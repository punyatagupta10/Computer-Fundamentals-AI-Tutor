OPERATING SYSTEM CONCEPTS NINTH EDITION OPERATING SYSTEM CONCEPTS ABRAHAM SILBERSCHATZ Yale University PETER BAER GALVIN Pluribus Networks GREG GAGNE Westminster College NINTH EDITION g561 Viceg561President g561andg561Executive g561Publisher g561g561 g561 Dong561Fowley g561 Executive g561Editor g561 g561g561g561g561 Bethg561Lang g561Golub g561 Editorial g561Assistant g561g561g561g561g561 Katherine g561Willis g561 Executive g561Marketing g561Manager g561g561g561g561 Christopher g561Ruelg561

g561g561g561g561 Christopher g561Ruelg561 Senior g561Production g561Editor g561 g561g561g561 Keng561Santor g561 Cover g561andg561titleg561page g561illustrations g561g561 g561 Susan g561Cyrg561 Cover g561Designer g561 g561g561g561g561 Madelyn g561Lesure g561 Textg561Designer g561g561 g561g561g561g561 Judyg561Allan g561 g561 g561 g561 g561 g561 Thisg561book g561wasg561setg561ing561Palatino g561byg561theg561author g561using g561LaTeXg561andg561printed g561andg561bound g561byg561Courier g556

g561andg561bound g561byg561Courier g556 Kendallville. g561Theg561cover g561wasg561printed g561byg561Courier. g561 g561 g561 Copyright g561g5612013, g5612012, g5612008g561Johng561Wiley g561g561Sons, g561Inc.g561g561Allg561rights g561reserved. g561 g561 Nog561partg561ofg561thisg561publication g561mayg561beg561reproduced, g561stored g561ing561ag561retrieval g561system g561org561transmitted g561ing561anyg561 form g561org561byg561anyg561means, g561electronic, g561mechan ical,g561photocopying,

g561electronic, g561mechan ical,g561photocopying, g561recording, g561scanning g561org561otherwise, g561 except g561asg561permitted g561under g561Sections g561107g561org561108g561ofg561theg5611976g561United g561States g561Copyright g561Act,g561without g561 either g561theg561prior g561written g561permission g561ofg561theg561Publisher, g561org561authorization g561through g561payment g561ofg561theg561 appropriate g561perg556copy g561feeg561tog561theg561Copyright g561Clearance g561Center,

g561Clearance g561Center, g561Inc.g561222g561Rosewood g561Drive, g561Danvers, g561 MAg56101923, g561(978)750 g5568400, g561faxg561(978)750 g5564470. g561Requests g561tog561theg561Publisher g561forg561permission g561should g561beg561 addressed g561tog561theg561Permissions g561Department, g561Johng561Wiley g561g561Sons, g561Inc.,g561111g561River g561Street, g561Hoboken, g561NJg561 07030 g561(201)748 g5566011, g561faxg561(201)748 g5566008, g561Eg556Mail: g561PERMREQWILEY.COM. g561g561g561 g561

g561PERMREQWILEY.COM. g561g561g561 g561 Evaluation g561copies g561areg561provided g561tog561qualified g561academics g561andg561profession alsg561forg561review g561purposes g561 only, g561forg561useg561ing561theirg561courses g561duringg561theg561nextg561academic g561year. g561g561These g561copies g561areg561licensed g561andg561mayg561 notg561beg561soldg561org561transf erred g561tog561ag561third g561party. g561g561Upon g561complet iong561ofg561theg561review g561period, g561please g561return g561

g561period, g561please g561return g561 theg561evaluation g561copy g561tog561Wiley. g561g561Return g561instructions g561andg561ag561freeg556ofg556charge g561return g561shipping g561labelg561areg561 available g561atg561www.wiley.comgoevalreturn. g561Outside g561ofg561theg561United g561States, g561please g561contact g561yourg561 localg561representative. g561 g561 Founded g561ing5611807, g561Johng561Wiley g561g561Sons, g561Inc.g561hasg561beeng561ag561valued g561sourceg561ofg561knowledge g561andg561

g561sourceg561ofg561knowledge g561andg561 understanding g561forg561more g561thang561200g561years, g561helping g561people g561around g561theg561world g561meet g561theirg561needs g561andg561 fulfill g561theirg561aspirations. g561Ourg561company g561isg561builtg561ong561ag561foundation g561ofg561principles g561thatg561include g561 responsibility g561tog561theg561communities g561weg561serve g561andg561where g561weg561liveg561andg561work. g561Ing5612008, g561weg561launched g561ag561 Corporate

g561weg561launched g561ag561 Corporate g561Citizenship g561Initiative, g561ag561global g561effort g561tog561address g561theg561environmental, g561social, g561economic, g561 andg561ethical g561challenges g561weg561faceg561ing561ourg561business. g561Among g561theg561issues g561weg561areg561addressing g561areg561carbon g561 impact, g561paper g561specifications g561andg561procurement, g561ethical g561conduct g561within g561ourg561business g561andg561among g561 ourg561vendors, g561andg561community

g561 ourg561vendors, g561andg561community g561andg561charitable g561support. g561Forg561more g561informatio n,g561please g561visitg561ourg561 website: g561www.wiley.com gocitizenship. g561g561g561 g561 g561 g561 ISBN: g561g561978g5561g556118g55606333 g5560g561 ISBN g561BRV: g561g561978g5561g556118g55612938 g5568g561 g561 Printed g561ing561theg561United g561States g561ofg561America g561 g561

g561States g561ofg561America g561 g561 10g561g561g5619g561g561g5618g561g561g5617g561g561g5616g561g561g5615g561g561g5614g561g561g5613g561g561g5612g561g561g5611g561To my children, Lemor, Sivan, and Aaron and my Nicolette Avi Silberschatz To Brendan and Ellen, and Barbara, Anne and Harold, and Walter and Rebecca Peter Baer Galvin To my Mom and Dad, Greg GagnePreface Operating systems are an essential part of any computer system. Similarly, ac o u r s eo no p e r a t i n gs y s t e m si sa ne s s e

s eo no p e r a t i n gs y s t e m si sa ne s s e n t i a lp a r to fa n yc o m p u t e rs c i e n c e education. This eld is undergoing rapid change, as computers are now prevalent in virtually every arena of daytoday lifefrom embedded devices in automobiles through the most sophisticated planning tools for governments and multinational rms. Yet the fundamental concepts remain fairly clear, and it is on these that we base this book. We wrote this book as a text for an introductory course in

this book as a text for an introductory course in operating systems at the junior or senior undergraduate level or at the rstyear graduate level. We hope that practitioners will also nd it useful. It provides a clear description of theconcepts that underlie operating systems. As prerequisites, we assume that the reader is familiar with basic data structures, computer organization, and ah i g h  l e v e ll a n g u a g e ,s u c ha sCo rJ a v a .T h eh a r d w a r et o p i c sr e q u i r e df o ra

h eh a r d w a r et o p i c sr e q u i r e df o ra n understanding of operating systems are covered in Chapter 1. In that chapter, we also include an overview of the fundamental data structures that are prevalent in most operating systems. For code examples, we use predominantly C, with some Java, but the reader can still understand the algorithms without a thorough knowledge of these languages. Concepts are presented using i ntuitive descriptions. Important theoretical results are covered, but

Important theoretical results are covered, but formal proofs are largely omitted. The bibliographical notes at the end of each chapter contain pointers to research papers in which results were rst presented and proved, as well as references to recent material for further reading. In place of proofs, gures and examples are used to suggest why we should expect the result in question to be true. The fundamental concepts and algorith ms covered in the book are often based on those used in both

in the book are often based on those used in both commercial and opensource operating systems. Our aim is to present these concepts and algorithms in a general setting that is not tied to one particular operating system. However, we present a large number of examples that pertain to the most popular and the most innovative operating systems, including Linux, Microsoft Windows, Apple Mac OS X ,a n d Solaris. We also include examples of both Android and i OS,c u r r e n t l yt h et w o dominant

and i OS,c u r r e n t l yt h et w o dominant mobile operating systems. The organization of the text reects our many years of teaching courses on operating systems, as well as curriculum guidelines published by the IEEE viiviii Preface Computing Society and the Association for Computing Machinery ( ACM ). Consideration was also given to the feedback provided by the reviewers of the text, along with the many comments and suggestions we received from readers of our previous editions and from our

from readers of our previous editions and from our current and former students. Content of This Book The text is organized in eight major parts: Overview .C h a p t e r s1a n d2e x p l a i nw h a to p e r a t i n gs y s t e m sa r e ,w h a t they do, and how they are designed and constructed. These chapters discuss what the common features of an operating system are and what an operating system does for the user. We include coverage of both traditional PCand server operating systems, as well as

PCand server operating systems, as well as operating systems for mobile devices. The presentation is motivational and explanatory in nature. We have avoided a discussion of how things are done internally in these chapters. Therefore, they are suitable for individual readers or for students in lowerlevel classes who want to learn what an operating system is without getting into the details of the internal algorithms. Process management .C h a p t e r s3t h r o u g h7d e s c r i b et h ep r o c e

e r s3t h r o u g h7d e s c r i b et h ep r o c e s sc o n c e p t and concurrency as the heart of modern operating systems. A process is the unit of work in a system. Such a system consists of a collection ofconcurrently executing processes, some of which are operatingsystem processes (those that execute system code) and the rest of which are user processes (those that execute user code). These chapters cover methods for process scheduling, interprocess communication, process synchronization,

communication, process synchronization, and deadlock handling. Also included is a discussion of threads, as well as an examination of issues related to multicore systems and parallel programming. Memory management . Chapters 8 and 9 deal with the management of main memory during the execution of a process. To improve both the utilization of the CPU and the speed of its response to its users, the computer must keep several processes in memory. There are many different memorymanagement schemes,

There are many different memorymanagement schemes, reecting various approaches to memory management, and the effectiven ess of a particular algorithm depends on the situation. Storage management .C h a p t e r s1 0t h r o u g h1 3d e s c r i b eh o wm a s ss t o r a g e , the le system, and IOare handled in a modern computer system. The le system provides the mechanism for online storage of and access to both data and programs. We describe the classic internal algorithms and structures of

the classic internal algorithms and structures of storage management and provide a rm practical understanding of the algorithms usedtheir properties, advantages, and disadvantages. Since the IOdevices that attach to a computer vary widely, the operating system needs to provide a wide range of functionality to applications to allow them to control all aspects of these devices. We discuss system IOin depth, including IOsystem design, interfaces, and internal system structures and functions. In

and internal system structures and functions. In many ways, IOdevices are the slowest major components of th ec o m p u t e r .B e c a u s et h e yr e p r e s e n taPreface ix performance bottleneck, we also examine performance issues associated with IOdevices. Protection and security .C h a p t e r s1 4a n d1 5d i s c u s st h em e c h a n i s m s necessary for the protection and security of computer systems. The processes in an operating system must be protected from one anothers activities,

must be protected from one anothers activities, and to provide such protection, we must ensure that only processes that have gained proper authorization from the operating system can operate on the les, memory, CPU, and other resources of the system. Protection is a mechanism for controlling the access of programs, processes, or users to computersystem resources. This mechanism must provide a means of specifying the controls to be imposed, as well as a means of enforcement. Security protects the

as a means of enforcement. Security protects the integrity of the information stored in the system (both data and code), as well as the physical resources of the system, from unauthorized access, malicious destruction or alteration, and accidental introduction of inconsistency. Advanced topics. Chapters 16 and 17 discuss virtual machines and distributed systems. Chapter 16 is a new chapter that provides an overview of virtual machines and their relationship to contemporary operating systems.

relationship to contemporary operating systems. Included is an overview of the hardware and software techniques that make virtualization possible. Chapter 17 condenses and updates the three chapters on distributed computing from the previous edition. This change is meant to make it easier for instructors to cover the material in the limited time available during a semester and for students to gain an understanding of the core ideas of distributed computing more quickly. Case studies .C h a p t e

computing more quickly. Case studies .C h a p t e r s1 8a n d1 9i nt h et e x t ,a l o n gw i t hA p p e n d i c e sAa n d B( w h i c ha r ea v a i l a b l eo n( http:www.osbook.com ), present detailed case studies of real operating systems, including Linux, Windows 7, FreeBSD,a n dM a c h .C o v e r a g eo fb o t hL i n u xa n dW i n d o w s7a r ep r e s e n t e d throughout this text; however, the case studies provide much more detail. It is especially interesting to compare and contrast the

especially interesting to compare and contrast the design of these two very different systems. Chapter 20 briey describes a few other inuential operating systems. The Ninth Edition As we wrote this Ninth Edition of Operating System Concepts, we were guided by the recent growth in three fundamental areas that affect operating systems: 1.Multicore systems 2.Mobile computing 3.Virtualization To emphasize these topics, we have integrated relevant coverage throughout this new editionand, in the case

throughout this new editionand, in the case of virtualization, have written an entirely new chapter. Additionally, we have rewritten material in almost every chapter by bringing older material up to date and removing material that is no longer interesting or relevant.x Preface We have also made substantial organizational changes. For example, we have eliminated the chapter on realtime systems and instead have integrated appropriate coverage of these systems throughout the text. We have reordered

systems throughout the text. We have reordered the chapters on storage management and have moved up the presentation of process synchronization so that it appears before process scheduling. Most of these organizational changes are base do no u re x p e r i e n c e sw h i l et e a c h i n g courses on operating systems. Below, we provide a brief outline of the major changes to the various chapters: Chapter 1, Introduction, includes updated coverage of multiprocessor and multicore systems, as well

of multiprocessor and multicore systems, as well as a new section on kernel data structures. Additionally, the coverage of computing environments now includes mobile systems and cloud computing. We also have incorporated an overview of realtime systems. Chapter 2, OperatingSystem Structures, provides new coverage of user interfaces for mobile devices, including discussions of i OSand Android, and expanded coverage of Mac OS X as a type of hybrid system. Chapter 3, Processes, now includes

hybrid system. Chapter 3, Processes, now includes coverage of multitasking in mobile operating systems, support for the multiprocess model in Googles Chrome web browser, and zombie and orphan processes in UNIX . Chapter 4, Threads, supplies expanded coverage of parallelism and Amdahls law. It also provides a new section on implicit threading, including Open MPand Apples Grand Central Dispatch. Chapter 5, Process Synchronization (previously Chapter 6), adds a new section on mutex locks as well as

6), adds a new section on mutex locks as well as coverage of synchronization using Open MP,a sw e l la sf u n c t i o n a ll a n g u a g e s . Chapter 6, CPU Scheduling (previously Chapter 5), contains new coverage of the Linux CFSscheduler and Windows use rmode scheduling. Coverage of realtime scheduling algorithms has also been integrated into this chapter. Chapter 7, Deadlocks, has no major changes. Chapter 8, Main Memory, includes new coverage of swapping on mobile systems and Intel 32 and

of swapping on mobile systems and Intel 32 and 64bit architectures. A new section discusses ARM architecture. Chapter 9, Virtual Memory, updates kernel memory management to include the Linux SLUB and SLOB memory allocators. Chapter 10, MassStorage Structure (previously Chapter 12), adds cover age of solidstate disks. Chapter 11, FileSystem Interface (previously Chapter 10), is updated with information about current technologies. Chapter 12, FileSystem Implementation (previously Chapter 11), is

Implementation (previously Chapter 11), is updated with coverage of current technologies. Chapter 13, IO, updates technologies and performance numbers, expands coverage of synchronousasynchronous and blockingnonblocking IO, and adds a section on vectored IO.Preface xi Chapter 14, Protection, has no major changes. Chapter 15, Security, has a revised cryptography section with modern notation and an improved explanation of various encryption methods and their uses. The chapter also includes new

and their uses. The chapter also includes new coverage of Windows 7 security. Chapter 16, Virtual Machines, is a new chapter that provides an overview of virtualization and how it relates to contemporary operating systems. Chapter 17, Distributed Systems, is a new chapter that combines and updates a selection of materials from previous Chapters 16, 17, and 18. Chapter 18, The Linux System (previously Chapter 21), has been updated to cover the Linux 3.2 kernel. Chapter 19, Windows 7, is a new

Linux 3.2 kernel. Chapter 19, Windows 7, is a new chapter presenting a case study of Windows 7. Chapter 20, Inuential Operating Systems (previously Chapter 23), has no major changes. Programming Environments This book uses examples of many realworld operating systems to illustrate fundamental operatingsystem concepts. Particular attention is paid to Linux and Microsoft Windows, but we also refer to various versions of UNIX (including Solaris, BSD,a n dM a c OS X ). The text also provides several

n dM a c OS X ). The text also provides several example programs written in C and Java. These programs are intended to run in the following programming environments: POSIX .POSIX (which stands for Portable Operating System Interface )r e p r e  sents a set of standards implemented primarily for UNIX based operating systems. Although Windows systems can also run certain POSIX programs, our coverage of POSIX focuses on UNIX and Linux systems. POSIX compliant systems must implement the POSIX core

compliant systems must implement the POSIX core standard ( POSIX .1); Linux, Solaris, and Mac OS X are examples of POSIX compliant systems. POSIX also denes several extensions to the standa rds, including realtime extensions (POSIX 1.b) and an extension for a threads library ( POSIX 1.c, better known as Pthreads). We provide several programming examples written in C illustrating the POSIX base API,a sw e l la sP t h r e a d sa n dt h ee x t e n s i o n sf o r realtime programming. These example

s i o n sf o r realtime programming. These example programs were tested on Linux 2.6 and 3.2 systems, Mac OS X 10.7, and Solaris 10 using the gcc4.0 compiler. Java .J a v ai saw i d e l yu s e dp r o g r a m m i n gl a n g u a g ew i t har i c h APIand builtin language support for thread creation and management. Java programs run on any operating system supporting a Java virtual machine (orJVM). We illustrate various operatingsystem and networking concepts with Java programs tested using the

concepts with Java programs tested using the Java 1.6 JVM. Windows systems .T h ep r i m a r yp r o g r a m m i n ge n v i r o n m e n tf o rW i n d o w s systems is the Windows API,w h i c hp r o v i d e sac o m p r e h e n s i v es e to ff u n c  tions for managing processes, threads, memory, and peripheral devices. We supply several C programs illustrating the use of this API.P r o g r a m s were tested on systems running Windows XPand Windows 7.xii Preface We have chosen these three

Windows 7.xii Preface We have chosen these three programmin ge n v i r o n m e n t sb e c a u s ew e believe that they best represent the two most popular operatingsystem models W indows and UNIX Linuxalong with the widely used Java environment. Most programming examples are written in C, and we expect readers to be comfortable with this language. Readers familiar with both the C and Java languages should easily understand most programs provided in this text. In some instancessuch as thread

in this text. In some instancessuch as thread creationwe illustrate a specic concept using all three programming environments, allowing the reader to contrast the three different libraries as they address the same task. In other situations, we may use just one of the APIst od e m o n s t r a t eac o n c e p t . For example, we illustrate shared memory using just the POSIX API ;s o c k e t programming in TCPIP is highlighted using the Java API. Linux Virtual Machine To help students gain a better

Virtual Machine To help students gain a better understanding of the Linux system, we provide a Linux virtual machine, including the Linux source code, that is available for download from the the website supporting this text ( http:www.osbook.com ). This virtual machine also includes a gcc development environment with c ompilers and editors. Most of the programming assignments in the book can be completed on this virtual machine, with the exception of assignmen ts that require Java or the Windows

of assignmen ts that require Java or the Windows API. We also provide three programming assignments that modify the Linux kernel through kernel modules: 1.Adding a basic kernel module to the Linux kernel. 2.Adding a kernel module that uses various kernel data structures. 3.Adding a kernel module that iterates over tasks in a running Linux system. Over time it is our intention to add additional kernel module assignments on the supporting website. Supporting Website When you visit the website

Supporting Website When you visit the website supporting this text at http:www.osbook.com , you can download the following resources: Linux virtual machine Ca n dJ a v as o u r c ec o d e Sample syllabi Set of Powerpoint slides Set of gures and illustrations FreeBSD and Mach case studiesPreface xiii Solutions to practice exercises Study guide for students Errata Notes to Instructors On the website for this text, we provide several sample syllabi that suggest various approaches for using the text

that suggest various approaches for using the text in both introductory and advanced courses. As a general rule, we encourage instructors to progress sequentially through the chapters, as this strategy provides the most thorough study of operating systems. However, by using the sample syllabi, an instructor can select a different ordering of chapters (or subsections of chapters). In this edition, we have added over sixty new written exercises and over twenty new programming problems and

and over twenty new programming problems and projects. Most of the new program ming assignments involve processes, threads, process synchronization, and memory management. Some involve addi ng kernel modules to the Linux system which requires using either the Linux virtual machine that accompanies this text or another suitable Linux distribution. Solutions to written exercises and programming assignments are available to instructors who have adopted this text for their operatingsystem class. To

this text for their operatingsystem class. To obtain these restricted supplements, contact your local John Wiley  Sons sales representative. You can nd your Wiley representative by going to http:www.wiley.comcollege and clicking Whos my rep?  Notes to Students We encourage you to take advantage of the practice exercises that appear at the end of each chapter. Solutions to the practice exercises are available for download from the supporting website http:www.osbook.com .W ea l s o encourage you

http:www.osbook.com .W ea l s o encourage you to read through the study guide, which was prepared by one of our students. Finally, for students who are unfamiliar with UNIX and Linux systems, we recommend that you download and install the Linux virtual machine that we include on the supporting website. Not only will this provide you with a new computing experience, but the opensource nature of Linux will allow you to easily examine the inner details of this popular operating system. We wish you

of this popular operating system. We wish you the very best of luck in your study of operating systems. Contacting Us We have endeavored to eliminate typos, bugs, and the like from the text. But, as in new releases of software, bugs almost surely remain. An uptodate errata list is accessible from the books website. We would be grateful if you would notify us of any errors or omissions in the book that are not on the current list of errata. We would be glad to receive suggestions on improvements

be glad to receive suggestions on improvements to the book. We also welcome any contributions to the book website that could be ofxiv Preface use to other readers, such as programming exercises, project suggestions, online labs and tutorials, and teaching tips. Email should be addressed to osbookauthorscs.yale.edu . Acknowledgments This book is derived from the previous editions, the rst three of which were coauthored by James Peterson. Others who helped us with previous editions include Hamid

who helped us with previous editions include Hamid Arabnia, Rida Bazzi, Randy Bentson, David Black, Joseph Boykin, Jeff Brumeld, Gael Buckley, Roy Campbell, P . C. Capon, John Carpenter, Gil Carrick, Thomas Casavant, Bart Childs, Ajoy Kumar Datta, Joe Deck, Sudarshan K. Dhall, Thomas Doeppner, Caleb Drake, M. Racsit Eskicio glu, Hans Flack, Robert Fowler, G. Scott Graham, Richard Guy, Max Hailperin, Rebecca Hartman, Wayne Hathaway, Christopher Haynes, Don Heller, Bruce Hillyer, Mark Holliday,

Haynes, Don Heller, Bruce Hillyer, Mark Holliday, Dean Hougen, Michael Huang, Ahmed Kamel, Morty Kewstel, Richard Kieburtz, Carol Kroll, Morty Kwestel, Thomas LeBlanc, John Leggett, Jerrold Leichter, Ted Leung, Gary Lippman, Carolyn Miller, Michael Molloy, Euripides Montagne, Yoichi Muraoka, Jim M. Ng, Banu Ozden, Ed Posnak, Boris Putanec, Charles Qualline, John Quarterman, Mike Reiter, Gustavo RodriguezRivera, Carolyn J. C. Schauble, Thomas P . Skinner, Yannis Smaragdakis, Jesse St. Laurent,

. Skinner, Yannis Smaragdakis, Jesse St. Laurent, John Stankovic, Adam Stauffer, Steven Stepanek, John Sterling, Hal Stern, Louis Stevens, Pete Thomas, David Umbaugh, Steve Vinoski, Tommy Wagner, Larry L. Wear, John Werth, James M. Westall, J. S. Weston, and Yang Xiang Robert Love updated both Chapter 18 and the Linux coverage throughout the text, as well as answering many of our Androidrelated questions. Chapter 19 was written by Dave Probert and was derived from Chapter 22 of the Eighth

and was derived from Chapter 22 of the Eighth Edition of Operating System Concepts. Jonathan Katz contributed to Chapter 15. Richard West provided input into Chapter 16. Salahuddin Khan updated Section 15.9 to provide new coverage of Windows 7 security. Parts of Chapter 17 were derived from a paper by Levy and Silberschatz [1990]. Chapter 18 was derived from an unpublished manuscript by Stephen Tweedie. Cliff Martin helped with updating the UNIX appendix to cover FreeBSD.S o m eo ft h ee x e r c

appendix to cover FreeBSD.S o m eo ft h ee x e r c i s e sa n da c c o m p a n y i n gs o l u t i o n sw e r es u p p l i e db y Arvind Krishnamurthy. Andrew DeNicola prepared the student study guide that is available on our website. Some of the the slides were prepeared by Marilyn Turnamian. Mike Shapiro, Bryan Cantrill, and Jim Mauro answered several Solaris related questions, and Bryan Cantrill from Sun Microsystems helped with the ZFScoverage. Josh Dees and Rob Reynolds con tributed coverage

Josh Dees and Rob Reynolds con tributed coverage of Microsofts NET.T h ep r o j e c tf o r POSIX message queues was contributed by John Trono of Saint Michaels College in Colchester, Vermont. Judi Paige helped with generating  gures and presentation of slides. Thomas Gagne prepared new artwork for this edition. Owen Galvin helped copyedit Chapter 16. Mark Wogahn has made sure that the software to produce this book ( LATEXand fonts) works properly. Ranjan Kumar Meher rewrote some of the

properly. Ranjan Kumar Meher rewrote some of the LATEXsoftware used in the production of this new text.Preface xv Our Executive Editor, Beth Lang Golub, provided expert guidance as we prepared this edition. She was assisted by Katherine Willis, who managed many details of the project smoothly. The Senior Production Editor, Ken Santor, was instrumental in handling all the production details. The cover illustrator was Susan Cyr, and the cover designer was Madelyn Lesure. Beverly Peavler copyedited

was Madelyn Lesure. Beverly Peavler copyedited the manusc ript. The freelance proofreader was Katrina Avery; the freelance indexer was WordCo, Inc. Abraham Silberschatz, New Haven, CT, 2012 Peter Baer Galvin, Boston, MA, 2012 Greg Gagne, Salt Lake City, UT, 2012Contents PART ONE OVERVIEW Chapter 1 Introduction 1.1 What Operating Systems Do 4 1.2 ComputerSystem Organization 7 1.3 ComputerSystem Architecture 12 1.4 OperatingSystem Structure 19 1.5 OperatingSystem Operations 21 1.6 Process

19 1.5 OperatingSystem Operations 21 1.6 Process Management 24 1.7 Memory Management 25 1.8 Storage Management 261.9 Protection and Security 30 1.10 Kernel Data Structures 31 1.11 Computing Environments 35 1.12 OpenSource Operating Systems 43 1.13 Summary 47 Exercises 49 Bibliographical Notes 52 Chapter 2 OperatingSystem Structures 2.1 OperatingSystem Services 55 2.2 User and OperatingSystem Interface 58 2.3 System Calls 62 2.4 Types of System Calls 66 2.5 System Programs 74 2.6 OperatingSystem

66 2.5 System Programs 74 2.6 OperatingSystem Design and Implementation 752.7 OperatingSystem Structure 78 2.8 OperatingSystem Debugging 86 2.9 OperatingSystem Generation 91 2.10 System Boot 92 2.11 Summary 93 Exercises 94 Bibliographical Notes 101 PART TWO PROCESS MANAGEMENT Chapter 3 Processes 3.1 Process Concept 105 3.2 Process Scheduling 110 3.3 Operations on Processes 115 3.4 Interprocess Communication 122 3.5 Examples of IPC Systems 1303.6 Communication in Client Server Systems 136 3.7

Communication in Client Server Systems 136 3.7 Summary 147 Exercises 149 Bibliographical Notes 161 xviixviii Contents Chapter 4 Threads 4.1 Overview 163 4.2 Multicore Programming 166 4.3 Multithreading Models 169 4.4 Thread Libraries 171 4.5 Implicit Threading 1774.6 Threading Issues 183 4.7 OperatingSystem Examples 188 4.8 Summary 191 Exercises 191 Bibliographical Notes 199 Chapter 5 Process Synchronization 5.1 Background 203 5.2 The CriticalSection Problem 206 5.3 Petersons Solution 207 5.4

Problem 206 5.3 Petersons Solution 207 5.4 Synchronization Hardware 209 5.5 Mutex Locks 212 5.6 Semaphores 213 5.7 Classic Problems of Synchronization 2195.8 Monitors 223 5.9 Synchronization Examples 232 5.10 Alternative Approaches 238 5.11 Summary 242 Exercises 242 Bibliographical Notes 258 Chapter 6 CPU Scheduling 6.1 Basic Concepts 261 6.2 Scheduling Criteria 265 6.3 Scheduling Algorithms 266 6.4 Thread Scheduling 277 6.5 MultipleProcessor Scheduling 278 6.6 RealTime CPU Scheduling 2836.7

Scheduling 278 6.6 RealTime CPU Scheduling 2836.7 OperatingSystem Examples 290 6.8 Algorithm Evaluation 300 6.9 Summary 304 Exercises 305 Bibliographical Notes 311 Chapter 7 Deadlocks 7.1 System Model 315 7.2 Deadlock Characterization 317 7.3 Methods for Handling Deadlocks 322 7.4 Deadlock Prevention 323 7.5 Deadlock Avoidance 3277.6 Deadlock Detection 333 7.7 Recovery from Deadlock 337 7.8 Summary 339 Exercises 339 Bibliographical Notes 346 PART THREE MEMORY MANAGEMENT Chapter 8 Main Memory 8.1

THREE MEMORY MANAGEMENT Chapter 8 Main Memory 8.1 Background 351 8.2 Swapping 358 8.3 Contiguous Memory Allocation 360 8.4 Segmentation 364 8.5 Paging 366 8.6 Structure of the Page Table 3788.7 Example: Intel 32 and 64bit Architectures 383 8.8 Example: ARM Architecture 388 8.9 Summary 389 Exercises 390 Bibliographical Notes 394Contents xix Chapter 9 Virtual Memory 9.1 Background 397 9.2 Demand Paging 401 9.3 CopyonWrite 408 9.4 Page Replacement 409 9.5 Allocation of Frames 421 9.6 Thrashing 425

409 9.5 Allocation of Frames 421 9.6 Thrashing 425 9.7 MemoryMapped Files 4309.8 Allocating Kernel Memory 436 9.9 Other Considerations 439 9.10 OperatingSystem Examples 445 9.11 Summary 448 Exercises 449 Bibliographical Notes 461 PART FOUR STORAGE MANAGEMENT Chapter 10 MassStorage Structure 10.1 Overview of MassStorage Structure 467 10.2 Disk Structure 470 10.3 Disk Attachment 471 10.4 Disk Scheduling 472 10.5 Disk Management 47810.6 SwapSpace Management 482 10.7 RAID Structure 484 10.8

Management 482 10.7 RAID Structure 484 10.8 StableStorage Implementation 494 10.9 Summary 496 Exercises 497 Bibliographical Notes 501 Chapter 11 FileSystem Interface 11.1 File Concept 503 11.2 Access Methods 513 11.3 Directory and Disk Structure 515 11.4 FileSystem Mounting 526 11.5 File Sharing 52811.6 Protection 533 11.7 Summary 538 Exercises 539 Bibliographical Notes 541 Chapter 12 FileSystem Implementation 12.1 FileSystem Structure 543 12.2 FileSystem Implementation 546 12.3 Directory

12.2 FileSystem Implementation 546 12.3 Directory Implementation 552 12.4 Allocation Methods 553 12.5 FreeSpace Management 561 12.6 Efciency and Performance 56412.7 Recovery 568 12.8 NFS 571 12.9 Example: The WAFL File System 577 12.10 Summary 580 Exercises 581 Bibliographical Notes 585 Chapter 13 IO Systems 13.1 Overview 587 13.2 IO Hardware 588 13.3 Application IO Interface 597 13.4 Kernel IO Subsystem 604 13.5 Transforming IO Requests to Hardware Operations 61113.6 STREAMS 613 13.7

to Hardware Operations 61113.6 STREAMS 613 13.7 Performance 615 13.8 Summary 618 Exercises 619 Bibliographical Notes 621xx Contents PART FIVE PROTECTION AND SECURITY Chapter 14 Protection 14.1 Goals of Protection 625 14.2 Principles of Protection 626 14.3 Domain of Protection 627 14.4 Access Matrix 632 14.5 Implementation of the Access Matrix 636 14.6 Access Control 63914.7 Revocation of Access Rights 640 14.8 CapabilityBased Systems 641 14.9 LanguageBased Protection 644 14.10 Summary 649

LanguageBased Protection 644 14.10 Summary 649 Exercises 650 Bibliographical Notes 652 Chapter 15 Security 15.1 The Security Problem 657 15.2 Program Threats 661 15.3 System and Network Threats 669 15.4 Cryptography as a Security Tool 674 15.5 User Authentication 685 15.6 Implementing Security Defenses 689 15.7 Firewalling to Protect Systems and Networks 69615.8 ComputerSecurity Classications 698 15.9 An Example: Windows 7 699 15.10 Summary 701 Exercises 702 Bibliographical Notes 704 PART SIX

Exercises 702 Bibliographical Notes 704 PART SIX ADV ANCED TOPICS Chapter 16 Virtual Machines 16.1 Overview 711 16.2 History 713 16.3 Benets and Features 714 16.4 Building Blocks 717 16.5 Types of Virtual Machines and Their Implementations 72116.6 Virtualization and OperatingSystem Components 728 16.7 Examples 735 16.8 Summary 737 Exercises 738 Bibliographical Notes 739 Chapter 17 Distributed Systems 17.1 Advantages of Distributed Systems 741 17.2 Types of Network based Operating Systems 743

17.2 Types of Network based Operating Systems 743 17.3 Network Structure 747 17.4 Communication Structure 751 17.5 Communication Protocols 75617.6 An Example: TCPIP 760 17.7 Robustness 762 17.8 Design Issues 764 17.9 Distributed File Systems 765 17.10 Summary 773 Exercises 774 Bibliographical Notes 777Contents xxi PART SEVEN CASE STUDIES Chapter 18 The Linux System 18.1 Linux History 781 18.2 Design Principles 786 18.3 Kernel Modules 789 18.4 Process Management 792 18.5 Scheduling 795 18.6

Process Management 792 18.5 Scheduling 795 18.6 Memory Management 800 18.7 File Systems 80918.8 Input and Output 815 18.9 Interprocess Communication 818 18.10 Network Structure 819 18.11 Security 821 18.12 Summary 824 Exercises 824 Bibliographical Notes 826 Chapter 19 Windows 7 19.1 History 829 19.2 Design Principles 831 19.3 System Components 838 19.4 Terminal Services and Fast User Switching 862 19.5 File System 86319.6 Networking 869 19.7 Programmer Interface 874 19.8 Summary 883 Exercises

Interface 874 19.8 Summary 883 Exercises 883 Bibliographical Notes 885 Chapter 20 Inuential Operating Systems 20.1 Feature Migration 887 20.2 Early Systems 888 20.3 Atlas 895 20.4 XDS940 896 20.5 THE 897 20.6 RC 4000 897 20.7 CTSS 898 20.8 MULTICS 899 20.9 IBM OS360 89920.10 TOPS20 901 20.11 CPM and MSDOS 901 20.12 Macintosh Operating System and Windows 902 20.13 Mach 902 20.14 Other Systems 904 Exercises 904 Bibliographical Notes 904 PART EIGHT APPENDICES Appendix A BSD UNIX A.1 UNIX History A1

APPENDICES Appendix A BSD UNIX A.1 UNIX History A1 A.2 Design Principles A6 A.3 Programmer Interface A8 A.4 User Interface A15 A.5 Process Management A18 A.6 Memory Management A22A.7 File System A24 A.8 IO System A32 A.9 Interprocess Communication A36 A.10 Summary A40 Exercises A41 Bibliographical Notes A42xxii Contents Appendix B The Mach System B.1 History of the Mach System B1 B.2 Design Principles B3 B.3 System Components B4 B.4 Process Management B7 B.5 Interprocess Communication B13B.6

B7 B.5 Interprocess Communication B13B.6 Memory Management B18 B.7 Programmer Interface B23 B.8 Summary B24 Exercises B25 Bibliographical Notes B26Part One Overview Anoperating system acts as an intermediary between the user of a computer and the computer hardware. The purpose of an operating system is to provide an environment in which a user can execute programs in a convenient andefcient manner. An operating system is software that manages the computer hard ware. The hardware must provide

the computer hard ware. The hardware must provide appropriate mechanisms to ensure the correct operation of the computer system and to prevent user programs from interfering with the proper operation of the system. Internally, operating systems vary greatly in their makeup, since they are organized along many different lines. The design of a new operating system is a major task. It is important that the goals of the system be well dened before the design begins. These goals form the basis for

the design begins. These goals form the basis for choices among various algorith ms and strategies. Because an operating system is large and complex, it must be created piece by piece. Each of these pieces should be a welldelineated portion of the system, with carefully dened inputs, outputs, and functions.1CHAPTER Introduction Anoperating system is a program that manages a computers hardware. It also provides a basis for application programs and acts as an intermediary between the computer user

acts as an intermediary between the computer user and the computer hardware. An amazing aspect of operating systems is how they vary in accomplishing these tasks. Mainframe operating systems are designed primarily to optimize utilization of hardware. Personal computer ( PC)o p e r a t i n gs y s t e m ss u p p o r tc o m p l e xg a m e s ,b u s i n e s s applications, and everything in between. Operating systems for mobile com puters provide an environment in which a user can easily interface

environment in which a user can easily interface with the computer to execute programs. Thus, some operating systems are designed to beconvenient, others to be efcient, and others to be some combination of the two. Before we can explore the details of computer system operation, we need to know something about system structure. We thus discuss the basic functions of system startup, IO,a n ds t o r a g ee a r l yi nt h i sc h a p t e r .W ea l s od e s c r i b e the basic computer architecture

s od e s c r i b e the basic computer architecture that makes it possible to write a functional operating system. Because an operating system is large and complex, it must be created piece by piece. Each of these pieces should be a welldelineated portion of the system, with carefully dened inputs, outputs, and functions. In this chapter, we provide a general overview of the major components of a contemporary computer system as well as the functions provided by the operating system. Additionally,

provided by the operating system. Additionally, we cover several other topics to help set the stage for the remainder of this text: data structures used in operating systems, computing environments, and opensource operating systems. CHAPTER OBJECTIVES To describe the basic organization of computer systems. To provide a grand tour of the major components of operating systems. To give an overview of the many types of computing environments. To explore several opensource operating systems. 34

explore several opensource operating systems. 34 Chapter 1 Introductionuser 1user 2user 3 computer hardwareoperating systemsystem and application programscompiler assembler text editor database systemuser n  Figure 1.1 Abstract view of the components of a computer system. 1.1 What Operating Systems Do We begin our discussion by looking at the operating systems role in the overall computer system. A computer system can be divided roughly into four components: the hardware, theoperating system,

components: the hardware, theoperating system, theapplication programs, and the users (Figure 1.1). The hardware the central processing unit (CPU),t h e memory ,a n dt h e inputoutput (IO)devices provides the basic computing resources for the system. The application programs such as word processors, spreadsheets, compilers, and Web browsersdene the ways in which these resources are used to solve users computing problems. The operating system controls the hardware and coordinates its use among

the hardware and coordinates its use among the various application programs for the various users. We can also view a computer system as consisting of hardware, software, and data. The operating system provides the means for proper use of these resources in the operation of the computer system. An operating system is similar to a government. Like a government, it performs no useful function by itself. It simply provides an environment within which other programs can do useful work. To understand

other programs can do useful work. To understand more fully the operating systems role, we next explore operating systems from two viewpoints: that of the user and that of the system. 1.1.1 User View The users view of the computer varies according to the interface being used. Most computer users sit in front of a PC,c o n s i s t i n go fam o n i t o r , keyboard, mouse, and system unit. Such a system is designed for one user1.1 What Operating Systems Do 5 to monopolize its resources. The goal

Systems Do 5 to monopolize its resources. The goal is to maximize the work (or play) that the user is performing. In this case, the operating system is designed mostly forease of use ,w i t hs o m ea t t e n t i o np a i dt op e r f o r m a n c ea n dn o n ep a i d toresource utilization how various hardware and software resources are shared. Performance is, of course, important to the user; but such systems are optimized for the singleuser experience rather than the requirements of multiple

rather than the requirements of multiple users. In other cases, a user sits at a terminal connected to a mainframe or a minicomputer .O t h e ru s e r sa r ea c c e s s i n gt h es a m ec o m p u t e rt h r o u g ho t h e r terminals. These users share resources and may exchange information. The operating system in such cases is designed to maximize resource utilization to assure that all available CPU time, memory, and IOare used efciently and that no individual user takes more than her fair

that no individual user takes more than her fair share. In still other cases, users sit at workstations connected to networks of other workstations and servers .T h e s eu s e r sh a v ed e d i c a t e dr e s o u r c e sa t their disposal, but they also share resources such as networking and servers, including le, compute, and print servers. Therefore, their operating system is designed to compromise between individual usability and resource utilization. Recently, many varieties of mobile

utilization. Recently, many varieties of mobile computers, such as smartphones and tablets, have come into fashion. Most mobile computers are standalone units for individual users. Quite often, they are conne cted to networks through cellular or other wireless technologies. Increasingly ,t h e s em o b i l ed e v i c e sa r er e p l a c i n g desktop and laptop computers for people who are primarily interested in using computers for email and web browsing. The user interface for mobile computers

browsing. The user interface for mobile computers generally features a touch screen ,w h e r et h eu s e ri n t e r a c t sw i t ht h e system by pressing and swiping ngers across the screen rather than using a physical keyboard and mouse. Some computers have little or no user view. For example, embedded computers in home devices and automobiles may have numeric keypads and may turn indicator lights on or off to show status, but they and their operating systems are designed primarily to run

operating systems are designed primarily to run without user intervention. 1.1.2 System View From the computers point of view, the operating system is the program most intimately involved with the hardware. In this context, we can view an operating system as a resource allocator .Ac o m p u t e rs y s t e mh a sm a n y resources that may be required to solve a problem: CPU time, memory space, lestorage space, IOdevices, and so on. The operating system acts as the manager of these resources.

system acts as the manager of these resources. Facing numerous and possibly conicting requests for resources, the operating system must decide how to allocate them to specic programs and users so that it can operate the computer system efciently and fairly. As we have seen, resource allocation is especially important where many users access the same mainframe or minicomputer. As l i g h t l yd i f f e r e n tv i e wo fa no p e r a t i n gs y s t e me m p h a s i z e st h en e e dt o control the

e me m p h a s i z e st h en e e dt o control the various IOdevices and user programs. An operating system is a control program. A control program manages the execution of user programs to prevent errors and improper use of th ec o m p u t e r .I ti se s p e c i a l l yc o n c e r n e d with the operation and control of IOdevices.6 Chapter 1 Introduction 1.1.3 Dening Operating Systems By now, you can probably see that the term operating system covers many roles and functions. That is the case,

covers many roles and functions. That is the case, at least in part, because of the myriad designs and uses of computers. Computers are present within toasters, cars, ships, spacecraft, homes, and businesses. They are the basis for game machines, music players, cable TVtuners, and industrial control systems. Although computers have a relatively short history, they have evolved rapidly. Computing started as an experiment to determine what could be done and quickly moved to xedpurpose systems for

done and quickly moved to xedpurpose systems for military uses, such as code breaking and trajectory plotting, and governmental uses, such as census calculation. Those early computers evolved into generalpurpose, multifunction mainframes, and thats when operating systems were born. In the 1960s, Moores Law predicted that the number of transistors on an integrated circuit would double every eighteen months, and that prediction has held true. Computers gained in functionality and shrunk in size,

gained in functionality and shrunk in size, leading to a vast number of uses and a vast number and variety of operating systems. (See Chapter 20 for more details on the history of operating systems.) How, then, can we dene what an operating system is? In general, we have no completely adequate denition of an operating system. Operating systems exist because they offer a reasonable way to solve the problem of creating a usable computing system. The fundamental goal of computer systems is to

The fundamental goal of computer systems is to execute user programs and to make solving user problems easier. Computer hardware is constructed toward this goal. Since bare hardware alone is not particularly easy to use, application programs are developed. These programs require certain common operations, such as those controlling the IOdevices. The common functions of controlling and allocating resources are then brought together into one piece of software: the operating system. In addition, we

of software: the operating system. In addition, we have no universally accepted denition of what is part of the operating system. A simple viewpoint is that it includes everything a vendor ships when you order the operating system. The features included, however, vary greatly across systems. Some systems take up less than a megabyte of space and lack even a fullscreen editor, whereas others require gigabytes of space and are based entirely on graphical windowing systems. A more common denition,

windowing systems. A more common denition, and the one that we usually follow, is that the operating system is the one program running at all times on the computerusually called thekernel .( A l o n gw i t ht h ek e r n e l ,t h e r ea r et w oo t h e rt y p e so fp r o g r a m s : system programs ,w h i c ha r ea s s o c i a t e dw i t ht h eo p e r a t i n gs y s t e mb u ta r en o t necessarily part of the kernel, and application programs, which include all programs not associated with the

which include all programs not associated with the operation of the system.) The matter of what constitutes an operating system became increasingly important as personal computers became more widespread and operating systems grew increasingly sophisticated. In 1998, the United States Department of Justice led suit against Microsoft, in essence claiming that Microsoft included too much functionality in its operating systems and thus prevented application vendors from competing. (For example, a

vendors from competing. (For example, a Web browser was an integral part of the operating systems.) As a result, Microsoft was found guilty of using its operatingsystem monopoly to limit competition. Today, however, if we look at operating systems for mobile devices, we see that once again the number of features constituting the operating system1.2 ComputerSystem Organization 7 is increasing. Mobile operating systems often include not only a core kernel but also middleware a set of software

core kernel but also middleware a set of software frameworks that provide additional services to application developers. For example, each of the two most promi nent mobile operating systemsApples i OSand Googles Androidfeatures ac o r ek e r n e la l o n gw i t hm i d d l e w a r et h a ts u p p o r t sd a t a b a s e s ,m u l t i m e d i a ,a n d graphics (to name a only few). 1.2 ComputerSystem Organization Before we can explore the details of how computer systems operate, we need general

of how computer systems operate, we need general knowledge of the structure of a computer system. In this section, we look at several parts of this structure. The section is mostly concerned with computersystem organization, so you can skim or skip it if you already understand the concepts. 1.2.1 ComputerSystem Operation Am o d e r ng e n e r a l  p u r p o s ec o m p u t e rs y s t e mc o n s i s t so fo n eo rm o r e CPUs and a number of device controllers connected through a common bus that

controllers connected through a common bus that provides access to shared memory (Figure 1.2). Each device controller is in charge of a specic type of device (for example, disk drives, audio devices, or video displays). The CPU and the device controllers can execute in parallel, competing for memory cycles. To ensure orderly access to the shared memory, a memory controller synchronizes access to the memory. For a computer to start runningfor instance, when it is powered up or rebootedit needs to

when it is powered up or rebootedit needs to have an initial program to run. This initial program, orbootstrap program ,t e n d st ob es i m p l e .T y p i c a l l y ,i ti ss t o r e dw i t h i n the computer hardware in readonly memory ( ROM )o re l e c t r i c a l l ye r a s a b l e programmable readonly memory ( EEPROM ), known by the general term rmware .I ti n i t i a l i z e sa l la s p e c t so ft h es y s t e m ,f r o m CPU registers to device controllers to memory contents. The

to device controllers to memory contents. The bootstrap program must know how to load the operating system and how to start executing that system. To accomplish USB controllerkeyboard printer mouse monitor disks graphics adapterdisk controller memoryCPUonline Figure 1.2 Am o d e r nc o m p u t e rs y s t e m .8 Chapter 1 Introductionuser process executingCPU IO interrupt processing IO requesttransfer doneIO requesttransfer doneIO deviceidle transferring Figure 1.3 Interrupt timeline for a single

Figure 1.3 Interrupt timeline for a single process doing output. this goal, the bootstrap program must locate the operatingsystem kernel and load it into memory. Once the kernel is loaded and executin g, it can start providing services to the system and its users. Some services are provided outside of the kernel, by system programs that are loaded into memory at boot time to become system processes ,o rsystem daemons that run the entire time the kernel is running. On UNIX ,t h e r s ts y s t e

kernel is running. On UNIX ,t h e r s ts y s t e mp r o c e s si s init,and it starts many other daemons. Once this phase is complete, the system is fully booted, and the system waits for some event to occur. The occurrence of an event is usually signaled by an interrupt from either the hardware or the software. Hardware may trigger an interrupt at any time by sending a signal to the CPU,u s u a l l yb yw a yo ft h es y s t e mb u s .S o f t w a r e may trigger an interrupt by executing a

t w a r e may trigger an interrupt by executing a special operation called a system call (also called a monitor call ). When the CPU is interrupted, it stops what it is doing and immediately transfers execution to a xed location. The xed location usually contains the starting address where the service routine for the interrupt is located. The interrupt service routine executes; on completion, the CPU resumes the interrupted computation. A timeline of this operation is shown in Figure 1.3.

timeline of this operation is shown in Figure 1.3. Interrupts are an important part of a computer architecture. Each computer design has its own interrupt mechanism, but several functions are common. The interrupt must transfer control to the appropriate interrupt service routine. The straightforward method for handling this transfer would be to invoke ag e n e r i cr o u t i n et oe x a m i n et h ei n t e r r u p ti n f o r m a t i o n .T h er o u t i n e ,i nt u r n , would call the

n .T h er o u t i n e ,i nt u r n , would call the interruptspecic handler. H owever, interrupts must be handled quickly. Since only a predened number of interrupts is possible, a table of pointers to interrupt routines can be used instead to provide the necessary speed. The interrupt routine is called indirectly through the table, with no intermediate routine needed. Generally, the table of pointers is stored in low memory (the rst hundred or so locations). These locations hold the addresses of

locations). These locations hold the addresses of the interrupt service routines for the various devices. This array, or interrupt vector ,o fa d d r e s s e si st h e ni n d e x e db yau nique device number, given with the interrupt request, to provide the addr ess of the interrupt service routine for1.2 ComputerSystem Organization 9 STORAGE DEFINITIONS AND NOTATION The basic unit of computer storage is the bit.Ab i tc a nc o n t a i no n eo ft w o values, 0 and 1. All other storage in a

eo ft w o values, 0 and 1. All other storage in a computer is based on collections of bits. Given enough bits, it is amazing how many things a computer can represent: numbers, letters, images, movies, sounds, documents, and programs, to name af e w .A byte is 8 bits, and on most computers it is the smallest convenient chunk of storage. For example, most computers dont have an instruction to move a bit but do have one to move a byte. A less common term is word , which is a given computer

common term is word , which is a given computer architectures native unit of data. A word is made up of one or more bytes. For example, a computer that has 64bit registers and 64bit memory addressing typically has 64bit (8byte) words. A computer executes many operations in its native word size rather than a byte at a time. Computer storage, along with most computer throughput, is generally measured and manipulated in bytes and collections of bytes. A kilobyte ,o r KB,i s1 , 0 2 4b y t e s ;a

bytes. A kilobyte ,o r KB,i s1 , 0 2 4b y t e s ;a megabyte ,o r MB,i s1 , 0 2 42bytes; a gigabyte ,o r GB,i s 1,0243bytes; a terabyte ,o rTB,i s1 , 0 2 44bytes; and a petabyte ,o rPB,i s1 , 0 2 45 bytes. Computer manufacturers often round off these numbers and say that am e g a b y t ei s1m i l l i o nb y t e sa n dag i g a b y t ei s1b i l l i o nb y t e s .N e t w o r k i n g measurements are an exception to this general rule; they are given in bits (because networks move data a bit at a

in bits (because networks move data a bit at a time). the interrupting device. Operating systems as different as Windows and UNIX dispatch interrupts in this manner. The interrupt architecture must also save the address of the interrupted instruction. Many old designs simply stored the interrupt address in a xed location or in a location indexed by the device number. More recent architectures store the return address on the system stack. If the interrupt routine needs to modify the processor

interrupt routine needs to modify the processor statefor instance, by modifying register valuesit must explicitly save the current state and then restore that state before returning. After the interr upt is serviced, the saved return address is loaded into the program counter, and the interrupted computation resumes as though the interrupt had not occurred. 1.2.2 Storage Structure The CPU can load instructions only from memory, so any programs to run must be stored there. Generalpurpose

to run must be stored there. Generalpurpose computers run most of their programs from rewritable memory, called main memory (also called randomaccess memory , orRAM ). Main memory commonly is implemented in a semiconductor technology called dynamic randomaccess memory (DRAM ). Computers use other forms of memory as well. We have already mentioned readonly memory, ROM )a n de l e c t r i c a l l ye r a s a b l ep r o g r a m m a b l er e a d  o n l y memory, EEPROM ). Because ROM cannot be

d  o n l y memory, EEPROM ). Because ROM cannot be changed, only static programs, such as the bootstrap program described earlier, are stored there. The immutability ofROM is of use in game cartridges. EEPROM can be changed but cannot be changed frequently and so contains mostly static programs. For example, smartphones have EEPROM to store their factoryinstalled programs.10 Chapter 1 Introduction All forms of memory provide an array of bytes. Each byte has its own address. Interaction is ac

Each byte has its own address. Interaction is ac hieved through a sequence of load orstore instructions to specic memory addresses. The load instruction moves a byte or word from main memory to an internal register within the CPU,w h e r e a st h e store instruction moves the content of a register to main memory. Aside from explicit loads and stores, the CPU automatically loads instructions from main memory for execution. At y p i c a li n s t r u c t i o n  e x e c u t i o nc y c l e ,a se x e

u c t i o n  e x e c u t i o nc y c l e ,a se x e c u t e do nas y s t e mw i t ha von Neumann architecture , r s tf e t c h e sa ni n s t r u c t i o nf r o mm e m o r ya n ds t o r e s that instruction in the instruction register .T h ei n s t r u c t i o ni st h e nd e c o d e d and may cause operands to be fetched from memory and stored in some internal register. After the instruction on the operands has been executed, the result may be stored back in memory. Notice that the memory unit sees

back in memory. Notice that the memory unit sees only as t r e a mo fm e m o r ya d d r e s s e s .I td o e sn o tk n o wh o wt h e ya r eg e n e r a t e d( b y the instruction counter, indexing, indirection, literal addresses, or some other means) or what they are for (instructions or data). Accordingly, we can ignore how am e m o r ya d d r e s si sg e n e r a t e db yap r o g r a m .W ea r ei n t e r e s t e do n l yi n the sequence of memory addresses generated by the running program.

memory addresses generated by the running program. Ideally, we want the programs and data to reside in main memory permanently. This arrangement usually is not possible for the following two reasons: 1.Main memory is usually too small to store all needed programs and data permanently. 2.Main memory is a volatile storage device that loses its contents when power is turned off or otherwise lost. Thus, most computer systems provide secondary storage as an extension of main memory. The main

storage as an extension of main memory. The main requirement for secondary storage is that it be able to hold large quantities of data permanently. The most common secondarystorage device is a magnetic disk ,w h i c h provides storage for both programs and data. Most programs (system and application) are stored on a disk until they are loaded into memory. Many programs then use the disk as both the source and the destination of their processing. Hence, the proper management of disk storage is of

Hence, the proper management of disk storage is of central importance to a computer system, as we discuss in Chapter 10. In a larger sense, however, the storage structure that we have described consisting of registers, main memory, and magnetic disksis only one of many possible storage systems. Others include cache memory, CDROM ,m a g n e t i c tapes, and so on. Each storage system provides the basic functions of storing ad a t u ma n dh o l d i n gt h a td a t u mu n t i li ti sr e t r i e v e

n gt h a td a t u mu n t i li ti sr e t r i e v e da tal a t e rt i m e .T h em a i n differences among the various storage systems lie in speed, cost, size, and volatility. The wide variety of storage systems can be organized in a hierarchy (Figure 1.4) according to speed and cost. The higher levels are expensive, but they are fast. As we move down the hierarchy, the cost per bit generally decreases, whereas the access time generally in creases. This tradeoff is reasonable; if a given storage

This tradeoff is reasonable; if a given storage system were both faster and less expensive than anotherother properties being the samethen there would be no reason to use the slower, more expensive memory. In fact, many early storage devices, including paper1.2 ComputerSystem Organization 11registers cache main memory solidstate disk magnetic disk optical disk magnetic tapes Figure 1.4 Storagedevice hierarchy. tape and core memories, are relegated to museums now that magnetic tape and

relegated to museums now that magnetic tape and semiconductor memory have become faster and cheaper. The top four levels of memory in Figure 1.4 may be constructed using semiconductor memory. In addition to differing in speed and cost, the various storage systems are either volatile or nonvolatile. As mentioned earlier, volatile storage loses its contents when the power to the device is removed. In the absence of expensive battery and generator backup systems, data must be written to nonvolatile

systems, data must be written to nonvolatile storage for safekeeping. In the hierarchy shown in Figure 1.4, the storage systems above the solidstate disk are volatile, whereas those including the solidstate disk and below are nonvolatile. Solidstate disks have several variants but in general are faster than magnetic disks and are nonvolatile. One type of solidstate disk stores data in a large DRAM array during normal operation but also contains a hidden magnetic hard disk and a battery for

a hidden magnetic hard disk and a battery for backup power. If external power is interrupted, this solidstate disks controller copies the data from RAM to the magnetic disk. When external power is restored, the controller copies the data back into RAM . Another form of solidstate disk is ash memory, which is popular in cameras and personal digital assistants (PDA s),i nr o b o t s ,a n di n c r e a s i n g l yf o rs t o r a g e on generalpurpose computers. Flash memory is slower than DRAM but

computers. Flash memory is slower than DRAM but needs no power to retain its contents. Another form of nonvolatile storage is NVRAM , which is DRAM with battery backup power. This memory can be as fast as DRAM and (as long as the battery lasts) is nonvolatile. The design of a complete memory system must balance all the factors just discussed: it must use only as much expensive memory as necessary while providing as much inexpensive, nonvolatile memory as possible. Caches can12 Chapter 1

memory as possible. Caches can12 Chapter 1 Introduction be installed to improve performance where a large disparity in access time or transfer rate exists between two components. 1.2.3 IO Structure Storage is only one of many types of IOdevices within a computer. A large portion of operating system code is dedicated to managing IO,b o t hb e c a u s e of its importance to the reliability and performance of a system and because of the varying nature of the devices. Next, we provide an overview of

of the devices. Next, we provide an overview of IO. Ag e n e r a l  p u r p o s ec o m p u t e rs y s t e mc o n s i s t so f CPUsa n dm u l t i p l ed e v i c e controllers that are connected through a common bus. Each device controller is in charge of a specic type of device. Depending on the controller, more than one device may be attached. For instance, seven or more devices can be attached to the small computersystems interface (SCSI )controller. A device controller maintains some local

A device controller maintains some local buffer storage and a set of specialpurpose registers. The device controller is respon sible for moving the data between the peripheral devices that it controls and its local buffer storage. Typically, operating systems have a device driver for each device controller. This device driver understands the device controller and provides the rest of the operating system with a uniform interface to the device. To start an IOoperation, the device driver loads the

start an IOoperation, the device driver loads the appropriate registers within the device controller. The device controller, in turn, examines the contents of these registers to determine what action to take (such as read ac h a r a c t e rf r o mt h ek e y b o a r d ). The controller starts the transfer of data from the device to its local buffer. Once the transfer of data is complete, the device controller informs the device driver via an interrupt that it has nished its operation. The device

that it has nished its operation. The device driver then returns control to the operating system, possibly returning the data or a pointer to the data if the operation was a read. For other operations, the device driver returns status information. This form of interruptdriven IOis ne for moving small amounts of data but can produce high overhead when used for bulk data movement such as disk IO.T os o l v et h i sp r o b l e m , direct memory access (DMA )is used. After setting up buffers,

access (DMA )is used. After setting up buffers, pointers, and counters for the IO device, the device controller transfers an entire block of data directly to or from its own buffer storage to memory, with no intervention by the CPU.O n l yo n ei n t e r r u p ti sg e n e r a t e dp e r block, to tell the device driver that the operation has completed, rather than the one interrupt per byte generated fo rl o w  s p e e dd e v i c e s .W h i l et h ed e v i c e controller is performing these

l et h ed e v i c e controller is performing these operations, the CPU is available to accomplish other work. Some highend systems use switch rather than bus architecture. On these systems, multiple components can talk to other components concurrently, rather than competing for cycles on a shared bus. In this case, DMA is even more effective. Figure 1.5 shows the interplay of all components of a computer system. 1.3 ComputerSystem Architecture In Section 1.2, we introduced the general structure

Section 1.2, we introduced the general structure of a typical computer system. A computer system can be organized in a number of different ways, which we1.3 ComputerSystem Architecture 13thread of executioninstructions and datainstruction execution cycle data movement DMA memory interruptcache data IO requestCPU (N) device (M) Figure 1.5 How a modern computer system works. can categorize roughly according to the number of generalpurpose processors used. 1.3.1 SingleProcessor Systems Until

used. 1.3.1 SingleProcessor Systems Until recently, most computer systems used a single processor. On a single processor system, there is one main CPU capable of executing a generalpurpose instruction set, including instructions from user processes. Almost all single processor systems have other specialpurpose processors as well. They may come in the form of devicespecic processors, such as disk, keyboard, and graphics controllers; or, on mainframes, they may come in the form of more

on mainframes, they may come in the form of more generalpurpose processors, such as IO processors that move data rapidly among the components of the system. All of these specialpurpose processors run a limited instruction set and do not run user processes. Sometimes, they are managed by the operating system, in that the operating system sends them information about their next task and monitors their status. For example, a diskcontroller microprocessor receives a sequence of requests from the

receives a sequence of requests from the main CPU and implements its own disk queue and scheduling algorithm. This arrangement relieves the main CPU of the overhead of disk scheduling. PCsc o n t a i nam i c r o p r o c e s s o ri nt h ek e y b o a r d to convert the keystrokes into codes to be sent to the CPU.I no t h e rs y s t e m s or circumstances, specialpurpose processors are lowlevel components built into the hardware. The operating system cannot communicate with these processors; they

cannot communicate with these processors; they do their jobs autonomously. The use of specialpurpose microprocessors is common and does not turn a singleprocessor system into14 Chapter 1 Introduction am u l t i p r o c e s s o r .I ft h e r ei so n l yo n eg e n e r a l  p u r p o s e CPU, then the system is as i n g l e  p r o c e s s o rs y s t e m . 1.3.2 Multiprocessor Systems Within the past several years, multiprocessor systems (also known as parallel systems ormulticore systems ) have

as parallel systems ormulticore systems ) have begun to dominate the landscape of computing. Such systems have two or more processors in close communication, sharing the computer bus and sometimes the clock, memory, and peripheral devices. Multiprocessor systems rst appeared prominently appeared in servers and have since migrated to desktop and laptop systems. Recently, multiple processors have appeared on mobile devices such as smartphones and tablet computers. Multiprocessor systems have three

computers. Multiprocessor systems have three main advantages: 1.Increased throughput .B yi n c r e a s i n gt h en u m b e ro fp r o c e s s o r s ,w ee x p e c t to get more work done in less time. The speedup ratio with Nprocessors is not N,however; rather, it is less than N.When multiple processors cooperate on a task, a certain amount of overhead is incurred in keeping all the parts working correctly. This overhead, plus contention for shared resources, lowers the expected gain from

shared resources, lowers the expected gain from additional processors. Similarly, Nprogrammers working closely together do not produce Ntimes the amount of work a single programmer would produce. 2.Economy of scale .M u l t i p r o c e s s o rs y s t e m sc a nc o s tl e s st h a ne q u i v a l e n t multiple singleprocessor systems, because they can share peripherals, mass storage, and power supplies. If several programs operate on the same set of data, it is cheaper to store those data on one

of data, it is cheaper to store those data on one disk and to have all the processors share them than to have many computers with local disks and many copies of the data. 3.Increased reliability . If functions can be distributed properly among several processors, then the failure of one processor will not halt the system, only slow it down. If we have ten processors and one fails, then each of the remaining nine processors can pick up a share of the work of the failed processor. Thus, the entire

the work of the failed processor. Thus, the entire system runs only 10 percent slower, rather than failing altogether. Increased reliability of a computer system is crucial in many applications. The ability to continue providing service proportional to the level of surviving hardware is called graceful degradation .S o m es y s t e m sg ob e y o n dg r a c e f u l degradation and are called fault tolerant ,b e c a u s et h e yc a ns u f f e raf a i l u r eo f any single component and still

raf a i l u r eo f any single component and still continue operation. Fault tolerance requires am e c h a n i s mt oa l l o wt h ef a i l u r et ob ed e t e c t e d ,d i a g n o s e d ,a n d ,i fp o s s i b l e , corrected. The HPNonStop (formerly Tandem) system uses both hardware and software duplication to ensure continued operation despite faults. The system consists of multiple pairs of CPUs, working in lockstep. Both processors in the pair execute each instruction and compare the results.

execute each instruction and compare the results. If the results differ, then one CPU of the pair is at fault, and both are halted. The process that was being executed is then moved to another pair of CPUs, and the instruction that failed1.3 ComputerSystem Architecture 15 is restarted. This solution is expensive, since it involves special hardware and considerable hardware duplication. The multipleprocessor systems in use today are of two types. Some systems use asymmetric multiprocessing , in

Some systems use asymmetric multiprocessing , in which each processor is assigned as p e c i  ct a s k .A boss processor controls the system; the other processors either look to the boss for instruction or have predened tasks. This scheme denes a bossworker relationship. The boss processor schedules and allocates work to the worker processors. The most common systems use symmetric multiprocessing (SMP ),i n which each processor performs all tasks within the operating system. SMP means that all

within the operating system. SMP means that all processors are peers; no bossworker relationship exists between processors. Figure 1.6 illustrates a typical SMP architecture. Notice that each processor has its own set of registers, as well as a privateor local cache. However , all processors share physical memory . An example of an SMP system is AIX,ac o m m e r c i a lv e r s i o no f UNIX designed by IBM.A n AIX system can be congured to employ dozens of processors. The benet of this model is

dozens of processors. The benet of this model is that many processes can run simultaneously Nprocesses can run if there are NCPUswithout causing performance to deteriorate signicantly . However, we must carefully control IO to ensure that the data reach the appropriate processor. Also, since the CPUs are separate, one may be sitting idle while another is overloaded, resulting in inefciencies. These inefciencies can be avoided if the processors share certain data structures. A multiprocessor

share certain data structures. A multiprocessor system of this form will allow processes and resourcessuch as memory to be shared dynamically among the various processors and can lower the variance among the processors. Such a system must be written carefully, as we shall see in Chapter 5. Virtually all modern operating systemsincluding Windows, Mac OS X ,a n dL i n u x  n o wp r o v i d es u p p o r tf o r SMP. The difference between symmetric and asymmetric multiprocessing may result from

and asymmetric multiprocessing may result from either hardware or software. Special hardware can differentiate the multiple processors, or the software can be written to allow only one boss and multiple workers. For instance, Sun Microsystems operating system SunOS Version 4 provided asymmetric multiprocessing, whereas Version 5 (Solaris) is symmetric on the same hardware. Multiprocessing adds CPUst oi n c r e a s ec o m p u t i n gp o w e r .I ft h e CPU has an integrated memory controller,

.I ft h e CPU has an integrated memory controller, then adding CPUsc a na l s oi n c r e a s et h ea m o u n tCPU 0 registers cacheCPU 1 registers cacheCPU 2 registers cache memory Figure 1.6 Symmetric multiprocessing architecture.16 Chapter 1 Introduction of memory addressable in the system. Either way, multiprocessing can cause as y s t e mt oc h a n g ei t sm e m o r ya c c e s sm o d e lf r o mu n i f o r mm e m o r ya c c e s s (UMA )t on o n  u n i f o r mm e m o r ya c c e s s( NUMA ).UMA

n  u n i f o r mm e m o r ya c c e s s( NUMA ).UMA is dened as the situation in which access to any RAM from any CPU takes the same amount of time. With NUMA , some parts of memory may take longer to access than other parts, creating a performance penalty. Operating systems can minimize the NUMA penalty through resource management, as discussed in Section 9.5.4. Ar e c e n tt r e n di n CPU design is to include multiple computing cores on a single chip. Such multiprocessor systems are termed

chip. Such multiprocessor systems are termed multicore .T h e y can be more efcient than multiple chips with single cores because onchip communication is faster than betweenchip communication. In addition, one chip with multiple cores uses signicantly less power than multiple singlecore chips. It is important to note that while multicore systems are multiprocessor systems, not all multiprocessor systems are multicore, as we shall see in Section 1.3.3. In our coverage of multiprocessor systems

1.3.3. In our coverage of multiprocessor systems throughout this text, unless we state otherwise, we generally use the more contemporary term multicore , which excludes some multiprocessor systems. In Figure 1.7, we show a dualcore design with two cores on the same chip. In this design, each core has its own register set as well as its own local cache. Other designs might use a shared cache or a combination of local and shared caches. Aside from architectural considerations, such as cache,

from architectural considerations, such as cache, memory, and bus contention, these multicore CPUsa p p e a rt ot h eo p e r a t i n gs y s t e ma s Nstandard processors. This characteristic puts pressure on operating system designersand application programmersto make use of those processing cores. Finally, blade servers are a relatively recent development in which multiple processor boards, IOboards, and networking boards are placed in the same chassis. The difference between these and

the same chassis. The difference between these and traditional multiprocessor systems is that each bladeprocessor board boots independently and runs its own operating system. Some bladeserver boards are multiprocessor as well, which blurs the lines between types of computers. In essence, these servers consist of multiple independent multiprocessor systems.CPU core 0 registers cacheCPU core 1 registers cache memory Figure 1.7 Ad u a l  c o r ed e s i g nw i t ht w oc o r e sp l a c e do nt h es a

s i g nw i t ht w oc o r e sp l a c e do nt h es a m ec h i p .1.3 ComputerSystem Architecture 17 1.3.3 Clustered Systems Another type of multiprocessor system is a clustered system ,w h i c hg a t h e r s together multiple CPUs. Clustered systems differ from the multiprocessor systems described in Section 1.3.2 in that they are composed of two or more individual systemsor nodesjoined together. Such systems are considered loosely coupled .E a c hn o d em a yb eas i n g l ep r o c e s s o rs y s

hn o d em a yb eas i n g l ep r o c e s s o rs y s t e mo ram u l t i c o r e system. We should note that the denition of clustered is not concrete; many commercial packages wrestle to dene a clustered system and why one form is better than another. The generally accepted denition is that clustered computers share storage and are closely linked via a localarea network LAN (as described in Chapter 17) or a faste ri n t e r c o n n e c t ,s u c ha sI n  n i B a n d . Clustering is usually used to

sI n  n i B a n d . Clustering is usually used to provide highavailability servicethat is, service will continue even if one or more systems in the cluster fail. Generally, we obtain high availability by adding a level of redundancy in the system. Al a y e ro fc l u s t e rs o f t w a r er u n so nt h ec l u s t e rn o d e s .E a c hn o d ec a nm o n i t o r one or more of the others (over the LAN ). If the monitored machine fails, the monitoring machine can take ownership of its storage and

machine can take ownership of its storage and restart the applications that were running on the failed machine. The users and clients of the applications see only a brief interruption of service. Clustering can be structured asymmetrically or symmetrically. In asym metric clustering ,o n em a c h i n ei si n hotstandby mode while the other is running the applications. The hotstandby host machine does nothing but monitor the active server. If that server fails, the hotstandby host becomes the

that server fails, the hotstandby host becomes the active server. In symmetric clustering ,t w oo rm o r eh o s t sa r er u n n i n g applications and are monitoring each other. This structure is obviously more efcient, as it uses all of the available hardware. However it does require that more than one application be available to run. Since a cluster consists of several computer systems connected via a network, clusters can also be used to provide highperformance computing environments. Such

highperformance computing environments. Such systems can supply signicantly greater computational power than singleprocessor or even SMP systems because they can run an application concurrently on all computers in the cluster. The application must have been written specically to take advantage of the cluster, however. This involves a technique known as parallelization ,w h i c hd i v i d e sap r o g r a mi n t o separate components that run in parallel on individual computers in the cluster.

parallel on individual computers in the cluster. Typically, these applications are designed so that once each computing node in the cluster has solved its portion of the problem, the results from all the nodes are combined into a nal solution. Other forms of clusters include parallel clusters and clustering over a widearea network ( WAN )( a sd e s c r i b e di nC h a p t e r1 7 ) .P a r a l l e lc l u s t e r sa l l o w multiple hosts to access the same data on shared storage. Because most

the same data on shared storage. Because most operating systems lack support for simultaneous data access by multiple hosts, parallel clusters usually require the use of special versions of software and special releases of applications. For example, Oracle Real Application Cluster is a version of Oracles database that has been designed to run on a parallel cluster. Each machine runs Oracle, and a layer of software tracks access to the shared disk. Each machine has full access to all data in the

Each machine has full access to all data in the database. To provide this shared access, the system must also supply access control and locking to18 Chapter 1 Introduction BEOWULF CLUSTERS Beowulf clusters are designed to solve highperformance computing tasks. AB e o w u l fc l u s t e rc o n s i s t so fc o m m o d i t yh a r d w a r e  s u c ha sp e r s o n a l computersconnected via a simple localarea network. No single specic software package is required to construct a cluster. Rather, the

is required to construct a cluster. Rather, the nodes use a set of opensource software libraries to communicate with one another. Thus, there are a variety of approaches to constructing a Beowulf cluster. Typically, though, Beowulf computing nodes run the Linux operating system. Since Beowulf clusters require no special hardware and operate using opensource software that is available free, they offer a lowcost strategy for building ah i g h  p e r f o r m a n c ec o m p u t i n gc l u s t e r .I

r f o r m a n c ec o m p u t i n gc l u s t e r .I nf a c t ,s o m eB e o w u l fc l u s t e r sb u i l t from discarded personal computers are using hundreds of nodes to solve computationally expensive scientic computing problems. ensure that no conicting operations occur. This function, commonly known as a distributed lock manager (DLM ),i si n c l u d e di ns o m ec l u s t e rt e c h n o l o g y . Cluster technology is changing rapidly. Some cluster products support dozens of systems in a

cluster products support dozens of systems in a cluster, as well as clustered nodes that are separated by miles. Many of these improvements are made possible by storagearea networks (SAN s),a sd e s c r i b e di nS e c t i o n1 0 . 3 . 3 ,w h i c ha l l o wm a n ys y s t e m s to attach to a pool of storage. If the applications and their data are stored on the SAN,t h e nt h ec l u s t e rs o f t w a r ec a na s s i g nt h ea p p l i c a t i o nt or u no na n y host that is attached to the SAN.I

or u no na n y host that is attached to the SAN.I ft h eh o s tf a i l s ,t h e na n yo t h e rh o s tc a nt a k e over. In a database cluster, dozens of hosts can share the same database, greatly increasing performance and reliability. Figure 1.8 depicts the general structure of a clustered system.computerinterconnectcomputerinterconnectcomputer storage area network Figure 1.8 General structure of a clustered system.1.4 OperatingSystem Structure 19job 10 Maxoperating system job 2 job 3 job 4

19job 10 Maxoperating system job 2 job 3 job 4 Figure 1.9 Memory layout for a multiprogramming system. 1.4 OperatingSystem Structure Now that we have discussed basic computersystem organization and archi tecture, we are ready to talk about operating systems. An operating system provides the environment within which programs are executed. Internally, operating systems vary greatly in their makeup, since they are organized along many different lines. There a re, however, many commonalities, which

There a re, however, many commonalities, which we consider in this section. One of the most important aspects of operating systems is the ability to multiprogram. A single program cannot, in general, keep either the CPU or the IO devices busy at all times. Single users frequently have multiple programs running. Multiprogramming increases CPU utilization by organizing jobs (code and data) so that the CPU always has one to execute. The idea is as follows: The operating system keeps several jobs in

The operating system keeps several jobs in memory simultaneously (Figure 1.9). Since, in general, main memory is too small to accommodate all jobs, the jobs are kept initially on the disk in the job pool . This pool consists of all processes residing on disk awaiting allocation of main memory. The set of jobs in memory can be a subset of the jobs kept in the job pool. The operating system picks and begins to execute one of the jobs in memory. Eventually, the job may have to wait for some task,

the job may have to wait for some task, such as an IO operation, to complete. In a nonmultiprogrammed system, the CPU would sit idle. In a multiprogrammed system, the operating system simply switches to, and executes, another job. When that job needs to wait, the CPU switches to another job, and so on. Eventually, the rst job nishes waiting and gets the CPU back. As long as at least one job needs to execute, the CPU is never idle. This idea is common in other life situations. A lawyer does not

common in other life situations. A lawyer does not work for only one client at a time, for example. While one case is waiting to go to trial or have papers typed, the lawyer can work on another case. If he has enough clients, the lawyer will never be idle for lack of work. (Idle lawyers tend to become politicians, so there is a certain social value in keeping lawyers busy.)20 Chapter 1 Introduction Multiprogrammed systems provide an environment in which the various system resources (for example,

which the various system resources (for example, CPU,m e m o r y ,a n dp e r i p h e r a ld e v i c e s )a r e utilized effectively, but they do not provide for user interaction with the computer system. Time sharing (ormultitasking )i sal o g i c a le x t e n s i o no f multiprogramming. In timesharing systems, the CPU executes multiple jobs by switching among them, but the switches occur so frequently that the users can interact with each program while it is running. Time sharing requires an

while it is running. Time sharing requires an interactive computer system, which provides direct communication between the user and the system. The user gives instructions to the operating system or to a program directly, using a input device such as a keyboard, mouse, touch pad, or touch screen, and waits for immediate results on an output device. Accordingly, the response time should be shorttypically less than one second. At i m e  s h a r e do p e r a t i n gs y s t e ma l l o w sm a n yu s

do p e r a t i n gs y s t e ma l l o w sm a n yu s e r st os h a r et h ec o m p u t e r simultaneously. Since each action or command in a timeshared system tends to be short, only a little CPU time is needed for each user. As the system switches rapidly from one user to the next, each user is given the impression that the entire computer system is dedicated to his use, even though it is being shared among many users. At i m e  s h a r e do p e r a t i n gs y s t e mu s e s CPU scheduling and

e r a t i n gs y s t e mu s e s CPU scheduling and multiprogram ming to provide each user with a small portion of a timeshared computer. Each user has at least one separate program in memory. A program loaded into memory and executing is called a process .W h e nap r o c e s se x e c u t e s ,i tt y p i c a l l y executes for only a short time before i te i t h e r n i s h e so rn e e d st op e r f o r m IO. IOmay be interactive; that is, output goes to a display for the user, and input comes

goes to a display for the user, and input comes from a user keyboard, mouse, or other device. Since interactive IO typically runs at people speeds, it may take a long time to complete. Input, for example, may be bounded by the users typing speed; seven characters per second is fast for people but incredibly slow for computers. Rather than let the CPU sit idle as this interactive input takes place, the operating system will rapidly switch the CPU to the program of some other user. Time sharing

to the program of some other user. Time sharing and multiprogramming require that several jobs be kept simultaneously in memory. If several jobs are ready to be brought into memory, and if there is not enough room for all of them, then the system must choose among them. Making this decision involves job scheduling ,w h i c hw ed i s c u s s in Chapter 6. When the operating system selects a job from the job pool, it loads that job into memory for execution. Having several programs in memory at

execution. Having several programs in memory at the same time requires some form of memory management, which we cover in Chapters 8 and 9. In addition, if several jobs are ready to run at the same time, the system must choose which job will run rst. Making this decision is CPU scheduling ,w h i c hi sa l s od i s c u s s e di nC h a p t e r6 .F i n a l l y ,r u n n i n gm u l t i p l e jobs concurrently requires that their ability to affect one another be limited in all phases of the operating

another be limited in all phases of the operating system, including process scheduling, disk storage, and memory management. We discuss these considerations throughout the text. In a timesharing system, the operating system must ensure reasonable response time. This goal is sometimes accomplished through swapping , whereby processes are swapped in and out of main memory to the disk. A more common method for ensuring reasonable response time is virtual memory ,a technique that allows the

is virtual memory ,a technique that allows the execution of a process that is not completely in1.5 OperatingSystem Operations 21 memory (Chapter 9). The main advantage of the virtualmemory scheme is that it enables users to run programs that are larger than actual physical memory . Further, it abstracts main memory into a large, uniform array of storage, separating logical memory as viewed by the user from physical memory. This arrangement frees programmers from concern over memorystorage

frees programmers from concern over memorystorage limitations. At i m e  s h a r i n gs y s t e mm u s ta l s op r o v i d ea l es y s t e m( C h a p t e r s1 1a n d 12). The le system resides on a collection of disks; hence, disk management must be provided (Chapter 10). In addition, a timesharing system provides am e c h a n i s mf o rp r o t e c t i n gr e s o u r c e sf r o mi n a p p r o p r i a t eu s e( C h a p t e r1 4 ) . To ensure orderly execution, the system must provide mechanisms

execution, the system must provide mechanisms for job synchronization and communication (Chapter 5), and it may ensure that jobs do not get stuck in a deadlock, forever wa iting for one another (Chapter 7). 1.5 OperatingSystem Operations As mentioned earlier, modern operating systems are interrupt driven .I ft h e r e are no processes to execute, no IOdevices to service, and no users to whom to respond, an operating system will sit quietly, waiting for something to happen. Events are almost

waiting for something to happen. Events are almost always signaled by the occurrence of an interrupt or a trap. A trap (or an exception ) is a softwaregenerated interrupt caused either by an error (for example, division by zero or invalid memory access) or by a specic request from a user program that an operatingsystem service be performed. The interruptdriven nature of an operating system denes that systems general structure. For each type of interrupt, separate segments of code in the

of interrupt, separate segments of code in the operating system determine what action should be taken. An interrupt service routine is pro vided to deal with the interrupt. Since the operating system and the users share the hardware and software resources of the computer system, we need to make sure that an error in a user program could cause problems only for the one program running. With sharing, many processes could be adversely affected by a bug in one program. For example, if a process gets

bug in one program. For example, if a process gets stuck in an innite loop, this loop could prevent the correct operation of many other processes. More subtle errors can occur in a multiprogramming system, where one erroneous program might modify another program, the data of another program, or even the operating system itself. Without protection against these sorts of errors, either the computer must execute only one process at a time or all output must be suspect. A properly designed operating

must be suspect. A properly designed operating system must ensure that an incorrect (or malicious) program cannot cause other programs to execute incorrectly. 1.5.1 DualMode and Multimode Operation In order to ensure the proper execution of the operating system, we must be able to distinguish between the execution of operatingsystem code and user dened code. The approach taken by most computer systems is to provide hardware support that allows us to differentiate among various modes of

allows us to differentiate among various modes of execution.22 Chapter 1 Introduction user process executinguser process kernelcalls system call return from system calluser mode (mode bit  1) trap mode bit  0return mode bit  1kernel mode (mode bit  0)execute system callFigure 1.10 Transition from user to kernel mode. At the very least, we need two separate modes of operation: user mode and kernel mode (also called supervisor mode ,system mode ,o rprivileged mode ). A bit, called the mode bit ,i

rprivileged mode ). A bit, called the mode bit ,i sa d d e dt ot h eh a r d w a r eo ft h ec o m p u t e r to indicate the current mode: kernel (0) or user (1). With the mode bit, we can distinguish between a task that is exec uted on behalf of the operating system and one that is executed on behalf of the user. When the computer system is executing on behalf of a user application, the system is in user mode. However, when a user application requests a service from the operating system (via a

a service from the operating system (via a system call), the system must transition from user to kernel mode to fulll the request. This is shown in Figure 1.10. As we shall see, this architectural enhancement is useful for many other aspects of system operation as well. At system boot time, the hardware starts in kernel mode. The operating system is then loaded and starts user applications in user mode. Whenever a trap or interrupt occurs, the hardware switches from user mode to kernel mode

hardware switches from user mode to kernel mode (that is, changes the state of the mode bit to 0). Thus, whenever the operating system gains control of the computer, it is in kernel mode. The system always switches to user mode (by setting the mode bit to 1) before passing control to au s e rp r o g r a m . The dual mode of operation provides us with the means for protecting the operating system from errant usersand errant users from one another. We accomplish this protection by designating some

We accomplish this protection by designating some of the machine instructions that may cause harm as privileged instructions . The hardware allows privileged instructions to be executed only in kernel mode. If an attempt is made to execute a privileged instruction in user mode, the hardware does not execute the instruction but rather treats it as illegal and traps it to the operating system. The instruction to switch to kernel mode is an example of a privileged instruction. Some other examples

of a privileged instruction. Some other examples include IOcontrol, timer management, and interrupt management. As we shall see throughout the text, there are many additional privileged instructions. The concept of modes can be extended beyond two modes (in which case the CPU uses more than one bit to set and test the mode). CPUst h a ts u p p o r t virtualization (Section 16.1) frequently have a separate mode to indicate when thevirtual machine manager (VMM )and the virtualization management

manager (VMM )and the virtualization management softwareis in control of the system. In this mode, the VMM has more privileges than user processes but fewer than the kernel. It needs that level of privilege so it can create and manage virtual machines, changing the CPU state to do so. Sometimes, too, different modes are used by various kernel1.5 OperatingSystem Operations 23 components. We should note that, as an alternative to modes, the CPU designer may use other methods to diffe rentiate

designer may use other methods to diffe rentiate operational privileges. The Intel 64 family of CPUs supports four privilege levels, for example, and supports virtualization but does not have a separate mode for virtualization. We can now see the life cycle of instruction execution in a computer system. Initial control resides in the operating system, where instructions are executed in kernel mode. When control is given to a user application, the mode is set to user mode. Eventually, control is

mode is set to user mode. Eventually, control is switched back to the operating system via an interrupt, a trap, or a system call. System calls provide the means for a user program to ask the operating system to perform tasks reserved for the operating system on the user programs behalf. A system call is invoked in a variety of ways, depending on the functionality provided by the underlying processor. In all forms, it is the method used by a process to request action by the operating system. A

to request action by the operating system. A system call usually takes the form of a trap to a specic location in the interrupt vector. This trap can be executed by a generic trap instruction, although some systems (such as MIPS )h a v eas p e c i  c syscall instruction to invoke a system call. When a system call is executed, it is typically treated by the hardware as a software interrupt. Control passes through the interrupt vector to a service routine in the operating system, and the mode bit

routine in the operating system, and the mode bit is set to kernel mode. The systemcall service routine is a part of the operating system. The kernel examines the interrupting instruction to determine what system call has occurred; a parameter indicates what type of service the user program is requesting. Additional information needed for the request may be passed in registers, on the stack, or in memory (with pointers to the memory locations passed in registers). The kernel veries th at the

passed in registers). The kernel veries th at the parameters are correct and legal, executes the request, and returns control to the instruction following the system call. We describe system calls more fully in Section 2.3. The lack of a hardwaresupported dual mode can cause serious shortcom ings in an operating system. For instance, MSDOS was written for the Intel 8088 architecture, which has no mode bit and therefore no dual mode. A user program running awry can wipe out the operating system

running awry can wipe out the operating system by writing over it with data; and multiple programs are able to write to a device at the same time, with potentially disastrous results. Modern versions of the Intel CPU do provide dualmode operation. Accordingly, most contemporary operating systemssuch as Microsoft Windows 7, as well as Unix and Linuxtake advantage of this dualmode feature and provide greater protection for the operating system. Once hardware protection is in place, it detects

Once hardware protection is in place, it detects errors that violate modes. These errors are normally handled by the operating system. If a user program fails in some waysuch as by making an attempt either to execute an illegal instruction or to access memory that is not in the users address spacethen the hardware traps to the operating system. The trap transfers control through the interrupt vector to the operating system, just as an interrupt does. When ap r o g r a me r r o ro c c u r s ,t h

does. When ap r o g r a me r r o ro c c u r s ,t h eo p e r a t i n gs y s t e mm u s tt e r m i n a t et h ep r o g r a m abnormally. This situation is handled by the same code as a userrequested abnormal termination. An appropriate error message is given, and the memory of the program may be dumped. The memory dump is usually written to a le so that the user or programmer can examine it and perhaps correct it and restart the program.24 Chapter 1 Introduction 1.5.2 Timer We must ensure that the

1 Introduction 1.5.2 Timer We must ensure that the operating system maintains control over the CPU. We cannot allow a user program to get stuck in an innite loop or to fail to call system services and never return control to the operating system. To accomplish this goal, we can use a timer .At i m e rc a nb es e tt oi n t e r r u p t the computer after a specied period. The period may be xed (for example, 160 second) or variable (for example, from 1 millisecond to 1 second). A variable timer is

1 millisecond to 1 second). A variable timer is generally implemented by a xedrate clock and a counter. The operating system sets the counter. Every time the clock ticks, the counter is decremented. When the counter reaches 0 , an interrupt occurs. For instance, a1 0  b i tc o u n t e rw i t ha1  m i l l i s e c o n dc l o c ka l l o w si n t e r r u p t sa ti n t e r v a l sf r o m 1m i l l i s e c o n dt o1 , 0 2 4m i l l i s e c o n d s ,i ns t e p so f1m i l l i s e c o n d . Before turning

t e p so f1m i l l i s e c o n d . Before turning over control to the user, the operating system ensures that the timer is set to interrupt. If the timer interrupts, control transfers automatically to the operating system, which may treat the interrupt as a fatal error or may give the program more time. Clearly, instructions that modify the content of the timer are privileged. We can use the timer to prevent a user program from running too long. A simple technique is to initialize a counter with

A simple technique is to initialize a counter with the amount of time that a program is allowed to run. A program with a 7minute time limit, for example, would have its counter initialized to 420. Every second, the timer interrupts, and the counter is decremented by 1. As long as the counter is positive, control is returned to the user program. When the counter becomes negative, the operating system terminates the program for exceeding the assigned time limit. 1.6 Process Management A program

time limit. 1.6 Process Management A program does nothing unless its instructions are executed by a CPU.A program in execution, as mentioned, is a process. A timeshared user program such as a compiler is a process. A wordprocessing program being run by an individual user on a PCis a process. A system task, such as sending output to a printer, can also be a process (or at least part of one). For now, you can consider a process to be a job or a timeshared program, but later you will learn that the

program, but later you will learn that the concept is more general. As we shall see in Chapter 3, it is possible to provide system calls that allow processes to create subprocesses to execute concurrently. Ap r o c e s sn e e d sc e r t a i nr e s o u r c e s  i n c l u d i n g CPU time, memory, les, and IOdevicesto accomplish its task. These resources are either given to the process when it is created or allocated to it while it is running. In addition to the various physical and logical

In addition to the various physical and logical resources that a process obtains when it is created, various initialization data (input) may be passed along. For example, consider a process whose function is to display the status of a le on the screen of a terminal. The process will be given the name of the le as an input and will execute the appropriate instructions and system calls to obtain and display the desired information on the terminal. When the process terminates, the operating system

When the process terminates, the operating system will reclaim any reusable resources. We emphasize that a program by itself is not a process. A program is a passive entity, like the contents of a le stored on disk, whereas a process1.7 Memory Management 25 is an active entity. A singlethreaded process has one program counter specifying the next instruction to execute. (Threads are covered in Chapter 4.) The execution of such a process must be sequential. The CPU executes one instruction of the

The CPU executes one instruction of the process after anoth er, until the process completes. Further, at any time, one instruction at most is exec uted on behalf of the process. Thus, although two processes may be associated with the same program, they are nevertheless considered two separate execution sequences. A multithreaded process has multiple program counters, each pointing to the next instruction to execute for a given thread. Ap r o c e s si st h eu n i to fw o r ki nas y s t e m .As y

s si st h eu n i to fw o r ki nas y s t e m .As y s t e mc o n s i s t so fac o l l e c t i o n of processes, some of which are operatingsystem processes (those that execute system code) and the rest of which are user processes (those that execute user code). All these processes can potentially execute concurrentlyby multiplexing on a single CPU,f o re x a m p l e . The operating system is responsible for the following activities in connec tion with process management: Scheduling processes and

with process management: Scheduling processes and threads on the CPUs Creating and deleting both user and system processes Suspending and resuming processes Providing mechanisms for process synchronization Providing mechanisms for process communication We discuss processmanagement techniques in Chapters 3 through 5. 1.7 Memory Management As we discussed in Section 1.2.2, the main memory is central to the operation of a modern computer system. Main memory is a large array of bytes, ranging in

Main memory is a large array of bytes, ranging in size from hundreds of thousands to billions. Each byte has its own address. Main memory is a repository of quickly accessible data shared by the CPU and IOdevices. The central processor reads instructions from main memory during the instructionfetch cycle and both reads and writes data from main memory during the datafetch cycle (on a von Neumann architecture). As noted earlier, the main memory is generally the only large storage device that the

generally the only large storage device that the CPU is able to address and access directly. For example, for the CPU to process data from disk, those data must rst be transferred to main memory by CPUgenerated IO calls. In the same way, instructions must be in memory for the CPU to execute them. For a program to be executed, it must be mapped to absolute addresses and loaded into memory. As the program executes, it accesses program instructions and data from memory by generating these absolute

and data from memory by generating these absolute addresses. Eventually, the program terminates, its memory space is declared available, and the next program can be loaded and executed. To improve both the utilization of the CPU and the speed of the computers response to its users, generalpurpose computers must keep several programs in memory, creating a need for memory management. Many different memory26 Chapter 1 Introduction management schemes are used. These schemes reect various approaches,

are used. These schemes reect various approaches, and the effectiveness of any given algorithm d epends on the situation. In selecting a memorymanagement scheme for a specic system, we must take into account many factorsespecially the hardware design of the system. Each algorithm requires its own hardware support. The operating system is responsible for the following activities in connec tion with memory management: Keeping track of which parts of memory are currently being used and who is using

memory are currently being used and who is using them Deciding which processes (or parts of processes) and data to move into and out of memory Allocating and deallocating memory space as needed Memorymanagement techniques are discussed in Chapters 8 and 9. 1.8 Storage Management To make the computer system convenient for users, the operating system provides a uniform, logical view of information storage. The operating system abstracts from the physical properties of its storage devices to dene a

properties of its storage devices to dene a logical storage unit, the le.T h eo p e r a t i n gs y s t e mm a p s l e so n t op h y s i c a lm e d i aa n d accesses these les via the storage devices. 1.8.1 FileSystem Management File management is one of the most visible components of an operating system. Computers can store information on several different types of physical media. Magnetic disk, optical disk, and magnetic tape are the most common. Each of these media has its own characteristics

Each of these media has its own characteristics and physical organization. Each medium is controlled by a device, such as a disk drive or tape drive, that also has its own unique characteristics. These properties include access speed, capacity, datatransfer rate, and access method (sequential or random). A l ei sac o l l e c t i o no fr e l a t e di n f o r m a t i o nd e  n e db yi t sc r e a t o r .C o m m o n l y , les represent programs (both source and object forms) and data. Data les may

source and object forms) and data. Data les may be numeric, alphabetic, alphanumeric, or binary. Files may be freeform (for example, text les), or they may be formatted rigidly (for example, xed elds). Clearly, the concept of a le is an extremely general one. The operating system implements the abstract concept of a le by managing massstorage media, such as tapes and disks, and the devices that control them. In addition, les are normally organized into directories to make them easier to use.

into directories to make them easier to use. Finally, when multiple users have access to les, it may be desirable to control which user may access a le and how that user may access it (for example, read, write, append). The operating system is responsible for the following activities in connec tion with le management: Creating and deleting les1.8 Storage Management 27 Creating and deleting directories to organize les Supporting primitives for manipulating les and directories Mapping les onto

manipulating les and directories Mapping les onto secondary storage Backing up les on stable (nonvolatile) storage media Filemanagement techniques are discu ssed in Chapters 11 and 12. 1.8.2 MassStorage Management As we have already seen, because main memory is too small to accommodate all data and programs, and because the data that it holds are lost when power is lost, the computer system must provide secondary storage to back up main memory. Most modern computer systems use disks as the

Most modern computer systems use disks as the principal online storage medium for both programs and data. Most programsincluding compilers, assemblers, word processors, editors, and formattersare stored on a disk until loaded into memory. They then use the disk as both the source and destination of their processing. Hence, the proper management of disk storage is of central importance to a computer system. The operating system is responsible for the following activities in connection with disk

the following activities in connection with disk management: Freespace management Storage allocation Disk scheduling Because secondary storage is used frequently, it must be used efciently. The entire speed of operation of a computer may hinge on the speeds of the disk subsystem and the algorithms that manipulate that subsystem. There are, however, many uses for storage that is slower and lower in cost (and sometimes of higher capacity) than secondary storage. Backups of disk data, storage of

storage. Backups of disk data, storage of seldomused data, and longterm archival storage are some examples. Magnetic tape drives and their tapes and CDand DVD drives and platters are typical tertiary storage devices. The media (tapes and optical platters) vary between WORM (writeonce, readmanytimes) and RW(read write) formats. Tertiary storage is not crucial to system performance, but it still must be managed. Some operating systems take on this task, while others leave tertiarystorage

on this task, while others leave tertiarystorage management to application programs. Some of the functions that operating systems can provide include mounting and unmounting media in devices, allocating and freeing the devices for exclusive use by processes, and migrating data from secondary to tertiary storage. Techniques for secondary and tertiary storage management are discussed in Chapter 10. 1.8.3 Caching Caching is an important principle of computer systems. Heres how it works. Information

computer systems. Heres how it works. Information is normally kept in some storage system (such as main memory). As it is used, it is copied into a faster storage systemthe cacheon a28 Chapter 1 Introduction temporary basis. When we need a particular piece of information, we rst check whether it is in the cache. If it is, we use the information directly from the cache. If it is not, we use the information from the source, putting a copy in the cache under the assumption that we will need it

cache under the assumption that we will need it again soon. In addition, internal programmable registers, such as index registers, provide a highspeed cache for main memory. The programmer (or compiler) implements the registerallocation and registerreplacement algorithms to decide which information to keep in registers and which to keep in main memory. Other caches are implemented totally in hardware. For instance, most systems have an instruction cache to hold the instructions expected to be

cache to hold the instructions expected to be executed next. Without this cache, the CPU would have to wait several cycles while an instruction was fetched from main memory. For similar reasons, most systems have one or more highspeed data caches in the memory hierarchy. We are not concerned with these hardwareonly caches in this text, since they are outside the control of the operating system. Because caches have limited size, cache management is an important design problem. Careful selection

is an important design problem. Careful selection of the cache size and of a replacement policy can result in greatly increased performance. Figure 1.11 compares storage performance in large workstations and small servers. Various replacement algorithms for softwarecontrolled caches are discussed in Chapter 9. Main memory can be viewed as a fast cache for secondary storage, since data in secondary storage must be copied into main memory for use and data must be in main memory before being moved

and data must be in main memory before being moved to secondary storage for safekeeping. The lesystem data, which resides permanently on secondary storage, may appear on several levels in the storage hierarchy. At the highest level, the operating system may maintain a cache of lesystem data in main memory. In addition, solidstate disks may be used for highspeed storage that is accessed through the lesystem interface. The bulk of secondary storage is on magnetic disks. The magneticdisk storage,

is on magnetic disks. The magneticdisk storage, in turn, is often backed up onto magnetic tapes or removable disks to protect against data loss in case of a harddisk failure. Some systems automatically archive old le data from secondary storage to tertiary storage, such as tape jukeboxes, to lower the storage cost (see Chapter 10). Level Name Typical size Implementation technology Access time (ns) Bandwidth (MBsec) Managed by Backed by1 registers  1 KB custom memory with multiple ports CMOS 0.25

1 KB custom memory with multiple ports CMOS 0.25  0.5 20,000  100,000 compiler cache2 cache  16MB onchip or offchip CMOS SRAM 0.5  25 5,000  10,000 hardware main memory3 main memory  64GB CMOS SRAM 80  250 1,000  5,000 operating system disk4 solid state disk  1 TB flash memory 25,000  50,000 500 operating system disk5 magnetic disk  10 TB magnetic disk 5,000,000 20  150 operating system disk or tape Figure 1.11 Performance of various levels of storage.1.8 Storage Management 29AAAmagnetic

of storage.1.8 Storage Management 29AAAmagnetic diskmain memoryhardware registercache Figure 1.12 Migration of integer A from disk to register. The movement of information between levels of a storage hierarchy may be either explicit or implicit, depending on the hardware design and the controlling operatingsystem software. For instance, data transfer from cache toCPU and registers is usually a hardware function, with no operatingsystem intervention. In contrast, transfer of data from disk to

In contrast, transfer of data from disk to memory is usually controlled by the operating system. In a hierarchical storage structure, the same data may appear in different levels of the storage system. For example, suppose that an integer A that is to be incremented by 1 is located in le B, and le B resides on magnetic disk. The increment operation proceeds by rst issuing an IOoperation to copy the disk block on which A resides to main memory. This operation is followed by copying A to the cache

operation is followed by copying A to the cache and to an internal register. Thus, the copy of A appears in several places: on the magnetic disk, in main memory, in the cache, and in an internal register (see Figure 1.12). Once the increment takes place in the internal register, the value of A differs in the various storage systems. The value of A becomes the same only after the new value of A is written from the internal register back to the magnetic disk. In a computing environment where only

disk. In a computing environment where only one process executes at a time, this arrangement poses no difculties, since an access to integer A will always be to the copy at the highest level of the hierarchy. However, in a multitasking environment, where the CPU is switched back and forth among various processes, extreme care must be taken to ensure that, if several processes wish to access A, then each of these processes will obtain the most recently updated value of A. The situation becomes

recently updated value of A. The situation becomes more complicated in a multiprocessor environment where, in addition to maintaining internal registers, each of the CPUsa l s o contains a local cache (Figure 1.6). In such an environment, a copy of A may exist simultaneously in several caches. Since the various CPUs can all execute in parallel, we must make sure that an update to the value of A in one cache is immediately reected in all other caches where A resides. This situation is called

caches where A resides. This situation is called cache coherency , and it is usually a hardware issue (handled below the operatingsystem level). In a distributed environment, the situation becomes even more complex. In this environment, several copies (or replicas) of the same le can be kept on different computers. Since the various replicas may be accessed and updated concurrently, some distributed systems ensure that, when a replica is updated in one place, all other replicas are brought up to

in one place, all other replicas are brought up to date as soon as possible. There are various ways to achieve this guarantee, as we discuss in Chapter 17. 1.8.4 IO Systems One of the purposes of an operating system is to hide the peculiarities of specic hardware devices from the user. For example, in UNIX ,t h ep e c u l i a r i t i e so f IO30 Chapter 1 Introduction devices are hidden from the bulk of the operating system itself by the IO subsystem .T h e IOsubsystem consists of several

subsystem .T h e IOsubsystem consists of several components: Am e m o r y  m a n a g e m e n tc o m p o n e n tt h a ti n c l u d e sb u f f e r i n g ,c a c h i n g ,a n d spooling Ag e n e r a ld e v i c e  d r i v e ri n t e r f a c e Drivers for specic hardware devices Only the device driver knows the pecu liarities of the specic device to which it is assigned. We discussed in Section 1.2.3 how interrupt handlers and device drivers are used in the construction of efcient IOsubsystems. In

in the construction of efcient IOsubsystems. In Chapter 13, we discuss how the IOsubsystem interfaces to the other system components, manages devices, transfers data, and detects IOcompletion. 1.9 Protection and Security If a computer system has multiple users and allows the concurrent execution of multiple processes, then access to data must be regulated. For that purpose, mechanisms ensure that les, memory segments, CPU,a n do t h e rr e s o u r c e sc a n be operated on by only those

e s o u r c e sc a n be operated on by only those processes that have gained proper authoriza tion from the operating system. For example, memoryaddressing hardware ensures that a process can execute on ly within its own address space. The timer ensures that no process can gain control of the CPU without eventually relinquishing control. Devicecontrol registers are not accessible to users, so the integrity of the various periph eral devices is protected. Protection ,t h e n ,i sa n ym e c h a n

Protection ,t h e n ,i sa n ym e c h a n i s mf o rc o n t r o l l i n gt h ea c c e s so fp r o c e s s e s or users to the resources dened by a computer system. This mechanism must provide means to specify the controls to be imposed and to enforce the controls. Protection can improve reliability by detecting latent errors at the interfaces between component subsystems. Early detection of interface errors can often prevent contamination of a healthy subsystem by another subsystem that is

a healthy subsystem by another subsystem that is malfunctioning. Furthermore, an unprotected resource cannot defend against use (or misuse) by an unauthorized or incompetent user. A protectionoriented system provides a means to distinguish between authorized and unauthorized usage, as we discuss in Chapter 14. As y s t e mc a nh a v ea d e q u a t ep r o t e c t i o nb u ts t i l lb ep r o n et of a i l u r ea n d allow inappropriate access. Consider a user whose authentication information (her

a user whose authentication information (her means of identifying herself to the system) is stolen. Her data could be copied or deleted, even though le and memory protection are working. It is the job of security to defend a system from external and internal attacks. Such attacks spread across a huge range and include viruses and worms, denialof service attacks (which use all of a systems resources and so keep legitimate users out of the system), identity theft, and theft of service

the system), identity theft, and theft of service (unauthorized use of a system). Prevention of some of these attacks is considered an operatingsystem function on some systems, while other systems leave it to policy or additional software. Due to the alarming rise in security incidents,1.10 Kernel Data Structures 31 operatingsystem security features represent a fastgrowing area of research and implementation. We discuss security in Chapter 15. Protection and security require the system to be

Protection and security require the system to be able to distinguish among all its users. Most operating systems maintain a list of user names and associated user identiers (user IDs).I nW i n d o w sp a r l a n c e ,t h i si sa security ID(SID).T h e s en u m e r i c a l IDsa r eu n i q u e ,o n ep e ru s e r .W h e nau s e rl o g si n to the system, the authentication stage determines the appropriate user IDfor the user. That user IDis associated with all of the users processes and threads.

with all of the users processes and threads. When an IDneeds to be readable by a user, it is translated back to the user name via the user name list. In some circumstances, we wish to distinguish among sets of users rather than individual users. For example, the owner of a le on a UNIX system may be allowed to issue all operations on that le, whereas a selected set of users may be allowed only to read the le. To accomplish this, we need to dene a group name and the set of users belonging to that

group name and the set of users belonging to that group. Group functionality can be implemented as a systemwide list of group names and group identiers . Au s e rc a nb ei no n eo rm o r eg r o u p s ,d e p e n d i n go no p e r a t i n g  s y s t e md e s i g n decisions. The users group IDsa r ea l s oi n c l u d e di ne v e r ya s s o c i a t e dp r o c e s s and thread. In the course of normal system use, the user IDand group IDfor a user are sufcient. However, a user sometimes needs to

are sufcient. However, a user sometimes needs to escalate privileges to gain extra permissions for an activity. The user may need access to a device that is restricted, for example. Operating systems provide various methods to allow privilege escalation. On UNIX ,f o ri n s t a n c e ,t h e setuid attribute on a program causes that program to run with the user IDof the owner of the le, rather than the current users ID.T h ep r o c e s sr u n sw i t ht h i s effective UID until it turns off the

i t ht h i s effective UID until it turns off the extra privileges or terminates. 1.10 Kernel Data Structures We turn next to a topic central to operatingsystem implementation: the way data are structured in the system. In this section, we briey describe several fundamental data structures used extensively in operating systems. Readers who require further details on these structures, as well as others, should consult the bibliography at the end of the chapter. 1.10.1 Lists, Stacks, and Queues An

the chapter. 1.10.1 Lists, Stacks, and Queues An array is a simple data structure in which each element can be accessed directly. For example, main memory is constructed as an array. If the data item being stored is larger than one byte, then multiple bytes can be allocated to the item, and the item is addressed as item number item size. But what about storing an item whose size may vary? And what about removing an item if the relative positions of the remaining items must be preserved? In such

of the remaining items must be preserved? In such situations, arrays give way to other data structures. After arrays, lists are perhaps the most fundamental data structures in computer science. Whereas each item in an array can be accessed directly, the items in a list must be accessed in a particular order. That is, a listrepresents a collection of data values as a sequence. The most common method for32 Chapter 1 Introductiondata data data null   Figure 1.13 Singly linked list. implementing

Figure 1.13 Singly linked list. implementing this structure is a linked list ,i nw h i c hi t e m sa r el i n k e dt oo n e another. Linked lists are of several types: In a singly linked list, each item points to its successor, as illustrated in Figure 1.13. In a doubly linked list, ag i v e ni t e mc a nr e f e re i t h e rt oi t sp r e d e c e s s o ro r to its successor, as illustrated in Figure 1.14. In a circularly linked list, the last element in the list refers to the rst element,

element in the list refers to the rst element, rather than to null, as illustrated in Figure 1.15. Linked lists accommodate items of varying sizes and allow easy insertion and deletion of items. One potential disadvantage of using a list is that performance for retrieving a specied item in a list of size nis linear  O(n), as it requires potentially traversing all nelements in the worst case. Lists are sometimes used directly by kernel algorithms. Frequently, though, they are used for

algorithms. Frequently, though, they are used for constructing more powerful data structures, such as stacks and queues. Astack is a sequentially ordered data structure that uses the last in, rst out(LIFO )principle for adding and removing items, meaning that the last item placed onto a stack is the rst i tem removed. The operations for inserting and removing items from a stack are known as push and pop,r e s p e c t i v e l y .A n operating system often uses a stack when invoking function

system often uses a stack when invoking function calls. Parameters, local variables, and the return address are pushed onto the stack when a function is called; returning from the function call pops those items off the stack. Aqueue ,i nc o n t r a s t ,i sas e q u e n t i a l l yo r d e r e dd a t as t r u c t u r et h a tu s e st h e rst in, rst out (FIFO )principle: items are removed from a queue in the order in which they were inserted. There are many everyday examples of queues, including

are many everyday examples of queues, including shoppers waiting in a checkout line at a store and cars waiting in line at a trafc signal. Queues are also quite common in operating systemsjobs that are sent to a printer are typically printed in the order in which they were submitted, for example. As we shall see in Chapter 6, tasks that are waiting to be run on an available CPU are often organized in queues.data null null data data data  Figure 1.14 Doubly linked list.1.10 Kernel Data Structures

Doubly linked list.1.10 Kernel Data Structures 33data data data data   Figure 1.15 Circularly linked list. 1.10.2 Trees Atree is a data structure that can be used to represent data hierarchically. Data values in a tree structure are linked through parentchild relationships. In a general tree , a parent may have an unlimited number of children. In a binary tree,ap a r e n tm a yh a v ea tm o s tt w oc h i l d r e n ,w h i c hw et e r mt h e left child and the right child .Abinary search tree

child and the right child .Abinary search tree additionally requires an ordering between the parents two children in which lef t child right child .F i g u r e 1.16 provides an example of a binary search tree. When we search for an item in ab i n a r ys e a r c ht r e e ,t h ew o r s t  c a s ep e r f o r m a n c ei s O(n)( c o n s i d e rh o wt h i sc a n occur). To remedy this situation, we can use an algorithm to create a balanced binary search tree. Here, a tree containing nitems has at most

tree. Here, a tree containing nitems has at most lg nlevels, thus ensuring worstcase performance of O(lg n). We shall see in Section 6.7.1 that Linux uses a balanced binary search tree as part its CPUscheduling algorithm. 1.10.3 Hash Functions and Maps Ahash function takes data as its input, performs a numeric operation on this data, and returns a numeric value. This numeric value can then be used as an index into a table (typically an array) to quickly retrieve the data. Whereas searching for a

quickly retrieve the data. Whereas searching for a data item through a list of size ncan require up to O(n) comparisons in the worst case, using a hash function for retrieving data from table can be as good as O(1) in the worst case, depending on implementation details. Because of this performance, has h functions are used extensively in operating systems.17 35 40 4212 14 6 Figure 1.16 Binary search tree.34 Chapter 1 Introduction01 .. n valuehash maphashfunction(key) Figure 1.17 Hash map. One

maphashfunction(key) Figure 1.17 Hash map. One potential difculty with hash functions is that two inputs can result in the same output valuethat is, they can link to the same table location. We can accommodate this hash collision by having a linked list at that table location that contains all of the items with the same hash value. Of course, the more collisions there are, the less efcient the hash function is. One use of a hash function is to implement a hash map ,w h i c ha s s o c i a t e s

implement a hash map ,w h i c ha s s o c i a t e s (ormaps ) [key:value] pairs using a hash function. For example, we can map the key operating to the value system .O n c et h em a p p i n gi se s t a b l i s h e d ,w ec a n apply the hash function to the key to obtain the value from the hash map (Figure 1.17). For example, suppose that a user name is mapped to a password. Password authentication then proceeds as follows: a user enters his user name and password. The hash function is applied to

name and password. The hash function is applied to the user name, which is then used to retrieve the password. The retrieved password is then compared with the password entered by the user for authentication. 1.10.4 Bitmaps Abitmap is a string of nbinary digits that can be used to represent the status of nitems. For example, suppose we have several resources, and the availability of each resource is indicated by the value of a binary digit: 0 means that the resource is available, while 1

0 means that the resource is available, while 1 indicates that it is unavailable (or viceversa). The value of the ithposition in the bitmap is associated with the ithresource. As an example, consider the bitmap shown below: 001011101 Resources 2, 4, 5, 6, and 8 are unavailable; resources 0, 1, 3, and 7 are available. The power of bitmaps becomes apparen t when we consider their space efciency. If we were to use an eightbit Boolean value instead of a single bit, the resulting data structure would

a single bit, the resulting data structure would be eight times larger. Thus, bitmaps are commonly used when there is a need to represent the availability of a large number of resources. Disk drives provide a nice illustration. A mediumsized disk drive might be divided into s everal thousand individual units, called disk blocks .Ab i t m a pc a nb eu s e dt oi n d i c a t et h ea v a i l a b i l i t yo fe a c hd i s kb l o c k . Data structures are pervasive in operating system implementations.

are pervasive in operating system implementations. Thus, we will see the structures discussed here, along with others, throughout this text as we explore kernel algorithms and their implementations.1.11 Computing Environments 35 LINUX KERNEL DATA STRUCTURES The data structures used in the Linux kernel are available in the kernel source code. The include lelinuxlist.h provides details of the linkedlist data structure used throughout the kernel. A queue in Linux is known as akfifo ,a n di t si m p

queue in Linux is known as akfifo ,a n di t si m p l e m e n t a t i o nc a nb ef o u n di nt h e kfifo.c le in the kernel directory of the source code. Linux also provides a balanced binary search tree implementation using redblack trees .D e t a i l sc a nb ef o u n di nt h e include le linuxrbtree.h . 1.11 Computing Environments So far, we have briey described several aspects of computer systems and the operating systems that manage them. We turn now to a discussion of how operating systems

turn now to a discussion of how operating systems are used in a variety of computing environments. 1.11.1 Traditional Computing As computing has matured, the lines separating many of the traditional com puting environments have blurred. Consider the typical ofce environment.  Just a few years ago, this environment consisted of PCsc o n n e c t e dt oan e t w o r k , with servers providing le and print services. Remote access was awkward, and portability was achieved by use of laptop computers.

was achieved by use of laptop computers. Terminals attached to mainframes were prevalent at many c ompanies as well, with even fewer remote access and portability options. The current trend is toward providing more ways to access these computing environments. Web technologies and increasing WAN bandwidth are stretching the boundaries of traditional computing. Companies establish portals ,w h i c h provide Web accessibility to their internal servers. Network computers (or thin clients )which are

Network computers (or thin clients )which are essentially terminals that understand webbased computingare used in place of traditional workstations where more security or easier maintenance is desired. Mobile computers can synchronize with PCs to allow very portable use of company information. Mobile computers can also connect to wireless networks and cellular data networks to use the companys Web portal (as well as the myriad other Web resources). At home, most users once had a single computer

At home, most users once had a single computer with a slow modem connection to the ofce, the Internet, or both. Today, networkconnection speeds once available only at great c ost are relatively inexpensive in many places, giving home users more access to more data. These fast data connections are allowing home computers to serve up Web pages and to run networks that include printers, client PCs, and servers. Many homes use rewalls to protect their networks from security breaches. In the latter

networks from security breaches. In the latter half of the 20th century, computing resources were relatively scarce. (Before that, they were nonexistent!) For a period of time, systems were either batch or interactive. Batch systems processed jobs in bulk, with predetermined input from les or other data sources. Interactive systems waited for input from users. To optimize the use of the computing resources, multiple users shared time on these systems. Timesharing systems used a36 Chapter 1

systems. Timesharing systems used a36 Chapter 1 Introduction timer and scheduling algorithms to cycle processes rapidly through the CPU, giving each user a share of the resources. Today, traditional timesharing systems are uncommon. The same schedul ing technique is still in use on desktop computers, laptops, servers, and even mobile computers, but frequently all the processes are owned by the same user (or a single user and the operating system). User processes, and system processes that

system). User processes, and system processes that provide services to the user ,a r em a n a g e ds ot h a te a c hf r e q u e n t l y gets a slice of computer time. Consider the windows created while a user is working on a PC,f o re x a m p l e ,a n dt h ef a c tt h a tt h e ym a yb ep e r f o r m i n g different tasks at the same time. Even a web browser can be composed of multiple processes, one for each website currently being visited, with time sharing applied to each web browser process.

time sharing applied to each web browser process. 1.11.2 Mobile Computing Mobile computing refers to computing on handheld smartphones and tablet computers. These devices share the disting uishing physical features of being portable and lightweight. Historically, compared with desktop and laptop computers, mobile systems gave up screen size, memory capacity, and overall functionality in return for handheld mobile access to services such as email and web browsing. Over the past few years,

email and web browsing. Over the past few years, however, features on mobile devices have become so rich that the distinction in functionality between, say, a consumer laptop and a tablet computer may be difcult to discern. In fact, we might argue that the features of a contemporary mobile device allow it to provide functionality that is either unavailable or impractical on a desktop or laptop computer. Today, mobile systems are used not only for email and web browsing but also for playing music

email and web browsing but also for playing music and video, reading digital books, taking photos, and recording highdenition video. Accordingly, tremendous growth continues in the wide range of applications tha tr u no ns u c hd e v i c e s .M a n yd e v e l o p e r s are now designing applications that take advantage of the unique features of mobile devices, such as global positioning system ( GPS)c h i p s ,a c c e l e r o m e t e r s , and gyroscopes. An embedded GPSchip allows a mobile

gyroscopes. An embedded GPSchip allows a mobile device to use satellites to determine its precise location on earth. That functionality is especially useful in designing applications that provide navigationfor example, telling users which way to walk or drive or perhaps directing them to nearby services, such as restaurants. An accelerometer allows a mobile device to detect its orientation with respect to the ground and to detect certain other forces, such as tilting and shaking. In several

forces, such as tilting and shaking. In several computer games that employ accelerometers, players interface with the system not by using a mouse or a keyboard but rather by tilting, rotating, and shaking the mobile device! Perhaps more a practical use of these features is found in augmentedreality applications, which overlay information on a display of the current environment. It is difcult to imagine how equivalent applications could be developed on traditional laptop or desktop computer

on traditional laptop or desktop computer systems. To provide access to online services, mobile devices typically use either IEEE standard 802.11 wireless or cellular data networks. The memory capacity and processing speed of mobile devices , however, are more limited than those ofPCs. Whereas a smartphone or tablet may have 64 GBin storage, it is not uncommon to nd 1 TBin storage on a desktop computer. Similarly, because1.11 Computing Environments 37 power consumption is such a concern, mobile

37 power consumption is such a concern, mobile devices often use processors that are smaller, are slower, and offer fewer processing cores than processors found on traditional desktop and laptop computers. Two operating systems currently dominate mobile computing: Apple i OS and Google Android .iOSwas designed to run on Apple iPhone and iPad mobile devices. Android powers smartphones and tablet computers available from many manufacturers. We examine these two mobile operating systems in further

these two mobile operating systems in further detail in Chapter 2. 1.11.3 Distributed Systems Ad i s t r i b u t e ds y s t e mi sac o l l e c t i o no fp h y s i c a l l ys e p a r a t e ,p o s s i b l yh e t e r o g e  neous, computer systems that are networked to provide users with access to the various resources that the system maintains. Access to a shared resource increases computation speed, functionality, data availability, and reliability. Some operating systems generalize network

Some operating systems generalize network access as a form of le access, with the details of networking contained in the network interfaces device driver. Others make users specically invoke network functions. Generally, systems contain a mix of the two modesfor example FTP and NFS.T h ep r o t o c o l s that create a distributed system can greatly affect that systems utility and popularity. Anetwork ,i nt h es i m p l e s tt e r m s ,i sac o m m u n i c a t i o np a t hb e t w e e n two or more

u n i c a t i o np a t hb e t w e e n two or more systems. Distributed systems depend on networking for their functionality. Networks vary by the pro tocols used, the distances between nodes, and the transport media. TCPIP is the most common network protocol, and it provides the fundamental architecture of the Internet. Most operating systems support TCPIP ,i n c l u d i n ga l lg e n e r a l  p u r p o s eo n e s .S o m es y s t e m s support proprietary protocols to suit their needs. To an

proprietary protocols to suit their needs. To an operating system, a network protocol simply needs an in terface devicea network adapter, for examplewith a device driver to manage it, as well as software to handle data. These concepts are discussed throughout this book. Networks are characterized based on th ed i s t a n c e sb e t w e e nt h e i rn o d e s . Alocalarea network (LAN )connects computers within a room, a building, or a campus. A widearea network (WAN )usually links buildings,

A widearea network (WAN )usually links buildings, cities, or countries. A global company may have a WAN to connect its ofces worldwide, for example. These networks may run one protocol or several protocols. The continuing advent of new technologies b rings about new forms of networks. For example, a metropolitanarea network (MAN )could link buildings within ac i t y .B l u e T o o t ha n d8 0 2 . 1 1d e v i c e su s ew i r e l e s st e c h n o l o g yt oc o m m u n i c a t e over a distance of

l o g yt oc o m m u n i c a t e over a distance of several feet, in essence creating a personalarea network (PAN)between a phone and a headset or a smartphone and a desktop computer. The media to carry networks are equally varied. They include copper wires, ber strands, and wireless transmissions between satellites, microwave dishes, and radios. When computing devices are connected to cellular phones, they create a network. Even very shortrange infrared communication can be used for networking.

infrared communication can be used for networking. At a rudimentary level, whenever computers communicate, they use or create a network. These n etworks also vary in their performance and reliability. Some operating systems have taken the concept of networks and dis tributed systems further than the notion of providing network connectivity.38 Chapter 1 Introduction Anetwork operating system is an operating system that provides features such as le sharing across the network, along with a

as le sharing across the network, along with a communication scheme that allows different processes on different computers to exchange messages. A computer running a network operating system acts autonomously from all other computers on the network, although it is aware of the network and is able to communicate with other networked computers. A distributed operating system provides a less autonomous environment. The different computers communicate closely enough to provide the illusion that only

closely enough to provide the illusion that only a single operating system controls the network. We cover computer networks and distributed systems in Chapter 17. 1.11.4 ClientServer Computing AsPCsh a v eb e c o m ef a s t e r ,m o r ep o w e rful, and cheaper, designers have shifted away from centralized system architecture. Terminals connected to centralized systems are now being supplanted by PCsa n dm o b i l ed e v i c e s .C o r r e s p o n d  ingly, userinterface functionality once

s p o n d  ingly, userinterface functionality once handled directly by centralized systems is increasingly being handled by PCs, quite often through a web interface. As ar e s u l t ,m a n yo ft o d a y  ss y s t e m sa c ta s server systems to satisfy requests generated by client systems .T h i sf o r mo fs p e c i a l i z e dd i s t r i b u t e ds y s t e m ,c a l l e d aclientserver system, has the general structure depicted in Figure 1.18. Server systems can be broadly categorized as compute

systems can be broadly categorized as compute servers and le servers: The computeserver system provides an interface to which a client can send a request to perform an action (for example, read data). In response, the server executes the action and sen ds the results to the client. A server running a database that responds to client requests for data is an example of such a system. The leserver system provides a lesystem interface where clients can create, update, read, and delete les. An

can create, update, read, and delete les. An example of such a system is a web server that delivers les to clients running web browsers. Server Networkclient desktop client laptopclient smartphone Figure 1.18 General structure of a clientserver system.1.11 Computing Environments 39 1.11.5 PeertoPeer Computing Another structure for a distributed system is the peertopeer ( P2P)s y s t e m model. In this model, clients and servers are not distinguished from one another. Instead, all nodes within

from one another. Instead, all nodes within the system are considered peers, and each may act as either a client or a server, d epending on whether it is requesting or providing a service. Peertopeer systems offer an advantage over traditional clientserver systems. In a clientserver system, the server is a bottleneck; but in a peertopeer system, services can be provided by several nodes distributed throughout the network. To participate in a peertopeer system, a node must rst join the network of

system, a node must rst join the network of peers. Once a node has joined the netw ork, it can begin providing services toand requesting services fromother nodes in the network. Determining what services are available is accomplished in one of two general ways: When a node joins a network, it registers its service with a centralized lookup service on the network. Any node desiring a specic service rst contacts this centralized lookup service to determine which node provides the service. The

to determine which node provides the service. The remainder of the communication takes place between the client and the service provider. An alternative scheme uses no centralized lookup service. Instead, a peer acting as a client must discover what node provides a desired service by broadcasting a request for the service to all other nodes in the network. The node (or nodes) providing that service responds to the peer making the request. To support this approach, a discovery protocol must be

this approach, a discovery protocol must be provided that allows peers to discover servic es provided by other peers in the network. Figure 1.19 illustrates such a scenario. Peertopeer networks gained widespread popularity in the late 1990s with several lesharing services, such as Napster and Gnutella, that enabled peers to exchange les with one another. The Napster system used an approach similar to the rst type described ab ove: a centralized server maintained an index of all les stored on

server maintained an index of all les stored on peer nodes in the Napster network, and the actualclient client client client client Figure 1.19 Peertopeer system with no centralized service.40 Chapter 1 Introduction exchange of les took place between the peer nodes. The Gnutella system used a technique similar to the second type: a client broadcasted le requests to other nodes in the system, and nodes that could service the request responded directly to the client. The future of exchanging les

to the client. The future of exchanging les remains uncertain because peertopeer networks can be used to exchange copyrighted materials (music, for example) anonymously, and there are laws governing the distribution of copyrighted material. Notably, Napster ran into legal trouble for copyright infringement and its services were shut down in 2001. Skype is another example of peertopeer computing. It allows clients to make voice calls and video calls and to s end text messages over the Internet

calls and to s end text messages over the Internet using a technology known as voice over IP(VoIP ).S k y p eu s e sah y b r i dp e e r  topeer approach. It includes a centralized login server, but it also incorporates decentralized peers and allows t wo peers to communicate. 1.11.6 Virtualization Virtualization is a technology that allows operating systems to run as appli cations within other operating systems. At rst blush, there seems to be little reason for such functionality. But the

be little reason for such functionality. But the virtualization industry is vast and growing, which is a testament to its utility and importance. Broadly speaking, virtualization is one member of a class of software that also includes emulation. Emulation is used when the source CPU type is different from the target CPU type. For example, when Apple switched from the IBM Power CPU to the Intel x86 CPU for its desktop and laptop computers, it included an emulation facility called Rosetta, which

an emulation facility called Rosetta, which allowed applications compiled for the IBM CPU to run on the Intel CPU.T h a ts a m ec o n c e p tc a nb e extended to allow an entire operating system written for one platform to run on another. Emulation comes at a heavy price, however. Every machinelevel instruction that runs natively on the source system must be translated to the equivalent function on the target system, frequently resulting in several target instructions. If the source and target

target instructions. If the source and target CPUsh a v es i m i l a rp e r f o r m a n c el e v e l s ,t h e emulated code can run much slower than the native code. A common example of emulation occurs when a computer language is not compiled to native code but instead is either executed in its highlevel form or translated to an intermediate form. This is known as interpretation . Some languages, such as BASIC ,c a nb ee i t h e rc o m p i l e do ri n t e r p r e t e d .J a v a ,i n contrast,

do ri n t e r p r e t e d .J a v a ,i n contrast, is always interpreted. Interpr etation is a form of emulation in that the highlevel language code is translated to native CPU instructions, emulating not another CPU but a theoretical virtual machine on which that language could run natively. Thus, we can run Java programs on Java virtual machines, but technically those virtual machines are Java emulators. With virtualization ,i nc o n t r a s t ,a no p e r a t i n gs y s t e mt h a ti sn a t i v

no p e r a t i n gs y s t e mt h a ti sn a t i v e l yc o m  piled for a particular CPU architecture runs within another operating system also native to that CPU.V i r t u a l i z a t i o n r s tc a m ea b o u to n IBM mainframes as a method for multiple users to run tasks concurrently. Running multiple virtual machines allowed (and still allows) many users to run tasks on a system designed for a single user. Later, in response to problems with running multiple Microsoft Windows XPapplications

running multiple Microsoft Windows XPapplications on the Intel x86 CPU,VMware created a new virtualization technology in the form of an application that ran on XP. That application ran one or more guest copies of Windows or other native1.11 Computing Environments 41 (a)processes hardwarekernel(b)programming interfaceprocesses processesprocesses kernel kernel kernel VM2 VM1 VM3 managerhardwarevirtual machine Figure 1.20 VMware. x86 operating systems, each running its own applications. (See Figure

each running its own applications. (See Figure 1.20.) Windows was the host operating system, and the VMware application was the virtual machine manager VMM .T h e VMM runs the guest operating systems, manages their resource use, and protects each guest from the others. Even though modern operating systems are fully capable of running multiple applications reliably, the use of virtualization continues to grow. On laptops and desktops, a VMM allows the user to install multiple operating systems

the user to install multiple operating systems for exploration or to run applications written for operating systems other than the native host. For example, an Apple laptop running Mac OS Xon the x86 CPU can run a Windows guest to allow execution of Windows applications. Companies writing software for multiple operating systems can use virtualization to run all of those operating systems on a single physical server for development, testing, an dd e b u g g i n g .W i t h i nd a t ac e n t e r s

e b u g g i n g .W i t h i nd a t ac e n t e r s , virtualization has become a common method of executing and managing computing environments. VMM sl i k e VMware, ESX,a n dC i t r i xX e n S e r v e rn o longer run on host operating systems but rather arethe hosts. Full details of the features and implementation of virtualization are found in Chapter 16. 1.11.7 Cloud Computing Cloud computing is a type of computing that delivers computing, storage, and even applications as a service across a

and even applications as a service across a network. In some ways, its a logical extension of virtualization, because it uses virtualization as a base for its functionality. For example, the Amazon Elastic Compute Cloud (EC2)facility has thousands of servers, millions of virtual machines, and petabytes of storage available for use by anyone on the Internet. Users pay per month based on how much of those resources they use. There are actually many types of cloud computing, including the

many types of cloud computing, including the following: Public cloud a cloud available via the Internet to anyone willing to pay for the services42 Chapter 1 Introduction Private cloud a cloud run by a company for that companys own use Hybrid cloud a cloud that includes both public and private cloud components Software as a service (SaaS )one or more applications (such as word processors or spreadsheets) available via the Internet Platform as a service (PaaS )a software stack ready for

as a service (PaaS )a software stack ready for application use via the Internet (for example, a database server) Infrastructure as a service (IaaS )servers or storage available over the Internet (for example, storage available for making backup copies of production data) These cloudcomputing types are not discrete, as a cloud computing environ ment may provide a combination of several types. For example, an organization may provide both SaaS and IaaS as a publicly available service. Certainly,

IaaS as a publicly available service. Certainly, there are traditional operating systems within many of the types of cloud infrastructure. Beyond those are the VMM st h a tm a n a g et h e virtual machines in which the user pr ocesses run. At a higher level, the VMM s themselves are managed by cloud management tools, such as Vware vCloud Director and the opensource Eucalyptus toolset. These tools manage the resources within a given cloud and provide interfaces to the cloud components, making a

interfaces to the cloud components, making a good argument for considering them a new type of operating system. Figure 1.21 illustrates a public cloud providing IaaS. Notice that both the cloud services and the cloud user interface are protected by a rewall. firewallcloud customer interface load balancer virtual machinesvirtual machines servers serversstorageInternet customer requests cloud management commands cloud managment services Figure 1.21 Cloud computing.1.12 OpenSource Operating Systems

Cloud computing.1.12 OpenSource Operating Systems 43 1.11.8 RealTime Embedded Systems Embedded computers are the most prevalent form of computers in existence. These devices are found everywhere, f rom car engines and manufacturing robots to DVD sa n dm i c r o w a v eo v e n s .T h e yt e n dt oh a v ev e r ys p e c i  ct a s k s . The systems they run on are usually primitive, and so the operating systems provide limited features. Usually, they have little or no user interface, preferring to

have little or no user interface, preferring to spend their time monitoring and managing hardware devices, such as automobile engines and robotic arms. These embedded systems vary considerably. Some are generalpurpose computers, running standard operating systemssuch as Linuxwith specialpurpose applications to implement the functionality. Others are hard ware devices with a specialpurpose embedded operating system providing just the functionality desired. Yet others are hardware devices with

desired. Yet others are hardware devices with application specic integrated circuits ( ASIC s)t h a tp e r f o r mt h e i rt a s k sw i t h o u ta no p e r a t  ing system. The use of embedded systems continues to expand. The power of these devices, both as standalone units and as elements of networks and the web, is sure to increase as well. Even now, entire houses can be computerized, so that a central computereither a generalpurpose computer or an embedded systemcan control heating and

or an embedded systemcan control heating and lighting, alarm systems, and even coffee makers. Web access can enable a home owner to tell the house to heat up before she arrives home. Someday, the refrigerator can notify the grocery store when it notices the milk is gone. Embedded systems almost always run realtime operating systems .A realtime system is used when rigid time requirements have been placed on the operation of a processor or the ow of data; thus, it is often used as a control device

data; thus, it is often used as a control device in a dedicated application. Sensors bring data to the computer. The computer must analyze the data and possibly adjust controls to modify the sensor inputs. Systems that control scientic experiments, medical imaging systems, industrial control systems, and certain display systems are real time systems. Some automobileengine fuelinjection systems, homeappliance controllers, and weapon systems are also realtime systems. Ar e a l  t i m es y s t e mh

realtime systems. Ar e a l  t i m es y s t e mh a sw e l l  d e  n e d , x e dt i m ec o n s t r a i n t s .P r o c e s s i n g must be done within the dened constraints, or the system will fail. For instance, it would not do for a robot arm to be instructed to halt after it had smashed into the car it was building. A realtime system functions correctly only if it returns the correct result within its time constraints. Contrast this system with a timesharing system, where it is desirable (but

a timesharing system, where it is desirable (but not mandatory) to respond quickly, or a batch system, which may have no time constraints at all. In Chapter 6, we consider the scheduling facility needed to implement realtime functionality in an operating system. In Chapter 9, we describe the design of memory management for realtime computing. Finally, in Chapters 18 and 19, we describe the realtime components of the Linux and Windows 7 operating systems. 1.12 OpenSource Operating Systems We

systems. 1.12 OpenSource Operating Systems We noted at the beginning of this chapter that the study of operating systems has been made easier by the availability of a vast number of opensource44 Chapter 1 Introduction releases. Opensource operating systems are those available in sourcecode format rather than as compiled binary code. Linux is the most famous open source operating system, while Microsoft Windows is a wellknown example of the opposite closedsource approach. Apples Mac OS X and i

closedsource approach. Apples Mac OS X and i OSoperating systems comprise a hybrid approach. They contain an opensource kernel named Darwin yet include proprietary, closedsource components as well. Starting with the source code allows the programmer to produce binary code that can be executed on a system. Doing the opposite reverse engi neering the source code from the binariesis quite a lot of work, and useful items such as comments are never recovered. Learning operating systems by examining

recovered. Learning operating systems by examining the source code has other benets as well. With the source code in hand, a student can modify the operating system and then compile and run the code to try out those changes, which is an excellent learning tool. This text includes projects that involve modifying operatingsystem source code, while also describing algorithms at ah i g hl e v e lt ob es u r ea l li m p o r t a n t operatingsystem topics are covered. Throughout the text, we provide

are covered. Throughout the text, we provide pointers to examples of opensource code for deeper study. There are many benets to opensource operating systems, including a community of interested (and usually unpaid) programmers who contribute to the code by helping to debug it, analyze it, provide support, and suggest changes. Arguably, opensource code is more secure than closedsource code because many more eyes are viewing the code. Certainly, opensource code has bugs, but opensource advocates

opensource code has bugs, but opensource advocates argue that bugs tend to be found and xed faster owing to the number of people using and viewing the code. Companies that earn revenue from selling their programs often hesitate to opensource their code, but Red Hat and a myriad of other companies are doing just that and showing that commercial companies benet, rather than suffer, when they opensource their code. Revenue can b eg e n e r a t e dt h r o u g hs u p p o r tc o n t r a c t s and the

h r o u g hs u p p o r tc o n t r a c t s and the sale of hardware on which the software runs, for example. 1.12.1 History In the early days of modern computing (that is, the 1950s), a great deal of software was available in opensource format. The original hackers (computer enthusiasts) at MITs Tech Model Railroad Club left their programs in drawers for others to work on. Homebrew user groups exchanged code during their meetings. Later, companyspecic user groups, such as Digital Equipment

user groups, such as Digital Equipment Corporations DEC, accepted contributions of sourcecode programs, collected them onto tapes, and distributed the tapes to interested members. Computer and software companies eventually sought to limit the use of their software to authorized computers and paying customers. Releasing only the binary les compiled from the source code, rather than the source code itself, helped them to achieve this goal, as well as protecting their code and their ideas from

well as protecting their code and their ideas from their competitors. Another issue involved copyrighted material. Operating systems and other programs can limit the ability to play back movies and music or display electronic books to authorized computers. Such copy protection ordigital rights management (DRM )would not be effective if the source code that implemented these limits were published. Laws in many countries, including the U.S. Digital Millennium Copyright Act ( DMCA ), make it

Digital Millennium Copyright Act ( DMCA ), make it illegal to reverseengineer DRM code or otherwise try to circumvent copy protection.1.12 OpenSource Operating Systems 45 To counter the move to limit software use and redistribution, Richard Stallman in 1983 started the GNU project to create a free, opensource, UNIX  compatible operating system. In 1985, he published the GNU Manifesto, which argues that all software should be free and opensourced. He also formed theFree Software Foundation

He also formed theFree Software Foundation (FSF)with the goal of encouraging the free exchange of software source code and the free use of that software. Rather than copyright its software, the FSFcopylefts the software to encourage sharing and improvement. The GNU General Public License (GPL)codies copylefting and is a common license under which free software is released. Fundamentally, GPL requires that the source code be distributed with any binaries and that any changes made to the source

binaries and that any changes made to the source code be released under the same GPL license. 1.12.2 Linux As an example of an opensource operating system, consider GNU Linux . The GNU project produced many UNIX compatible tools, including compilers, editors, and utilities, but never released a kernel. In 1991, a student in Finland, Linus Torvalds, released a rudimentary UNIX like kernel using the GNU compilers and tools and invited contributions worldwide. The advent of the Internet meant that

worldwide. The advent of the Internet meant that anyone interested could download the source code, modify it, and submit changes to Torvalds. Releasing updates once a week allowed this socalled Linux operating system to grow rapidly, enhanced by several thousand programmers. The resulting GNU Linux operating system has spawned hundreds of unique distributions ,o rc u s t o mb u i l d s ,o ft h es y s t e m .M a j o rd i s t r i b u t i o n s include RedHat, SUSE ,F e d o r a ,D e b i a n ,S l a

RedHat, SUSE ,F e d o r a ,D e b i a n ,S l a c k w a r e ,a n dU b u n t u .D i s t r i b u t i o n s vary in function, utility, installed applications, hardware support, user inter face, and purpose. For example, RedHat Enterprise Linux is geared to large commercial use. PCLinux OSis a LiveCD an operating system that can be booted and run from a CDROM without being installed on a systems hard disk. One variant of PCLinux OScalled PCLinux OSSupergamer DVD is a LiveDVD that includes graphics

DVD is a LiveDVD that includes graphics drivers and games. A gamer can run it on any compatible system simply by booting from the DVD .W h e nt h eg a m e ri s nished, a reboot of the system resets it to its installed operating system. You can run Linux on a Windows system using the following simple, free approach: 1.Download the free VMware Player tool from http:www.vmware.comdownloadplayer and install it on your system. 2.Choose a Linux version from among the hundreds of appliances, or virtual

from among the hundreds of appliances, or virtual machine images, available from VMware at http:www.vmware.comappliances These images are preinstalled with operating systems and applications and include many avors of Linux.46 Chapter 1 Introduction 3.Boot the virtual machine within VMware Player. With this text, we provide a virtual machine image of Linux running the Debian release. This image contains the Linux source code as well as tools for software development. We cover examples involving

software development. We cover examples involving tha tL i n u xi m a g et h r o u g h o u tt h i s text, as well as in a detailed case study in Chapter 18. 1.12.3 BSD UNIX BSD UNIX has a longer and more complicated history than Linux. It started in 1978 as a derivative of ATT sUNIX .R e l e a s e sf r o mt h eU n i v e r s i t yo fC a l i f o r n i a at Berkeley ( UCB)c a m ei ns o u r c ea n db i n a r yf o r m ,b u tt h e yw e r en o to p e n  source because a license from ATT was required.

n  source because a license from ATT was required. BSD UNIX s development was slowed by a lawsuit by ATT , but eventually a fully functional, opensource version, 4.4BSDlite, was released in 1994. Just as with Linux, there are many distributions of BSD UNIX ,i n c l u d i n g FreeBSD,NetBSD,Open BSD,a n dD r a g o n  y BSD.T oe x p l o r et h es o u r c ec o d e ofFreeBSD,s i m p l yd o w n l o a dt h ev i r t u a lm a c h i n ei m a g eo ft h ev e r s i o no f interest and boot it within VMware,

e r s i o no f interest and boot it within VMware, as described above for Linux. The source code comes with the distribution and is stored in usrsrc .T h ek e r n e l source code is in usrsrcsys .F o re x a m p l e ,t oe x a m i n et h ev i r t u a lm e m o r y implementation code in the FreeBSD kernel, see the les in usrsrcsysvm . Darwin, the core kernel component of Mac OS X ,i sb a s e do n BSD UNIX and is opensourced as well. That source code is available from http:www.opensource.apple.com

is available from http:www.opensource.apple.com .E v e r yM a c OS X release has its open source components posted at that site. The name of the package that contains the kernel begins with xnu. Apple also provides extensive developer tools, documentation, and support at http:connect.apple.com .F o rm o r ei n f o r m a  tion, see Appendix A. 1.12.4 Solaris Solaris is the commercial UNIX based operating system of Sun Microsystems. Originally, Suns SunOS operating system was based on BSD UNIX .S

SunOS operating system was based on BSD UNIX .S u nm o v e d toATT s System V UNIX as its base in 1991. In 2005, Sun opensourced most of the Solaris code as the OpenSolaris project. The purchase of Sun by Oracle in 2009, however, left the state of this project unclear. The source code as it was in 2005 is still available via a source code browser and for download at http:src.opensolaris.orgsource . Several groups interested in using OpenSolaris have started from that base and expanded its

have started from that base and expanded its features. Their working set is Project Illumos, which has expanded from the OpenSolaris base to include more features and to be the basis for several products. Illumos is available at http:wiki.illumos.org . 1.12.5 OpenSource Systems as Learning Tools The free software movement is driving legions of programmers to create thousands of opensource projects, including operating systems. Sites like http:freshmeat.net and http:distrowatch.com provide

and http:distrowatch.com provide portals to many of these projects. As we stated earlier, open source projects enable students to use source code as a learning tool. They can modify programs and test them,1.13 Summary 47 help nd and x bugs, and otherwise explore mature, fullfeatured operating systems, compilers, tools, user interfaces, and other types of programs. The availability of source code for historic projects, such as Multics, can help students to understand those projects and to build

students to understand those projects and to build knowledge that will help in the implementation of new projects. GNU Linux and BSD UNIX are all opensource operating systems, but each has its own goals, utility, licensing, and purpose. Sometimes, licenses are not mutually exclusive and crosspollination occurs, allowing rapid improvements in operatingsystem projects. For example, several major components of OpenSolaris have been ported to BSD UNIX . The advantages of free software and open

UNIX . The advantages of free software and open sourcing are likely to increase the number and quality of opensource projects, leading to an increase in the number of individuals and companies that use these projects. 1.13 Summary An operating system is software that manages the computer hardware, as well as providing an environment for application programs to run. Perhaps the most visible aspect of an operating system is the interface to the computer system it provides to the human user. For a

system it provides to the human user. For a computer to do its job of executing programs, the programs must be in main memory. Main memory is the only large storage area that the processor can access directly. It is an array of bytes, ranging in size from millions to billions. Each byte in memory has its own address. The main memory is usually a volatile storage device that loses its contents when power is turned off or lost. Most computer systems provide secondary storage as an extension of

provide secondary storage as an extension of main memory. Secondary storage provides a form of nonvolatile storage that is capable of holding large quantities of data permanently. The most common secondarystorage device is a magnetic disk, which provides storage of both programs and data. The wide variety of storage systems in a computer system can be organized in a hierarchy according to speed and cost. The higher levels are expensive, but they are fast. As we move down the hierarchy, the cost

are fast. As we move down the hierarchy, the cost per bit generally decreases, whereas the access time generally increases. There are several different strategies for designing a computer system. Singleprocessor systems have only one processor, while multiprocessor systems contain two or more processors that share physical memory and peripheral devices. The most common multiprocessor design is symmetric multiprocessing (or SMP), where all processors are considered peers and run independently of

are considered peers and run independently of one another. Clustered systems are a specialized form of multiprocessor systems and consist of multiple computer systems connected by a localarea network. To best utilize the CPU,m o d e r no p e r a t i n gs y s t e m se m p l o ym u l t i p r o g r a m  ming, which allows several jobs to be in memory at the same time, thus ensuring that the CPU always has a job to execute. Timesharing systems are an exten sion of multiprogramming wherein CPU

are an exten sion of multiprogramming wherein CPU scheduling algorithms rapidly switch between jobs, thus providing the illusion that each job is running concurrently. The operating system must ensure correct operation of the computer system. To prevent user programs from interfering with the proper operation of48 Chapter 1 Introduction THE STUDY OF OPERATING SYSTEMS There has never been a more interesting time to study operating systems, and it has never been easier. The opensource movement has

has never been easier. The opensource movement has overtaken operating systems, causing many of them to be made available in both source and binary (executable) format. The list of operating systems available in both formats includes Linux, BSD UNIX ,S o l a r i s ,a n dp a r to fM a c OS X .T h ea v a i l a b i l i t y of source code allows us to study operating systems from the inside out. Questions that we could once answer only by looking at documentation or the behavior of an operating

at documentation or the behavior of an operating system we can now answer by examining the code itself. Operating systems that are no longer commercially viable have been opensourced as well, enabling us to study how systems operated in a time of fewer CPU ,m e m o r y ,a n ds t o r a g er e s o u r c e s .A ne x t e n s i v eb u t incomplete list of opensource operatingsystem projects is available from http:dmoz.orgComputersSoftwareOperating SystemsOpen Source . In addition, the rise of

SystemsOpen Source . In addition, the rise of virtualization as a mainstream (and frequently free) computer function makes it possible to run many operating systems on top of one core system. For example, VMware ( http:www.vmware.com )p r o v i d e s af r e e player for Windows on which hundreds of free virtual appliances  can run. Virtualbox ( http:www.virtualbox.com )p r o v i d e saf r e e ,o p e n  source virtual machine manager on many operating systems. Using such tools, students can try

systems. Using such tools, students can try out hundreds of operating systems without dedicated hardware. In some cases, simulators of specic hardware are also available, allowing the operating system to run on native hardware, all within the connes of a modern computer and modern operating system. For example, a DECSYSTEM20 simulator running on Mac OS X can boot TOPS20 ,l o a dt h e source tapes, and modify and compile a new TOPS20 kernel. An interested student can search the Internet to nd the

student can search the Internet to nd the original papers that describe the operating system, as well as the original manuals. The advent of opensource operating systems has also made it easier to make the move from student to operatingsystem developer. With some knowledge, some effort, and an Internet connection, a student can even create a new operatingsystem distribution. Just a few years ago, it was difcult or impossible to get access to source code. Now, such access is limited only by how

code. Now, such access is limited only by how much interest, time, and disk space a student has. the system, the hardware has two modes: user mode and kernel mode. Various instructions (such as IOinstructions and halt instructions) are privileged and can be executed only in kernel mode. The memory in which the operating system resides must also be protected from modication by the user. A timer prevents innite loops. These facilities (dual mode, privileged instructions, memory protection, and

privileged instructions, memory protection, and timer interrupt) are basic building blocks used by operating systems to achieve correct operation. Ap r o c e s s( o rj o b )i st h ef u n d a m e n t a lu n i to fw o r ki na no p e r a t i n gs y s t e m . Process management includes creating and deleting processes and providing mechanisms for processes to communicate and synchronize with each other.Practice Exercises 49 An operating system manages memory by keeping track of what parts of memory

memory by keeping track of what parts of memory are being used and by whom. The operating system is also responsible for dynamically allocating and freeing memory space. Storage space is also managed by the operating system; this includes providing le systems for representing les and directories and managing space on massstorage devices. Operating systems must also be concerned with protecting and securing the operating system and users. Protection measures control the access of processes or

measures control the access of processes or users to the resources made available by the computer system. Security measures are responsible for defending a computer system from external or internal attacks. Several data structures that are fundamental to computer science are widely used in operating systems, including lists, stacks, queues, trees, hash functions, maps, and bitmaps. Computing takes place in a variety of environments. Traditional computing involves desktop and laptop PCs, usually

computing involves desktop and laptop PCs, usually connected to a computer network. Mobile computing refers to computing on handheld smartphones and tablet computers, which offer several unique features. Distributed systems allow users to share resources on geographically dispersed hosts connected via ac o m p u t e rn e t w o r k .S e r v i c e sm a yb ep rovided through either the client server model or the peertopeer model. Virtualization involves abstracting ac o m p u t e r  sh a r d w a r

abstracting ac o m p u t e r  sh a r d w a r ei n t os e v e r a ld i f f e r e n te x e c u t i o ne n v i r o n m e n t s .C l o u d computing uses a distributed system to abstract services into a cloud, where users may access the services from remote locations. Realtime operating systems are designed for embedded environments, such as consumer devices, automobiles, and robotics. The free software movement has created thousands of opensource projects, including operating systems. Because of

projects, including operating systems. Because of these projects, students are able to use source code as a learning tool. They can modify programs and test them, help nd and x bugs, and otherwise explore mature, fullfeatured operating systems, compilers, tools, user interfaces, and other types of programs. GNU Linux and BSD UNIX are opensource operating systems. The advan tages of free software and open sourcing are likely to increase the number and quality of opensource projects, leading to an

and quality of opensource projects, leading to an increase in the number of individuals and companies that use these projects. Practice Exercises 1.1 What are the three main purposes of an operating system? 1.2 We have stressed the need for an operating system to make efcient use of the computing hardware. When is it appropriate for the operating system to forsake this principle and to waste resources? Why is such as y s t e mn o tr e a l l yw a s t e f u l ? 1.3 What is the main difculty that a

s t e f u l ? 1.3 What is the main difculty that a programmer must overcome in writing an operating system for a realtime environment? 1.4 Keeping in mind the various denitions of operating system, consider whether the operating system should include applications such as web browsers and mail programs. Argue both that it should and that it should not, and support your answers.50 Chapter 1 Introduction 1.5 How does the distinction between kernel mode and user mode function as a rudimentary form

mode and user mode function as a rudimentary form of protection (security) system? 1.6 Which of the following instructions should be privileged? a. Set value of timer. b. Read the clock. c. Clear memory. d. Issue a trap instruction. e. Turn off interrupts. f. Modify entries in devicestatus table. g. Switch from user to kernel mode. h. Access IOdevice. 1.7 Some early computers protected the operating system by placing it in am e m o r yp a r t i t i o nt h a tc o u l dn o tb em o d i  e db ye i t

i o nt h a tc o u l dn o tb em o d i  e db ye i t h e rt h eu s e rj o b or the operating system itself. Describe two difculties that you think could arise with such a scheme. 1.8 Some CPUsp r o v i d ef o rm o r et h a nt w om o d e so fo p e r a t i o n .W h a ta r e two possible uses of these multiple modes? 1.9 Timers could be used to compute the current time. Provide a short description of how this could be accomplished. 1.10 Give two reasons why caches are useful. What problems do they

why caches are useful. What problems do they solve? What problems do they cause? If a cache can be made as large as the device for which it is caching (for instance, a cache as large as a disk), why not make it that large and eliminate the device? 1.11 Distinguish between the client server and peertopeer models of distributed systems. Exercises 1.12 In a multiprogramming and timesharing environment, several users share the system simultaneously. This situation can result in various security

This situation can result in various security problems. a. What are two such problems? b. Can we ensure the same degree of security in a timeshared machine as in a dedicated machine? Explain your answer. 1.13 The issue of resource utilization shows up in different forms in different types of operating systems. List what resources must be managed carefully in the following settings: a. Mainframe or minicomputer systems b. Workstations connected to servers c. Mobile computersExercises 51 1.14

to servers c. Mobile computersExercises 51 1.14 Under what circumstances would a user be better off using a time sharing system than a PCor a singleuser workstation? 1.15 Describe the differences between symmetric and asymmetric multipro cessing. What are three advantages and one disadvantage of multipro cessor systems? 1.16 How do clustered systems differ from multiprocessor systems? What is required for two machines belonging to a cluster to cooperate to provide ah i g h l ya v a i l a b l es

to provide ah i g h l ya v a i l a b l es e r v i c e ? 1.17 Consider a computing cluster consisting of two nodes running a database. Describe two ways in which the cluster software can manage access to the data on the disk. Discuss the benets and disadvantages of each. 1.18 How are network computers different from traditional personal com puters? Describe some usage scenarios in which it is advantageous to use network computers. 1.19 What is the purpose of interrupts? How does an interrupt

the purpose of interrupts? How does an interrupt differ from a trap? Can traps be generated intentionally by a user program? If so, for what purpose? 1.20 Direct memory access is used for highspeed IO devices in order to avoid increasing the CPUs execution load. a. How does the CPU interface with the device to coordinate the transfer? b. How does the CPU know when the memory operations are com plete? c. The CPU is allowed to execute other programs while the DMA controller is transferring data.

while the DMA controller is transferring data. Does this process interfere with the execution of the user programs? If so, describe what forms of interference are caused. 1.21 Some computer systems do not provide a privileged mode of operation in hardware. Is it possible to construct a secure operating system for these computer systems? Give arguments both that it is and that it is not possible. 1.22 Many SMP systems have different levels of caches; one level is local to each processing core,

one level is local to each processing core, and another level is shared among all processing cores. Why are caching systems designed this way? 1.23 Consider an SMP system similar to the one shown in Figure 1.6. Illustrate with an example how data residing in memory could in fact have a different value in each of the local caches. 1.24 Discuss, with examples, how the problem of maintaining coherence of cached data manifests itself in the followin gp r o c e s s i n ge n v i r o n m e n t s : a.

gp r o c e s s i n ge n v i r o n m e n t s : a. Singleprocessor systems b. Multiprocessor systems c. Distributed systems52 Chapter 1 Introduction 1.25 Describe a mechanism for enforcing memory protection in order to prevent a program from modifying the memory associated with other programs. 1.26 Which network conguration LAN orWAN would best suit the following environments? a. A campus student union b. Several campus locations across a statewide university system c. A neighborhood 1.27 Describe

university system c. A neighborhood 1.27 Describe some of the challenges of designing operating systems for mobile devices compared with designing operating systems for tradi tional PCs. 1.28 What are some advantages of peertopeer systems over clientserver systems? 1.29 Describe some distributed applications that would be appropriate for a peertopeer system. 1.30 Identify several advantages and several disadvantages of opensource operating systems. Include the types of people who would nd each

Include the types of people who would nd each aspect to be an advantage or a disadvantage. Bibliographical Notes [Brookshear (2012)] provides an overview of computer science in general. Thorough coverage of data structures can be found in [Cormen et al. (2009)]. [Russinovich and Solomon (2009)] give an overview of Microsoft Windows and covers considerable technical detail about the system internals and components. [McDougall and Mauro (2007)] cover the internals of the Solaris operating system.

the internals of the Solaris operating system. Mac OS X internals are discussed in [Singh (2007)]. [Love (2010)] provides an overview of the Linux operating system and great detail about data structures used in the Linux kernel. Many general textbooks cover operating systems, including [Stallings (2011)], [Deitel et al. (2004)], and [Tanenbaum (2007)]. [Kurose and Ross (2013)] provides a general overview of computer networks, including a discussion of clientserver and peertopeer systems.

discussion of clientserver and peertopeer systems. [Tarkoma and Lagerspetz (2011)] examines several different mobile operating systems, including Android and iOS. [Hennessy and Patterson (2012)] provide coverage of IOsystems and buses and of system architecture in general. [Bryant and OHallaron (2010)] provide at h o r o u g ho v e r v i e wo fac o m p u t e rs y s t e mf r o mt h ep e r s p e c t i v eo fac o m p u t e r programmer. Details of the Intel 64 instruction set and privilege modes

the Intel 64 instruction set and privilege modes can be found in [Intel (2011)]. The history of open sourcing and its benets and challenges appears in [Raymond (1999)]. The Free Software Foundation has published its philosophy inhttp:www.gnu.orgphilosophyfreesoftwareforfreedom.html .T h eo p e n source of Mac OS X are available from http:www.apple.comopensource .Bibliography 53 Wikipedia has an informative entry about the contributions of Richard Stallman at http:en.wikipedia.orgwikiRichard

Stallman at http:en.wikipedia.orgwikiRichard Stallman . The source code of Multics is available at http:web.mit.edumulticshistory sourceMultics Internet ServerMultics sources.html . Bibliography [Brookshear (2012)] J. G. Brookshear, Computer Science: An Overview, Eleventh Edition, AddisonWesley (2012). [Bryant and OHallaron (2010)] R. Bryant and D. OHallaron, Computer Systems: AP r o g r a m m e r sP e r s p e c t i v e , Second Edition, AddisonWesley (2010). [Cormen et al. (2009)] T. H. Cormen,

(2010). [Cormen et al. (2009)] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms, Third Edition, MIT Press (2009). [Deitel et al. (2004)] H. Deitel, P . Deitel, and D. Choffnes, Operating Systems, Third Edition, Prentice Hall (2004). [Hennessy and Patterson (2012)] J. Hennessy and D. Patterson, Computer Archi tecture: A Quantitative Approach, Fifth Edition, Morgan Kaufmann (2012). [Intel (2011)] Intel 64 and IA32 Architectures Software Developers Manual, Com

IA32 Architectures Software Developers Manual, Com bined Volumes: 1, 2A, 2B, 3A and 3B .I n t e lC o r p o r a t i o n( 2 0 1 1 ) . [Kurose and Ross (2013)] J. Kurose and K. Ross, Computer NetworkingA Top Down Approach, Sixth Edition, AddisonWesley (2013). [Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developers Library (2010). [McDougall and Mauro (2007)] R. McDougall and J. Mauro, Solaris Internals, Second Edition, Prentice Hall (2007). [Raymond (1999)] E. S. Raymond, The

Hall (2007). [Raymond (1999)] E. S. Raymond, The Cathedral and the Bazaar ,O  R e i l l y Associates (1999). [Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon, Win dows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition, Microsoft Press (2009). [Singh (2007)] A. Singh, Mac OS X Internals: A Systems Approach ,A d d i s o n  Wesley (2007). [Stallings (2011)] W. Stallings, Operating Systems, Seventh Edition, Prentice Hall (2011). [Tanenbaum (2007)] A. S.

Prentice Hall (2011). [Tanenbaum (2007)] A. S. Tanenbaum, Modern Operating Systems, Third Edition, Prentice Hall (2007). [Tarkoma and Lagerspetz (2011)] S. Tarkoma and E. Lagerspetz, Arching over the Mobile Computing Chasm: Platforms and Runtimes ,IEEE Computer , Volume 44, (2011), pages 2228.2CHAPTER Operating System Structures An operating system provides the environment within which programs are executed. Internally, operating systems vary greatly in their makeup, since they are organized

greatly in their makeup, since they are organized along many differ ent lines. The design of a new operating system is a major task. It is important that the goals of the system be well dened before the design begins. These goals form the basis for choices among various algorithms and strategies. We can view an operating system from several vantage points. One view focuses on the services that the system provides; another, on the interface that it makes available to users and programmers; a

it makes available to users and programmers; a third, on its components and their interconnections. In this chapter, we explore all three aspects of operating systems, showing the viewpoints of users, programmers, and operating system designers. We consider what services an operating system provides, how they are provided, how they are debugged, and what the various methodologies are for designing such systems. Finally, we describe how operating systems are created and how a computer starts its

systems are created and how a computer starts its operating system. CHAPTER OBJECTIVES To describe the services an operating system provides to users, processes, and other systems. To discuss the various ways of structuring an operating system. To explain how operating systems are installed and customized and how they boot. 2.1 OperatingSystem Services An operating system provides an environment for the execution of programs. It provides certain services to programs and to the users of those

services to programs and to the users of those programs. The specic services provided, of course, differ from one operating system to another, but we can identify common classes. These operating system services are provided for the convenience of the programmer, to make the programming 5556 Chapter 2 OperatingSystem Structuresuser and other system programs services operating system hardwaresystem callsGUI batch user interfacescommand line program executionIO operationsfile

line program executionIO operationsfile systemscommunicationresource allocationaccounting protection and securityerror detection Figure 2.1 Av i e wo fo p e r a t i n gs y s t e ms e r v i c e s . task easier. Figure 2.1 shows one view of the various operatingsystem services and how they interrelate. One set of operating system services provides functions that are helpful to the user. User interface .A l m o s ta l lo p e r a t i n gs y s t e m sh a v ea user interface (UI). This interface can

sh a v ea user interface (UI). This interface can take several forms. One is a commandline interface (CLI),w h i c hu s e st e x tc o m m a n d sa n dam e t h o df o re n t e r i n gt h e m( s a y , ak e y b o a r df o rt y p i n gi nc o m m a n d si nas p e c i  cf o r m a tw i t hs p e c i  c options). Another is a batch interface ,i nw h i c hc o m m a n d sa n dd i r e c t i v e s to control those commands are entered into les, and those les are executed. Most commonly, a graphical user

les are executed. Most commonly, a graphical user interface (GUI)is used. Here, the interface is a window system with a pointing device to direct IO, choose from menus, and make selections and a keyboard to enter text. Some systems provide two or all three of these variations. Program execution .T h es y s t e mm u s tb ea b l et ol o a dap r o g r a mi n t o memory and to run that program. The program must be able to end its execution, either normally or abnormally (indicating error).

either normally or abnormally (indicating error). IOoperations .Ar u n n i n gp r o g r a mm a yr e q u i r e IO,w h i c hm a yi n v o l v ea le or an IOdevice. For specic devices, spec ial functions may be desired (such as recording to a CDorDVD drive or blanking a display screen). For efciency and protection, users usually cannot control IOdevices directly. Therefore, the operating system must provide a means to do IO. Filesystem manipulation .T h e l es y s t e mi so fp a r t i c u l a ri n t

h e l es y s t e mi so fp a r t i c u l a ri n t e r e s t .O b v i  ously, programs need to read and write les and directories. They also need to create and delete them by name, search for a given le, and list le information. Finally, some operating systems include permissions management to allow or deny access to les or directories based on le ownership. Many operating systems provide a variety of le systems, sometimes to allow personal choice and sometimes to provide specic features or

choice and sometimes to provide specic features or performance characteristics.2.1 OperatingSystem Services 57 Communications .T h e r ea r em a n yc i r c u m s t a n c e si nw h i c ho n ep r o c e s s needs to exchange information with another process. Such communication may occur between processes that are executing on the same computer or between processes that are executing on different computer systems tied together by a computer network. Communications may be implemented viashared memory

Communications may be implemented viashared memory ,i nw h i c ht w oo rm o r ep r o c e s s e sr e a da n dw r i t et o as h a r e ds e c t i o no fm e m o r y ,o r message passing ,i nw h i c hp a c k e t so f information in predened formats are moved between processes by the operating system. Error detection .T h eo p e r a t i n gs y s t e mn e e d st ob ed e t e c t i n ga n dc o r r e c t i n g errors constantly. Errors may occur in the CPU and memory hardware (such as a memory error or a

and memory hardware (such as a memory error or a power failure), in IOdevices (such as a parity error on disk, a connection failure on a network, or lack of paper in the printer), and in the user program (such as an arithmetic overow, an attempt to access an illegal memory location, or a toogreat use of CPU time). For each type of error, the operating system should take the appropriate action to ensure correct and consistent computing. Sometimes, it has no choice but to halt the system. At other

it has no choice but to halt the system. At other times, it might terminate an errorcausing process or return an error code to a process for the process to detect and possibly correct. Another set of operating system functions exists not for helping the user but rather for ensuring the efcient operation of the system itself. Systems with multiple users can gain efciency by sharing the computer resources among the users. Resource allocation .W h e nt h e r ea r em u l t i p l eu s e r so rm u l t

nt h e r ea r em u l t i p l eu s e r so rm u l t i p l ej o b s running at the same time, resources must be allocated to each of them. The operating system manages many different types of resources. Some (such asCPU cycles, main memory, and le storage) may have special allocation code, whereas others (such as IOdevices) may have much more general request and release code. For instance, in determining how best to use the CPU,o p e r a t i n gs y s t e m sh a v e CPUscheduling routines that take

s t e m sh a v e CPUscheduling routines that take into account the speed of the CPU,t h ej o b st h a tm u s tb ee x e c u t e d ,t h en u m b e ro f registers available, and other factors. There may also be routines to allocate printers, USB storage drives, and other peripheral devices. Accounting .W ew a n tt ok e e pt r a c ko fw h i c hu s e r su s eh o wm u c ha n d what kinds of computer resources. This record keeping may be used for accounting (so that users can be billed) or simply for

(so that users can be billed) or simply for accumulating usage statistics. Usage statistics may be a valuable tool for researchers who wish to recongure the system to improve computing services. Protection and security . The owners of information stored in a multiuser or networked computer system may want to control use of that information. When several separate processes execute concurrently, it should not be possible for one process to interfere with the others or with the operating system

with the others or with the operating system itself. Protection involves ensuring that all access to system resources is controlled. Security of the system from outsiders is also important. Such security starts with requiring each user to authenticate58 Chapter 2 OperatingSystem Structures himself or herself to the system, usually by means of a password, to gain access to system resources. It extends to defending external IOdevices, including network adapters, from invalid access attempts and to

adapters, from invalid access attempts and to recording all such connections for detection of breakins. If a system is to be protected and secure, precautions must be instituted throughout it. A chain is only as strong as its weakest link. 2.2 User and OperatingSystem Interface We mentioned earlier that there are several ways for users to interface with the operating system. Here, we discuss two fundamental approaches. One provides a commandline interface, or command interpreter ,t h a ta l l o

interface, or command interpreter ,t h a ta l l o w su s e r s to directly enter commands to be performed by the operating system. The other allows users to interface with the operating system via a graphical user interface, or GUI. 2.2.1 Command Interpreters Some operating systems include the command interpreter in the kernel. Others, such as Windows and UNIX , treat the command interpreter as a special program that is running when a job is initiated or when a user rst logs on (on interactive

or when a user rst logs on (on interactive systems). On systems with multiple command interpreters to choose from, the interpreters are known as shells .F o re x a m p l e ,o n UNIX and Linux systems, a user may choose among several different shells, including the Bourne shell ,C shell ,BourneAgain shell ,Korn shell ,a n do t h e r s .T h i r d  p a r t ys h e l l sa n df r e e userwritten shells are also available. Most shells provide similar functionality, and a users choice of which shell to

and a users choice of which shell to use is generally based on personal preference. Figure 2.2 shows the Bourne s hell command interpreter being used on Solaris 10. The main function of the command interpreter is to get and execute the next userspecied command. Many of the commands given at this level manipulate les: create, delete, list, print, copy, execute, and so on. The MSDOS and UNIX shells operate in this way. These commands can be implemented in two general ways. In one approach, the

in two general ways. In one approach, the command interpreter itself contains the code to execute the command. For example, a command to delete a le may cause the command interpreter to jump to a section of its code that sets up the parameters and makes the appropriate system call. In this case, the number of commands that can be given determines the size of the command interpreter, since each command requires its own implementing code. An alternative approachused by UNIX , among other operating

approachused by UNIX , among other operating systems implements most commands through system programs. In this case, the command interpreter does not understand the command in any way; it merely uses the command to identify a le to be loaded into memory and executed. Thus, the UNIX command to delete a le rm file.txt would search for a le called rm,l o a dt h e l ei n t om e m o r y ,a n de x e c u t ei tw i t h the parameter file.txt .T h ef u n c t i o na s s o c i a t e dw i t ht h e rmcommand

c t i o na s s o c i a t e dw i t ht h e rmcommand would2.2 User and OperatingSystem Interface 59 Figure 2.2 The Bourne shell command interpreter in Solrais 10. be dened completely by the code in the le rm.I nt h i sw a y ,p r o g r a m m e r sc a n add new commands to the system easily by creating new les with the proper names. The commandinterpreter program, which can be small, does not have to be changed for new commands to be added. 2.2.2 Graphical User Interfaces As e c o n ds t r a t e g

User Interfaces As e c o n ds t r a t e g yf o ri n t e r f a c i n gw i t ht h eo p e r a t i n gs y s t e mi st h r o u g hau s e r  friendly graphical user interface, or GUI.H e r e ,r a t h e rt h a ne n t e r i n gc o m m a n d s directly via a commandline interfac e, users employ a mousebased window andmenu system characterized by a desktop metaphor. The user moves the mouse to position its pointer on images, or icons ,o nt h es c r e e n( t h ed e s k t o p ) that represent programs, les,

t h ed e s k t o p ) that represent programs, les, directories, and system functions. Depending on the mouse pointers location, clicking a button on the mouse can invoke a program, select a le or directoryknown as a folder or pull down a menu that contains commands. Graphical user interfaces rst appear ed due in part to research taking place in the early 1970s at Xerox PARC research facility. The rst GUI appeared on the Xerox Alto computer in 1973. However, graphical interfaces became more

in 1973. However, graphical interfaces became more widespread with the advent of Apple Macintosh computers in the 1980s. The user interface for the Macintosh operating system (Mac OS)h a su n d e r g o n e various changes over the years, the most signicant being the adoption of theAqua interface that appeared with Mac OS X .M i c r o s o f t  s r s tv e r s i o no f WindowsVersion 1.0was based on the addition of a GUI interface to the MSDOS operating system. Later versions of Windows have made

system. Later versions of Windows have made cosmetic60 Chapter 2 OperatingSystem Structures changes in the appearance of the GUI along with several enhancements in its functionality. Because a mouse is impractical for most mobile systems, smartphones and handheld tablet computers typically use a touchscreen interface. Here, users interact by making gestures on the touchscreenfor example, pressing and swiping ngers across the screen. Figure 2.3 illustrates the touchscreen of the Apple iPad.

2.3 illustrates the touchscreen of the Apple iPad. Whereas earlier smartphones included a physical keyboard, most smartphones now simulate a keyboard on the touchscreen. Traditionally, UNIX systems have been dominated by commandline inter faces. Various GUIinterfaces are available, however .T h e s ei n c l u d et h eC o m m o n Desktop Environment ( CDE)a n dX  W i n d o w ss y s t e m s ,w h i c ha r ec o m m o n on commercial versions of UNIX ,s u c ha sS o l a r i sa n dI B M  s AIX system.

,s u c ha sS o l a r i sa n dI B M  s AIX system. In addition, there has been signicant development in GUI designs from various opensource projects, such as KD e s k t o pE n v i r o n m e n t (orKDE)a n dt h e GNOME desktop by the GNU project. Both the KDE and GNOME desktops run on Linux and various UNIX systems and are available under opensource licenses, which means their source code is readily available for reading and for modication under specic license terms. Figure 2.3 The iPad

under specic license terms. Figure 2.3 The iPad touchscreen.2.2 User and OperatingSystem Interface 61 2.2.3 Choice of Interface The choice of whether to use a commandline or GUI interface is mostly one of personal preference. System administrators who manage computers and power users who have deep knowledge of a system frequently use the commandline interface. For them, it is more efcient, giving them faster access to the activities they need to perform. Indeed, on some systems, only a subset of

perform. Indeed, on some systems, only a subset of system functions is available via the GUI, leaving the less common tasks to those who are commandline knowledgeable. Further, command line interfaces usually make repetitive tasks easier, in part because they have their own programmability. For example, if a frequent task requires a set of commandline steps, those steps can be recorded into a le, and that le can be run just like a program. The program is not compiled into executable code but

program is not compiled into executable code but rather is interpreted by the commandline interface. These shell scripts are very common on systems that are commandline oriented, such as UNIX and Linux. In contrast, most Windows users are happy to use the Windows GUI environment and almost never use the MSDOS shell interface. The various changes undergone by the Macintosh operating systems provide a nice study in contrast. Historically, Mac OShas not provided a commandline interface, always

OShas not provided a commandline interface, always requiring its users to interface with the operating system using its GUI. However, with the release of Mac OS X (which is in part implemented using a UNIX kernel), the operating system now provides both a Aqua interface and a commandline interface. Figure 2.4 is a screenshot of the Mac OS X GUI . Figure 2.4 The Mac OS X GUI.62 Chapter 2 OperatingSystem Structures The user interface can vary from system to system and even from user to user within

system to system and even from user to user within a system. It typically is substantially removed from the actual system structure. The design of a useful and friendly user interface is therefore not a direct function of the operating system. In this book, we concentrate on the fundamental problems of providing adequate service to user programs. From the point of view of the operating system, we do not distinguish between user programs and system programs. 2.3 System Calls System calls provide

programs. 2.3 System Calls System calls provide an interface to the services made available by an operating system. These calls are generally available as routines written in C and C, although certain lowlevel tasks (for example, tasks where hardware must be accessed directly) may have to be written using assemblylanguage instructions. Before we discuss how an operating system makes system calls available, lets rst use an example to illustrate how system calls are used: writing a simple program

system calls are used: writing a simple program to read data from one le and copy them to another le. The rst input that the program will need is the names of the two les: the input le and the output le. These names can be specied in many ways, depending on the operatingsystem design. One approach is for the program to ask the user for the names. In an interactive system, this approach will require a sequence of system calls, rst to write a prompting message on the screen and then to read from

message on the screen and then to read from the keyboard the characters that dene the two les. On mousebased and iconbased systems, a menu of le names is usually displayed in a window. The user can then use the mouse to select the source name, and a window can be opened for the destination name to be specied. This sequence requires many IOsystem calls. Once the two le names have been ob tained, the program must open the input le and create the output le. Each of these operations requires another

le. Each of these operations requires another system call. Possible error conditions for each operation can require additional system calls. When the program tries to open the input le, for example, it may nd that there is no le of that name or that the le is protected against access. In these cases, the program should print a message on the console (another sequence of system calls) and then terminate abnormally (another system call). If the input le exists, then we must create a new output le.

le exists, then we must create a new output le. We may nd that there is already an output le with the same name. This situation may cause the program to abort (a system call), or we may delete the existing le (another system call) and create a new one (yet another system call). Another option, in an interactive system, is to ask the user (via a sequence of system calls to output the prompting message and to read the response from the terminal) whether to replace the existing le or to abort the

whether to replace the existing le or to abort the program. When both les are set up, we enter a loop that reads from the input le (a system call) and writes to the output le (another system call). Each read and write must return status information regarding various possible error conditions. On input, the program may nd that the end of the le has been reached or that there was a hardware failure in the read (such as a parity error). The write operation may encounter various errors, depending on

may encounter various errors, depending on the output device (for example, no more disk space).2.3 System Calls 63 Finally, after the entire le is copied, the program may close both les (another system call), write a message to the console or window (more system calls), and nally terminate normally (the nal system call). This systemcall sequence is shown in Figure 2.5. As you can see, even simple programs may make heavy use of the operating system. Frequently, systems execute thousands of system

Frequently, systems execute thousands of system calls per second. Most programmers never see this level of detail, however. Typically, application developers design programs according to an application programming interface (API).T h e API species a set of functions that are available to an application programmer, including the parameters that are passed to each function and the return values the programmer can expect. Three of the most common APIsa v a i l a b l et oa p p l i c a t i o np r o g

v a i l a b l et oa p p l i c a t i o np r o g r a m m e r sa r e the Windows APIfor Windows systems, the POSIX API forPOSIX based systems (which include virtually all versions of UNIX , Linux, and Mac OS X), and the Java APIfor programs that run on the Java virtual machine. A programmer accesses anAPIvia a library of code provided by the operating system. In the case of UNIX and Linux for programs written in the C language, the library is called libc. Note thatunless speciedthe systemcall names

libc. Note thatunless speciedthe systemcall names used throughout this text are generic examples. Each operating system has its own name for each system call. Behind the scenes, the functions that make up an APItypically invoke the actual system calls on behalf of the application programmer. For example, the Windows function CreateProcess() (which unsurprisingly is used to create an e wp r o c e s s )a c t u a l l yi n v o k e st h e NTCreateProcess() system call in the Windows kernel. Why would

system call in the Windows kernel. Why would an application programmer prefer programming according to anAPIrather than invoking actual system calls? There are several reasons for doing so. One benet concerns program portability. An application programsource file destination file Example System Call Sequence Acquire input file name Write prompt to screen Accept input Acquire output file name Write prompt to screen Accept input Open the input file if file doesnt exist, abort Create output file if

if file doesnt exist, abort Create output file if file exists, abort Loop Read from input file Write to output file Until read fails Close output file Write completion message to screen Terminate normally Figure 2.5 Example of how system calls are used.64 Chapter 2 OperatingSystem Structures EXAMPLE OF STANDARD API As an example of a standard API,c o n s i d e rt h e read() function that is available in UNIX and Linux systems. The APIfor this function is obtained from the man page by invoking

function is obtained from the man page by invoking the command man read on the command line. A description of this APIappears below: include unistd.h ssizet read(int fd, void buf, sizet count) return valuefunct ion nameparameters Ap r o g r a mt h a tu s e st h e read() function must include the unistd.h header le, as this le denes the ssize tand size tdata types (among other things). The parameters passed to read() are as follows: int fd the le descriptor to be read void buf a buffer where the

descriptor to be read void buf a buffer where the data will be read into size tc o u n t the maximum number of bytes to be read into the buffer On a successful read, the number of bytes read is returned. A return value of 0i n d i c a t e se n do f l e .I fa ne r r o ro c c u r s , read() returns 1. mer designing a program using an APIcan expect her program to compile and run on any system that supports the same API(although, in reality, architectural differences often make this more difcult

differences often make this more difcult than it may appear). Furthermore, actual system calls can often be more detailed and difcult to work with than theAPIavailable to an application programmer. Nevertheless, there often exists as t r o n gc o r r e l a t i o nb e t w e e naf u n c t i o ni nt h e APIand its associated system call within the kernel. In fact, many of the POSIX and Windows APIsa r es i m i l a rt o the native system calls provided by the UNIX ,L i n u x ,a n dW i n d o w so p e

by the UNIX ,L i n u x ,a n dW i n d o w so p e r a t i n g systems. For most programming languages, the runtime support system (a set of functions built into libraries included with a compiler) provides a system call interface that serves as the link to system calls made available by the operating system. The systemcall interface intercepts function calls in the API and invokes the necessary system calls within the operating system. Typically, a number is associated with each system call, and

a number is associated with each system call, and the systemcall interface maintains a table indexed according to these numbers. The system call interface2.3 System Calls 65 Implementat ion of open ( ) system callopen ( )user mode returnuser appl ication system call interface kernel mode iopen ( ) Figure 2.6 The handling of a user application invoking the open() system call. then invokes the intended system call in the operatingsystem kernel and returns the status of the system call and any

and returns the status of the system call and any return values. The caller need know nothing about how the system call is implemented or what it does during execution. Rather, the caller need only obey the APIand understand what the operating system will do as a result of the execution of that system call. Thus, most of the details of the operatingsystem interface are hidden from the programmer by the APIand are managed by the runtime support library. The relationship between an API, the

library. The relationship between an API, the systemcall interface, and the operating system is shown in Figure 2.6, which illustrates how the operating system handles a user application invoking the open() system call. System calls occur in different ways, depending on the computer in use. Often, more information is required than simply the identity of the desired system call. The exact type and amount of information vary according to the particular operating system and call. For example, to

operating system and call. For example, to get input, we may need to specify the le or device to use as the source, as well as the address and length of the memory buffer into which the input should be read. Of course, the device or le and length may be implicit in the call. Three general methods are used to pass parameters to the operating system. The simplest approach is to pass the parameters in registers. In some cases, however, there may be more parameters than registers. In these cases,

be more parameters than registers. In these cases, the parameters are generally stored in a block, or table, in memory, and the address of the block is passed as a parameter in a register (Figure 2.7). This is the approach taken by Linux and Solaris. Parameters also can be placed, orpushed ,o n t ot h e stack by the program and popped off the stack by the operating system. Some operating systems prefer the block or stack method because those approaches do not limit the number or length of

approaches do not limit the number or length of parameters being passed.66 Chapter 2 OperatingSystem Structures code for system call 13 operat ing systemuser programuse parameters from table XregisterX X: parameters for call load address X system call 13 Figure 2.7 Passing of parameters as a table. 2.4 Types of System Calls System calls can be grouped roughly into six major categories: process control ,le manipulation ,device manipulation ,information maintenance , communications ,a n d

,information maintenance , communications ,a n d protection .I nS e c t i o n s2 . 4 . 1t h r o u g h2 . 4 . 6 ,w eb r i e  y discuss the types of system calls that may be provided by an operating system. Most of these system calls support, or are supported by, concepts and functions that are discussed in later chapters. Figure 2.8 summarizes the types of system calls normally provided by an operating system. As mentioned, in this text, we normally refer to the system calls by generic names.

refer to the system calls by generic names. Throughout the text, however, we provide examples of the a ctual counterparts to the system calls for Windows, UNIX ,a n dL i n u xs y s t e m s . 2.4.1 Process Control Ar u n n i n gp r o g r a mn e e d st ob ea b l et oh a l ti t se x e c u t i o ne i t h e rn o r m a l l y (end() )o ra b n o r m a l l y( abort() ). If a system call is made to terminate the currently running program abnormally, or if the program runs into a problem and causes an

if the program runs into a problem and causes an error trap, a dump of memory is sometimes taken and an error message generated. The dump is written to disk and may be examined by adebugger a system program designed to aid the programmer in nding and correcting errors, or bugs to determine the cause of the problem. Under either normal or abnormal circumstances, the operating system must transfer control to the invoking command interpreter. The command interpreter then reads the next command. In

interpreter then reads the next command. In an interactive system, the command interpreter simply continues with the next command; it is assumed that the user will issue an appropriate command to respond to any error. In a GUI system, a popup window might alert the user to the error and ask for guidance. In a batch system, the command interpreter usually terminates the entire job and continues with the next job. Some systems may allow for special recovery actions in case an error occurs. If the

recovery actions in case an error occurs. If the program discovers an error in its input and wants to terminate abnormally, it may also want to dene an error level. More severe errors can be indicated by a higherlevel error parameter. It is then2.4 Types of System Calls 67 Process control end, abort load, execute create process, terminate process get process attributes, set process attributes wait for time wait event, signal event allocate and free memory File management create le, delete le

free memory File management create le, delete le open, close read, write, reposition get le attributes, set le attributes Device management request device, release device read, write, reposition get device attributes, set device attributes logically attach or detach devices Information maintenance get time or date, set time or date get system data, set system data get process, le, or device attributes set process, le, or device attributes Communications create, delete communication connection

create, delete communication connection send, receive messages transfer status information attach or detach remote devices Figure 2.8 Types of system calls. possible to combine normal and abnormal termination by dening a normal termination as an error at level 0. The command interpreter or a following program can use this error level to determine the next action automatically. Ap r o c e s so rj o be x e c u t i n go n ep r o g r a mm a yw a n tt o load() and execute() another program. This

n tt o load() and execute() another program. This feature allows the command interpreter to execute a program as directed by, for example, a user command, the click of a68 Chapter 2 OperatingSystem Structures EXAMPLES OF WINDOWS AND UNIX SYSTEM CALLS Windows Unix Process CreateProcess() fork() Control ExitProcess() exit() WaitForSingleObject() wait() File CreateFile() open() Manipulation ReadFile() read() WriteFile() write() CloseHandle() close() Device SetConsoleMode() ioctl() Manipulation

Device SetConsoleMode() ioctl() Manipulation ReadConsole() read() WriteConsole() write() Information GetCurrentProcessID() getpid() Maintenance SetTimer() alarm() Sleep() sleep() Communication CreatePipe() pipe() CreateFileMapping() shm open() MapViewOfFile() mmap() Protection SetFileSecurity() chmod() InitlializeSecurityDescriptor() umask() SetSecurityDescriptorGroup() chown() mouse, or a batch command. An interesting question is where to return control when the loaded program terminates. This

control when the loaded program terminates. This question is related to whether the existing program is lost, saved, or allowed to continue execution concurrently with the new program. If control returns to the existing program when the new program termi nates, we must save the memory image of the existing program; thus, we have effectively created a mechanism for one program to call another program. If both programs continue concurrently, we have created a new job or process to be

we have created a new job or process to be multiprogrammed. Often, there is a system call specically for this purpose (create process() orsubmit job() ). If we create a new job or process, or perhaps even a set of jobs or processes, we should be able to control its execution. This control requires the ability to determine and reset the attributes of a job or process, includ ing the jobs priority, its maximum allowable execution time, and so on (get process attributes() andset process

and so on (get process attributes() andset process attributes() ). We may also want to terminate a job or process that we created ( terminate process() )i f we nd that it is incorrect or is no longer needed.2.4 Types of System Calls 69 EXAMPLE OF STANDARD C LIBRARY The standard C library provides a portion of the systemcall interface for many versions of UNIX and Linux. As an example, lets assume a C program invokes the printf() statement. The C library intercepts this call and invokes the

The C library intercepts this call and invokes the necessary system call (or calls) in the operating systemin this instance, the write() system call. The C library takes the value returned by write() and passes it back to the user program. This is shown below: write ( ) system calluser mode kernel modeinclude std io.h int ma in ( )     printf (Greet ings);    return 0;  standard C l ibrary write ( ) Having created new jobs or processes, we may need to wait for them to nish their execution. We

need to wait for them to nish their execution. We may want to wait for a certain amount of time to pass ( wait time() ). More probably, we will want to wait for a specic event to occur ( wait event() ). The jobs or processes should then signal when that event has occurred ( signal event() ). Quite often, two or more processes may share data. To ensure the integrity of the data being shared, operating systems often provide system calls allowing ap r o c e s st o lock shared data. Then, no other

ap r o c e s st o lock shared data. Then, no other process can access the data until the lock is released. Typically, such system calls include acquire lock() and release lock() .S y s t e mc a l l so ft h e s et y p e s ,d e a l i n gw i t ht h ec o o r d i n a t i o no f concurrent processes, are discussed in great detail in Chapter 5. There are so many facets of and variations in process and job control that we next use two examplesone involving a singletasking system and the other a

involving a singletasking system and the other a multitasking systemto clarify these concepts. The MSDOS operating system is an example of a singletasking system. It has a command interpreter that is invoked when the computer is started (Figure 2.9(a)). Because MSDOS is singletasking, it uses a simple method to run a program and does not create an e wp r o c e s s .I tl o a d st h ep r o g r a mi n t om e m o r y ,w r i t i n go v e rm o s to fi t s e l ft o70 Chapter 2 OperatingSystem

o s to fi t s e l ft o70 Chapter 2 OperatingSystem Structures (a) (b)free memory command interpreter kernelprocessfree memory command interpreter kernel Figure 2.9 MSDOS execution. (a) At system startup. (b) Running a program. give the program as much memory as possible (Figure 2.9(b)). Next, it sets the instruction pointer to the rst instruction of the program. The program then runs, and either an error causes a trap, or the program executes a system call to terminate. In either case, the error

call to terminate. In either case, the error code is saved in the system memory for later use. Following this action, the small portion of the command interpreter that was not overwritten resumes execution. Its rst task is to reload the rest of the command interpreter from disk. Then the command interpreter makes the previous error code available to the user or to the next program. FreeBSD (derived from Berkeley UNIX )i sa ne x a m p l eo fam u l t i t a s k i n g system. When a user logs on to

l t i t a s k i n g system. When a user logs on to the system, the shell of the users choice is run. This shell is similar to the MSDOS shell in that it accepts commands and executes programs that the user requests. However, since FreeBSD is a multitasking system, the command interpreter may continue running while another program is executed (Figure 2.10). To start a new process, the shellfree memory interpreter kernelprocess D process C process B Figure 2.10 FreeBSD running multiple

C process B Figure 2.10 FreeBSD running multiple programs.2.4 Types of System Calls 71 executes a fork() system call. Then, the selected program is loaded into memory via an exec() system call, and the program is executed. Depending on the way the command was issued, the shell then either waits for the process to nish or runs the process in the background. In the latter case, the shell immediately requests another command. When a process is running in the background, it cannot receive input dire

in the background, it cannot receive input dire ctly from the keyboard, because the shell is using this resource. IOis therefore done through les or through a GUI interface. Meanwhile, the user is free to ask the shell to run other programs, to monitor the progress of the running process, to change that programs priority, and so on. When the process is done, it executes an exit() system call to terminate, returning to the invoking process a status code of 0 or a nonzero error code. This status

code of 0 or a nonzero error code. This status or error code is th en available to the shell or other programs. Processes are discussed in Chapter 3 with a program example using thefork() andexec() system calls. 2.4.2 File Management The le system is discussed in more detail in Chapters 11 and 12. We can, however, identify several common system calls dealing with les. We rst need to be able to create() anddelete() les. Either system call requires the name of the le and perhaps some of the les

the name of the le and perhaps some of the les attributes. Once the le is created, we need to open() it and to use it. We may also read() , write() ,o rreposition() (rewind or skip to the end of the le, for example). Finally, we need to close() the le, indicating that we are no longer using it. We may need these same sets of operations for directories if we have a directory structure for organizing les in the le system. In addition, for either les or directories, we need to be able to determine

or directories, we need to be able to determine the values of various attributes and perhaps to reset them if necessary. File attributes include the le name, le type, protection codes, accounting information, and so on. At least two system calls, get file attributes() andset file attributes() ,a r e required for this function. Some operating systems provide many more calls, such as calls for le move() and copy() .O t h e r sm i g h tp r o v i d ea n APIthat performs those operations using code

ea n APIthat performs those operations using code and other system calls, and others might provide system programs to perform those tasks. If the system programs are callable by other programs, then each can be considered an APIby other system programs. 2.4.3 Device Management Ap r o c e s sm a yn e e ds e v e r a lr e s o u r c e st oe x e c u t e  m a i nm e m o r y ,d i s kd r i v e s , access to les, and so on. If the resour ces are available, they can be granted, and control can be returned

they can be granted, and control can be returned to the user process. Otherwise, the process will have to wait until sufcient resources are available. The various resources controlled by the operating system can be thought of as devices. Some of these devices are physical devices (for example, disk drives), while others can be thought of as abstract or virtual devices (for example, les). A system with multiple users may require us to rst request() ad e v i c e ,t oe n s u r ee x c l u s i v eu s

ad e v i c e ,t oe n s u r ee x c l u s i v eu s eo fi t .A f t e r we are nished with the device, we release() it. These functions are similar to the open() and close() system calls for les. Other operating systems allow unmanaged access to devices.72 Chapter 2 OperatingSystem Structures The hazard then is the potential for device contention and perhaps deadlock, which are described in Chapter 7. Once the device has been requested (and allocated to us), we can read() , write() ,a n d( p o s s i

to us), we can read() , write() ,a n d( p o s s i b l y ) reposition() the device, just as we can with les. In fact, the similarity between IOdevices and les is so great that many operating systems, including UNIX ,m e r g et h et w oi n t oac o m b i n e d l ed e v i c es t r u c t u r e . In this case, a set of system calls is used on both les and devices. Sometimes, IOdevices are identied by special l en a m e s ,d i r e c t o r yp l a c e m e n t ,o r l e attributes. The user interface can

e n t ,o r l e attributes. The user interface can also make  les and devices appear to be similar, even though the underlying system calls are dissimilar. This is another example of the many design decisions that go into building an operating system and user interface. 2.4.4 Information Maintenance Many system calls exist simply for the purpose of transferring information between the user program and the operating system. For example, most systems have a system call to return the current time()

have a system call to return the current time() and date() .O t h e r system calls may return information about the system, such as the number of current users, the version number of the operating system, the amount of free memory or disk space, and so on. Another set of system calls is helpful in debugging a program. Many systems provide system calls to dump() memory. This provision is useful for debugging. A program trace lists each system call as it is executed. Even microprocessors provide a

as it is executed. Even microprocessors provide a CPU mode known as single step ,i nw h i c hat r a p is executed by the CPU after every instruction. The trap is usually caught by a debugger. Many operating systems provide a time prole of a program to indicate the amount of time that the program executes at a particular location or set of locations. A time prole requires either a tracing facility or regular timer interrupts. At every occurrence of the timer interrupt, the value of the program

of the timer interrupt, the value of the program counter is recorded. With sufciently frequent timer interrupts, a statistical picture of the time spent on various parts of the program can be obtained. In addition, the operating system keeps information about all its processes, and system calls are used to access this information. Generally, calls are also used to reset the process information ( get process attributes() and set process attributes() ). In Section 3.1.3, we discuss what

attributes() ). In Section 3.1.3, we discuss what information is normally kept. 2.4.5 Communication There are two common models of interpr ocess communication: the message passing model and the sharedmemory model. In the messagepassing model , the communicating processes exchange messages with one another to transfer information. Messages can be exchanged between the processes either directly or indirectly through a common mailbox. Before communication can take place, a connection must be

communication can take place, a connection must be opened. The name of the other communicator must be known, be it another process on the same system or a process on another computer connected by a communications network. Each computer in an e t w o r kh a sa host name by which it is commonly known. A host also has a2.4 Types of System Calls 73 network identier, such as an IPaddress. Similarly, each process has a process name , and this name is translated into an identier by which the operating

translated into an identier by which the operating system can refer to the process. The get hostid() and get processid() system calls do this translation. The identiers are then passed to the general purpose open() and close() calls provided by the le system or to specic open connection() andclose connection() system calls, depending on the systems model of communication. The recipient process usually must give its permission for communication to take place with an accept connection() call. Most

take place with an accept connection() call. Most processes that will be receiving connections are specialpurpose daemons ,w h i c ha r es y s t e mp r o g r a m sp r o v i d e df o rt h a tp u r p o s e .T h e ye x e c u t e await for connection() call and are awakened when a connection is made. The source of the communication, known as the client , and the receiving daemon, known as a server, then exchange messages by using read message() andwrite message() system calls. The close connection()

message() system calls. The close connection() call terminates the communication. In the sharedmemory model ,p r o c e s s e su s e shared memory create() andshared memory attach() system calls to create and gain access to regions of memory owned by other processes. Recall that, normally, the operating system tries to prevent one process from accessing another processs memory. Shared memory requires that two or more processes agree to remove this restriction. They can then exchange information

restriction. They can then exchange information by reading and writing data in the shared areas. The form of the data is determined by the processes and is not under the operating systems control. The processes are also responsible for ensuring that they are not writing to the same location simultaneously. Such mechanisms are discussed in Chapter 5. In Chapter 4, we look at a variation of the process schemethreadsin which memory is shared by default. Both of the models just discussed are common

Both of the models just discussed are common in operating systems, and most systems implement both. Message passing is useful for exchanging smaller amounts of data, because no conicts need be avoided. It is also easier to implement than is shared memory for intercomputer communication. Shared memory allows maximum speed and convenience of communication, since it can be done at memory transfer speeds when it takes place within a computer. Problems exist, however, in the areas of protection and

exist, however, in the areas of protection and synchronization between the processes sharing memory. 2.4.6 Protection Protection provides a mechanism for controlling access to the resources provided by a computer system. Historically, protection was a concern only on multiprogrammed computer systems with several users. However, with the advent of networking and the Internet, all computer systems, from servers to mobile handheld devices, must be concerned with protection. Typically, system calls

concerned with protection. Typically, system calls providing protection include set permission() and get permission() ,w h i c hm a n i p u l a t et h ep e r m i s s i o ns e t t i n g so f resources such as les and disks. The allow user() anddeny user() system calls specify whether particular users canor cannotbe allowed access to certain resources. We cover protection in Chapter 14 and the much larger issue of security in Chapter 15.74 Chapter 2 OperatingSystem Structures 2.5 System Programs

2 OperatingSystem Structures 2.5 System Programs Another aspect of a modern system is its collection of system programs. Recall Figure 1.1, which depicted the logical c omputer hierarchy. At the lowest level is hardware. Next is the operating system, then the system programs, and nally the application programs. System programs ,a l s ok n o w na s system utilities , provide a convenient environment fo rp r o g r a md e v e l o p m e n ta n de x e c u t i o n . Some of them are simply user

de x e c u t i o n . Some of them are simply user interfaces to system calls. Others are considerably more complex. They can be divided into these categories: File management .T h e s ep r o g r a m sc r e a t e ,d e l e t e ,c o p y ,r e n a m e ,p r i n t , dump, list, and generally manipulate les and directories. Status information .S o m ep r o g r a m ss i m p l ya s kt h es y s t e mf o rt h ed a t e , time, amount of available memory or disk space, number of users, or similar status

or disk space, number of users, or similar status information. Others are more complex, providing detailed performance, logging, and debugging information. Typically, these pro grams format and print the output to the terminal or other output devices or les or display it in a window of the GUI.S o m es y s t e m sa l s os u p p o r ta registry ,w h i c hi su s e dt os t o r ea n dr e t r i e v ec o n  g u r a t i o ni n f o r m a t i o n . File modication .S e v e r a lt e x te d i t o r sm a yb

.S e v e r a lt e x te d i t o r sm a yb ea v a i l a b l et oc r e a t ea n d modify the content of les stored on disk or other storage devices. There may also be special commands to search contents of les or perform transformations of the text. Programminglanguage support .C o m p i l e r s ,a s s e m b l e r s ,d e b u g g e r s ,a n d interpreters for common programming languages (such as C, C, Java, and PERL )a r eo f t e np r o v i d e dw i t ht h eo p e r a t i n gs y s t e mo ra v a i l

i t ht h eo p e r a t i n gs y s t e mo ra v a i l a b l ea sa separate download. Program loading and execution . Once a program is assembled or com piled, it must be loaded into memory to be executed. The system may provide absolute loaders, relocatable loaders, linkage editors, and overlay loaders. Debugging systems for either higherlevel languages or machine language are needed as well. Communications .T h e s ep r o g r a m sp r o v i d et h em e c h a n i s mf o rc r e a t i n g virtual

h em e c h a n i s mf o rc r e a t i n g virtual connections among processes, users, and computer systems. They allow users to send messages to one anothers screens, to browse Web pages, to send email messages, to log in remotely, or to transfer les from one machine to another. Background services .A l lg e n e r a l  p u r p o s es y s t e m sh a v em e t h o d sf o r launching certain systemprogram processes at boot time. Some of these processes terminate after completing their tasks, while

terminate after completing their tasks, while others continue to run until the system is halted. Constantly running systemprogram processes are known as services ,subsystems ,o rd a e m o n s .O n ee x a m p l ei s the network daemon discussed in Section 2.4.5. In that example, a system needed a service to listen for network connections in order to connect those requests to the correct processes. Other examples include process schedulers that start processes according to a specied schedule,

start processes according to a specied schedule, system error monitoring services, and print servers. Typical systems have dozens2.6 OperatingSystem Design and Implementation 75 of daemons. In addition, operating systems that run important activities in user context rather than in kernel context may use daemons to run these activities. Along with system programs, most operating systems are supplied with programs that are useful in solving common problems or performing common operations. Such

problems or performing common operations. Such application programs include Web browsers, word proces sors and text formatters, spreadsheets, database systems, compilers, plotting and statisticalanalysis packages, and games. The view of the operating system seen by most users is dened by the application and system programs, rather than by the actual system calls. Consider a users PC.W h e nau s e r  sc o m p u t e ri sr u n n i n gt h eM a c OS X operating system, the user might see the GUI,

OS X operating system, the user might see the GUI, featuring a mouseandwindows interface. Alternatively, or even in one of the windows, the user might have a commandline UNIX shell. Both use the same set of system calls, but the system calls look different and act in different ways. Further confusing the user view, consider the user dualbooting from Mac OS X into Windows. Now the same user on the same hardware has two entirely different interfaces and two sets of applications using the same

and two sets of applications using the same physical resources. On the same hardware, then, au s e rc a nb ee x p o s e dt om u l t i p l eu s e ri nterfaces sequentially or concurrently. 2.6 OperatingSystem Design and Implementation In this section, we discuss problems we face in designing and implementing an operating system. There are, of course, no complete solutions to such problems, but there are approaches that have proved successful. 2.6.1 Design Goals The rst problem in designing a

2.6.1 Design Goals The rst problem in designing a system is to dene goals and specications. At the highest level, the design of the system will be affected by the choice of hardware and the type of system: batch, time sharing, single user, multiuser, distributed, real time, or general purpose. Beyond this highest design level, the requirements may be much harder to specify. The requirements can, however, be divided into two basic groups: user goals and system goals . Users want certain obvious

and system goals . Users want certain obvious properties in a system. The system should be convenient to use, easy to learn and t ou s e ,r e l i a b l e ,s a f e ,a n df a s t .O fc o u r s e , these specications are not particularly useful in the system design, since there is no general agreement on how to achieve them. As i m i l a rs e to fr e q u i r e m e n t sc a nb ed e  n e db yt h o s ep e o p l ew h om u s t design, create, maintain, and operate the system. The system should be easy

and operate the system. The system should be easy to design, implement, and maintain; and it should be exible, reliable, error free, and efcient. Again, these require ments are vague and may be interpreted in various ways. There is, in short, no unique solution to the problem of dening the requirements for an operating system. The wide range of systems in existence shows that different requirements can result in a large variety of solutions for different environments. For example, the

for different environments. For example, the requirements for VxWorks, a real76 Chapter 2 OperatingSystem Structures time operating system for embedded systems, must have been substantially different from those for MVS ,al a r g em u l t i u s e r ,m u l t i a c c e s so p e r a t i n gs y s t e m forIBMmainframes. Specifying and designing an operating system is a highly creative task. Although no textbook can tell you how to do it, general principles have been developed in the eld of software

have been developed in the eld of software engineering ,a n dw et u r nn o wt o a discussion of some of these principles. 2.6.2 Mechanisms and Policies One important principle is the separation of policy from mechanism .M e c h a  nisms determine how to do something; policies determine what will be done. For example, the timer construct (see Section 1.5.2) is a mechanism for ensuring CPU protection, but deciding how long the timer is to be set for a particular user is a policy decision. The

for a particular user is a policy decision. The separation of policy and mechanism is important for exibility. Policies are likely to change across places or over time. In the worst case, each change in policy would require a change in the underlying mechanism. A general mechanism insensitive to changes in policy would be more desirable. A change in policy would then require redenition of only certain parameters of the system. For instance, consider a mechanism for giving priority to certain

a mechanism for giving priority to certain types of programs over others. If the mechanism is properly separated from policy, it can be used either to support a policy decision that IOintensive programs should have priority over CPUintensive ones or to support the opposite policy. Microkernelbased operating systems (Section 2.7.3) take the separation of mechanism and policy to one extreme by implementing a basic set of primitive building blocks. These blocks are almost policy free, allowing more

These blocks are almost policy free, allowing more advanced mechanisms and policies to be added via usercreated kernel modules or user programs themselves. As an example, consider the history of UNIX .A t r s t , it had a timesharing scheduler. In the latest version of Solaris, scheduling is controlled by loadable tables. Depending on the table currently loaded, the system can be time sharing, batch processing, real time, fair share, or any combination. Making the scheduling mechanism general

Making the scheduling mechanism general purpose allows vast policy changes to be made with a single loadnewtable command. At the other extreme is a system such as Windows, in which both mechanism and policy are encoded in the system to enforce a global look and feel. All applications have similar interfaces, because the interface itself is built into the kernel and system libraries. The Mac OS X operating system has similar functionality. Policy decisions are important for all resource

Policy decisions are important for all resource allocation. Whenever it is necessary to decide whether or not to allocate a resource, a policy decision must be made. Whenever the question is how rather than what ,i ti sam e c h a n i s m that must be determined. 2.6.3 Implementation Once an operating system is designed, it must be implemented. Because operating systems are collections of many programs, written by many people over a long period of time, it is difcult to make general statements

of time, it is difcult to make general statements about how they are implemented.2.6 OperatingSystem Design and Implementation 77 Early operating systems were written in assembly language. Now, although some operating systems are still written in assembly language, most are written in a higherlevel language such as C or an even higherlevel language such as C. Actually, an operating system can be written in more than one language. The lowest levels of the kernel might be assembly language.

levels of the kernel might be assembly language. Higherlevel routines might be in C, and system programs might be in C or C, in interpreted scripting languages like PERL or Python, or in shell scripts. In fact, a given Linux distribution probably includes programs written in all of those languages. The rst system that was not written in assembly language was probably the Master Control Program ( MCP ) for Burroughs computers. MCP was written in a variant of ALGOL .MULTICS ,d e v e l o p e da t

a variant of ALGOL .MULTICS ,d e v e l o p e da t MIT,w a sw r i t t e nm a i n l yi n the system programming language PL1 .T h eL i n u xa n dW i n d o w so p e r a t i n g system kernels are written mostly in C, although there are some small sections of assembly code for device drivers and for saving and restoring the state of registers. The advantages of using a higherlevel language, or at least a systems implementation language, for implementing operating systems are the same as those gained

operating systems are the same as those gained when the language is used for application programs: the code can be written faster, is more compact, and is easier to understand and debug. In addition, improvements in compiler technology will improve the generated code for the entire operating system by simple recompilation. Finally, an operating system is far easier to port to move to some other hardware if it is written in a higherlevel language. For example, MSDOS was written in Intel 8088

For example, MSDOS was written in Intel 8088 assembly language. Consequently, it runs natively only on the Intel X86 family of CPUs. (Note that although MSDOS runs natively only on Intel X86, emulators of the X86 instruction set allow the operating system to run on other CPUsbutmoreslowly ,andwithhigherresourceuse.Aswementioned in Chapter 1, emulators are programs that duplicate the functionality of one system on another system.) The Linux operating system, in contrast, is written mostly in C

system, in contrast, is written mostly in C and is available natively on a number of different CPUs, including Intel X86, Oracle SPARC ,a n d IBMPowerPC. The only possible disadvantages of implementing an operating system in a higherlevel language are reduced speed and increased storage requirements. This, however, is no longer a major issue in todays systems. Although an expert assemblylanguage programmer can produce efcient small routines, for large programs a modern compiler can perform

for large programs a modern compiler can perform complex analysis and apply sophisticated optimizations that produce excellent code. Modern processors have deep pipelining and multiple functional units that can handle the details of complex dependencies much more easily than can the human mind. As is true in other systems, major performance improvements in oper ating systems are more likely to be the result of better data structures and algorithms than of excellent assemblylanguage code. In

than of excellent assemblylanguage code. In addition, although operating systems are large, only a small amount of the code is critical to high performance; the interrupt handler, IOmanager, memory manager, and CPU scheduler are probably the most critical routines. After the system is written and is working correctly, bottleneck routines can be identied and can be replaced with assemblylanguage equivalents.78 Chapter 2 OperatingSystem Structures 2.7 OperatingSystem Structure As y s t e ma sl a r

2.7 OperatingSystem Structure As y s t e ma sl a r g ea n dc o m p l e xa sam o d e r no p e r a t i n gs y s t e mm u s tb e engineered carefully if it is to function properly and be modied easily. A common approach is to partition the task into small components, or modules, rather than have one monolithic system. Each of these modules should be aw e l l  d e  n e dp o r t i o no ft h es y s t e m ,w i t hc a r e f u l l yd e  n e di n p u t s ,o u t p u t s , and functions. We have already

s ,o u t p u t s , and functions. We have already discussed briey in Chapter 1 the common components of operating systems. In this section, we discuss how these components are interconne cted and melded into a kernel. 2.7.1 Simple Structure Many operating systems do not have welldened structures. Frequently, such systems started as small, simple, and limited systems and then grew beyond their original scope. MSDOS is an example of such a system. It was originally designed and implemented by a

It was originally designed and implemented by a few people who had no idea that it would become so popular. It was written to provide the most functionality in the least space, so it was not carefully divided into modules. Figure 2.11 shows its structure. InMSDOS ,t h ei n t e r f a c e sa n dl e v e l so ff u n c t i o n a l i t ya r en o tw e l ls e p a r a t e d . For instance, application programs are able to access the basic IOroutines to write directly to the display and disk drives. Such

directly to the display and disk drives. Such freedom leaves MSDOS vulnerable to errant (or malicious) programs, causing entire system crashes when user programs fail. Of course, MSDOS was also limited by the hardware of its era. Because the Intel 8088 for which it was written provides no dual mode and no hardware protection, the designers of MSDOS had no choice but to leave the base hardware accessible. Another example of limited structuring is the original UNIX operating system. Like MSDOS

is the original UNIX operating system. Like MSDOS ,UNIX initially was limited by hardware functionality. It consists of two separable parts: the kernel and the system programs. The kernel ROM BIOS device dr iversapplication program MSDOS device dr iversresident system program Figure 2.11 MSDOS layer structure.2.7 OperatingSystem Structure 79kernel(the users) shells and commands comp ilers and interpreters system l ibraries systemcall interface to the kernel signals term inal handl ing character

the kernel signals term inal handl ing character IO system terminal dr iversfile system swapp ing block IO system disk and tape dr iversCPU schedul ing page replacement demand pag ing virtual memory kernel interface to the hardware terminal controllers terminalsdevice controllers disks and tapesmemory controllers phys ical memory Figure 2.12 Traditional UNIX system structure. is further separated into a series of in terfaces and device drivers, which have been added and expanded over the years

which have been added and expanded over the years as UNIX has evolved. We can view the traditional UNIX operating system as being layered to some extent, as shown in Figure 2.12. Everything below the systemcall interface and above the physical hardware is the kernel. The kernel provides the le system, CPU scheduling, memory management, and other operatingsystem functions through system calls. Taken in sum, that is an enormous amount of functionality to be combined into one level. This monolithic

to be combined into one level. This monolithic structure was difcult to implement and maintain. It had a distinct performance advan tage, however: there is very little overhead in the system call interface or in communication within the kernel. We still see evidence of this simple, monolithic structure in the UNIX ,L i n u x , and Windows operating systems. 2.7.2 Layered Approach With proper hardware support, operating systems can be broken into pieces that are smaller and more appropriate than

pieces that are smaller and more appropriate than those allowed by the original MSDOS and UNIX systems. The operating system can then retain much greater control over the computer and over the applications that make use of that computer. Implementers have more freedom in changing the inner workings of the system and in creating modular operating systems. Under a top down approach, the overall functionality and features are determined and are separated into components. Information hiding is also

into components. Information hiding is also important, because it leaves programmers free to implement the lowlevel routines as they see t, provided that the external inte rface of the routine stays unchanged and that the routine itself performs the advertised task. As y s t e mc a nb em a d em o d u l a ri nm a n yw a y s .O n em e t h o di st h e layered approach ,i nw h i c ht h eo p e r a t i n gs y s t e mi sb r o k e ni n t oan u m b e ro fl a y e r s (levels). The bottom layer (layer 0)

fl a y e r s (levels). The bottom layer (layer 0) is the hardware; the highest (layer N)i st h e user interface. This layering structure is depicted in Figure 2.13.80 Chapter 2 OperatingSystem Structureslayer N user interface  layer 1 layer 0 hardware Figure 2.13 Al a y e r e do p e r a t i n gs y s t e m . An operatingsystem layer is an implementation of an abstract object made up of data and the operations that can manipulate those data. A typical operatingsystem layersay, layer Mconsists of

operatingsystem layersay, layer Mconsists of data structures and a set of routines that can be invoked by higherlevel layers. Layer M,in turn, can invoke operations on lowerlevel layers. The main advantage of the layered approach is simplicity of construction and debugging. The layers are selected so that each uses functions (operations) and services of only lowerlevel layers. Th is approach simplies debugging and system verication. The rst layer can be debugged without any concern for the rest

can be debugged without any concern for the rest of the system, because, by denition, it uses only the basic hardware (which is assumed correct) to implement its functions. Once the rst layer is debugged, its correct functioning can be assumed while the second layer is debugged, and so on. If an error is found during the debugging of a particular layer, the error must be on that layer, because the layers below it are already debugged. Thus, the design and implementation of the system are

the design and implementation of the system are simplied. Each layer is implemented only with operations provided by lowerlevel layers. A layer does not need to know how these operations are implemented; it needs to know only what these operations do. Hence, each layer hides the existence of certain data structures, operations, and hardware from higherlevel layers. The major difculty with the layered approach involves appropriately dening the various layers. Because a layer can use only

the various layers. Because a layer can use only lowerlevel layers, careful planning is necessary. For example, the device driver for the backing store (disk space used by virtualmemory algorithms) must be at a lower level than the memorymanagement ro utines, because memory management requires the ability to use the backing store. Other requirements may not be so obvious. The backingstore driver would normally be above the CPU scheduler, because the driver may need to wait for IOand the CPU can

the driver may need to wait for IOand the CPU can be rescheduled during this time. However, on a large2.7 OperatingSystem Structure 81 system, the CPU scheduler may have more information about all the active processes than can t in memory. Therefore, this information may need to be swapped in and out of memory, requiring the backingstore driver routine to be below the CPU scheduler. A n a lp r o b l e mw i t hl a y e r e di m p l e m e n t a t i o n si st h a tt h e yt e n dt ob el e s s efcient

o n si st h a tt h e yt e n dt ob el e s s efcient than other types. For instance, when a user program executes an IO operation, it executes a system call that is trapped to the IOlayer, which calls the memorymanagement layer, which in turn calls the CPUscheduling layer, which is then passed to the hardware. At each layer, the parameters may be modied, data may need to be passed, and so on. Each layer adds overhead to the system call. The net result is a system call that takes longer than does

is a system call that takes longer than does one on a nonlayered system. These limitations have caused a small backlash against layering in recent years. Fewer layers with more functionality are being designed, providing most of the advantages of modularized code while avoiding the problems of layer denition and interaction. 2.7.3 Microkernels We have already seen that as UNIX expanded, the kernel became large and difcult to manage. In the mid1980s, researchers at Carnegie Mellon University

researchers at Carnegie Mellon University developed an operating system called Mach that modularized the kernel using the microkernel approach. This method structures the operating system by removing all nonessential components from the kernel and implementing them as system and userlevel programs. The result is a smaller kernel. There is little consensus regarding which services should remain in the kernel and which should be implemented in user space. Typically, however, microkernels provide

space. Typically, however, microkernels provide minimal process and memory management, in addition to a communication facility. Figure 2.14 illustrates the architecture of a typical microkernel. The main function of the microkernel is to provide communication between the client program and the various services that are also running in user space. Communication is provided through message passing, which was described in Section 2.4.5. For example, if the client program wishes to access a le,

if the client program wishes to access a le, itApplication ProgramFile SystemDevice DriverInterprocess Communicationmemory managmentCPU schedulingmessages messages microkernel hardwareuser mode kernel mode Figure 2.14 Architecture of a typical microkernel.82 Chapter 2 OperatingSystem Structures must interact with the le server. The c lient program and service never interact directly. Rather, they communicate indirectly by exchanging messages with the microkernel. One benet of the microkernel a

the microkernel. One benet of the microkernel a pproach is that it makes extending the operating system easier. All new services are added to user space and consequently do not require modication of the kernel. When the kernel does have to be modied, the changes tend to be fewer, because the microkernel is as m a l l e rk e r n e l .T h er e s u l t i n go p e r a t i n gs y s t e mi se a s i e rt op o r tf r o mo n e hardware design to another. The microkernel also provides more security and

The microkernel also provides more security and reliability, since most services are running as userrather than kernel processes. If a service fails, the rest of the operating system remains untouched. Some contemporary operating systems have used the microkernel approach. Tru64 UNIX (formerly Digital UNIX )p r o v i d e sa UNIX interface to the user, but it is implemented with a Mac hk e r n e l .T h eM a c hk e r n e lm a p s UNIX system calls into messages to the appropriate userlevel

calls into messages to the appropriate userlevel services. The Mac OS X kernel (also known as Darwin )i sa l s op a r t l yb a s e do nt h eM a c hm i c r o k e r n e l . Another example is QNX ,ar e a l  t i m eo p e r a t i n gs y s t e mf o re m b e d d e d systems. The QNX Neutrino microkernel provides services for message passing and process scheduling. It also handles lowlevel network communication and hardware interrupts. All other services in QNX are provided by standard processes that

in QNX are provided by standard processes that run outside the kernel in user mode. Unfortunately, the performance of microkernels can suffer due to increased systemfunction overhead. Consider the history of Windows NT.T h e r s t release had a layered microkernel organization. This versions performance was low compared with that of Windows 95. Windows NT4.0 partially corrected the performance problem by moving layers from user space to kernel space and integrating them more closely. By the time

and integrating them more closely. By the time Windows XP was designed, Windows architecture had become more monolithic than microkernel. 2.7.4 Modules Perhaps the best current m ethodology for operatingsystem design involves using loadable kernel modules .H e r e ,t h ek e r n e lh a sas e to fc o r ec o m p o n e n t s and links in additional services via modules, either at boot time or during run time. This type of design is common in modern implementations of UNIX ,s u c h as Solaris, Linux,

of UNIX ,s u c h as Solaris, Linux, and Mac OS X ,a sw e l la sW i n d o w s . The idea of the design is for the kernel to provide core services while other services are implemented dynamically, as the kernel is running. Linking services dynamically is preferable to adding n ew features directly to the kernel, which would require recompiling the kernel every time a change was made. Thus, for example, we might build CPU scheduling and memory management algorithms directly into the kernel and then

algorithms directly into the kernel and then add support for different le systems by way of loadable modules. The overall result resembles a layered system in that each kernel section has dened, protected interfaces; but it is more exible than a layered system, because any module can call any other module. The approach is also similar to the microkernel approach in that the primary module has only core functions and knowledge of how to load and communicate with other modules; but it2.7

load and communicate with other modules; but it2.7 OperatingSystem Structure 83 core Solaris kernelfile systems loadable system calls executable formatsSTREAMS modulesmiscellaneous modulesdevice and bus driversscheduling classes Figure 2.15 Solaris loadable modules. is more efcient, because modules do not need to invoke message passing in order to communicate. The Solaris operating system structure, shown in Figure 2.15, is organized around a core kernel with seven types of loadable kernel

a core kernel with seven types of loadable kernel modules: 1.Scheduling classes 2.File systems 3.Loadable system calls 4.Executable formats 5.STREAMS modules 6.Miscellaneous 7.Device and bus drivers Linux also uses loadable kernel modules, primarily for supporting device drivers and le systems. We cover creating loadable kernel modules in Linux as a programming exercise at the end of this chapter. 2.7.5 Hybrid Systems In practice, very few operating systems adopt a single, strictly dened

operating systems adopt a single, strictly dened structure. Instead, they combine dif ferent structures, resulting in hybrid systems that address performance, security, and usability issues. For example, both Linux and Solaris are monolithic, because having the operating system in a single address space provides very efcient performance. However, they are also modular, so that new functionality can be dynamically added to the kernel. Windows is largely monolithic as well (again primarily for

is largely monolithic as well (again primarily for performance reasons), but it retains some behavior typical of microkernel systems, including providing support for separate subsystems (known as operatingsystem personalities ) that run as usermode processes. Windows systems also provide support for dynamically loadable kernel modules. We provide case studies of Linux and Windows 7 in in Chapters 18 and 19, respectively. In the remainder of this section, we explore the structure of84 Chapter 2

section, we explore the structure of84 Chapter 2 OperatingSystem Structures three hybrid systems: the Apple Mac OS X operating system and the two most prominent mobile operating systemsi OSand Android. 2.7.5.1 Mac OS X The Apple Mac OS X operating system uses a hybrid structure. As shown in Figure 2.16, it is a layered system. The top layers include the Aqua user interface (Figure 2.4) and a set of application environments and services. Notably, theCocoa environment species an API for the

theCocoa environment species an API for the ObjectiveC programming language, which is used for writing Mac OS X applications. Below these layers is the kernel environment ,w h i c hc o n s i s t sp r i m a r i l yo ft h eM a c h microkernel and the BSD UNIX kernel. Mach provides memory management; support for remote procedure calls ( RPCs) and interprocess communication (IPC) facilities, including message passing; and thread scheduling. The BSD component provides a BSD commandline interface,

component provides a BSD commandline interface, support for networking and le systems, and an implementation of POSIX API s, including Pthreads. In addition to Mach and BSD,t h ek e r n e le n v i r o n m e n tp r o v i d e sa n IO kit for development of device drivers and dyn amically loadable modules (which Mac OS X refers to as kernel extensions ). As shown in Figure 2.16, the BSD application environment can make use of BSD facilities directly. 2.7.5.2 iOS iOSis a mobile operating system

2.7.5.2 iOS iOSis a mobile operating system designed by Apple to run its smartphone, the iPhone ,a sw e l la si t st a b l e tc o m p u t e r ,t h e iPad .iOSis structured on the Mac OS X operating system, with added functionality pertinent to mobile devices, but does not directly run Mac OS X applications. The structure of i OSappears in Figure 2.17. Cocoa Touch is an APIfor ObjectiveC that provides several frameworks for developing applications that run on i OSdevices. The fundamental

that run on i OSdevices. The fundamental difference between Cocoa, mentioned earlier, and Cocoa T ouch is that the latter provides support for hardware features unique to mobile devices, such as touch screens. The media services layer provides services for graphics, audio, and video.graphical user interfaceAqua application environments and services kernel environmentJava Cocoa Quicktime BSD Mach IO kit kernel extensionsBSD Figure 2.16 The Mac OS X structure.2.7 OperatingSystem Structure 85Cocoa

X structure.2.7 OperatingSystem Structure 85Cocoa Touch Media Serv ices Core Serv ices Core OS Figure 2.17 Architecture of Apples iOS. The core services layer provides a variety of features, including support for cloud computing and databases. The bottom layer represents the core operating system, which is based on the kernel environment shown in Figure 2.16. 2.7.5.3 Android The Android operating system was designed by the Open Handset Alliance (led primarily by Google) and was developed for

(led primarily by Google) and was developed for Android smartphones and tablet computers. Whereas i OSis designed to run on Apple mobile devices and is closesourced, Android runs on a variety of mobile platforms and is opensourced, partly explaining its rapid rise in popularity. The structure of Android appears in Figure 2.18. Android is similar to i OSin that it is a layered stack of software that provides a rich set of frameworks for d eveloping mobile applications. At the bottom of this

mobile applications. At the bottom of this software stack is the Linux kernel, although it has been modied by Google and is currently outside the normal distribution of Linux releases.Applications Application Framework Android runtime Core Libraries Dalvik virtual machineLibraries Linux kernelSQLite openGL surface manager webkit libcmedia framework Figure 2.18 Architecture of Googles Android.86 Chapter 2 OperatingSystem Structures Linux is used primarily for process, memory, and devicedriver

primarily for process, memory, and devicedriver support for hardware and has been expanded to include power management. The Android runtime environment includes a core set of libraries as well as the Dalvik virtual machine. Software designers for Android devices develop applications in the Java language. However, rather than using the standard Java API,G o o g l eh a s designed a separate Android APIfor Java development. The Java class les are rst compiled to Java bytecode and the nt r a n s l a

compiled to Java bytecode and the nt r a n s l a t e di n t oa ne x e c u t a b l e l et h a t runs on the Dalvik virtual machine. The Dalvik virtual machine was designed for Android and is optimized for mobile devices with limited memory and CPU processing capabilities. The set of libraries available for Android applications includes frameworks for developing web browsers (webkit), database support (SQLite), and multi media. The libc library is similar to the standard C library but is much

is similar to the standard C library but is much smaller and has been designed for the slower CPUst h a tc h a r a c t e r i z em o b i l ed e v i c e s . 2.8 OperatingSystem Debugging We have mentioned debugging frequently in this chapter. Here, we take a closer look. Broadly, debugging is the activity of nding and xing errors in a system, both in hardware and in software. Performance problems are considered bugs, so debugging can also include performance tuning ,w h i c hs e e k st oi m p r o

performance tuning ,w h i c hs e e k st oi m p r o v e performance by removing processing bottlenecks . In this section, we explore debugging process and kernel errors and performance problems. Hardware debugging is outside the scope of this text. 2.8.1 Failure Analysis If a process fails, most operating systems write the error information to a log leto alert system operators or users that the problem occurred. The operating system can also take a core dump a capture of the memory of the process

a core dump a capture of the memory of the process and store it in a le for later analysis. (Memory was referred to as the core in the early days of computing.) Running programs and core dumps can be probed by a debugger, which allows a programmer to explore the code and memory of a process. Debugging userlevel process code is a challenge. Operatingsystem kernel debugging is even more complex bec ause of the size and complexity of the kernel, its control of the hardware, and the lack of

its control of the hardware, and the lack of userlevel debugging tools. Af a i l u r ei nt h ek e r n e li sc a l l e da crash .W h e nac r a s ho c c u r s ,e r r o ri n f o r m a t i o n is saved to a log le, and the memory state is saved to a crash dump . Operatingsystem debugging and process debugging frequently use dif ferent tools and techniques due to the very different nature of these two tasks. Consider that a kernel failure in the lesystem code would make it risky for the kernel to try

code would make it risky for the kernel to try to save its state to a le on the le system before rebooting. Ac o m m o nt e c h n i q u ei st os a v et h ek e r n e l  sm e m o r ys t a t et oas e c t i o no fd i s k set aside for this purpose that contains no le system. If the kernel detects an unrecoverable error, it writes the entire contents of memory, or at least the kernelowned parts of the system memory, to the disk area. When the system reboots, a process runs to gather the data from

reboots, a process runs to gather the data from that area and write it to a crash2.8 OperatingSystem Debugging 87 Kernighans Law Debugging is twice as hard as writing the code in the rst place. Therefore, if you write the code as cleverly as possible, you are, by denition, not smart enough to debug it.  dump le within a le system for analysis. Obviously, such strategies would be unnecessary for debugging ordinary userlevel processes. 2.8.2 Performance Tuning We mentioned earlier that performance

Tuning We mentioned earlier that performance tuning seeks to improve performance by removing processing bottlenecks. To identify bottlenecks, we must be able to monitor system performance. Thus, the operating system must have some means of computing and displaying measures of system behavior. In a number of systems, the operating system does this by producing trace listings of system behavior. All interesting events ar el o g g e dw i t ht h e i rt i m ea n di m p o r t a n t parameters and are

rt i m ea n di m p o r t a n t parameters and are written to a le. Later, an analysis program can process the log le to determine system performance and to identify bottlenecks and inefciencies. These same traces can be run as input for a simulation of a suggested improved system. Traces also can help people to nd errors in operatingsystem behavior. Another approach to performance tuning uses singlepurpose, interactive tools that allow users and administrators to question the state of various

administrators to question the state of various system components to look for bottlenecks. One such tool employs the UNIX command top to display the resources used on the system, as well as a sorted list of thetopresourceusing processes. Other tools display the state of disk IO, memory allocation, and network trafc. The Windows Task Manager is a similar tool for Windows systems. The task manager includes information for current applications as well as processes, CPU and memory usage, and

as well as processes, CPU and memory usage, and networking statistics. A screen shot of the task manager appears in Figure 2.19. Making operating systems easier to understand, debug, and tune as they run is an active area of research and implementation. A new generation of kernelenabled performance analysis tools has made signicant improvements in how this goal can be achieved. Next, we discuss a leading example of such at o o l :t h eS o l a r i s1 0 DTrace dynamic tracing facility. 2.8.3

a r i s1 0 DTrace dynamic tracing facility. 2.8.3 DTrace DTrace is a facility that dynamically adds probes to a running system, both in user processes and in the kernel. These probes can be queried via the D programming language to determine an astonishing amount about the kernel, the system state, and process activities. For example, Figure 2.20 follows an application as it executes a system call ( ioctl() )a n ds h o w st h ef u n c t i o n a l calls within the kernel as they execute to

o n a l calls within the kernel as they execute to perform the system call. Lines ending with Uare executed in user mode, and lines ending in Kin kernel mode.88 Chapter 2 OperatingSystem Structures Figure 2.19 The Windows task manager. Debugging the interactions between userlevel and kernel code is nearly impossible without a toolset that understands both sets of code and can instrument the interactions. For that toolset to be truly useful, it must be able to debug any area of a system,

it must be able to debug any area of a system, including areas that were not written with debugging in mind, and do so without affecting system reliability. This tool must also have a minimum performance impactideally it should have no impact when not in use and a proportional impact during use. The DTrace tool meets these requirements and provides a dynamic, safe, lowimpact debugging environment. Until the DTrace framework and tools became available with Solaris 10, kernel debugging was usually

with Solaris 10, kernel debugging was usually shrouded in mystery and accomplished via happenstance and archaic code and tools. For example, CPUsh a v eab r e a k p o i n t feature that will halt execution and allow a debugger to examine the state of the system. Then execution can continue until the next breakpoint or termination. This method cannot be used in a multiuser operatingsystem kernel without negatively affecting all of the users on the system. Proling ,w h i c hp e r i o d i c a l l y

system. Proling ,w h i c hp e r i o d i c a l l y samples the instruction pointer to determine which code is being executed, can show statistical trends but not individual activities. Code can be included in the kernel to emit specic data under specic circumstances, but that code slows down the kernel and tends not to be included in the part of the kernel where the specic problem being debugged is occurring.2.8 OperatingSystem Debugging 89 .all.d pgrep xclock XEventsQueued dtrace: script .all.d

pgrep xclock XEventsQueued dtrace: script .all.d matched 52377 probes CPU FUNCTION 0  XEventsQueued U 0  XEventsQueued U 0  X11TransBytesReadable U 0  X11TransBytesReadable U 0  X11TransSocketBytesReadable U 0  X11TransSocketBytesreadable U 0  ioctl U 0  ioctl K 0  getf K 0  setactivefd K 0  setactivefd K 0  getf K 0  getudatamodel K 0  getudatamodel K ... 0  releasef K 0  clearactivefd K 0  clearactivefd K 0  cvbroadcast K 0  cvbroadcast K 0  releasef K 0  ioctl K 0  ioctl U 0  XEventsQueued U

K 0  ioctl K 0  ioctl U 0  XEventsQueued U 0  XEventsQueued U Figure 2.20 Solaris 10 dtrace follows a system call within the kernel. In contrast, DTrace runs on production systemssystems that are running important or critical applicationsand causes no harm to the system. It slows activities while enabled, but after execution it resets the system to its predebugging state. It is also a broad and deep tool. It can broadly debug everything happening in the system (both at the user and kernel levels

in the system (both at the user and kernel levels and between the user and kernel layers). It ca na l s od e l v ed e e pi n t oc o d e ,s h o w i n g individual CPU instructions or kernel subroutine activities. DTrace is composed of a compiler, a framework, providers ofprobes written within that framework, and consumers of those probes. DTrace providers create probes. Kernel structures exist to keep track of all probes that the providers have created. The probes are stored in a hashtable data

created. The probes are stored in a hashtable data structure that is hashed by name and indexed according to unique probe identiers. When a probe is enabled, a bit of code in the area to be probed is rewritten to call dtrace probe(probe identifier) and then continue with the codes original operation. Different providers create different kinds of probes. For example, a kernel systemcall probe works differently from a userprocess probe, and that is different from an IOprobe. DTrace features a

is different from an IOprobe. DTrace features a compiler that generates a byte code that is run in the kernel. This code is assured to be safeby the compiler. For example, no loops are allowed, and only specic kernel state modications are allowed when specically requested. Only users with DTrace privileges (orrootusers)90 Chapter 2 OperatingSystem Structures are allowed to use DTrace, as it can retrieve private kernel data (and modify data if requested). The generated code r uns in the kernel

requested). The generated code r uns in the kernel and enables probes. It also enables consumers in user mode an de n a b l e sc o m m u n i c a t i o n sb e t w e e n the two. ADTrace consumer is code that is interested in a probe and its results. Ac o n s u m e rr e q u e s t st h a tt h ep r o v i d e rc r e a t eo n eo rm o r ep r o b e s .W h e na probe res, it emits data that are managed by the kernel. Within the kernel, actions called enabling control blocks ,o r ECBs, are performed when

control blocks ,o r ECBs, are performed when probes re. One probe can cause multiple ECBst oe x e c u t ei fm o r et h a no n ec o n s u m e r is interested in that probe. Each ECB contains a predicate ( if statement )t h a t can lter out that ECB.O t h e r w i s e ,t h el i s to fa c t i o n si nt h e ECB is executed. The most common action is to capture some bit of data, such as a variables value at that point of the probe execution. By gathering such data, a complete picture of au s e ro rk e

such data, a complete picture of au s e ro rk e r n e la c t i o nc a nb eb u i l t .F u r t h e r ,p r o b e s r i n gf r o mb o t hu s e rs p a c e and the kernel can show how a user level action caused kernellevel reactions. Such data are invaluable for performance monitoring and code optimization. Once the probe consumer terminates, its ECBsa r er e m o v e d .I ft h e r ea r en o ECBs consuming a probe, the probe is removed. That involves rewriting the code to remove the dtrace probe() call

the code to remove the dtrace probe() call and put back the original code. Thus, before a probe is created and after it is destroyed, the system is exactly the same, as if no probing occurred. DTrace takes care to assure that probes do not use too much memory or CPU capacity, which could harm the running system. The buffers used to hold the probe results are monitored for exceeding default and maximum limits. CPU time for probe execution is monitored as well. If limits are exceeded, the consumer

as well. If limits are exceeded, the consumer is terminated, along with the offending probes. Buffers are allocated per CPU to avoid contention and data loss. An example of D code and its output shows some of its utility. The following program shows the DTrace code to enable scheduler probes and record the amount of CPU time of each process running with user ID101 while those probes are enabled (that is, while the program runs): sched:::oncpu uid  101  selfts  timestamp;  sched:::offcpu selfts

101  selfts  timestamp;  sched:::offcpu selfts  time[execname]  sum(timestamp  selfts); selfts  0;  The output of the program, showing the processes and how much time (in nanoseconds) they spend running on the CPUs, is shown in Figure 2.21. Because DTrace is part of the opensource OpenSolaris version of the Solaris 10 operating system, it has been added to other operating systems when those2.9 OperatingSystem Generation 91 d t r a c e ss c h e d . d dtrace: script sched.d matched 6 probes C

e d . d dtrace: script sched.d matched 6 probes C gnomesettingsd 142354 gnomevfsdaemon 158243 dsdm 189804 wnckapplet 200030 gnomepanel 277864 clockapplet 374916 mappingdaemon 385475 xscreensaver 514177 metacity 539281 Xorg 2579646 gnometerminal 5007269 mixer applet2 7388447 java 10769137 Figure 2.21 Output of the D code. systems do not have conicting license agreements. For example, DTrace has been added to Mac OS X and FreeBSD and will likely spread further due to its unique capabilities. Other

further due to its unique capabilities. Other operating systems, especially the Linux derivatives, are adding kerneltracing functionality as well. Still other operating systems are beginning to include performance and tracing tools fostered by research at various institutions, including the Paradyn project. 2.9 OperatingSystem Generation It is possible to design, code, and implement an operating system specically for one machine at one site. More commonly, however, operating systems are designed

commonly, however, operating systems are designed to run on any of a class of machines at a variety of sites with a variety of peripheral congurations. The system must then be congured or generated for each specic computer site, a process sometimes known as system generation SYSGEN . The operating system is normally distributed on disk, on CDROM or DVDROM ,o ra sa n ISOimage, which is a le in the format of a CDROM orDVDROM .T og e n e r a t eas y s t e m ,w eu s eas p e c i a lp r o g r a m .T h

s t e m ,w eu s eas p e c i a lp r o g r a m .T h i s SYSGEN program reads from a given le, or asks the operator of the system for information concerning the specic conguration of the hardware system, or probes the hardware directly to d etermine what components are there. The following kinds of information must be determined. What CPU is to be used? What options (extended instruction sets, oating point arithmetic, and so on) are installed? For multiple CPU systems, each CPU may be described.

multiple CPU systems, each CPU may be described. How will the boot disk be formatted? How many sections, or partitions,  will it be separated into, and what will go into each partition?92 Chapter 2 OperatingSystem Structures How much memory is available? Some systems will determine this value themselves by referencing memory location after memory location until an illegal address fault is generated. This procedure denes the nal legal address and hence the amount of available memory. What devices

hence the amount of available memory. What devices are available? The system will need to know how to address each device (the device number), the devic ei n t e r r u p tn u m b e r ,t h ed e v i c e  s type and model, and any special device characteristics. What operatingsystem options are desired, or what parameter values are to be used? These options or values might include how many buffers of which sizes should be used, what type of CPUscheduling algorithm is desired, what the maximum

algorithm is desired, what the maximum number of processes to be supported is, and so on. Once this information is determined, it c an be used in several ways. At one extreme, a system administrator can use it to modify a copy of the source code of the operating system. The operating system then is completely compiled. Data declarations, initializations, and constants, along with conditional compilation, produce an outputobject version of the operating system that is tailored to the system

operating system that is tailored to the system described. At a slightly less tailored level, the system description can lead to the creation of tables and the selection of modules from a precompiled library. These modules are linked together to form the generated operating system. Selection allows the library to contain the device drivers for all supported IO devices, but only those needed are linked into the operating system. Because the system is not recompiled, system generation is faster,

is not recompiled, system generation is faster, but the resulting system may be overly general. At the other extreme, it is possible to construct a system that is completely table driven. All the code is always part of the system, and selection occurs at execution time, rather than at compile or link time. System generation involves simply creating the appropriate tables to describe the system. The major differences among these approaches are the size and generality of the generated system and

size and generality of the generated system and the ease of modifying it as the hardware conguration changes. Consider the cost of modifying the system to support a newly acquired graphics terminal or another disk drive. Balanced against that cost, of course, is the frequency (or infrequency) of such changes. 2.10 System Boot After an operating system is generated, it must be made available for use by the hardware. But how does the hardware know where the kernel is or how to load that kernel?

where the kernel is or how to load that kernel? The procedure of starting a computer by loading the kernel is known as booting the system. On most computer systems, a small piece of code known as the bootstrap program orbootstrap loader locates the kernel, loads it into main memory, and starts its execution. Some computer systems, such as PCs, use a twostep process in which a simple bootstrap loader fetches am o r ec o m p l e xb o o tp r o g r a mf r o md i s k ,w h i c hi nt u r nl o a d st h

a mf r o md i s k ,w h i c hi nt u r nl o a d st h ek e r n e l . When a CPU receives a reset eventfor instance, when it is powered up or rebootedthe instruction register is loaded with a predened memory2.11 Summary 93 location, and execution starts there. At that location is the initial bootstrap program. This program is in the form of readonly memory ( ROM ), because the RAM is in an unknown state at system startup. ROM is convenient because it needs no initialization and cannot easily be

it needs no initialization and cannot easily be infected by a computer virus. The bootstrap program can perform a variety of tasks. Usually, one task is to run diagnostics to determine the state of the machine. If the diagnostics pass, the program can continue with the booting steps. It can also initialize all aspects of the system, from CPU registers to device controllers and the contents of main memory. Sooner or later, it starts the operating system. Some systemssuch as cellular phones,

system. Some systemssuch as cellular phones, tablets, and game consolesstore the entire operating system in ROM .S t o r i n gt h eo p e r a t i n gs y s t e mi n ROM is suitable for small operating systems, simple supporting hardware, and rugged operation. A problem with this approach is that changing the bootstrap code requires changing the ROM hardware chips. Some systems resolve this problem by using erasable programmable readonly memory (EPROM ),w h i c hi sr e a d  only except when

(EPROM ),w h i c hi sr e a d  only except when explicitly given a command to become writable. All forms ofROM are also known as rmware ,s i n c et h e i rc h a r a c t e r i s t i c sf a l ls o m e w h e r e between those of hardware and those of software. A problem with rmware in general is that executing code the re is slower than executing code in RAM . Some systems store the operating system in rmware and copy it to RAM for fast execution. A nal issue with rmware is that it is relatively

A nal issue with rmware is that it is relatively expensive, so usually only small amounts are available. For large operating systems (including most generalpurpose operating systems like Windows, Mac OS X ,a n d UNIX )o rf o rs y s t e m st h a tc h a n g e frequently, the bootstrap loader is stored in rmware, and the operating system is on disk. In this case, the bootstrap runs diagnostics and has a bit of code that can read a single block at a xed location (say block zero) from disk into

at a xed location (say block zero) from disk into memory and execute the code from that boot block . The program stored in the boot block may be sophisticated enough to load the entire operating system into memory and begin its execution. More typically, it is simple code (as it ts in a single disk block) and knows only the address on disk and length of the remainder of the bootstrap program. GRUB is an example of an opensource bootstrap program for Linux systems. All of the diskbound bootstrap,

for Linux systems. All of the diskbound bootstrap, and the operating system itself, can be easily changed by writing new versions to disk. A disk that has a boot partition (more on that in Section 10.5.1) is called a boot disk orsystem disk . Now that the full bootstrap program has been loaded, it can traverse the le system to nd the operating system kernel, load it into memory, and start its execution. It is only at this point that the system is said to be running . 2.11 Summary Operating

is said to be running . 2.11 Summary Operating systems provide a number of services. At the lowest level, system calls allow a running program to make requests from the operating system directly. At a higher level, the comman di n t e r p r e t e ro rs h e l lp r o v i d e sa mechanism for a user to issue a request without writing a program. Commands may come from les during batchmode execution or directly from a terminal or desktop GUIwhen in an interactive or timeshared mode. System programs

an interactive or timeshared mode. System programs are provided to satisfy many common user requests.94 Chapter 2 OperatingSystem Structures The types of requests vary according to level. The systemcall level must provide the basic functions, such as process control and le and device manipulation. Higherlevel requests, satis ed by the command interpreter or system programs, are translated into a sequence of system calls. System services can be classied into several categories: program control,

classied into several categories: program control, status requests, and IOrequests. Program errors can be considered implicit requests for service. The design of a new operating system is a major task. It is important that the goals of the system be well dened before the design begins. The type of system desired is the foundation for choices among various algorithms and strategies that will be needed. Throughout the entire design cycle, we must be careful to separate policy decisions from

must be careful to separate policy decisions from implementation details (mechanisms). This separation allows maximum exibility if policy decisions are to be changed later. Once an operating system is designed, it must be implemented. Oper ating systems today are almost always written in a systemsimplementation language or in a higherlevel lang uage. This feature improves their implemen tation, maintenance, and portability. As y s t e ma sl a r g ea n dc o m p l e xa sam o d e r no p e r a t i n

ea n dc o m p l e xa sam o d e r no p e r a t i n gs y s t e mm u s t be engineered carefully. Modularity is important. Designing a system as a sequence of layers or using a microkernel is considered a good technique. Many operating systems now support dynamically loaded modules, which allow adding functionality to an operating system while it is executing. Generally, operating systems adopt a hybrid approach that combines several different types of structures. Debugging process and kernel

types of structures. Debugging process and kernel failures can be accomplished through the use of debuggers and other tools that analyze core dumps. Tools such as DTrace analyze production systems to nd bottlenecks and understand other system behavior. To create an operating system for a particular machine conguration, we must perform system generation. For the computer system to begin running, theCPU must initialize and start executing the bootstrap program in rmware. The bootstrap can execute

program in rmware. The bootstrap can execute the operating system directly if the operating system is also in the rmware, or it can complete a sequence in which it loads progressively smarter programs from rmware and disk until the operating system itself is loaded into memory and executed. Practice Exercises 2.1 What is the purpose of system calls? 2.2 What are the ve major activities of an operating system with regard to process management? 2.3 What are the three major activities of an

2.3 What are the three major activities of an operating system with regard to memory management? 2.4 What are the three major activities of an operating system with regard to secondarystorage management? 2.5 What is the purpose of the command interpreter? Why is it usually separate from the kernel?Exercises 95 2.6 What system calls have to be executed by a command interpreter or shell in order to start a new process? 2.7 What is the purpose of system programs? 2.8 What is the main advantage of

system programs? 2.8 What is the main advantage of the layered approach to system design? What are the disadvantages of the layered approach? 2.9 List ve services provided by an operating system, and explain how each creates convenience for users. In which cases would it be impossible for userlevel programs to provide these services? Explain your answer. 2.10 Why do some systems store the operating system in rmware, while others store it on disk? 2.11 How could a system be designed to allow a

2.11 How could a system be designed to allow a choice of operating systems from which to boot? What would the bootstrap program need to do? Exercises 2.12 The services and functions provided by an operating system can be divided into two main categories. Briey describe the two categories, and discuss how they differ. 2.13 Describe three general methods for passing parameters to the operating system. 2.14 Describe how you could obtain a statistical prole of the amount of time spent by a program

prole of the amount of time spent by a program executing different sections of its code. Discuss the importance of obtaining such a statistical prole. 2.15 What are the ve major activities of an operating system with regard to le management? 2.16 What are the advantages and disadvantages of using the same system call interface for manipulating both les and devices? 2.17 Would it be possible for the user to develop a new command interpreter using the systemcall interface provided by the operating

the systemcall interface provided by the operating system? 2.18 What are the two models of interprocess communication? What are the strengths and weaknesses of the two approaches? 2.19 Why is the separation of mechanism and policy desirable? 2.20 It is sometimes difcult to achieve a layered approach if two components of the operating system are dependent on each other. Identify a scenario in which it is unclear how to layer two system components that require tight coupling of their

components that require tight coupling of their functionalities. 2.21 What is the main advantage of the microkernel approach to system design? How do user programs and system services interact in a microkernel architecture? What are the disadvantages of using the microkernel approach? 2.22 What are the advantages of using loadable kernel modules?96 Chapter 2 OperatingSystem Structures 2.23 How are i OSand Android similar? How are they different? 2.24 Explain why Java programs running on Android

2.24 Explain why Java programs running on Android systems do not use the standard Java APIand virtual machine. 2.25 The experimental Synthesis operating s ystem has an assembler incor porated in the kernel. To optimize systemcall performance, the kernel assembles routines within kernel space to minimize the path that the system call must take through the kernel. This approach is the antithesis of the layered approach, in which the path through the kernel is extended to make building the

the kernel is extended to make building the operating system easier. Discuss the pros and cons of the Synthesis approach to kernel design and systemperformance optimization. Programming Problems 2.26 In Section 2.3, we described a program that copies the contents of one le to a destination le. This program works by rst prompting the user for the name of the source and destination les. Write this program using either the Windows or POSIX API . Be sure to include all necessary error checking,

. Be sure to include all necessary error checking, including ensuring that the source le exists. Once you have correctly designed and tested the program, if you used a system that supports it, run the program using a utility that traces system calls. Linux systems provide the strace utility, and Solaris and Mac OS X systems use the dtrace command. As Windows systems do not provide such features, you will have to trace through the Windows version of this program using a debugger. Programming

of this program using a debugger. Programming Projects Linux Kernel Modules In this project, you will learn how to create a kernel module and load it into the Linux kernel. The project can be completed using the Linux virtual machine that is available with this text. Although you may use an editor to write these Cp r o g r a m s ,y o uw i l lh a v et ou s et h e terminal application to compile the programs, and you will have to enter commands on the command line to manage the modules in the

on the command line to manage the modules in the kernel. As youll discover, the advantage of d eveloping kernel modules is that it is a relatively easy method of interacting wit ht h ek e r n e l ,t h u sa l l o w i n gy o ut o write programs that directly invoke kernel functions. It is important for you to keep in mind that you are indeed writing kernel code that directly interacts with the kernel. That normally means that any errors in the code could crash the system! However, since you will

could crash the system! However, since you will be using a virtual machine, any failures will at worst only require rebooting the system. Part ICreating Kernel Modules The rst part of this project involves following a series of steps for creating and inserting a module into the Linux kernel.Programming Projects 97 You can list all kernel modules that are currently loaded by entering the command lsmod This command will list the current kernel modules in three columns: name, size, and where the

in three columns: name, size, and where the module is being used. The following program (named simple.c and available with the source code for this text) illustrates a very basic kernel module that prints appropriate messages when the kernel module is loaded and unloaded. include linuxinit.h include linuxkernel.h include linuxmodule.h  This function is called when the module is loaded.  int simple init(void)  printk(KERN INFO Loading Module n); return 0;   This function is called when the module

0;   This function is called when the module is removed.  void simple exit(void)  printk(KERN INFO Removing Module n);   Macros for registering module entry and exit points.  module init(simple init); module exit(simple exit); MODULE LICENSE(GPL); MODULE DESCRIPTION(Simple Module); MODULE AUTHOR(SGG); The function simple init() is the module entry point ,w h i c hr e p r e s e n t s the function that is invoked when the mo dule is loaded into the kernel. Similarly, the simple exit() function is

kernel. Similarly, the simple exit() function is the module exit point the function that is called when the module is removed from the kernel. The module entry point function m ust return an integer value, with 0 representing success and any other value representing failure. The module exit point function returns void .N e i t h e rt h em o d u l ee n t r yp o i n tn o rt h em o d u l e exit point is passed any parameters. The two following macros are used for registering the module entry and

are used for registering the module entry and exit points with the kernel: module init() module exit()98 Chapter 2 OperatingSystem Structures Notice how both the module entry and exit point functions make calls to the printk() function. printk() is the kernel equivalent of printf() , yet its output is sent to a kernel log buffer whose contents can be read by thedmesg command. One difference between printf() andprintk() is that printk() allows us to specify a priority ag whose values are given in

to specify a priority ag whose values are given in the linuxprintk.h include le. In this instance, the priority is KERN INFO , which is dened as an informational message. The nal lines MODULE LICENSE() ,MODULE DESCRIPTION() ,a n d MOD ULE AUTHOR() represent details regarding the software license, description of the module, and author. For our purposes, we do not depend on this information, but we include it because it is standard practice in developing kernel modules. This kernel module simple.c

kernel modules. This kernel module simple.c is compiled using the Makefile accom panying the source code with this project. To compile the module, enter the following on the command line: make The compilation produces several les. The le simple.ko represents the compiled kernel module. The following step illustrates inserting this module into the Linux kernel. Loading and Removing Kernel Modules Kernel modules are loaded using the insmod command, which is run as follows: sudo insmod simple.ko To

which is run as follows: sudo insmod simple.ko To check whether the module has loaded, enter the lsmod command and search for the module simple . Recall that the module entry point is invoked when the module is inserted into the kernel. To check the contents of this message in the kernel log buffer, enter the command dmesg You should see the message  Loading Module.  Removing the kernel module involves invoking the rmmod command (notice that the .kosufx is unnecessary): sudo rmmod simple Be sure

.kosufx is unnecessary): sudo rmmod simple Be sure to check with the dmesg command to ensure the module has been removed. Because the kernel log buffer can ll up quickly, it often makes sense to clear the buffer periodically. This can be accomplished as follows: sudo dmesg cProgramming Projects 99 Part I Assignment Proceed through the steps described ab ove to create the kernel module and to load and unload the module. Be sure to che ck the contents of the kernel log buffer using dmesg to ensure

of the kernel log buffer using dmesg to ensure you have properly followed the steps. Part IIKernel Data Structures The second part of this projec t involves modifying the kernel module so that it uses the kernel linkedlist data structure. In Section 1.10, we covered various data structures that are common in operating systems. The Linux kernel provides several of these structures. Here, we explore using the circular, doubly linked list that is available to kernel developers. Much of what we

is available to kernel developers. Much of what we discuss is available in the Linux source code in this instance, the include le linuxlist.h and we recommend that you examine this le as you proceed through the following steps. Initially, you must dene a struct containing the elements that are to be inserted in the linked list. The following C struct denes birthdays: struct birthday  int day; int month; int year; struct list head list;  Notice the member struct list head list .T h e list head

the member struct list head list .T h e list head structure is dened in the include le linuxtypes.h .I t si n t e n t i o ni st oe m b e dt h e linked list within the nodes that comprise the list. This list head structure is quite simpleit merely holds two members, next andprev ,t h a tp o i n tt ot h e next and previous entries in the list. By embedding the linked list within the structure, Linux makes it possible to manage the data structure with a series of macro functions. Inserting Elements

a series of macro functions. Inserting Elements into the Linked List We can declare a list head object, which we use as a reference to the head of the list by using the LIST HEAD() macro static LIST HEAD(birthday list); This macro denes and initializes the variable birthday list ,w h i c hi so ft y p e struct list head .100 Chapter 2 OperatingSystem Structures We create and initialize instances of struct birthday as follows: struct birthday person; person  kmalloc(sizeof(person), GFP KERNEL);

person  kmalloc(sizeof(person), GFP KERNEL); personday  2; personmonth 8; personyear  1995; INIT LIST HEAD(personlist); The kmalloc() function is the kernel equivalent of the userlevel malloc() function for allocating memory, except that kernel memory is being allocated. (The GFP KERNEL ag indicates routine kernel memory allocation.) The macro INIT LIST HEAD() initializes the list member in struct birthday .W ec a n then add this instance to the end of the linked list using the list add tail()

end of the linked list using the list add tail() macro: list add tail(personlist, birthday list); Traversing the Linked List Traversing the list involves using the list for each entry() Macro, which accepts three parameters: Ap o i n t e rt ot h es t r u c t u r eb e i n gi t e r a t e do v e r Ap o i n t e rt ot h eh e a do ft h el i s tb e i n gi t e r a t e do v e r The name of the variable containing the list head structure The following code illustrates this macro: struct birthday ptr; list

illustrates this macro: struct birthday ptr; list for each entry(ptr, birthday list, list)   on each iteration ptr points   to the next birthday struct   Removing Elements from the Linked List Removing elements from the list involves using the list del() macro, which is passed a pointer to struct list head list del(struct list head element) This removes element from the list while maintaining the structure of the remainder of the list. Perhaps the simplest approach for removing all elements from

simplest approach for removing all elements from a linked list is to remove each element as you traverse the list. The macro list for each entry safe() behaves much like list for each entry()Bibliographical Notes 101 except that it is passed an additional argument that maintains the value of the next pointer of the item being deleted. (This is necessary for preserving the structure of the list.) The following code example illustrates this macro: struct birthday ptr, next list for each entry

struct birthday ptr, next list for each entry safe(ptr,next,birthday list,list)   on each iteration ptr points   to the next birthday struct  list del(ptrlist); kfree(ptr);  Notice that after deleting each element, we return memory that was previously allocated with kmalloc() back to the kernel with the call to kfree() . Careful memory managementwhich includes releasing memory to prevent memory leaks is crucial when developing kernellevel code. Part II Assignment In the module entry point,

Part II Assignment In the module entry point, create a linked list containing ve struct birthday elements. Traverse the linked list and output its co ntents to the kernel log buffer. Invoke the dmesg command to ensure the list is properly constructed once the kernel module has been loaded. In the module exit point, delete the elemen ts from the linked list and return the free memory back to the kernel. Again, invoke the dmesg command to check that the list has been removed once the kernel module

the list has been removed once the kernel module has been unloaded. Bibliographical Notes [Dijkstra (1968)] advocated the layered approach to operatingsystem design. [BrinchHansen (1970)] was an early proponent of constructing an operating system as a kernel (or nucleus) on which more complete systems could be built. [Tarkoma and Lagerspetz (2011)] provide an overview of various mobile operating systems, including Android and i OS. MSDOS , Version 3.1, is described in [Microsoft (1986)]. Windows

3.1, is described in [Microsoft (1986)]. Windows NT and Windows 2000 are described by [Solomon (1998)] and [Solomon and Russinovich (2000)]. Windows XPinternals are described in [Russinovich and Solomon (2009)]. [Hart (2005)] covers Windows systems programming in detail. BSD UNIX is described in [McKusick et al. (1996)]. [Love (2010)] and [Mauerer (2008)] thoroughly discuss the Linux kernel. In particular, [Love (2010)] covers Linux kernel modules as well as kernel data structures. Several UNIX

as well as kernel data structures. Several UNIX systemsincluding Machare treated in detail in [Vahalia (1996)]. Mac OS X is presented at http:www.apple.commacosx and in [Singh (2007)]. Solaris is fully described in [McDougall and Mauro (2007)]. DTrace is discussed in [Gregg and Mauro (2011)]. The DTrace source code is available at http:src.opensolaris.orgsource .102 Chapter 2 OperatingSystem Structures Bibliography [BrinchHansen (1970)] P. B r i n c h  H a n s e n , The Nucleus of a Multiprogram

n c h  H a n s e n , The Nucleus of a Multiprogram ming System ,Communications of the ACM ,V o l u m e1 3 ,N u m b e r4( 1 9 7 0 ) ,p a g e s 238241 and 250. [Dijkstra (1968)] E. W. Dijkstra, The Structure of the THE Multiprogramming System ,Communications of the ACM ,V o l u m e1 1 ,N u m b e r5( 1 9 6 8 ) ,p a g e s 341346. [Gregg and Mauro (2011)] B. Gregg and J. Mauro, DTraceDynamic Tracing in Oracle Solaris, Mac OS X, and FreeBSD ,P r e n t i c eH a l l( 2 0 1 1 ) . [Hart (2005)] J. M.

n t i c eH a l l( 2 0 1 1 ) . [Hart (2005)] J. M. Hart, Windows System Programming, Third Edition, Addison Wesley (2005). [Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developers Library (2010). [Mauerer (2008)] W. Mauerer, Professional Linux Kernel Architecture ,J o h nW i l e y and Sons (2008). [McDougall and Mauro (2007)] R. McDougall and J. Mauro, Solaris Internals, Second Edition, Prentice Hall (2007). [McKusick et al. (1996)] M. K. McKusick, K. Bostic, and M. J. Karels,

M. K. McKusick, K. Bostic, and M. J. Karels, The Design and Implementation of the 4.4 BSD UNIX Operating System ,J o h nW i l e ya n dS o n s (1996). [Microsoft (1986)] Microsoft MSDOS Users Reference and Microsoft MSDOS Programmers Reference .M i c r o s o f tP r e s s( 1 9 8 6 ) . [Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon, Win dows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition, Microsoft Press (2009). [Singh (2007)] A. Singh, Mac OS X

Press (2009). [Singh (2007)] A. Singh, Mac OS X Internals: A Systems Approach ,A d d i s o n  Wesley (2007). [Solomon (1998)] D. A. Solomon, Inside Windows NT, Second Edition, Microsoft Press (1998). [Solomon and Russinovich (2000)] D. A. Solomon and M. E. Russinovich, Inside Microsoft Windows 2000, Third Edition, Microsoft Press (2000). [Tarkoma and Lagerspetz (2011)] S. Tarkoma and E. Lagerspetz, Arching over the Mobile Computing Chasm: Platforms and Runtimes ,IEEE Computer , Volume 44,

Platforms and Runtimes ,IEEE Computer , Volume 44, (2011), pages 2228. [Vahalia (1996)] U. Vahalia, Unix Internals: The New Frontiers ,P r e n t i c eH a l l (1996).Part Two Process Management Aprocess can be thought of as a program in execution. A process will need certain resourcessuch as CPU time, memory, les, and IOdevices to accomplish its task. These resources are allocated to the process either when it is created or while it is executing. Ap r o c e s si st h eu n i to fw o r ki nm o s ts

Ap r o c e s si st h eu n i to fw o r ki nm o s ts y s t e m s .S y s t e m sc o n s i s to f ac o l l e c t i o no fp r o c e s s e s :o p e r a t i n g  s y s t e mp r o c e s s e se x e c u t es y s t e m code, and user processes execute user code. All these processes may execute concurrently. Although traditionally a proce ss contained only a single thread of control as it ran, most modern operating systems now support processes that have multiple threads. The operating system is responsible

threads. The operating system is responsible for several important aspects of process and thread management: the creation and deletion of both user and system processes; the scheduling of processes; and the provision of mechanisms for synchronization, c ommunication, and deadlock handling for processes.3CHAPTER Processes Early computers allowed only one program to be executed at a time. This program had complete control of the system and had access to all the systems resources. In contrast,

access to all the systems resources. In contrast, contemporary computer systems allow multiple pro grams to be loaded into memory and executed concurrently. This evolution required rmer control and more compartmentalization of the various pro grams; and these needs resulted in the notion of a process ,w h i c hi sap r o g r a m in execution. A process is the unit of work in a modern timesharing system. The more complex the operating system is, the more it is expected to do on behalf of its

is, the more it is expected to do on behalf of its users. Although its main concern is the execution of user programs, it also needs to take care of various system tasks that are better left outside the kernel itself. A system therefore consists of a collection of processes: operating system processes executing system code and user processes executing user code. Potentially, all these processes ca ne x e c u t ec o n c u r r e n t l y ,w i t ht h e CPU (or CPUs) multiplexed among them. By

t ht h e CPU (or CPUs) multiplexed among them. By switching the CPU between processes, the operating system can make the computer more productive. In this chapter, you will read about what processes are and how they work. CHAPTER OBJECTIVES To introduce the notion of a process  a program in execution, which forms the basis of all computation. To describe the various features of processes, including scheduling, creation, and termination. To explore interprocess communication using shared memory

interprocess communication using shared memory and mes sage passing. To describe communication in client  server systems. 3.1 Process Concept Aq u e s t i o nt h a ta r i s e si nd i s c u s s i n go p e r a t i n gs y s t e m si n v o l v e sw h a tt oc a l l all the CPU activities. A batch system executes jobs ,w h e r e a sat i m e  s h a r e d 105106 Chapter 3 Processes system has user programs ,o rtasks .E v e no nas i n g l e  u s e rs y s t e m ,au s e rm a y be able to run several

rs y s t e m ,au s e rm a y be able to run several programs at one time: a word processor, a Web browser, and an email package. And even if a user can execute only one program at a time, such as on an embedded device that does not support multitasking, the operating system may need to support its own internal programmed activities, such as memory management. In many respects, all these activities are similar, so we call all of them processes . The terms joband process are used almost

. The terms joband process are used almost interchangeably in this text. Although we personally prefer the term process ,m u c ho fo p e r a t i n g  s y s t e m theory and terminology was developed during a time when the major activity of operating systems was job processing. It would be misleading to avoid the use of commonly accepted terms that include the word job(such as job scheduling )s i m p l yb e c a u s e process has superseded job. 3.1.1 The Process Informally, as mentioned earlier,

The Process Informally, as mentioned earlier, a process is a program in execution. A process is more than the program code, which is sometimes known as the text section . It also includes the current activity, as represented by the value of the program counter and the contents of the processors registers. A process generally also includes the process stack ,w h i c hc o n t a i n st e m p o r a r yd a t a( s u c ha sf u n c t i o n parameters, return addresses, and local variables), and a data

return addresses, and local variables), and a data section ,w h i c h contains global variables. A process may also include a heap ,w h i c hi sm e m o r y that is dynamically allocated during process run time. The structure of a process in memory is shown in Figure 3.1. We emphasize that a program by itself is not a process. A program is a passive entity, such as a le containing a list of instructions stored on disk (often called an executable le ). In contrast, a process is an active entity,

le ). In contrast, a process is an active entity, with a program counter specifying the next instruction to execute and a set of associated resources. A program becomes a process when an executable le is loaded into memory. Two common tec hniques for loading executable les text 0max dataheapstack Figure 3.1 Process in memory.3.1 Process Concept 107 are doubleclicking an icon representing the executable le and entering the name of the executable le on the command line (as in prog.exe ora.out ).

le on the command line (as in prog.exe ora.out ). Although two processes may be associated with the same program, they are nevertheless considered two separate exec ution sequences. For instance, several users may be running different copies of the mail program, or the same user may invoke many copies of the web browser program. Each of these is a separate process; and although the text sections are equivalent, the data, heap, and stack sections vary. It is also common to have a process that

vary. It is also common to have a process that spawns many processes as it runs. We discuss such matters in Section 3.4. Note that a process itself can be an execution environment for other code. The Java programming environment provides a good example. In most circumstances, an executable Java program is executed within the Java virtual machine ( JVM). The JVM executes as a process that interprets the loaded Java code and takes actions (via native machine instructions) on behalf of that code.

machine instructions) on behalf of that code. For example, to run the compiled Java program Program.class , we would enter java Program The command javaruns the JVM as an ordinary process, which in turns executes the Java program Programin the virtual machine. The concept is the same as simulation, except that the code, instead of being written for a different instruction set, is written in the Java language. 3.1.2 Process State As a process executes, it changes state .T h es t a t eo fap r o c

it changes state .T h es t a t eo fap r o c e s si sd e  n e di np a r t by the current activity of that process. A process may be in one of the following states: New .T h ep r o c e s si sb e i n gc r e a t e d . Running .I n s t r u c t i o n sa r eb e i n ge x e c u t e d . Waiting .T h ep r o c e s si sw a i t i n gf o rs o m ee v e n tt oo c c u r( s u c ha sa n IO completion or reception of a signal). Ready . The process is waiting to be assigned to a processor. Terminated .T h ep r o c e

to a processor. Terminated .T h ep r o c e s sh a s n i s h e de x e c u t i o n . These names are arbitrary, and they vary across operating systems. The states that they represent are found on all systems, however. Certain operating systems also more nely delineate process states. It is important to realize that only one process can be running on any processor at any instant. Many processes may be ready and waiting, however. The state diagram corresponding to these states is presented in Figure

to these states is presented in Figure 3.2. 3.1.3 Process Control Block Each process is represented in the operating system by a process control block (PCB)alsocalleda task control block .APCBis shown in Figure 3.3. It contains many pieces of information associated with a specic process, including these:108 Chapter 3 Processesnew terminated running readyadmitted interrupt scheduler dispatchIO or event completion IO or event waitexit waiting Figure 3.2 Diagram of process state. Process state .T h

3.2 Diagram of process state. Process state .T h es t a t em a yb en e w ,r e a d y ,r u n n i n g ,w a i t i n g ,h a l t e d ,a n d so on. Program counter .T h ec o u n t e ri n d i c a t e st h ea d d r e s so ft h en e x ti n s t r u c t i o n to be executed for this process. CPU registers .T h er e g i s t e r sv a r yi nn u m b e ra n dt y p e ,d e p e n d i n go n the computer architecture. They include accumulators, index registers, stack pointers, and generalpurpose registers, plus any

pointers, and generalpurpose registers, plus any conditioncode information. Along with the program counter, this state information must be saved when an interrupt occurs, to allow the process to be continued correctly afterward (Figure 3.4). CPUscheduling information .T h i si n f o r m a t i o ni n c l u d e sap r o c e s sp r i o r i t y , pointers to scheduling queues, and any other scheduling parameters. (Chapter 6 describes process scheduling.) Memorymanagement information .T h i si n f o r

Memorymanagement information .T h i si n f o r m a t i o nm a yi n c l u d es u c h items as the value of the base and limit registers and the page tables, or the segment tables, depending on the memory system used by the operating system (Chapter 8). process state process number program counter memory limits list of open filesregisters    Figure 3.3 Process control block (PCB).3.1 Process Concept 109process P0 process P1 save state into PCB0 save state into PCB1reload state from PCB1 reload

save state into PCB1reload state from PCB1 reload state from PCB0operating system idle idleexecuting idleexecuting executinginterrupt or system call interrupt or system call      Figure 3.4 Diagram showing CPU switch from process to process. Accounting information .T h i si n f o r m a t i o ni n c l u d e st h ea m o u n to f CPU and real time used, time limits, account numbers, job or process numbers, and so on. IOstatus information .T h i si n f o r m a t i o ni n c l u d e st h el i s to f

n f o r m a t i o ni n c l u d e st h el i s to f IOdevices allocated to the process, a list of open les, and so on. In brief, the PCB simply serves as the repository for any information that may vary from process to process. 3.1.4 Threads The process model discussed so far has implied that a process is a program that performs a single thread of execution. For example, when a process is running aw o r d  p r o c e s s o rp r o g r a m ,as i n g l et h r e a do fi n s t r u c t i o n si sb e i n

l et h r e a do fi n s t r u c t i o n si sb e i n ge x e c u t e d . This single thread of control allows the process to perform only one task at at i m e .T h eu s e rc a n n o ts i m u l t a n e o u s l yt y p ei nc h a r a c t e r sa n dr u nt h es p e l l checker within the same process, for example. Most modern operating systems have extended the process concept to allow a process to have multiple threads of execution and thus to perform more than one task at a time. This feature is

more than one task at a time. This feature is especially benecial on multicore systems, where multiple threads can run in parallel. On a system that supports threads, the PCB is expanded to include information for each thread. Other changes throughout the system are also needed to support threads. Chapter 4 explores threads in detail.110 Chapter 3 Processes PROCESS REPRESENTATION IN LINUX The process control block in the Linux operating system is represented by the C structure task struct ,w h i

represented by the C structure task struct ,w h i c hi sf o u n di nt h e linuxsched.h include le in the kernel sourcecode directory. This structure contains all the necessary information for representing a process, including the state of the process, scheduling and memorymanagement information, list of open les, and pointers to the processs parent and a list of its children and siblings. (A processs parent is the process that created it; its children are any processes that it creates. Its

children are any processes that it creates. Its siblings are children with the same parent process.) Some of these elds include: long state;  state of the process  struct sched entity se;  scheduling information  struct task struct parent;  this processs parent  struct list head children;  this processs children  struct files struct files;  list of open files  struct mm struct mm;  address space of this process  For example, the state of a process is represented by the eld long state in this

is represented by the eld long state in this structure. Within the Linux kernel, all active processes are represented using a doubly linked list of task struct .T h ek e r n e lm a i n t a i n sap o i n t e r current totheprocesscurrentlyexecutingonthesystem,asshownbelow:struct taskstruct process information   struct taskstruct process information    current (currently executing proccess)struct taskstruct process information      As an illustration of how the kernel might manipulate one of the

of how the kernel might manipulate one of the elds in thetask struct for a specied process, lets assume the system would like to change the state of the process currently running to the value new state . Ifcurrent is a pointer to the process currently executing, its state is changed with the following: currentstate  new state; 3.2 Process Scheduling The objective of multiprogramming is to have some process running at all times, to maximize CPU utilization. The objective of time sharing is to

utilization. The objective of time sharing is to switch the CPU among processes so frequently that users can interact with each program3.2 Process Scheduling 111queue header PCB7 PCB3 PCB5PCB14 PCB6PCB2 head head head head headready queue disk unit 0 terminal unit 0mag tape unit 0 mag tape unit 1tail registers registers tail tail tail tail        Figure 3.5 The ready queue and various IO device queues. while it is running. To meet these objectives, the process scheduler selects an available

the process scheduler selects an available process (possibly from a set of several available processes) for program execution on the CPU.F o ras i n g l e  p r o c e s s o rs y s t e m ,t h e r ew i l ln e v e r be more than one running process. If there are more processes, the rest will have to wait until the CPU is free and can be rescheduled. 3.2.1 Scheduling Queues As processes enter the system, they are put into a job queue ,w h i c hc o n s i s t s of all processes in the system. The

o n s i s t s of all processes in the system. The processes that are residing in main memory and are ready and waiting to execute are kept on a list called the ready queue . This queue is generally stored as a linked list. A readyqueue header contains pointers to the rst and nal PCBsi nt h el i s t .E a c h PCB includes a pointer eld that points to the next PCB in the ready queue. The system also includes other queues. When a process is allocated the CPU, it executes for a while and eventually

the CPU, it executes for a while and eventually quits, is interrupted, or waits for the occurrence of a particular event, such as the completion of an IOrequest. Suppose the process makes an IOrequest to a shared device, such as a disk. Since there are many processes in the system, the disk may be busy with the IOrequest of some other process. The process therefore may have to wait for the disk. The list of processes waiting for a particular IOdevice is called a device queue .E a c hd e v i c eh

is called a device queue .E a c hd e v i c eh a si t so w nd e v i c eq u e u e( F i g u r e3 . 5 ) .112 Chapter 3 Processesready queue CPU IO IO queue IO request time slice expired fork a child wait for an interruptinterrupt occurschild executes Figure 3.6 Queueingdiagram representation of process scheduling. Ac o m m o nr e p r e s e n t a t i o no fp r o c e s ss c h e d u l i n gi sa queueing diagram , such as that in Figure 3.6. Each rectangular box represents a queue. Two types of queues

box represents a queue. Two types of queues are present: the ready queue and a set of device queues. The circles represent the resources that serve the queues, and the arrows indicate the ow of processes in the system. An e wp r o c e s si si n i t i a l l yp u ti nt h er e a d yq u e u e .I tw a i t st h e r eu n t i li ti s selected for execution, or dispatched . Once the process is allocated the CPU and is executing, one of several events could occur: The process could issue an IOrequest and

occur: The process could issue an IOrequest and then be placed in an IOqueue. The process could create a new child process and wait for the childs termination. The process could be removed forcibly from the CPU,a sar e s u l to fa n interrupt, and be put back in the ready queue. In the rst two cases, the process eventually switches from the waiting state to the ready state and is then put back in the ready queue. A process continues this cycle until it terminates, at which time it is removed

until it terminates, at which time it is removed from all queues and has its PCB and resources deallocated. 3.2.2 Schedulers A process migrates among the various scheduling queues throughout its lifetime. The operating system must select, for scheduling purposes, processes from these queues in some fashion. The selection process is carried out by the appropriate scheduler . Often, in a batch system, more processes are submitted than can be executed immediately. These processes are spooled to a

immediately. These processes are spooled to a massstorage device (typically a disk), where they are kept for later execution. The longterm scheduler ,o rjob scheduler ,s e l e c t sp r o c e s s e sf r o mt h i sp o o la n dl o a d st h e mi n t om e m o r yf o r3.2 Process Scheduling 113 execution. The shortterm scheduler ,o r CPU scheduler , selects from among the processes that are ready to execute and allocates the CPU to one of them. The primary distinction between these two schedulers lies

distinction between these two schedulers lies in frequency of execution. The shortterm scheduler must select a new process for the CPU frequently. A process may execute for only a few milliseconds before waiting for an IOrequest. Often, the shortterm scheduler executes at least once every 100 milliseconds. Because of the short time between executions, the shortterm scheduler must be fast. If it takes 10 milliseconds to decide to execute a process for 100 milliseconds, then 10(100  10)  9 percent

for 100 milliseconds, then 10(100  10)  9 percent of the CPU is being used (wasted) simply for scheduling the work. The longterm scheduler executes much less frequently; minutes may sep arate the creation of one new process and the next. The longterm scheduler controls the degree of multiprogramming (the number of processes in mem ory). If the degree of multiprogramming is stable, then the average rate of process creation must be equal to the average departure rate of processes leaving the

average departure rate of processes leaving the system. Thus, the longterm scheduler may need to be invoked only when a process leaves the system. Because of the longer interval between executions, the longterm scheduler can a fford to take more time to decide which process should be selected for execution. It is important that the longterm scheduler make a careful selection. In general, most processes can be described as either IObound or CPU bound. An IObound process is one that spends more of

An IObound process is one that spends more of its time doing IO than it spends doing computations. A CPUbound process ,i nc o n t r a s t ,g e n e r a t e s IO requests infrequently, using more of its time doing computations. It is important that the longterm scheduler select a good process mix ofIObound and CPUbound processes. If all processes are IObound, the ready queue will almost always be empty, and the shortterm scheduler will have little to do. If all processes are CPU bound, the

little to do. If all processes are CPU bound, the IOwaiting queue will almost always be empty, devices will go unused, and again the system will be unbalanced. The system with the best performance will thus have a combination of CPUbound and IObound processes. On some systems, the longterm scheduler may be absent or minimal. For example, timesharing systems such as UNIX and Microsoft Windows systems often have no longterm scheduler but simply put every new process in memory for the shortterm

put every new process in memory for the shortterm scheduler. The stability of these systems depends either on a physical limitation (such as the number of available terminals) or on the selfadjusting nature of human users. If performance declines to unacceptable levels on a multiuser system, some users will simply quit. Some operating systems, such as timesharing systems, may introduce an additional, intermediate level of scheduling. This mediumterm scheduler is diagrammed in Figure 3.7. The key

scheduler is diagrammed in Figure 3.7. The key idea behind a mediumterm scheduler is that sometimes it can be advantageous to remove a process from memory (and from active contention for the CPU)a n dt h u sr e d u c et h ed e g r e eo f multiprogramming. Later, the process can be reintroduced into memory, and its execution can be continued where it left off. This scheme is called swapping . The process is swapped out, and is later swapped in, by the mediumterm scheduler. Swapping may be

in, by the mediumterm scheduler. Swapping may be necessary to improve the process mix or because a change in memory requirements has overcommitted available memory, requiring memory to be freed up. Swapping is discussed in Chapter 8.114 Chapter 3 Processesswap in swap out end CPU IOIO waiting queuesready queuepartially executed swappedout processes Figure 3.7 Addition of mediumterm scheduling to the queueing diagram. 3.2.3 Context Switch As mentioned in Section 1.2.1, interrupts cause the

mentioned in Section 1.2.1, interrupts cause the operating system to change aCPU from its current task and to run a kernel routine. Such operations happen frequently on generalpurpose systems. When an interrupt occurs, the system needs to save the current context of the process running on the CPU so that it can restore that context when its processing is done, essentially suspending the process and then resuming it. The context is represented in the PCB of the process. It includes the value of

the PCB of the process. It includes the value of the CPU registers, the process state (see Figure 3.2), and memorymanagement information. Generically, we perform a state save of the current state of the CPU,b ei ti nk e r n e lo ru s e rm o d e ,a n dt h e na state restore to resume operations. Switching the CPU to another process requires performing a state save of the current process and a state restore of a different process. This task is known as acontext switch .W h e nac o n t e x ts w i t

as acontext switch .W h e nac o n t e x ts w i t c ho c c u r s ,t h ek e r n e ls a v e st h ec o n t e x t of the old process in its PCB and loads the saved context of the new process scheduled to run. Contextswitch time is pure overhead, because the system does no useful work while switching. Switching speed varies from machine to machine, depending on the memory speed, the number of registers that must be copied, and the existence of special instructions (such as a single instruction to load

instructions (such as a single instruction to load or store all registers). A typical speed is a few milliseconds. Contextswitch times are highly dependent on hardware support. For instance, some processors (such as the Sun Ultra SPARC )p r o v i d em u l t i p l es e t s of registers. A context switch here simply requires changing the pointer to the current register set. Of course, if there a re more active processes than there are register sets, the system resorts to copying register data to

the system resorts to copying register data to and from memory, as before. Also, the more complex the operating system, the greater the amount of work that must be done during a context switch. As we will see in Chapter 8, advanced memorymanagement techniques may require that extra data be switched with each context. For instance , the address space of the current process must be preserved as the space of the next task is prepared for use. How the address space is preserved, and what amount of

the address space is preserved, and what amount of work is needed to preserve it, depend on the memorymanagement method of the operating system.3.3 Operations on Processes 115 MULTITASKING IN MOBILE SYSTEMS Because of the constraints imposed on mobile devices, early versions of i OS did not provide userapplication multitasking; only one application runs in the foreground and all other user applications are suspended. Operating system tasks were multitasked because they were written by Apple and

multitasked because they were written by Apple and well behaved. However, beginning with i OS4, Apple now provides a limited form of multitasking for user applications, thus allowing a single foreground application to run concurrently with multiple background applications. (On am o b i l ed e v i c e ,t h e foreground application is the application currently open and appearing on the display. The background application remains in memory, but does not occupy the display screen.) The i OS4p r o g

not occupy the display screen.) The i OS4p r o g r a m m i n g API provides support for multitasking, thus allowing a process to run in the background without being suspended. However, it is limited and only available for a limited number of application types, including applications running a single, nitelength task (such as completing a download of content from a network); receiving notications of an event occurring (such as a new email message); with longrunning background tasks (such as an

with longrunning background tasks (such as an audio player.) Apple probably limits multitasking due to battery life and memory use concerns. The CPU certainly has the features to support multitasking, but Apple chooses to not take advantage of some of them in order to better manage resource use. Android does not place such constraints on the types of applications that can run in the background. If an application requires processing while in the background, the application must use a service ,as

background, the application must use a service ,as e p a r a t ea p p l i c a t i o n component that runs on behalf of the background process. Consider a streaming audio application: if the application moves to the background, the service continues to send audio les to the audio device driver on behalf of the background application. In fact, the service will continue to run even if the background application is suspended. Services do not have a user interface and have a small memory footprint,

user interface and have a small memory footprint, thus providing an efcient technique for multitasking in a mobile environment. 3.3 Operations on Processes The processes in most systems can execute concurrently, and they may be created and deleted dynamically. Thus, these systems must provide a mechanism for process creation and termination. In this section, we explore the mechanisms involved in creating processes and illustrate process creation onUNIX and Windows systems.116 Chapter 3 Processes

onUNIX and Windows systems.116 Chapter 3 Processes 3.3.1 Process Creation During the course of execution, a process may create several new processes. As mentioned earlier, the creating process is called a parent process, and the new processes are called the children of that process. Each of these new processes may in turn create other processes, forming a tree of processes. Most operating systems (including UNIX ,L i n u x ,a n dW i n d o w s )i d e n t i f y processes according to a unique

s )i d e n t i f y processes according to a unique process identier (orpid), which is typically an integer number. The pid provides a unique value for each process in the system, and it can be used as an index to access various attributes of a process within the kernel. Figure 3.8 illustrates a typical process tree for the Linux operating system, showing the name of each process and its pid. (We use the term process rather loosely, as Linux prefers the term task instead.) The init process (which

the term task instead.) The init process (which always has a pid of 1) serves as the root parent process for all user processes. Once the system has booted, the init process can also create various user processes, such as a web or print server, an sshserver, and the like. In Figure 3.8, we see two children of init kthreadd and sshd .T h e kthreadd process is responsible for creating additional processes that perform tasks on behalf of the kernel (in this situation, khelper and pdflush ). The

(in this situation, khelper and pdflush ). The sshd process is responsible for managing clients that connect to the system by using ssh(which is short for secure shell ). The login process is responsible for managing clients that directly log onto the system. In this example, a client has logged on and is using the bash shell, which has been assigned pid 8416. Using the bash commandline interface, this user has created the process psas well as the emacs editor. On UNIX and Linux systems, we can

emacs editor. On UNIX and Linux systems, we can obtain a listing of processes by using thepscommand. For example, the command ps el will list complete information for all processes currently active in the system. It is easy to construct a process tree similar to the one shown in Figure 3.8 by recursively tracing parent processes all the way to the init process. init pid  1 sshd pid  3028login pid  8415kthreadd pid  2 sshd pid  3610pdflush pid  200khelper pid  6tcsch pid  4005emacs pid  9204bash

pid  6tcsch pid  4005emacs pid  9204bash pid  8416ps pid  9298 Figure 3.8 At r e eo fp r o c e s s e so nat y p i c a lL i n u xs y s t e m .3.3 Operations on Processes 117 In general, when a process creates a child process, that child process will need certain resources ( CPU time, memory, les, IOdevices) to accomplish its task. A child process may be able to obtain its resources directly from the operating system, or it may be constrained to a subset of the resources of the parent process. The

subset of the resources of the parent process. The parent may have to partition its resources among its children, or it may be able to share some resources (such as memory or les) among several of its children. Restric ting a child process to a subset of the parents resources prevents any process from overloading the system by creating too many child processes. In addition to supplying various physical and logical resources, the parent process may pass along initialization data (input) to the

may pass along initialization data (input) to the child process. For example, consider a process whose function is to display the contents of a le say , image.jpg on the screen of a terminal. When the process is created, it will get, as an input from its parent process, the name of the le image.jpg . Using that le name, it will open the le and write the contents out. It may also get the name of the output device. Alternatively, some operating systems pass resources to child processes. On such a

pass resources to child processes. On such a system, the new process may get two open les, image.jpg and the terminal device, and may simply transfer the datum between the two. When a process creates a new process, two possibilities for execution exist: 1.The parent continues to execute co ncurrently with its children. 2.The parent waits until some or all of its children have terminated. There are also two addressspace possibilities for the new process: 1.The child process is a duplicate of the

process: 1.The child process is a duplicate of the parent process (it has the same program and data as the parent). 2.The child process has a new program loaded into it. To illustrate these differences, lets rst consider the UNIX operating system. InUNIX ,a sw e  v es e e n ,e a c hp r o c e s si s identied by its process identier, which is a unique integer. A new process is created by the fork() system call. The new process consists of a copy of the address space of the original process. This

of the address space of the original process. This mechanism allows the parent process to communicate easily with its child process. Both processes (the parent and the child) continue execution at the instruction after the fork() ,w i t ho n ed i f f e r e n c e :t h er e t u r nc o d ef o r thefork() is zero for the new (child) process, whereas the (nonzero) process identier of the child is returned to the parent. After a fork() system call, one of the two processes typically uses the exec()

one of the two processes typically uses the exec() system call to replace the processs memory space with a new program. The exec() system call loads a binary le into memory (destroying the memory image of the program containing the exec() system call) and starts its execution. In this manner, the two processes are able to communicate and then go their separate ways. The parent can then create more children; or, if it has nothing else to do while the child runs, it can issue a wait() system call

the child runs, it can issue a wait() system call to move itself off the ready queue until the termination of the child. Because the118 Chapter 3 Processes include systypes.h  include stdio.h  include unistd.h  int main()  pid tp i d ;  fork a child process  pid  fork(); if (pid  0)  error occurred  fprintf(stderr, Fork Failed); return 1;  else if (pid  0)  child process  execlp(binls,ls,NULL);  else  parent process   parent will wait for the child to complete  wait(NULL); printf(Child

the child to complete  wait(NULL); printf(Child Complete);  return 0;  Figure 3.9 Creating a separate process using the UNIX fork() system call. call to exec() overlays the processs address space with a new program, the call to exec() does not return control unless an error occurs. The C program shown in Figure 3.9 illustrates the UNIX system calls previously described. We now have two different processes running copies of the same program. The only difference is that the value of pid(the

The only difference is that the value of pid(the process identier) for the child process is zero, while that for the parent is an integer value greater than zero (in fact, it is the actual pid of the child process). The child process inherits privileges and scheduling attributes from the parent, as well certain resources, such as open les. The child process then overlays its address space with the UNIX command binls (used to get a directory listing) using the execlp() system call ( execlp() is a

using the execlp() system call ( execlp() is a version of the exec() system call). The parent waits for the child process to complete with the wait() system call. When the child process completes (by either implicitly or explicitly invoking exit() ), the parent process resumes from the call to wait() , where it completes using the exit() system call. This is also illustrated in Figure 3.10. Of course, there is nothing to prevent the child from notinvoking exec() and instead continuing to execute

exec() and instead continuing to execute as a copy of the parent process. In this scenario, the parent and child are concurrent processes running the same code3.3 Operations on Processes 119 pid  fork() exec()parentparent (pid  0) child (pid  0)wait() exit()parent resumes Figure 3.10 Process creation using the fork() system call. instructions. Because the child is a copy of the parent, each process has its own copy of any data. As an alternative example, we next consider process creation in

example, we next consider process creation in Windows. Processes are created in the Windows APIusing the CreateProcess() func tion, which is similar to fork() in that a parent creates a new child process. However, whereas fork() has the child process inheriting the address space of its parent, CreateProcess() requires loading a specied program into the address space of the child process at process creation. Furthermore, whereas fork() is passed no parameters, CreateProcess() expects no fewer

no parameters, CreateProcess() expects no fewer than ten parameters. The C program shown in Figure 3.11 illustrates the CreateProcess() function, which creates a child process that loads the application mspaint.exe . We opt for many of the default values of the ten parameters passed to CreateProcess() .R e a d e r si n t e r e s t e di np u r s u i n gt h ed e t a i l so fp r o c e s s creation and management in the Windows APIare encouraged to consult the bibliographical notes at the end of

to consult the bibliographical notes at the end of this chapter. The two parameters passed to the CreateProcess() function are instances of the STARTUPINFO and PROCESS INFORMATION structures. STARTUPINFO species many properties of the new process, such as window size and appearance and handles to standard input and output les. The PRO CESS INFORMATION structure contains a handle and the identiers to the newly created process and its thread. We invoke the ZeroMemory() func tion to allocate memory

the ZeroMemory() func tion to allocate memory for each of these structures before proceeding with CreateProcess() . The rst two parameters passed to CreateProcess() are the application name and commandline parameters. If the application name is NULL (as it is in this case), the commandline parameter species the application to load. In this instance, we are loading the Microsoft Windows mspaint.exe application. Beyond these two initial parameters, we use the default parameters for inheriting

we use the default parameters for inheriting process and thread handles as well as specifying that there will be no creation ags. We also use the parents existing environment block and starting directory. Last, we provide two pointers to the STARTUPINFO andPROCESS  INFORMATION structures created at the beginning of the program. In Figure 3.9, the parent process waits for the child to complete by invoking the wait() system call. The equivalent of this in Windows is WaitForSingleObject() , which

this in Windows is WaitForSingleObject() , which is passed a handle of the child process pi.hProcess and waits for this process to complete. Once the child process exits, control returns from the WaitForSingleObject() function in the parent process.120 Chapter 3 Processes include stdio.h  include windows.h  int main(VOID)  STARTUPINFO si; PROCESS INFORMATION pi;  allocate memory  ZeroMemory(si, sizeof(si)); si.cb  sizeof(si); ZeroMemory(pi, sizeof(pi));  create child process  if

sizeof(pi));  create child process  if (!CreateProcess(NULL,  use command line  C:WINDOWS system32 mspaint.exe,  command  NULL,  dont inherit process handle  NULL,  dont inherit thread handle  FALSE,  disable handle inheritance  0,  no creation flags  NULL,  use parents environment block  NULL,  use parents existing directory  si, pi))  fprintf(stderr, Create Process Failed); return 1;   parent will wait for the child to complete  WaitForSingleObject(pi.hProcess, INFINITE); printf(Child

INFINITE); printf(Child Complete);  close handles  CloseHandle(pi.hProcess); CloseHandle(pi.hThread);  Figure 3.11 Creating a separate process using the Windows API. 3.3.2 Process Termination Ap r o c e s st e r m i n a t e sw h e ni t n i s h e se x e c uting its nal statement and asks the operating system to delete it by using the exit() system call. At that point, the process may return a status value (typically an integer) to its parent process (via the wait() system call). All the resources

(via the wait() system call). All the resources of the processincluding physical and virtual memory, open les, and IO buffersare deallocated by the operating system. Termination can occur in other circumstances as well. A process can cause the termination of another process via an appropriate system call (for example, TerminateProcess() in Windows). Usually, such a system call can be invoked3.3 Operations on Processes 121 only by the parent of the process th at is to be terminated. Otherwise,

the process th at is to be terminated. Otherwise, users could arbitrarily kill each others jobs. Note that a parent needs to know the identities of its children if it is to terminate them. Thus, when one process creates a new process, the identity of the newly cre ated process is passed to the parent. Ap a r e n tm a yt e r m i n a t et h ee x e c u t i o no fo n eo fi t sc h i l d r e nf o rav a r i e t yo f reasons, such as these: The child has exceeded its usage of some of the resources that

exceeded its usage of some of the resources that it has been allocated. (To determine whether this has occurred, the parent must have am e c h a n i s mt oi n s p e c tt h es t a t eo fi t sc h i l d r e n . ) The task assigned to the child is no longer required. The parent is exiting, and the operating system does not allow a child to continue if its parent terminates. Some systems do not allow a child to exist if its parent has terminated. In such systems, if a process terminates (either

In such systems, if a process terminates (either normally or abnormally), then all its children must also be termina ted. This phenomenon, referred to as cascading termination ,i sn o r m a l l yi n i t i a t e db yt h eo p e r a t i n gs y s t e m . To illustrate process execution and termination, consider that, in Linux and UNIX systems, we can terminate a process by using the exit() system call, providing an exit status as a parameter:  exit with status 1  exit(1); In fact, under normal

exit with status 1  exit(1); In fact, under normal termination, exit() may be called either directly (as shown above) or indirectly (by a return statement in main() ). Ap a r e n tp r o c e s sm a yw a i tf o rt h et e r m i n a t i o no fac h i l dp r o c e s sb yu s i n g thewait() system call. The wait() system call is passed a parameter that allows the parent to obtain the exit status of the child. This system call also returns the process identier of the terminated child so that the parent

of the terminated child so that the parent can tell which of its children has terminated: pid tp i d ; int status; pid  wait(status); When a process terminates, its resources are deallocated by the operating system. However, its entry in the process table must remain there until the parent calls wait() ,b e c a u s et h ep r o c e s st a b l ec o n t a i n st h ep r o c e s s  se x i ts t a t u s . Ap r o c e s st h a th a st e r m i n a t e d ,b u tw h o s ep a r e n th a sn o ty e tc a l l e d

u tw h o s ep a r e n th a sn o ty e tc a l l e d wait() ,i s known as a zombie process. All processes transition to this state when they terminate, but generally they exist as zomb ies only briey. Once the parent calls wait() ,t h ep r o c e s si d e n t i  e ro ft h ez o m b i ep r o c e s sa n di t se n t r yi nt h e process table are released. Now consider what would happen if a parent did not invoke wait() and instead terminated, thereby leaving its child processes as orphans .L i n u xa n

its child processes as orphans .L i n u xa n d UNIX address this scenario by assigning the init process as the new parent to122 Chapter 3 Processes orphan processes. (Recall from Figure 3.8 that the init process is the root of the process hierarchy in UNIX and Linux systems.) The init process periodically invokes wait() ,t h e r e b ya l l o w i n gt h ee x i ts t a t u so fa n yo r p h a n e dp r o c e s st ob e collected and releasing the orphans process identier and processtable entry. 3.4

process identier and processtable entry. 3.4 Interprocess Communication Processes executing concurrently in the operating system may be either independent processes or cooperating processes. A process is independent if it cannot affect or be affected by the other processes executing in the system. Any process that does not share data with any other process is independent. A process is cooperating if it can affect or be affected by the other processes executing in the system. Clearly, any process

executing in the system. Clearly, any process that shares data with other processes is a cooperating process. There are several reasons for providing an environment that allows process cooperation: Information sharing . Since several users may be interested in the same piece of information (for instance, a shared le), we must provide an environment to allow concurrent access to such information. Computation speedup .I fw ew a n tap a r t i c u l a rt a s kt or u nf a s t e r ,w em u s t break it

a rt a s kt or u nf a s t e r ,w em u s t break it into subtasks, each of which will be executing in parallel with the others. Notice that such a speedup ca nb ea c h i e v e do n l yi ft h ec o m p u t e r has multiple processing cores. Modularity .W em a yw a n tt oc o n s t r u c tt h es y s t e mi nam o d u l a rf a s h i o n , dividing the system functions into separate processes or threads, as we discussed in Chapter 2. Convenience .E v e na ni n d i v i d u a lu s e rm a yw o r ko nm a n

na ni n d i v i d u a lu s e rm a yw o r ko nm a n yt a s k sa tt h e same time. For instance, a user may be editing, listening to music, and compiling in parallel. Cooperating processes require an interprocess communication (IPC)mech anism that will allow them to exchange data and information. There are two fundamental models of interpr ocess communication: shared memory andmes sage passing .I nt h es h a r e d  m e m o r ym o d e l ,ar e g i o no fm e m o r yt h a ti ss h a r e d by

e g i o no fm e m o r yt h a ti ss h a r e d by cooperating processes is established. Processes can then exchange informa tion by reading and writing data to the shared region. In the messagepassing model, communication takes place by means of messages exchanged between the cooperating processes. The two communications models are contrasted in Figure 3.12. Both of the models just mentioned are common in operating systems, and many systems implement both. Message passing is useful for exchanging

both. Message passing is useful for exchanging smaller amounts of data, because no conicts need be avoided. Message passing is also easier to implement in a distributed system than shared memory. (Although there are systems that provide distributed shared memory, we do not consider them in this text.) Shared memory can be faster than message passing, since messagepassing systems are typically implemented using system calls3.4 Interprocess Communication 123 MULTIPROCESS ARCHITECTURECHROME BROWSER

123 MULTIPROCESS ARCHITECTURECHROME BROWSER Many websites contain active content such as JavaScript, Flash, and HTML5 to provide a rich and dynamic webbrowsing experience. Unfortunately, these web applications may also contain software bugs, which can result in sluggish response times and can even cause the web browser to crash. This isnt a big problem in a web browser that displays content from only one website. But most contemporary web browsers provide tabbed browsing, which allows a single

provide tabbed browsing, which allows a single instance of a web browser application to open several websites at the same time, with each site in a separate tab. To switch between the different sites , a user need only click on the appropriate tab. This arrangement is illustrated below: Ap r o b l e mw i t ht h i sa p p r o a c hi st h a ti faw e ba p p l i c a t i o ni na n yt a bc r a s h e s , the entire processincluding all other tabs displaying additional websites crashes as well. Googles

additional websites crashes as well. Googles Chrome web browser was designed to address this issue by using a multiprocess architecture. Chrome identies three different types of processes: browser, renderers, and plugins. The browser process is responsible for managing the user interface as well as disk and network IO.An e wb r o w s e rp r o c e s si sc r e a t e dw h e n Chrome is started. Only one browser process is created. Renderer processes contain logic for rendering web pages. Thus, they

contain logic for rendering web pages. Thus, they contain the logic for handling HTML ,J a v a s c r i p t ,i m a g e s ,a n ds of o r t h .A s ag e n e r a lr u l e ,an e wr e n d e r e rp r o c e s si sc r e a t e df o re a c hw e b s i t eo p e n e d in a new tab, and so several renderer processes may be active at the same time. Aplugin process is created for each type of plugin (such as Flash or QuickTime) in use. Plugin processes contain the code for the plugin as well as additional code

the code for the plugin as well as additional code that enables the plugin to communicate with associated renderer processes and the browser process. The advantage of the multiprocess approach is that websites run in isolation from one another. If one website crashes, only its renderer process is affected; all other processes remain unharmed. Furthermore, renderer processes run in a sandbox ,w h i c hm e a n st h a ta c c e s st od i s ka n dn e t w o r k IOis restricted, minimizing the effects

t w o r k IOis restricted, minimizing the effects of any security exploits. and thus require the more timecon suming task of kernel intervention. In sharedmemory systems, system calls are required only to establish shared124 Chapter 3 Processes process A message queue kernel (a) (b)process A shared memory kernelprocess B m0m1m2 ...m3 mnprocess B Figure 3.12 Communications models. (a) Message passing. (b) Shared memory. memory regions. Once shared memory is established, all accesses are treated

memory is established, all accesses are treated as routine memory accesses, and no assistance from the kernel is required. Recent research on systems with several processing cores indicates that message passing provides better performance than shared memory on such systems. Shared memory suffers from cache coherency issues, which arise because shared data migrate among the several caches. As the number of processing cores on systems increases, it is possible that we will see message passing as

it is possible that we will see message passing as the preferred mechanism for IPC. In the remainder of this section, we explore sharedmemory and message passing systems in more detail. 3.4.1 SharedMemory Systems Interprocess communication using shared memory requires communicating processes to establish a region of shared memory. Typically, a sharedmemory region resides in the address space of the process creating the sharedmemory segment. Other processes that wish to communicate using this

processes that wish to communicate using this sharedmemory segment must attach it to their address space. Recall that, normally, the operating system tries to prevent one process from accessing another processs memory. Shared memory requires that two or more processes agree to remove this restriction. They can then exchange information by reading and writing data in the shared areas. The form of the data and the location are determined by these processes and are not under the operating systems

processes and are not under the operating systems control. The processes are also responsible for ensuring that they are not writing to the same location simultaneously. To illustrate the concept of cooperating processes, lets consider the producerconsumer problem, which is a common paradigm for cooperating processes. A producer process produces information that is consumed by a consumer process. For example, a compiler may produce assembly code that is consumed by an assembler. The assembler,

that is consumed by an assembler. The assembler, in turn, may produce object modules that are consumed by the loader. The producerconsumer problem3.4 Interprocess Communication 125 item next produced; while (true)   produce an item in next produced  while (((in  1) B U F F E R SIZE) o u t ) ; d on o t h i n g  buffer[in]  next produced; in  (in  1) B U F F E R SIZE;  Figure 3.13 The producer process using shared memory. also provides a useful metaphor for the clientserver paradigm. We generally

for the clientserver paradigm. We generally think of a server as a producer and a client as a consumer. For example, a web server produces (that is, provides) HTML les and images, which are consumed (that is, read) by the client web browser requesting the resource. One solution to the producerconsumer problem uses shared memory. To allow producer and consumer processes to run concurrently, we must have available a buffer of items that can be lled by the producer and emptied by the consumer. This

by the producer and emptied by the consumer. This buffer will reside in a region of memory that is shared by the producer and consumer processes. A producer can produce one item while the consumer is consuming another item. The producer and consumer must be synchronized, so that the consumer does not try to consume an item that has not yet been produced. Two types of buffers can be used. The unbounded buffer places no practical limit on the size of the buffer. The consumer may have to wait for

of the buffer. The consumer may have to wait for new items, but the producer can always produce new items. The bounded buffer assumes a x e db u f f e rs i z e .I nt h i sc a s e ,t h ec o n s u m e rm u s tw a i ti ft h eb u f f e ri se m p t y , and the producer must wait if the buffer is full. Lets look more closely at how the bounded buffer illustrates interprocess communication using shared memory. The following variables reside in a region of memory shared by the producer and consumer

of memory shared by the producer and consumer processes: define BUFFER SIZE 10 typedef struct  ... item; item buffer[BUFFER SIZE]; int in  0; int out  0; The shared buffer is implemented as a circular array with two logical pointers: inand out.T h ev a r i a b l e inpoints to the next free position in the buffer; out points to the rst full position in the buffer. The buffer is empty when in out;t h eb u f f e ri sf u l lw h e n( ( in1 ) B U F F E R SIZE)  out. The code for the producer process

E R SIZE)  out. The code for the producer process is shown in Figure 3.13, and the code for the consumer process is shown in Figure 3.14. The producer process has a126 Chapter 3 Processes item next consumed; while (true)  while (in  out) ; d on o t h i n g  next consumed  buffer[out]; out  (out  1) B U F F E R SIZE;  consume the item in next consumed   Figure 3.14 The consumer process using shared memory. local variable next produced in which the new item to be produced is stored. The consumer

new item to be produced is stored. The consumer process has a local variable next consumed in which the item to be consumed is stored. This scheme allows at most BUFFER SIZE 1items in the buffer at the same time. We leave it as an exercise for you to provide a solution in which BUFFER SIZE items can be in the buffer at the same time. In Section 3.5.1, we illustrate the POSIX API for shared memory. One issue this illustration does not address concerns the situation in which both the producer

concerns the situation in which both the producer process and the consumer process attempt to access the shared buffer concurrently. In Chapter 5, we discuss how synchronization among cooperating processes can be implemented effectively in a shared memory environment. 3.4.2 MessagePassing Systems In Section 3.4.1, we showed how cooperating processes can communicate in a sharedmemory environment. The sche me requires that these processes share a region of memory and that the code for accessing

a region of memory and that the code for accessing and manipulating the shared memory be written explicitly by the application programmer. Another way to achieve the same effect is for the operating system to provide the means for cooperating processes to communicate with each other via a messagepassing facility. Message passing provides a mechanism to allow processes to communicate and to synchronize their actions without sharing the same address space. It is particularly useful in a

same address space. It is particularly useful in a distributed environment ,w h e r et h ec o m m u n i c a t i n g processes may reside on different computers connected by a network. For example, an Internet chat program could be designed so that chat participants communicate with one another by exchanging messages. Am e s s a g e  p a s s i n gf a c i l i t yp r o v i d e sa tl e a s tt w oo p e r a t i o n s : send(message) receive(message) Messages sent by a process can be either xed or

Messages sent by a process can be either xed or variable in size. If only xedsized messages can be sent, the systemlevel implementation is straight forward. This restriction, however, m akes the task of programming more difcult. Conversely, variablesized messages require a more complex system3.4 Interprocess Communication 127 level implementation, but the programming task becomes simpler. This is a common kind of tradeoff seen throughout operatingsystem design. If processes PandQwant to

operatingsystem design. If processes PandQwant to communicate, they must send messages to and receive messages from each other: a communication link must exist between them. This link can be implemented in a variety of ways. We are concerned here not with the links physical implementation (such as shared memory, hardware bus, or network, which are covered in Chapter 17) but rather with its logical implementation. Here are several methods fo rl o g i c a l l yi m p l e m e n t i n gal i n k and

o g i c a l l yi m p l e m e n t i n gal i n k and the send() receive() operations: Direct or indirect communication Synchronous or asynchronous communication Automatic or explicit buffering We look at issues related to each of these features next. 3.4.2.1 Naming Processes that want to communicate must have a way to refer to each other. They can use either direct or indirect communication. Under direct communication ,e a c hp r o c e s st h a tw a n t st oc o m m u n i c a t e must explicitly

tw a n t st oc o m m u n i c a t e must explicitly name the recipient or sender of the communication. In this scheme, the send() andreceive() primitives are dened as: send(P, message) Send a message to process P. receive(Q, message) Receive a message from process Q. Ac o m m u n i c a t i o nl i n ki nt h i ss c h e m eh a st h ef o l l o w i n gp r o p e r t i e s : A link is established automatically be tween every pair of processes that want to communicate. The processes need to know only

to communicate. The processes need to know only each others identity to communicate. Al i n ki sa s s o c i a t e dw i t he x a c t l yt w op r o c e s s e s . Between each pair of processes, there exists exactly one link. This scheme exhibits symmetry in addressing; that is, both the sender process and the receiver process must name the other to communicate. A variant of this scheme employs asymmetry in addressing. Here, only the sender names the recipient; the recipient is not required to name

recipient; the recipient is not required to name the sender. In this scheme, the send() andreceive() primitives are dened as follows: send(P, message) Send a message to process P. receive(id, message) Receive a message from any process. The variable idis set to the name of the process with which communication has taken place.128 Chapter 3 Processes The disadvantage in both of these schemes (symmetric and asymmetric) is the limited modularity of the resulting process denitions. Changing the

of the resulting process denitions. Changing the identier of a process may necessitate examining all other process denitions. All references to the old identier must be found, so that they can be modied to the new identier. In general, any such hardcoding techniques, where identiers must be explicitly stated, are less desirable than techniques involving indirection, as described next. With indirect communication ,t h em e s s a g e sa r es e n tt oa n dr e c e i v e df r o m mailboxes ,o rports

oa n dr e c e i v e df r o m mailboxes ,o rports .Am a i l b o xc a nb ev i e w e da b s t r a c t l ya sa no b j e c ti n t ow h i c h messages can be placed by processes and from which messages can be removed. Each mailbox has a unique identication. For example, POSIX message queues use an integer value to identify a mailbox. A process can communicate with another process via a number of different mailboxes, but two processes can communicate only if they have a shared mailbox. The send()

only if they have a shared mailbox. The send() andreceive() primitives are dened as follows: send(A, message) Send a message to mailbox A. receive(A, message) Receive a message from mailbox A. In this scheme, a communication link has the following properties: Al i n ki se s t a b l i s h e db e t w e e nap a i ro fp r ocesses only if both members of the pair have a shared mailbox. Al i n km a yb ea s s o c i a t e dw i t hm o r et h a nt w op r o c e s s e s . Between each pair of communicating

o c e s s e s . Between each pair of communicating processes, a number of different links may exist, with each link corresponding to one mailbox. Now suppose that processes P1,P2,a n d P3all share mailbox A.P r o c e s s P1sends a message to A, while both P2and P3execute a receive() from A. Which process will receive the message sent by P1?T h ea n s w e rd e p e n d so n which of the following methods we choose: Allow a link to be associated with two processes at most. Allow at most one process

two processes at most. Allow at most one process at a time to execute a receive() operation. Allow the system to select arbitrarily which process will receive the message (that is, either P2orP3,b u tn o tb o t h ,w i l lr e c e i v et h em e s s a g e ) .T h e system may dene an algorithm for selecting which process will receive the message (for example, round robin, where processes take turns receiving messages). The system may identify the receiver to the sender. A mailbox may be owned either

to the sender. A mailbox may be owned either by a process or by the operating system. If the mailbox is owned by a process (that is, the mailbox is part of the address space of the process), then we distinguish between the owner (which can only receive messages through this mailbox) and the user (which can only send messages to the mailbox). Since e ach mailbox has a unique owner, there can be no confusion about which process should receive a message sent to this mailbox. When a process that

message sent to this mailbox. When a process that owns a mailbox terminates, the mailbox3.4 Interprocess Communication 129 disappears. Any process that subsequently sends a message to this mailbox must be notied that the mailbox no longer exists. In contrast, a mailbox that is owned by the operating system has an existence of its own. It is independent and is not attached to any particular process. The operating system then must provide a mechanism that allows a process to do the following:

that allows a process to do the following: Create a new mailbox. Send and receive messages through the mailbox. Delete a mailbox. The process that creates a new mailbox is that mailboxs owner by default. Initially, the owner is the only process that can receive messages through this mailbox. However, the ownership and receiving privilege may be passed to other processes through appropriate system calls. Of course, this provision could result in multiple receivers for each mailbox. 3.4.2.2

in multiple receivers for each mailbox. 3.4.2.2 Synchronization Communication between processes takes place through calls to send() and receive() primitives. There are different design options for implementing each primitive. Message passing may be either blocking ornonblocking  also known as synchronous and asynchronous .( T h r o u g h o u tt h i st e x t ,y o u will encounter the concepts of synchronous and asynchronous behavior in relation to various operatingsystem algorithms.) Blocking

to various operatingsystem algorithms.) Blocking send .T h es e n d i n gp r o c e s si sb l o c k e du n t i lt h em e s s a g ei s received by the receiving process or by the mailbox. Nonblocking send .T h es e n d i n gp r o c e s ss e n d st h em e s s a g ea n dr e s u m e s operation. Blocking receive .T h er e c e i v e rb l o c k su n t i lam e s s a g ei sa v a i l a b l e . Nonblocking receive .T h er e c e i v e rr e t r i e v e se i t h e rav a l i dm e s s a g eo ra null. Different

t h e rav a l i dm e s s a g eo ra null. Different combinations of send() andreceive() are possible. When both send() and receive() are blocking, we have a rendezvous between the sender and the receiver. The solution to the producerconsumer problem becomes trivial when we use blocking send() and receive() statements. The producer merely invokes the blocking send() call and waits until the message is delivered to either the receiver or the mailbox. Likewise, when the consumer invokes receive() ,i

Likewise, when the consumer invokes receive() ,i tb l o c k su n t i lam e s s a g ei sa v a i l a b l e .T h i si s illustrated in Figures 3.15 and 3.16. 3.4.2.3 Buffering Whether communication is direct or ind irect, messages exchanged by commu nicating processes reside in a temporary queue. Basically, such queues can be implemented in three ways:130 Chapter 3 Processes message next produced; while (true)   produce an item in next produced  send(next produced);  Figure 3.15 The producer

send(next produced);  Figure 3.15 The producer process using message passing. Zero capacity .T h eq u e u eh a sam a x i m u ml e n g t ho fz e r o ;t h u s ,t h el i n k cannot have any messages waiting in it. In this case, the sender must block until the recipient receives the message. Bounded capacity .T h eq u e u eh a s n i t el e n g t h n;thus, at most nmessages can reside in it. If the queue is not full when a new message is sent, the message is placed in the queue (either the message

message is placed in the queue (either the message is copied or a pointer to the message is kept), and the sender can continue execution without waiting. The links capacity is nite, ho wever. If the link is full, the sender must block until space is available in the queue. Unbounded capacity . The queues length is potentially innite; thus, any number of messages can wait in it. The sender never blocks. The zerocapacity case is sometimes referred to as a message system with no buffering. The

to as a message system with no buffering. The other cases are referred to as systems with automatic buffering. 3.5 Examples of IPC Systems In this section, we explore three different IPCsystems. We rst cover the POSIX APIfor shared memory and then discuss message passing in the Mach operating system. We conclude with Windows, which interestingly uses shared memory as a mechanism for providing certain types of message passing. 3.5.1 An Example: POSIX Shared Memory Several IPCmechanisms are

POSIX Shared Memory Several IPCmechanisms are available for POSIX systems, including shared memory and message passing. Here, we explore the POSIX API for shared memory. POSIX shared memory is organized using memorymapped les, which associate the region of shared memory with a le. A process must rst create message next consumed; while (true)  receive(next consumed);  consume the item in next consumed   Figure 3.16 The consumer process using message passing.3.5 Examples of IPC Systems 131 as h a

passing.3.5 Examples of IPC Systems 131 as h a r e d  m e m o r yo b j e c tu s i n gt h e shm open() system call, as follows: shm fd  shm open(name, O CREAT  O RDRW, 0666); The rst parameter species the name of the sharedmemory object. Processes that wish to access this shared memory must refer to the object by this name. The subsequent parameters specify that the sharedmemory object is to be created if it does not yet exist ( O CREAT )a n dt h a tt h eo b j e c ti so p e nf o rr e a d i n g

h a tt h eo b j e c ti so p e nf o rr e a d i n g and writing ( O RDRW ). The last parameter establishes the directory permissions of the sharedmemory object. A successful call to shm open() returns an integer le descriptor for the sharedmemory object. Once the object is established, the ftruncate() function is used to congure the size of the object in bytes. The call ftruncate(shm fd, 4096); sets the size of the object to 4,096 bytes. Finally, the mmap() function establishes a memorymapped le

the mmap() function establishes a memorymapped le containing the sharedmemory object. It also returns a pointer to the memorymapped le that is used for accessing the sharedmemory object. The programs shown in Figure 3.17 and 3.18 use the producerconsumer model in implementing shared memory. The producer establishes a shared memory object and writes to shared memory, and the consumer reads from shared memory. The producer, shown in Figure 3.17, creates a sharedmemory object named OSand writes the

a sharedmemory object named OSand writes the infamous string Hello World! to shared memory. The program memorymaps a sharedmemory object of the specied size and allows writing to the object. (Obviously, only writing is necessary for the producer.) The ag MAP SHARED species that changes to the sharedmemory object will be visible to all processes sharing the object. Notice that we write to the sharedmemory object by calling the sprintf() function and writing the formatted string to the pointer

and writing the formatted string to the pointer ptr.A f t e re a c hw r i t e ,w em u s ti n c r e m e n tt h e pointer by the number of bytes written. The consumer process, shown in Figure 3.18, reads and outputs the contents of the shared memory. The consumer also invokes the shm unlink() function, which removes the sharedmemory segment after the consumer has accessed it. We provide further exercises using the POSIX sharedmemory API in the programming exercises at the end of this chapter.

programming exercises at the end of this chapter. Additionally, we provide more detailed coverage of memory mapping in Section 9.7. 3.5.2 An Example: Mach As an example of message passing, we next consider the Mach operating system. You may recall that we introduced Mach in Chapter 2 as part of the Mac OS X operating system. The Mach kernel supports the creation and destruction of multiple tasks, which are similar to processes but have multiple threads of control and fewer associated resources.

threads of control and fewer associated resources. Most communication in Mach including all intertask informationis carried out by messages . Messages are sent to and received from mailboxes, called ports in Mach.132 Chapter 3 Processes include stdio.h  include stlib.h  include string.h  include fcntl.h  include sysshm.h  include sysstat.h  int main()   the size (in bytes) of shared memory object  const int SIZE 4096;  name of the shared memory object  const char name  OS;  strings written to

object  const char name  OS;  strings written to shared memory  const char message 0 H e l l o  ; const char message 1 W o r l d !  ;  shared memory file descriptor  int shm fd;  pointer to shared memory obect  void ptr;  create the shared memory object  shm fd  shm open(name, O CREAT  O RDRW, 0666);  configure the size of the shared memory object  ftruncate(shm fd, SIZE);  memory map the shared memory object  ptr  mmap(0, SIZE, PROT WRITE, MAP SHARED, shm fd, 0);  write to the shared memory

SHARED, shm fd, 0);  write to the shared memory object  sprintf(ptr,s,message 0); ptr  strlen(message 0); sprintf(ptr,s,message 1); ptr  strlen(message 1); return 0;  Figure 3.17 Producer process illustrating POSIX sharedmemory API. Even system calls are made by messages. When a task is created, two special mailboxesthe Kernel mailbox and the Notify mailboxare also created. The kernel uses the Kernel mailbox to communicate with the task and sends notication of event occurren ces to the Notify

notication of event occurren ces to the Notify port. Only three system calls are needed for message transfer. The msg send() call sends a message to a mailbox. A message is received via msg receive() .R e m o t ep r o c e d u r e calls ( RPCs) are executed via msg rpc() ,w h i c hs e n d sam e s s a g ea n dw a i t sf o r exactly one return message from the sender. In this way, the RPC models a3.5 Examples of IPC Systems 133 include stdio.h  include stlib.h  include fcntl.h  include sysshm.h

stlib.h  include fcntl.h  include sysshm.h  include sysstat.h  int main()   the size (in bytes) of shared memory object  const int SIZE 4096;  name of the shared memory object  const char name  OS;  shared memory file descriptor  int shm fd;  pointer to shared memory obect  void ptr;  open the shared memory object  shm fd  shm open(name, O RDONLY, 0666);  memory map the shared memory object  ptr  mmap(0, SIZE, PROT READ, MAP SHARED, shm fd, 0);  read from the shared memory object  printf(s,(char

read from the shared memory object  printf(s,(char )ptr);  remove the shared memory object  shm unlink(name); return 0;  Figure 3.18 Consumer process illustrating POSIX sharedmemory API. typical subroutine procedure call but can work between systemshence the term remote .R e m o t ep r o c e d u r ec a l l sa r ec o v e r e di nd e t a i li nS e c t i o n3 . 6 . 2 . The port allocate() system call creates a new mailbox and allocates space for its queue of messages. The maximum size of the

for its queue of messages. The maximum size of the message queue defaults to eight messages. The task that creates the mailbox is that mailboxs owner. The owner is also allowed to receive from the mailbox. Only one task at a time can either own or receive from a mailbox, but these rights can be sent to other tasks. The mailboxs message queue is initially empty. As messages are sent to the mailbox, the messages are copied into the mailbox. All messages have the same priority. Mach guarantees that

have the same priority. Mach guarantees that multiple messages from the same sender are queued in rstin, rstout ( FIFO ) order but does not guarantee an absolute ordering. For instance, messages from two senders may be queued in any order. The messages themselves consist of a xedlength header followed by a variablelength data portion. The header indicates the length of the message and includes two mailbox names. One mailbox name species the mailbox134 Chapter 3 Processes to which the message is

Chapter 3 Processes to which the message is being sent. Commonly, the sending thread expects a reply, so the mailbox name of the sender is passed on to the receiving task, which can use it as a return address.  The variable part of a message is a list of typed data items. Each entry in the list has a type, size, and value. The type of the objects specied in the message is important, since objects dened by the operating systemsuch as ownership or receive access rights, task states, and memory

or receive access rights, task states, and memory segmentsmay be sent in messages. The send and receive operations themselves are exible. For instance, when a message is sent to a mailbox, the mailbox may be full. If the mailbox is not full, the message is copied to the mailbox, and the sending thread continues. If the mailbox is full, the sending thread has four options: 1.Wait indenitely until there is room in the mailbox. 2.Wait at most nmilliseconds. 3.Do not wait at all but rather return

3.Do not wait at all but rather return immediately. 4.Temporarily cache a message. Here, a message is given to the operating system to keep, even though the mailbox to which that message is being sent is full. When the message can be put in the mailbox, a message is sent back to the sender. Only one message to a full mailbox can be pending at any time for a given sending thread. The nal option is meant for server tasks, such as a lineprinter driver. After nishing a request, such tasks may need

After nishing a request, such tasks may need to send a onetime reply to the task that requested service, but they must also continue with other service requests, even if the reply mailbox for a client is full. The receive operation must specify the mailbox or mailbox set from which a message is to be received. A mailbox set is a collection of mailboxes, as declared by the task, which can be grouped together and treated as one mailbox for the purposes of the task. Threads in a task can receive

of the task. Threads in a task can receive only from a mailbox or mailbox set for which the task has receive access. A port status() system call returns the number of messages in a given mailbox. The receive operation attempts to receive from (1) any mailbox in a mailbox set or (2) a specic (named) mailbox. If no message is waiting to be received, the receiving thread can either wait at most nmilliseconds or not wait at all. The Mach system was especially designed for distributed systems, which

especially designed for distributed systems, which we discuss in Chapter 17, but Mach was shown to be suitable for systems with fewer processing cores, as evidenced by its inclusion in the Mac OS X system. The major problem with message systems has generally been poor performance caused by double copying of messages: the message is copied rst from the sender to the mailbox an d then from the mailbox to the receiver. The Mach message system attempts to avoid doublecopy operations by using

attempts to avoid doublecopy operations by using virtualmemorymanagement techniques (Chapter 9). Essentially, Mach maps the address space containing the se nders message into the receivers address space. The message itself is never ac tually copied. This messagemanagement technique provides a large performance boost but works for only intrasystem messages. The Mach operating system is discussed in more detail in the online Appendix B.3.5 Examples of IPC Systems 135 3.5.3 An Example: Windows The

of IPC Systems 135 3.5.3 An Example: Windows The Windows operating system is an example of modern design that employs modularity to increase functionality and decrease the time needed to imple ment new features. Windows provides support for multiple operating envi ronments, or subsystems. Application programs communicate with these subsystems via a messagepassing mechanism. Thus, application programs can be considered clients of a subsystem server. The messagepassing facility in Windows is

server. The messagepassing facility in Windows is called the advanced local procedure call (ALPC )facility. It is used for communication between two processes on the same machine. It is similar to the standard remote procedure call ( RPC)m e c h a n i s mt h a ti sw i d e l yu s e d ,b u ti ti so p t i m i z e df o ra n ds p e c i  c to Windows. (Remote procedure calls are covered in detail in Section 3.6.2.) Like Mach, Windows uses a port object to establish and maintain a connection between

to establish and maintain a connection between two processes. Windows uses two types of ports: connection ports and communication ports . Server processes publish connectionport objects that are visible to all processes. When a client wants services from a subsystem, it opens a handle to the servers connectionport object and send sac o n n e c t i o nr e q u e s tt ot h a tp o r t . The server then creates a channel and re turns a handle to the client. The channel consists of a pair of private

client. The channel consists of a pair of private communication ports: one for clientserver messages, the other for serverclient messages. Additionally, communication channels support a callback mechanism that allows the client and server to accept requests when they would normally be expecting a reply. When an ALPC channel is created, one of three messagepassing techniques is chosen: 1.For small messages (up to 256 bytes), the ports message queue is used as intermediate storage, and the

queue is used as intermediate storage, and the messages are copied from one process to the other. 2.Larger messages must be passed through a section object ,w h i c hi sa region of shared memory associated with the channel. 3.When the amount of data is too large to t into a section object, an APIis available that allows server processes to read and write directly into the address space of a client. The client has to decide when it sets up the channel whether it will need to send a large message.

whether it will need to send a large message. If the client determines that it does want to send large messages, it asks for a section object to be created. Similarly, if the server decides that replies will be large, it creates a section object. So that the section object can be used, a small message is sent that contains a pointer and size information about the section object. This method is more complicated than the rst method listed above, but it a voids data copying. The structure of

but it a voids data copying. The structure of advanced local procedure calls in Windows is shown in Figure 3.19. It is important to note that the ALPC facility in Windows is not part of the Windows APIand hence is not visible to the application programmer. Rather, applications using the Windows APIinvoke standard remote procedure calls. When the RPC is being invoked on a process on the same system, the RPC is handled indirectly through an ALPC .p r o c e d u r ec a l l .A d d i t i o n a l l y

.p r o c e d u r ec a l l .A d d i t i o n a l l y ,m a n yk e r n e l services use ALPC to communicate with client processes.136 Chapter 3 Processes Connection PortConnection request Handle Handle HandleClient Communication Port Server Communication Port Shared Section Object ( 256 bytes)Server Client Figure 3.19 Advanced local procedure calls in Windows. 3.6 Communication in ClientServer Systems In Section 3.4, we described how processes can communicate using shared memory and message passing.

using shared memory and message passing. These techniques can be used for communica tion in clientserver systems (Section 1.11.4) as well. In this section, we explore three other strategies for communica tion in clientserver systems: sockets, remote procedure calls ( RPCs), and pipes. 3.6.1 Sockets Asocket is dened as an endpoint for communication. A pair of processes communicating over a network employs a pair of socketsone for each process. A socket is identied by an IPaddress concatenated

A socket is identied by an IPaddress concatenated with a port number. In general, sockets use a clien tserver architecture. The server waits for incoming client requests by listening to a specied port. Once a request is received, the server accepts a connection from the client socket to complete the connection. Servers implementing specic services (such as telnet, FTP,a n d HTTP )l i s t e nt ow e l l  k n o w np o r t s( at e l n e ts e r v e rl i s t e n st op o r t2 3 ;a n FTP server listens

i s t e n st op o r t2 3 ;a n FTP server listens to port 21; and a web, or HTTP ,s e r v e rl i s t e n st op o r t8 0 ) .A l l ports below 1024 are considered well known; we can use them to implement standard services. When a client process initiates a request for a connection, it is assigned a port by its host computer. This port has some arbitrary number greater than 1024. For example, if a client on host Xwith IPaddress 146.86.5.20 wishes to establish a connection with a web server (which is

establish a connection with a web server (which is listening on port 80) at address 161.25.19.8, host Xmay be assigned port 1625. The connection will consist of a pair of sockets: (146.86.5.20:1625) on host Xand (161.25.19.8:80) on the web server. This situation is illustrated in Figure 3.20. The packets traveling between the hosts are delivered to the appropriate process based on the destination port number. All connections must be unique. Therefore, if another process also on host Xwished to

if another process also on host Xwished to establish another connection with the same web server, it would be assigned a port number greater than 1024 and not equal to 1625. This ensures that all connections consist of a unique pair of sockets.3.6 Communication in ClientServer Systems 137socket (146.86.5.20:1625)host X (146.86.5.20) socket (161.25.19.8:80)web server (161.25.19.8) Figure 3.20 Communication using sockets. Although most program examples in this text use C, we will illustrate

examples in this text use C, we will illustrate sockets using Java, as it provides a muc he a s i e ri n t e r f a c et os o c k e t sa n dh a sa rich library for networking utilities. Those interested in socket programming in C or C should consult the bibliographical notes at the end of the chapter. Java provides three different types of sockets. Connectionoriented ( TCP) sockets are implemented with the Socket class. Connectionless ( UDP )s o c k e t s use the DatagramSocket class. Finally,

o c k e t s use the DatagramSocket class. Finally, the MulticastSocket class is a subclass of the DatagramSocket class. A multicast socket allows data to be sent to multiple recipients. Our example describes a date server that uses connectionoriented TCP sockets. The operation allows clients to request the current date and time from the server. The server listens to port 6013, although the port could have any arbitrary number greater than 1024. When a connection is received, the server returns

When a connection is received, the server returns the date and time to the client. The date server is shown in Figure 3.21. The server creates a ServerSocket that species that it will listen to port 6013. The server then begins listening to the port with the accept() method. The server blocks on the accept() method waiting for a client to request a connection. When a connection request is received, accept() returns a socket that the server can use to communicate with the client. The details of

use to communicate with the client. The details of how the server communicates with the socket are as follows. The server rst establishes a PrintWriter object that it will use to communicate with the client. A PrintWriter object allows the server to write to the socket using the routine print() and println() methods for output. The server process sends the date to the client, calling the method println() . Once it has written the date to the socket, the server closes the socket to the client and

the server closes the socket to the client and resumes listening for more requests. Ac l i e n tc o m m u n i c a t e sw i t ht h es e r v e rb yc r e a t i n gas o c k e ta n dc o n n e c t i n g to the port on which the server is listening. We implement such a client in the Java program shown in Figure 3.22. The client creates a Socket and requests ac o n n e c t i o nw i t ht h es e r v e ra t IPaddress 127.0.0.1 on port 6013. Once the connection is made, the client can read from the socket

is made, the client can read from the socket using normal stream IOstatements. After it has received the date from the server, the client closes138 Chapter 3 Processes import java.net.; import java.io.; public class DateServer  public static void main(String[] args)  try  ServerSocket sock  new ServerSocket(6013);  now listen for connections  while (true)  Socket client  sock.accept(); PrintWriter pout  new PrintWriter(client.getOutputStream(), true);  write the Date to the socket

true);  write the Date to the socket  pout.println(new java.util.Date().toString());  close the socket and resume   listening for connections  client.close();   catch (IOException ioe)  System.err.println(ioe);    Figure 3.21 Date server. the socket and exits. The IPaddress 127.0.0.1 is a special IPaddress known as the loopback .W h e nac o m p u t e rr e f e r st o IPaddress 127.0.0.1, it is referring to itself. This mechanism allows a client and server on the same host to communicate using the

server on the same host to communicate using the TCPIP protocol. The IPaddress 127.0.0.1 could be replaced with the IPaddress of another host running the date server. In addition to an IPaddress, an actual host name, such as www.westminstercollege.edu ,c a nb eu s e da s well. Communication using socketsalthough common and efcientis con sidered a lowlevel form of communica tion between distributed processes. One reason is that sockets allow only an unstructured stream of bytes to be exchanged

an unstructured stream of bytes to be exchanged between the communicating threads. It is the responsibility of the client or server application to impose a structure on the data. In the next two subsections, we look at two higher level methods of communication: remote procedure calls ( RPCs) and pipes. 3.6.2 Remote Procedure Calls One of the most common forms of remote service is the RPC paradigm, which we discussed briey in Section 3.5.2. The RPC was designed as a way to3.6 Communication in

RPC was designed as a way to3.6 Communication in ClientServer Systems 139 import java.net.; import java.io.; public class DateClient  public static void main(String[] args)  try   make connection to server socket  Socket sock  new Socket(127.0.0.1,6013); InputStream in  sock.getInputStream(); BufferedReader bin  new BufferedReader(new InputStreamReader(in));  read the date from the socket  String line; while ( (line  bin.readLine()) ! null) System.out.println(line);  close the socket connection

close the socket connection sock.close();  catch (IOException ioe)  System.err.println(ioe);    Figure 3.22 Date client. abstract the procedurecall mechanism for use between systems with network connections. It is similar in many respects to the IPCmechanism described in Section 3.4, and it is usually built on top of such a system. Here, however, because we are dealing with an environment in which the processes are executing on separate systems, we must use a messagebased communication scheme

we must use a messagebased communication scheme to provide remote service. In contrast to IPCmessages, the messages exchanged in RPCcommunication are well structured and are thus no longer just packets of data. Each message is addressed to an RPC daemon listening to a port on the remote system, and each contains an identier specifying the function to execute and the parameters to pass to that function. The function is then executed as requested, and any output is sent back to the requester in a

and any output is sent back to the requester in a separate message. Aport is simply a number included at the start of a message packet. Whereas a system normally has one network address, it can have many ports within that address to differentiate the many network services it supports. If a remote process needs a service, it addresses a message to the proper port. For instance, if a system wished to allow other systems to be able to list its current users, it would have a daemon supporting such

users, it would have a daemon supporting such an RPC attached to a port say, port 3027. Any remote system could obtain the needed information (that140 Chapter 3 Processes is, the list of current users) by sending an RPC message to port 3027 on the server. The data would be received in a reply message. The semantics of RPCsa l l o w sac l i e n tt oi n v o k eap r o c e d u r eo nar e m o t e host as it would invoke a procedure locally. The RPC system hides the details that allow communication to

hides the details that allow communication to take place by providing a stub on the client side. Typically, a separate stub exists for each separate remote procedure. When the client invokes a remote procedure, the RPC system calls the appropriate stub, passing it the parameters provided to the remote procedure. This stub locates the port on the server and marshals the parameters. Parameter marshalling involves packaging the parameters into a form that can be transmitted over an e t w o r k .T h

that can be transmitted over an e t w o r k .T h es t u bt h e nt r a n s m i t sam e s s a g et ot h es e r v e ru s i n gm e s s a g e passing. A similar stub on the server side receives this message and invokes the procedure on the server. If necessary, return values are passed back to the client using the same technique. On Windows systems, stub code is compiled from a specication written in the Microsoft Interface Denition Language (MIDL ),w h i c hi su s e df o rd e  n i n gt h ei n t e r

h i c hi su s e df o rd e  n i n gt h ei n t e r f a c e sb e t w e e nc l i e n ta n ds e r v e r programs. One issue that must be dealt with concern sd i f f e r e n c e si nd a t ar e p r e s e n t a  tion on the client and server machines. Consider the representation of 32bit integers. Some systems (known as bigendian )s t o r et h em o s ts i g n i  c a n tb y t e rst, while other systems (known as littleendian )s t o r et h el e a s ts i g n i  c a n t byte rst. Neither order is better per

n i  c a n t byte rst. Neither order is better per se; rather, the choice is arbitrary within ac o m p u t e ra r c h i t e c t u r e .T or e s o l v ed i f f e r e n c e sl i k et h i s ,m a n y RPC systems dene a machineindependent representation of data. One such representation is known as external data representation (XDR ).O nt h ec l i e n ts i d e ,p a r a m e t e r marshalling involves converting the machinedependent data into XDR before they are sent to the server. On the server side,

they are sent to the server. On the server side, the XDR data are unmarshalled and converted to the machinedependent representation for the server. Another important issue involves the semantics of a call. Whereas local procedure calls fail only under extreme circumstances, RPCsc a nf a i l ,o rb e duplicated and executed more than on ce, as a result of common network errors. One way to address this problem is for the operating system to ensure that messages are acted on exactly once, rather

that messages are acted on exactly once, rather than at most once. Most local procedure calls have the exactly once functionality, but it is more difcult to implement. First, consider at most once. This semantic can be implemented by attaching a timestamp to each message. The server must keep a history of all the timestamps of messages it has already processed or a history large enough to ensure that repeated messages are detected. Incoming messages that have a timestamp already in the history

that have a timestamp already in the history are ignored. The client can then send a message one or more times and be assured that it only executes once. Forexactly once, we need to remove the risk that the server will never receive the request. To accomplish this, the server must implement the at most once protocol described above but must also acknowledge to the client that the RPC call was received and executed. These ACK messages are common throughout networking. The client must resend each

throughout networking. The client must resend each RPC call periodically until it receives the ACK for that call. Yet another important issue concerns the communication between a server and a client. With standard procedure calls, some form of binding takes place during link, load, or execution time (Chapter 8) so that a procedure calls name3.6 Communication in ClientServer Systems 141 client user calls kernel to send RPC message to procedure X matchmaker receives message, looks up answer

X matchmaker receives message, looks up answer matchmaker replies to client with port P daemon listening to port P receives message daemon processes request and processes send outputkernel sends message to matchmaker to find port numberFrom: client To: server Port: matchmaker Re: address for RPC X From: client To: server Port: port P contents  From: RPC Port: P To: client Port: kernel output From: server To: client Port: kernel Re: RPC X Port: Pkernel places port P in user RPC message kernel

Pkernel places port P in user RPC message kernel sends RPC kernel receives reply, passes it to usermessages server Figure 3.23 Execution of a remote procedure call (RPC). is replaced by the memory address of the procedure call. The RPC scheme requires a similar binding of the client and the server port, but how does a client know the port numbers on the server? Neither system has full information about the other, because they do not share memory. Two approaches are common. First, the binding

Two approaches are common. First, the binding information may be predetermined, in the form of xed port addresses. At compile time, an RPC call has a xed port number associated with it. Once a program is compiled, the server cannot change the port nu mber of the requested service. Second, binding can be done dynamically by a rendezvous mechanism. Typically, an operating system provides a rendezvous (also called a matchmaker )d a e m o n on a xed RPC port. A client then sends a message containing

RPC port. A client then sends a message containing the name of the RPC to the rendezvous daemon requesting the port address of the RPC it needs to execute. The port number is returned, and the RPC calls can be sent to that port until the process terminates (or the server crashes). This method requires the extra overhead of the initial request but is more exible than the rst approach. Figure 3.23 shows a sample interaction. The RPC scheme is useful in implementing a distributed le system (Chapter

in implementing a distributed le system (Chapter 17). Such a system can be implemented as a set of RPC daemons142 Chapter 3 Processes and clients. The messages are addressed to the distributed le system port on a server on which a le operation is to take place. The message contains the disk operation to be performed. The disk operation might be read ,write ,rename , delete ,o rstatus , corresponding to the usual lerelated system calls. The return message contains any data resulting from that

message contains any data resulting from that call, which is executed by the DFSdaemon on behalf of the client. For instance, a message might contain ar e q u e s tt ot r a n s f e raw h o l e l et oac l i e n to rb el i m i t e dt oas i m p l eb l o c k request. In the latter case, several requests may be needed if a whole le is to be transferred. 3.6.3 Pipes Apipe acts as a conduit allowing two processes to communicate. Pipes were one of the rst IPCmechanisms in early UNIX systems. They

the rst IPCmechanisms in early UNIX systems. They typically provide one of the simpler ways for processes to communicate with one another, although they also have some limitations. In implementing a pipe, four issues must be considered: 1.Does the pipe allow bidirectional communication, or is communication unidirectional? 2.If twoway communication is allowed, is it half duplex (data can travel only one way at a time) or full duplex (data can travel in both directions at the same time)? 3.Must a

in both directions at the same time)? 3.Must a relationship (such as parentchild )e x i s tb e t w e e nt h ec o m m u n i  cating processes? 4.Can the pipes communicate over a network, or must the communicating processes reside on the same machine? In the following sections, we explore two common types of pipes used on both UNIX and Windows systems: ordinary pipes and named pipes. 3.6.3.1 Ordinary Pipes Ordinary pipes allow two processes to communicate in standard producer consumer fashion: the

in standard producer consumer fashion: the producer writes to one end of the pipe (the writeend ) and the consumer reads from the other end (the readend ). As a result, ordinary pipes are unidirectional, allowing only oneway communication. If twoway communication is required, two pipes must be used, with each pipe sending data in a different direction. We next illustrate constructing ordinary pipes on both UNIX and Windows systems. In both program examples, one process writes the message

program examples, one process writes the message Greetings to the pipe, while the other process reads this message from the pipe. On UNIX systems, ordinary pipes are constructed using the function pipe(int fd[]) This function creates a pipe that is accessed through the int fd[] le descriptors: fd[0] is the readend of the pipe, and fd[1] is the writeend.3.6 Communication in ClientServer Systems 143 parent fd(0) fd(1)child fd(0) fd(1)pipe Figure 3.24 File descriptors for an ordinary pipe. UNIX

3.24 File descriptors for an ordinary pipe. UNIX treats a pipe as a special type of le. Thus, pipes can be accessed using ordinary read() andwrite() system calls. An ordinary pipe cannot be accessed from outside the process that created it. Typically, a parent process creates a pipe and uses it to communicate with ac h i l dp r o c e s st h a ti tc r e a t e sv i a fork() . Recall from Section 3.3.1 that a child process inherits open les from its parent. Since a pipe is a special type of le, the

parent. Since a pipe is a special type of le, the child inherits the pipe from its parent process. Figure 3.24 illustrates the relationship of the le descriptor fdto the parent and child processes. In the UNIX program shown in Figure 3.25, the parent process creates a pipe and then sends a fork() call creating the child process. What occurs after thefork() call depends on how the data are to ow through the pipe. In this instance, the parent writes to the pipe, and the child reads from it. It is

to the pipe, and the child reads from it. It is important to notice that both the parent process and the child process initially close their unused ends of the pipe. Although the program shown in Figure 3.25 does not require this action, it is an important step to ensure that a process reading from the pipe can detect endofle ( read() returns 0) when the writer has closed its end of the pipe. Ordinary pipes on Windows systems are termed anonymous pipes ,a n d they behave similarly to their UNIX

pipes ,a n d they behave similarly to their UNIX counterparts: they are unidirectional and include systypes.h  include stdio.h  include string.h  include unistd.h  define BUFFER SIZE 25 define READ END 0 define WRITE END 1 int main(void)  char write msg[BUFFER SIZE]  Greetings; char read msg[BUFFER SIZE]; int fd[2]; pid tp i d ;  Program continues in Figure 3.26  Figure 3.25 Ordinary pipe in UNIX.144 Chapter 3 Processes  create the pipe  if (pipe(fd)  1)  fprintf(stderr,Pipe failed); return 1;

1)  fprintf(stderr,Pipe failed); return 1;   fork a child process  pid  fork(); if (pid 0) error occurred  fprintf(stderr, Fork Failed); return 1;  if (pid 0) parent process   close the unused end of the pipe  close(fd[READ END]);  write to the pipe  write(fd[WRITE END], write msg, strlen(write msg)1);  close the write end of the pipe  close(fd[WRITE END]);  else  child process   close the unused end of the pipe  close(fd[WRITE END]);  read from the pipe  read(fd[READ END], read msg, BUFFER

from the pipe  read(fd[READ END], read msg, BUFFER SIZE); printf(read s,read msg);  close the write end of the pipe  close(fd[READ END]);  return 0;  Figure 3.26 Figure 3.25, continued. employ parentchild relationships betw een the communicating processes. In addition, reading and writing to the pipe can be accomplished with the ordinary ReadFile() and WriteFile() functions. The Windows API for creating pipes is the CreatePipe() function, which is passed four parameters. The parameters provide

is passed four parameters. The parameters provide separate handles for (1) reading and (2) writing to the pipe, as well as (3) an instance of the STARTUPINFO structure, which is used to specify that the child process is to inherit the handles of the pipe. Furthermore, (4) the size of the pipe (in bytes) may be specied. Figure 3.27 illustrates a parent process creating an anonymous pipe for communicating with its child. Unlike UNIX systems, in which a child process3.6 Communication in

in which a child process3.6 Communication in ClientServer Systems 145 include stdio.h  include stdlib.h  include windows.h  define BUFFER SIZE 25 int main(VOID)  HANDLE ReadHandle, WriteHandle; STARTUPINFO si; PROCESS INFORMATION pi; char message[BUFFER SIZE]  Greetings; DWORD written;  Program continues in Figure 3.28  Figure 3.27 Windows anonymous pipeparent process. automatically inherits a pipe created by its parent, Windows requires the programmer to specify which attributes the child

programmer to specify which attributes the child process will inherit. This is accomplished by rst initializing the SECURITY ATTRIBUTES structure to allow handles to be inherited and then redire cting the child processs handles for standard input or standard output to the read or write handle of the pipe. Since the child will be reading from the pipe, the parent must redirect the childs standard input to the read handle of the pipe. Furthermore, as the pipes are half duplex, it is necessary to

as the pipes are half duplex, it is necessary to prohibit the child from inheriting the writeend of the pipe. The program to create the child process is similar to the program in Figure 3.11, except that the fth parameter is set to TRUE ,i n d i c a t i n g that the child process is to inherit d esignated handles from its parent. Before writing to the pipe, the parent rst closes its unused read end of the pipe. The child process that reads from the pipe is shown in Figure 3.29. Before reading

the pipe is shown in Figure 3.29. Before reading from the pipe, this program obtains the read handle to the pipe by invoking GetStdHandle() . Note that ordinary pipes require a parentchild relationship between the communicating processes on both UNIX and Windows systems. This means that these pipes can be used only for co mmunication between processes on the same machine. 3.6.3.2 Named Pipes Ordinary pipes provide a simple mechanism for allowing a pair of processes to communicate. However,

a pair of processes to communicate. However, ordinary pipes exist only while the processes are communicating with one another. On both UNIX and Windows systems, once the processes have nished communicating and have terminated, the ordinary pipe ceases to exist. Named pipes provide a much more powerful communication tool. Com munication can be bidirectional, and no parentchild relationship is required. Once a named pipe is established, several processes can use it for communi cation. In fact, in

can use it for communi cation. In fact, in a typical scenario, a named pipe has several writers. Addi tionally, named pipes continue to exist after communicating processes have146 Chapter 3 Processes  set up security attributes allowing pipes to be inherited  SECURITY ATTRIBUTES sa  sizeof( SECURITY ATTRIBUTES ),NULL ,TRUE ;  allocate memory  ZeroMemory(pi, sizeof(pi));  create the pipe  if (!CreatePipe(ReadHandle, WriteHandle, sa, 0))  fprintf(stderr, Create Pipe Failed); return 1;   establish

Create Pipe Failed); return 1;   establish the START INFO structure for the child process  GetStartupInfo(si); si.hStdOutput  GetStdHandle( STD OUTPUT HANDLE );  redirect standard input to the read end of the pipe  si.hStdInput  ReadHandle; si.dwFlags  STARTF USESTDHANDLES ;  dont allow the child to inherit the write end of pipe  SetHandleInformation(WriteHandle, HANDLE FLAG INHERIT ,0 ) ;  create the child process  CreateProcess( NULL , c h i l d . e x e  , NULL, NULL , TRUE,  inherit handles

d . e x e  , NULL, NULL , TRUE,  inherit handles  0,NULL, NULL , s i , p i ) ;  close the unused end of the pipe  CloseHandle(ReadHandle);  the parent writes to the pipe  if (!WriteFile(WriteHandle, message, BUFFER SIZE,written, NULL )) fprintf(stderr, Error writing to pipe.);  close the write end of the pipe  CloseHandle(WriteHandle);  wait for the child to exit  WaitForSingleObject(pi.hProcess, INFINITE ); CloseHandle(pi.hProcess); CloseHandle(pi.hThread); return 0;  Figure 3.28 Figure 3.27,

return 0;  Figure 3.28 Figure 3.27, continued. nished. Both UNIX and Windows systems support named pipes, although the details of implementation differ greatly. Next, we explore named pipes in each of these systems.3.7 Summary 147 include stdio.h  include windows.h  define BUFFER SIZE 25 int main( VOID )  HANDLE Readhandle; CHAR buffer[ BUFFER SIZE]; DWORD read;  get the read handle of the pipe  ReadHandle  GetStdHandle( STD INPUT HANDLE );  the child reads from the pipe  if

INPUT HANDLE );  the child reads from the pipe  if (ReadFile(ReadHandle, buffer, BUFFER SIZE, r e a d , NULL )) printf(child read s,buffer); else fprintf(stderr, Error reading from pipe); return 0;  Figure 3.29 Windows anonymous pipeschild process. Named pipes are referred to as FIFO si n UNIX systems. Once created, they appear as typical les in the le system. A FIFO is created with the mkfifo() system call and manipulated with the ordinary open() ,read() ,write() , and close() system calls. It

,read() ,write() , and close() system calls. It will continue to exist until it is explicitly deleted from the le system. Although FIFO sa l l o wb i d i r e c t i o n a lc o m m u n i c a t i o n ,o n l y halfduplex transmission is permitted. If data must travel in both directions, two FIFO sa r et y p i c a l l yu s e d .A d d i t i o n a l l y ,t h ec o m m u n i c a t i n gp r o c e s s e sm u s t reside on the same machine. If intermachine communication is required, sockets (Section 3.6.1)

communication is required, sockets (Section 3.6.1) must be used. Named pipes on Windows systems provide a richer communication mech anism than their UNIX counterparts. Fullduplex communication is allowed, and the communicating processes may reside on either the same or different machines. Additionally, only byteoriented data may be transmitted across a UNIX FIFO ,w h e r e a sW i n d o w ss y s t e m sa l l o we i t h e rb y t e o rm e s s a g e  o r i e n t e d data. Named pipes are created

g e  o r i e n t e d data. Named pipes are created with the CreateNamedPipe() function, and a client can connect to a named pipe using ConnectNamedPipe() .C o m m u n i  cation over the named pipe can be accomplished using the ReadFile() and WriteFile() functions. 3.7 Summary Ap r o c e s si sap r o g r a mi ne x e c u t i o n .A sap r o c e s se x e c u t e s ,i tc h a n g e ss t a t e .T h e state of a process is dened by that proce sss current activity. Each process may be in one of the

activity. Each process may be in one of the following states: new, ready, running, waiting, or terminated.148 Chapter 3 Processes PIPES IN PRACTICE Pipes are used quite often in the UNIX commandline environment for situations in which the output of one command serves as input to another. For example, the UNIX lscommand produces a directory listing. For especially long directory listings, the output may scroll through several screens. The command more manages output by displaying only one screen

more manages output by displaying only one screen of output at at i m e ;t h eu s e rm u s tp r e s st h es p a c eb a rt om o v ef r o mo n es c r e e nt ot h en e x t . Setting up a pipe between the lsandmore commands (which are running as individual processes) allows the output of lsto be delivered as the input to more ,e n a b l i n gt h eu s e rt od i s p l a yal a r g ed i r e c t o r yl i s t i n gas c r e e na tat i m e . Ap i p ec a nb ec o n s t r u c t e do nt h ec o m m a n dl i n eu

o n s t r u c t e do nt h ec o m m a n dl i n eu s i n gt h e character. The complete command is ls  more In this scenario, the lscommand serves as the producer, and its output is consumed by the more command. Windows systems provide a more command for the DOS shell with functionality similar to that of its UNIX counterpart. The DOS shell also uses thecharacter for establishing a pipe. The only difference is that to get ad i r e c t o r yl i s t i n g , DOS uses the dir command rather than ls,a

i n g , DOS uses the dir command rather than ls,a ss h o w n below: dir  more Each process is represented in the operating system by its own process control block ( PCB). Ap r o c e s s ,w h e ni ti sn o te x e c u t i n g ,i sp l a c e di ns o m ew a i t i n gq u e u e .T h e r e are two major classes of queues in an operating system: IOrequest queues and the ready queue. The ready queue contains all the processes that are ready to execute and are waiting for the CPU.E a c hp r o c e s si sr e

are waiting for the CPU.E a c hp r o c e s si sr e p r e s e n t e db ya PCB. The operating system must select processes from various scheduling queues. Longterm (job) scheduling is the selection of processes that will be allowed to contend for the CPU.N o r m a l l y ,l o n g  t e r ms c h e d u l i n gi sh e a v i l y inuenced by resourceallocation considerations, especially memory manage ment. Shortterm ( CPU)s c h e d u l i n gi st h es e l e c t i o no fo n ep r o c e s sf r o mt h e ready

e c t i o no fo n ep r o c e s sf r o mt h e ready queue. Operating systems must provide a mechanism for parent processes to create new child processes. The parent may wait for its children to terminate before proceeding, or the parent and ch ildren may execute concurrently. There are several reasons for allowing conc urrent execution: information sharing, computation speedup, modularity, and convenience. The processes executing in the operating system may be either independent processes or

system may be either independent processes or cooperating processes. Cooperating processes require an interpro cess communication mechanism to communicate with each other. Principally, communication is achieved through two schemes: shared memory and mes sage passing. The sharedmemory method requires communicating processesPractice Exercises 149 include systypes.h  include stdio.h  include unistd.h  int value  5; int main()  pid tp i d ; pid  fork(); if (pid  0)  child process  value  15; return

if (pid  0)  child process  value  15; return 0;  else if (pid  0)  parent process  wait(NULL); printf(PARENT: value  d,value);  LINE A  return 0;   Figure 3.30 What output will be at Line A? to share some variables. The processes are expected to exchange information through the use of these shared variables. In a sharedmemory system, the responsibility for providing communication rests with the application pro grammers; the operating system needs to provide only the shared memory. The

needs to provide only the shared memory. The messagepassing method allows the processes to exchange messages. The responsibility for providing communication may rest with the operating system itself. These two schemes are not mutually exclusive and can be used simultaneously within a single operating system. Communication in clientserver systems may use (1) sockets, (2) remote procedure calls ( RPCs), or (3) pipes. A socket is dened as an endpoint for communication. A connection between a pair

for communication. A connection between a pair of applications consists of a pair of sockets, one at each end of the communication channel. RPCsa r ea n o t h e r form of distributed communication. An RPC occurs when a process (or thread) calls a procedure on a remote application. Pipes provide a relatively simple ways for processes to communicate with one another. Ordinary pipes allow communication between parent and child processes, while named pipes permit unrelated processes to communicate.

pipes permit unrelated processes to communicate. Practice Exercises 3.1 Using the program shown in Figure 3.30, explain what the output will be at LINE A . 3.2 Including the initial parent process, how many processes are created by the program shown in Figure 3.31?150 Chapter 3 Processes include stdio.h  include unistd.h  int main()   fork a child process  fork();  fork another child process  fork();  and fork another  fork(); return 0;  Figure 3.31 How many processes are created? 3.3 Original

3.31 How many processes are created? 3.3 Original versions of Apples mobile i OSoperating system provided no means of concurrent processing. Discuss three major complications that concurrent processing adds to an operating system. 3.4 The Sun UltraSPARC processor has multiple register sets. Describe what happens when a context switch occurs if the new context is already loaded into one of the register sets. What happens if the new context is in memory rather than in a register set and all the

memory rather than in a register set and all the register sets are in use? 3.5 When a process creates a new process using the fork() operation, which of the following states is shared between the parent process and the child process? a. Stack b. Heap c. Shared memory segments 3.6 Consider the exactly once semantic with respect to the RPC mechanism. Does the algorithm for implementing this semantic execute correctly even if the ACK message sent back to the client is lost due to a network problem?

to the client is lost due to a network problem? Describe the sequence of messages, and discuss whether exactly once is still preserved. 3.7 Assume that a distributed system is susceptible to server failure. What mechanisms would be required to guarantee the exactly once semantic for execution of RPCs? Exercises 3.8 Describe the differences among shortterm, mediumterm, and long term scheduling.Exercises 151 include stdio.h  include unistd.h  int main()  int i; for (i  0; i  4; i) fork(); return

main()  int i; for (i  0; i  4; i) fork(); return 0;  Figure 3.32 How many processes are created? 3.9 Describe the actions taken by a kernel to contextswitch between processes. 3.10 Construct a process tree similar to Figure 3.8. To obtain process infor mation for the UNIX or Linux system, use the command ps ael . include systypes.h  include stdio.h  include unistd.h  int main()  pid tp i d ;  fork a child process  pid  fork(); if (pid  0)  error occurred  fprintf(stderr, Fork Failed); return 1;

occurred  fprintf(stderr, Fork Failed); return 1;  else if (pid  0)  child process  execlp(binls,ls,NULL); printf(LINE J);  else  parent process   parent will wait for the child to complete  wait(NULL); printf(Child Complete);  return 0;  Figure 3.33 When will LINE J be reached?152 Chapter 3 Processes Use the command man ps to get more information about the pscom mand. The task manager on Windows systems does not provide the parent process ID,b u tt h e process monitor tool, available from tech

u tt h e process monitor tool, available from tech net.microsoft.com ,p r o v i d e sap r o c e s s  t r e et o o l . 3.11 Explain the role of the init process on UNIX and Linux systems in regard to process termination. 3.12 Including the initial parent process, how many processes are created by the program shown in Figure 3.32? 3.13 Explain the circumstances under which which the line of code marked printf(LINE J) in Figure 3.33 will be reached. 3.14 Using the program in Figure 3.34, identify

3.14 Using the program in Figure 3.34, identify the values of pidat lines A,B, C,a n d D.( A s s u m et h a tt h ea c t u a lp i d so ft h ep a r e n ta n dc h i l da r e2 6 0 0 and 2603, respectively.) include systypes.h  include stdio.h  include unistd.h  int main()  pid tp i d ,p i d 1 ;  fork a child process  pid  fork(); if (pid  0)  error occurred  fprintf(stderr, Fork Failed); return 1;  else if (pid  0)  child process  pid1  getpid(); printf(child: pid  d,pid);  A  printf(child: pid1

printf(child: pid  d,pid);  A  printf(child: pid1  d,pid1);  B   else  parent process  pid1  getpid(); printf(parent: pid  d,pid);  C  printf(parent: pid1  d,pid1);  D  wait(NULL);  return 0;  Figure 3.34 What are the pid values?Exercises 153 include systypes.h  include stdio.h  include unistd.h  define SIZE 5 int nums[SIZE]  0,1,2,3,4 ; int main()  int i; pid tp i d ; pid  fork(); if (pid  0)  for (i  0; i  SIZE; i)  nums[i]  i; printf(CHILD: d ,nums[i]);  LINE X    else if (pid  0)

d ,nums[i]);  LINE X    else if (pid  0)  wait(NULL); for (i  0; i  SIZE; i) printf(PARENT: d ,nums[i]);  LINE Y   return 0;  Figure 3.35 What output will be at Line X and Line Y? 3.15 Give an example of a situation in which ordinary pipes are more suitable than named pipes and an example of a situation in which named pipes are more suitable than ordinary pipes. 3.16 Consider the RPC mechanism. Describe the undesirable consequences that could arise from not enforcing either the at most once

arise from not enforcing either the at most once orexactly once semantic. Describe possible uses for a mechanism that has neither of these guarantees. 3.17 Using the program shown in Figure 3.35, explain what the output will be at lines XandY. 3.18 What are the benets and the disadvantages of each of the following? Consider both the system level and the programmer level. a. Synchronous and asynchronous communication b. Automatic and explicit buffering c. Send by copy and send by reference d.

buffering c. Send by copy and send by reference d. Fixedsized and variablesized messages154 Chapter 3 Processes Programming Problems 3.19 Using either a UNIX or a Linux system, write a C program that forks ac h i l dp r o c e s st h a tu l t i m a t e l yb e c o m e s az o m b i ep r o c e s s .T h i sz o m b i e process must remain in the system for at least 10 seconds. Process states can be obtained from the command ps l The process states are shown below the Scolumn; processes with a state

shown below the Scolumn; processes with a state ofZare zombies. The process identier (pid) of the child process is listed in the PIDcolumn, and that of the parent is listed in the PPID column. Perhaps the easiest way to determine that the child process is indeed az o m b i ei st or u nt h ep r o g r a mt h a ty o uh a v ew r i t t e ni nt h eb a c k g r o u n d (using the )a n dt h e nr u nt h ec o m m a n d ps l to determine whether the child is a zombie process. Because you do not want too

is a zombie process. Because you do not want too many zombie processes existing in the system, you will need to remove the one that you have created. The easiest way to do that is to terminate the parent process using the kill command. For example, if the process id of the parent is 4884, you would enter kill 9 4884 3.20 An operating systems pid manager is responsible for managing process identiers. When a process is rst created, it is assigned a unique pid by the pid manager. The pid is

a unique pid by the pid manager. The pid is returned to the pid manager when the process completes execution, and the manager may later reassign this pid. Process identiers are discussed more fully in Section 3.3.1. What is most important here is to recognize that process identiers must be unique; no two active processes can have the same pid. Use the following constants to identify the range of possible pid values: define MIN PID 300 define MAX PID 5000 You may use any data structure of your

PID 5000 You may use any data structure of your choice to represent the avail ability of process identiers. One strategy is to adopt what Linux has done and use a bitmap in which a value of 0 at position iindicates that ap r o c e s si do fv a l u e iis available and a value of 1 indicates that the process id is currently in use. Implement the following APIfor obtaining and releasing a pid: int allocate map(void) Createsandinitializesadatastructure for representing pids; returns1 if

for representing pids; returns1 if unsuccessful, 1 if successful int allocate pid(void) Allocatesandreturnsapid;returns 1i fu n a b l et oa l l o c a t eap i d( a l lp i d sa r ei nu s e ) void release pid(int pid) Releases a pid This programming problem will be modied later on in Chpaters 4 and 5.Programming Problems 155 3.21 The Collatz conjecture concerns what happens when we take any positive integer nand apply the following algorithm: nbraceleftbigg n2, if n is even 3n1,if n is odd The

n2, if n is even 3n1,if n is odd The conjecture states that when this algorithm is continually applied, all positive integers will eventually reach 1. For example, if n35, the sequence is 35, 106, 53, 160, 80, 40, 20, 10, 5, 16, 8, 4, 2, 1 Write a C program using the fork() system call that generates this sequence in the child process. The starting number will be provided from the command line. For example, if 8 is passed as a parameter on the command line, the child process will output 8

the command line, the child process will output 8 ,4,2,1. Because the parent and child processes have their own copies of the data, it will be necessary for the child to output the sequence. Have the parent invoke thewait() call to wait for the child process to complete before exiting the program. Perform necessary error checking to ensure that a positive integer is passed on the command line. 3.22 In Exercise 3.21, the child process must output the sequence of numbers generated from the

output the sequence of numbers generated from the algorithm specied by the Collatz conjecture because the parent and child have their own copies of the data. Another approach to designing this program is to establish a sharedmemory object between the parent and child processes. This technique allows the child to write the contents of th es e q u e n c et ot h es h a r e d  m e m o r yo b j e c t . The parent can then output the sequence when the child completes. Because the memory is shared, any

child completes. Because the memory is shared, any changes the child makes will be reected in the parent process as well. This program will be structured using POSIX shared memory as described in Section 3.5.1. The parent process will progress through the following steps: a. Establish the sharedmemory object ( shm open() ,ftruncate() , andmmap() ). b. Create the child process and wait for it to terminate. c. Output the contents of shared memory. d. Remove the sharedmemory object. One area of

d. Remove the sharedmemory object. One area of concern with c ooperating processes involves synchro nization issues. In this exercise, the parent and child processes must be coordinated so that the parent does not output the sequence until the child nishes execution. These two processes will be synchronized using thewait() system call: the parent process will invoke wait() ,w h i c h will suspend it until the child process exits. 3.23 Section 3.6.1 describes port numbers below 1024 as being well

describes port numbers below 1024 as being well known that is, they provide standard services. Port 17 is known as the quoteof156 Chapter 3 Processes theday service. When a client connects to port 17 on a server, the server responds with a quote for that day. Modify the date server shown in Figure 3.21 so that it delivers a quote of the day rather than the current date. The quotes should be printable ASCII characters and should contain fewer than 512 characters, although multiple lines are

than 512 characters, although multiple lines are allowed. Since port 17 is well known and therefore unavailable, have your server listen to port 6017. The date client shown in Figure 3.22 can be used to read the quotes returned by your server. 3.24 Ahaiku is a threeline poem in which the rst line contains ve syllables, the second line contains seven syllables, and the third line contains ve syllables. Write a haiku server that listens to port 5575. When a client connects to this port, the server

When a client connects to this port, the server responds with a haiku. The date client shown in Figure 3.22 can be used to read the quotes returned by your haiku server. 3.25 An echo server echoes back whatever it receives from a client. For example, if a client sends the server the string Hello there! ,t h es e r v e r will respond with Hello there! Write an echo server using the Java networking API described in Section 3.6.1. This server will wait for a client connection using the accept()

wait for a client connection using the accept() method. When a client connect ion is received, the server will loop, performing the following steps: Read data from the socket into a buffer. Write the contents of the buffer back to the client. The server will break out of the loop only when it has determined that the client has closed the connection. The date server shown in Figure 3.21 uses the java.io.BufferedReader class. BufferedReader extends the java.io.Reader class, which is used for

the java.io.Reader class, which is used for reading character streams. However, the echo server cannot guarantee that it will read characters from clients; it may receive binary data as well. The class java.io.InputStream deals with data at the byte level rather than the character level. Thus, your echo server must use an object that extends java.io.InputStream .T h e read() method in the java.io.InputStream class returns 1w h e nt h ec l i e n th a sc l o s e di t s end of the socket

l i e n th a sc l o s e di t s end of the socket connection. 3.26 Design a program using ordinary pipes in which one process sends a string message to a second process, and the second process reverses the case of each character in the message and sends it back to the rst process. For example, if the rst process sends the message Hi There ,t h e second process will return hI tHERE .T h i sw i l lr e q u i r eu s i n gt w op i p e s , one for sending the original message from the rst to the second

the original message from the rst to the second process and the other for sending the modied message from the second to the rst process. You can write this program using either UNIX or Windows pipes. 3.27 Design a lecopying program named filecopy using ordinary pipes. This program will be passed two parameters: the name of the le to beProgramming Projects 157 copied and the name of the copied le. The program will then create an ordinary pipe and write the contents of the le to be copied to the

write the contents of the le to be copied to the pipe. The child process will read this le from the pipe and write it to the destination le. For example, if we invoke the program as follows: filecopy input.txt copy.txt the le input.txt will be written to the pipe. The child process will read the contents of this le and write it to the destination le copy.txt . You may write this program using either UNIX or Windows pipes. Programming Projects Project 1UNIX Shell and History Feature This project

1UNIX Shell and History Feature This project consists of designing a C p rogram to serve as a shell interface that accepts user commands and t hen executes each command in a separate process. This project can be completed on any Linux, UNIX ,o rM a c OS X system. As h e l li n t e r f a c eg i v e st h eu s e rap r o m p t ,a f t e rw h i c ht h en e x tc o m m a n d is entered. The example below illustrates the prompt osh and the users next command: cat prog.c . (This command displays the le

cat prog.c . (This command displays the le prog.c on the terminal using the UNIX catcommand.) osh cat prog.c One technique for implementing a shell interface is to have the parent process rst read what the user enters on the command line (in this case, cat prog.c ), and then create a separate child process that performs the command. Unless otherwise specied, the parent process waits for the child to exit before continuing. This is similar in functionality to the new process creation illustrated

to the new process creation illustrated in Figure 3.10. However, UNIX shells typically also allow the child process to run in the background, or concurrently. To accomplish this, we add an ampersand ( )a tt h ee n do ft h ec o m m a n d .T h u s ,i fw er e w r i t et h ea b o v e command as osh cat prog.c  the parent and child processes will run concurrently. The separate child process is created using the fork() system call, and the users command is executed using one of the system calls in the

is executed using one of the system calls in the exec() family (as described in Section 3.3.1). A C program that provides the general operations of a commandline shell is supplied in Figure 3.36. The main() function presents the prompt osh and outlines the steps to be taken after in put from the user has been read. The main() function continually loops as long as should runequals 1; when the user enters exit at the prompt, your program will set should run to 0 and terminate. This project is

set should run to 0 and terminate. This project is organized into two parts: (1) creating the child process and executing the command in the child, and (2) modifying the shell to allow a history feature.158 Chapter 3 Processes include stdio.h  include unistd.h  define MAX LINE 80  The maximum length command  int main(void)  char args[MAX LINE2  1];  command line arguments  int should run  1;  flag to determine when to exit program  while (should run)  printf(osh); fflush(stdout);  A f t e rr e a

run)  printf(osh); fflush(stdout);  A f t e rr e a d i n gu s e ri n p u t ,t h es t e p sa r e : ( 1 )f o r kac h i l dp r o c e s su s i n gf o r k ( ) ( 2 )t h ec h i l dp r o c e s sw i l li n v o k ee x e c v p ( ) ( 3 )i fc o m m a n di n c l u d e d ,p a r e n tw i l li n v o k ew a i t ( )   return 0;  Figure 3.36 Outline of simple shell. Part I Creating a Child Process The rst task is to modify the main() function in Figure 3.36 so that a child process is forked and executes the comman

a child process is forked and executes the comman d specied by the user. This will require parsing what the user has entered into separate tokens and storing the tokens in an array of character strings ( args in Figure 3.36). For example, if the user enters the command ps ael at the osh prompt, the values stored in the args array are: args[0]  ps args[1]  ael args[2]  NULL This args array will be passed to the execvp() function, which has the following prototype: execvp(char command, char

the following prototype: execvp(char command, char params[]); Here, command represents the command to be performed and params stores the parameters to this command. For this project, the execvp() function should be invoked as execvp(args[0], args) . Be sure to check whether the user included an  to determine whether or not the parent process is to wait for the child to exit.Programming Projects 159 Part IICreating a History Feature The next task is to modify the shell interface program so that

is to modify the shell interface program so that it provides ahistory feature that allows the user to access the most recently entered commands. The user will be able to access up to 10 commands by using the feature. The commands will be consec utively numbered starting at 1, and the numbering will continue past 10. For example, if the user has entered 35 commands, the 10 most recent commands will be numbered 26 to 35. The user will be able to list the command history by entering the command

list the command history by entering the command history at the osh prompt. As an example, assume that the history consists of the commands (from most to least recent): ps, ls l, top, cal, who, date The command history will output: 6p s 5l s l 4t o p 3c a l 2w h o 1d a t e Your program should support two techniques for retrieving commands from the command history: 1.When the user enters !!,t h em o s tr e c e n tc o m m a n di nt h eh i s t o r yi s executed. 2.When the user enters a single

o r yi s executed. 2.When the user enters a single !followed by an integer N,t h e Nth command in the history is executed. Continuing our example from above, if the user enters !!,t h e pscommand will be performed; if the user enters !3,t h ec o m m a n d cal will be executed. Any command executed in this fashion should be echoed on the users screen. The command should also be placed in the history buffer as the next command. The program should also manage basic error handling. If there are no

also manage basic error handling. If there are no commands in the history, entering !!should result in a message No commands in history. If there is no command corresponding to the number entered with the single !,t h ep r o g r a ms h o u l do u t p u t No such command in history.  Project 2Linux Kernel Module for Listing Tasks In this project, you will write a kernel module that lists all current tasks in a Linux system. Be sure to review the programming project in Chapter 2, which deals with

programming project in Chapter 2, which deals with creating Linux kernel modules, before you begin this project. The project can be completed using the Linux virtual machine provided with this text.160 Chapter 3 Processes Part IIterating over Tasks Linearly As illustrated in Section 3.1, the PCB in Linux is represented by the structure task struct ,w h i c hi sf o u n di nt h e linuxsched.h include le. In Linux, thefor each process() macro easily allows iteration over all current tasks in the

allows iteration over all current tasks in the system: include linuxsched.h struct task struct task; for each process(task)   on each iteration task points to the next task   The various elds in task struct can then be displayed as the program loops through the for each process() macro. Part I Assignment Design a kernel module that iterates through all tasks in the system using the for each process() macro. In particular, output the task name (known as executable name ), state, and process id of

as executable name ), state, and process id of each task. (You will probably have to read through the task struct structure in linuxsched.h to obtain the names of these elds.) Write this code in the module entry point so that its contents will appear in the kernel log buffer, which can be viewed using the dmesg command. To verify that your code is working correctly, compare the contents of the kernel log buffer with the output of the following command, which lists all tasks in the system: ps el

which lists all tasks in the system: ps el The two values should be very similar. Because tasks are dynamic, however, it is possible that a few tasks may appear in one listing but not the other. Part IIIterating over Tasks with a DepthFirst Search Tree The second portion of this project involves iterating over all tasks in the system using a depthrst search ( DFS)t r e e .( A sa ne x a m p l e :t h e DFS iteration of the processes in Figure 3.8 is 1, 8415, 8416, 9298, 9204, 2, 6, 200, 3028,

3.8 is 1, 8415, 8416, 9298, 9204, 2, 6, 200, 3028, 3610, 4005.) Linux maintains its process tree as a series of lists. Examining the task struct inlinuxsched.h ,w es e et w o struct list head objects: children and siblingBibliographical Notes 161 These objects are pointers to a list of the tasks children, as well as its sib lings. Linux also maintains references to the init task ( struct task struct init task ). Using this information as well as macro operations on lists, we can iterate over the

macro operations on lists, we can iterate over the children of init as follows: struct task struct task; struct list head list; list for each(list, init taskchildren)  task  list entry(list, struct task struct, sibling);  task points to the next child in the list   The list for each() macro is passed two parameters, both of type struct list head : Ap o i n t e rt ot h eh e a do ft h el i s tt ob et r a v e r s e d Ap o i n t e rt ot h eh e a dn o d eo ft h el i s tt ob et r a v e r s e d At each

d eo ft h el i s tt ob et r a v e r s e d At each iteration of list for each() ,t h e r s tp a r a m e t e ri ss e tt ot h e list structure of the next child. We then use this value to obtain each structure in the list using the list entry() macro. Part II Assignment Beginning from the init task, design a kernel module that iterates over all tasks in the system using a DFS tree. Just as in the rst part of this project, output the name, state, and pid of each task. Perform this iteration in the

pid of each task. Perform this iteration in the kernel entry module so that its output appears in the kernel log buffer. If you output all tasks in the system, you may see many more tasks than appear with the ps ael command. This is because some threads appear as children but do not show up as ordinary processes. Therefore, to check the output of the DFStree, use the command ps eLf This command lists all tasksincluding threadsin the system. To verify that you have indeed performed an appropriate

that you have indeed performed an appropriate DFSiteration, you will have to examine the relationships among the various tasks output by the pscommand. Bibliographical Notes Process creation, management, and IPC inUNIX and Windows systems, respectively, are discussed in [Robbins and Robbins (2003)] and [Russinovich and Solomon (2009)]. [Love (2010)] covers support for processes in the Linux kernel, and [Hart (2005)] covers Windows systems programming in detail. Coverage of the multiprocess model

in detail. Coverage of the multiprocess model used in Googles Chrome can be found at http:blog.chromium.org200809multiprocessarchitecture.html .162 Chapter 3 Processes Message passing for multicore systems is discussed in [Holland and Seltzer (2011)]. [Baumann et al. (2009)] describe performance issues in shared memory and messagepassing systems. [Vahalia (1996)] describes interprocess communication in the Mach system. The implementation of RPCs is discussed by [Birrell and Nelson (1984)].

RPCs is discussed by [Birrell and Nelson (1984)]. [Staunstrup (1982)] discusses procedure calls versus messagepassing com munication. [Harold (2005)] provides coverage of socket programming in Java. [Hart (2005)] and [Robbins and Robbins (2003)] cover pipes in Windows and UNIX systems, respectively. Bibliography [Baumann et al. (2009)] A. Baumann, P . Barham, P .E. Dagand, T. Harris, R. Isaacs, P . Simon, T. Roscoe, A. Sch upbach, and A. Singhania, The multikernel: an e wO Sa r c h i t e c t u r

The multikernel: an e wO Sa r c h i t e c t u r ef o rs c a l a b l em u l t i c o r es y s t e m s (2009), pages 2944. [Birrell and Nelson (1984)] A. D. Birrell and B. J. Nelson, Implementing Remote Procedure Calls ,ACM Transactions on Computer Systems ,V o l u m e2 , Number 1 (1984), pages 3959. [Harold (2005)] E. R. Harold, Java Network Programming, Third Edition, OReilly A s s o c i a t e s( 2 0 0 5 ) . [Hart (2005)] J. M. Hart, Windows System Programming, Third Edition, Addison Wesley

System Programming, Third Edition, Addison Wesley (2005). [Holland and Seltzer (2011)] D. Holland and M. Seltzer, Multicore OSes: look ing forward from 1991, er, 2011 ,Proceedings of the 13th USENIX conference on Hot topics in operating systems (2011), pages 3333. [Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developers Library (2010). [Robbins and Robbins (2003)] K. Robbins and S. Robbins, Unix Systems Pro gramming: Communication, Concurrency and Threads, Second Edition,

Concurrency and Threads, Second Edition, Prentice Hall (2003). [Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon, Win dows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition, Microsoft Press (2009). [Staunstrup (1982)] J. Staunstrup, Message Passing Communication Versus Procedure Call Communication ,SoftwarePractice and Experience ,V o l u m e1 2 , Number 3 (1982), pages 223234. [Vahalia (1996)] U. Vahalia, Unix Internals: The New Frontiers ,P r e n t i

Unix Internals: The New Frontiers ,P r e n t i c eH a l l (1996).4CHAPTER Threads The process model introduced in Chapter 3 assumed that a process was an executing program with a single thread of control. Virtually all modern operating systems, however, provide features enabling a process to contain multiple threads of control. In this chapter, we introduce many concepts associated with multithreaded computer systems, including a discussion of the APIsf o rt h eP t h r e a d s ,W i n d o w s ,a

APIsf o rt h eP t h r e a d s ,W i n d o w s ,a n dJ a v at h r e a dl i b r a r i e s .W el o o ka ta number of issues related to multithreaded programming and its effect on the design of operating systems. Finally, we explore how the Windows and Linux operating systems support threads at the kernel level. CHAPTER OBJECTIVES To introduce the notion of a thread  a fundamental unit of CPU utilization that forms the basis of multithreaded computer systems. To discuss the APIsf o rt h eP t h r e a

systems. To discuss the APIsf o rt h eP t h r e a d s ,W i n d o w s ,a n dJ a v at h r e a dl i b r a r i e s . To explore several strategies that provide implicit threading. To examine issues related to multithreaded programming. To cover operating system support for threads in Windows and Linux. 4.1 Overview At h r e a di sab a s i cu n i to f CPU utilization; it comprises a thread ID,ap r o g r a m counter, a register set, and a stack. It shares with other threads belonging to the same

It shares with other threads belonging to the same process its code section, data section, and other operatingsystem resources, such as open les and signals. A traditional (or heavyweight ) process has a single thread of control. If a process has multiple threads of control, it can perform more than one task at a time. Figure 4.1 illustrates the difference between a traditional singlethreaded process and a multithreaded process. 4.1.1 Motivation Most software applications that run on modern

Most software applications that run on modern computers are multithreaded. An application typically is implemented as a separate process with several 163164 Chapter 4 Threadsregisterscode data files stack registers registers registerscode data files stack stack stack thread thread singlethreaded process multithreaded process Figure 4.1 Singlethreaded and multithreaded processes. threads of control. A web browser might have one thread display images or text while another thread retrieves data

images or text while another thread retrieves data from the network, for example. A word processor may have a thread for displaying graphics, another thread for responding to keystrokes from the user, and a third thread for performing spelling and grammar checking in the background. Applications can also be designed to leverage processing capabilities on multicore systems. Such applications can perform several CPUintensive tasks in parallel across the multiple computing cores. In certain

across the multiple computing cores. In certain situations, a single application may be required to perform several similar tasks. For example, a web server accepts client requests for web pages, images, sound, and so forth. A busy web server may have several (perhaps thousands of) clients concurrently accessing it. If the web server ran as a traditional singlethreaded process, it would be able to service only one client at a time, and a client might have to wait a very long time for its request

have to wait a very long time for its request to be serviced. One solution is to have the server run as a single process that accepts requests. When the server receives a request, it creates a separate process to service that request. In fact, this processcreation method was in common use before threads became popular. Process creation is time consuming and resource intensive, however. If the new process will perform the same tasks as the existing process, why incur all that overhead? It is

process, why incur all that overhead? It is generally more efcient to use one process that contains multiple threads. If the webserver process is multithreaded, the server will create a separate thread that listens for client requests. When a request is made, rather th an creating another process, the server creates a new thread to service the request and resume listening for additional requests. This is illustrated in Figure 4.2. Threads also play a vital role in remote procedure call ( RPC)s y

a vital role in remote procedure call ( RPC)s y s t e m s .R e c a l l from Chapter 3 that RPCsa l l o wi n t e r p r o c e s sc o m m u n i c a t i o nb yp r o v i d i n ga communication mechanism similar to ordinary function or procedure calls. Typically, RPC servers are multithreaded. When a server receives a message, it4.1 Overview 165client(1) request(2) create new thread to service the request (3) resume listening for additional client requestsserver thread Figure 4.2 Multithreaded server

thread Figure 4.2 Multithreaded server architecture. services the message using a separate thread. This allows the server to service several concurrent requests. Finally, most operatingsystem kernels are now multithreaded. Several threads operate in the kernel, and ea ch thread performs a specic task, such as managing devices, managing memory, or interrupt handling. For example, Solaris has a set of threads in the kernel specically for interrupt handling; Linux uses a kernel thread for managing

handling; Linux uses a kernel thread for managing the amount of free memory in the system. 4.1.2 Benets The benets of multithreaded programmin gc a nb eb r o k e nd o w ni n t of o u r major categories: 1.Responsiveness .M u l t i t h r e a d i n ga ni n t e r a c t i v ea p p l i c a t i o nm a ya l l o w a program to continue running even if part of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user. This quality is especially useful in designing

This quality is especially useful in designing user interfaces. For instance, consider what happens when a user clicks a button that results in the performance of a timeconsuming operation. A singlethreaded application would be unresponsive to the user until the operation had completed. In contrast, if the timeconsuming operation is performed in as e p a r a t et h r e a d ,t h ea p p l i c a t i o nr e m a i n sr e s p o n s i v et ot h eu s e r . 2.Resource sharing .P r o c e s s e sc a no n l

. 2.Resource sharing .P r o c e s s e sc a no n l ys h a r er e s o u r c e st h r o u g ht e c h n i q u e s such as shared memory and message passing. Such techniques must be explicitly arranged by the programmer. However, threads share the memory and the resources of the pr ocess to which they belong by default. The benet of sharing code and data is that it allows an application to have several different threads of activity within the same address space. 3.Economy .A l l o c a t i n gm e m o

space. 3.Economy .A l l o c a t i n gm e m o r ya n dr e s o u r c e sf o rp r o c e s sc r e a t i o ni sc o s t l y . Because threads share the resources of the process to which they belong, it is more economical to create and contextswitch threads. Empirically gauging the difference in overhead can be difcult, but in general it is signicantly more time consuming to create and manage processes than threads. In Solaris, for example, creating a process is about thirty times166 Chapter 4 Threads

process is about thirty times166 Chapter 4 Threads T1 T2 T3 T4 T1 T2 T3 T4 T1 single core time Figure 4.3 Concurrent execution on a singlecore system. slower than is creating a thread, and context switching is about ve times slower. 4.Scalability. The benets of multithreading can be even greater in a multiprocessor architecture, where threads may be running in parallel on different processing cores. A singlethreaded process can run on only one processor, regardless how many are available. We

processor, regardless how many are available. We explore this issue further in the following section. 4.2 Multicore Programming Earlier in the history of computer design, in response to the need for more computing performance, single CPU systems evolved into multi CPU systems. Am o r er e c e n t ,s i m i l a rt r e n di ns y s t e md e s i g ni st op l a c em u l t i p l ec o m p u t i n g cores on a single chip. Each core appears as a separate processor to the operating system (Section 1.3.2).

processor to the operating system (Section 1.3.2). Whether the cores appear across CPU chips or within CPU chips, we call these systems multicore ormultiprocessor systems. Multithreaded programming provides a mechanism for more efcient use of these multiple computing cores and improved concurrency. Consider an application with four threads. On a system with a single computing core, concurrency merely means that the execution of the threads will be interleaved over time (Figure 4.3), because the

be interleaved over time (Figure 4.3), because the processing core is capable of executing only one thread at a time. On a system with multiple cores, however, concurrency means that the threads can run in parallel, because the system can assign a separate thread to each core (Figure 4.4). Notice the distinction between parallelism and concurrency in this discus sion. A system is parallel if it can perform more than one task simultaneously. In contrast, a concurrent system supports more than one

a concurrent system supports more than one task by allowing all the tasks to make progress. Thus, it is possible to have concurrency without parallelism. Before the advent of SMP and multicore architectures, most com puter systems had only a single processor. CPU schedulers were designed to provide the illusion of parallelism by rapidly switching between processes in T1 T3 T1 T3 T1core 1 T2 T4 T2 T4 T2core 2 time  Figure 4.4 Parallel execution on a multicore system.4.2 Multicore Programming 167

a multicore system.4.2 Multicore Programming 167 AMDAHLS LAW Amdahls Law is a formula that identies potential performance gains from adding additional computing cores to an application that has both serial (nonparallel) and parallel components. If Sis the portion of the application that must be performed serially on a system with Nprocessing cores, the formula appears as follows: speedup 1 S(1S) N As an example, assume we have an application that is 75 percent parallel and 25 percent serial. If

is 75 percent parallel and 25 percent serial. If we run this application on a system with two processing cores, we can get a speedup of 1.6 times. If we add two additional cores (for a total of four), the speedup is 2.28 times. One interesting fact about Amdahls Law is that as Napproaches innity, the speedup converges to 1 S.F o re x a m p l e ,i f4 0p e r c e n to fa na p p l i c a t i o n is performed serially, the maximum speedup is 2.5 times, regardless of the number of processing cores we

regardless of the number of processing cores we add. This is the fundamental principle behind Amdahls Law: the serial portion of an application can have a disproportionate effect on the performance we gain by adding additional computing cores. Some argue that Amdahls Law does not take into account the hardware performance enhancements used in the design of contemporary multicore systems. Such arguments suggest Amdahls Law may cease to be applicable as the number of processing cores continues to

as the number of processing cores continues to increase on modern computer systems. the system, thereby allowing each process to make progress. Such processes were running concurrently, but not in parallel. As systems have grown from tens of threads to thousands of threads, CPU designers have improved system performance by adding hardware to improve thread performance. Modern Intel CPUs frequently support two threads per core, while the Oracle T4 CPU supports eight threads per core. This support

CPU supports eight threads per core. This support means that multiple threads can be loaded into the core for fast switching. Multicore computers will no doubt continue to increase in core counts and hardware thread support. 4.2.1 Programming Challenges The trend towards multicore systems continues to place pressure on system designers and application programmers to make better use of the multiple computing cores. Designers of operating systems must write scheduling algorithms that use multiple

must write scheduling algorithms that use multiple processing cores to allow the parallel execution shown in Figure 4.4. For application programmers, the challenge is to modify existing programs as well as design new programs that are multithreaded. In general, ve areas present challenges in programming for multicore systems:168 Chapter 4 Threads 1.Identifying tasks . This involves examining applications to nd areas that can be divided into separate, concurrent tasks. Ideally, tasks are

separate, concurrent tasks. Ideally, tasks are independent of one another and thus can run in parallel on individual cores. 2.Balance .W h i l ei d e n t i f y i n gt a s k st h a tc a nr u ni np a r a l l e l ,p r o g r a m m e r s must also ensure that the tasks perform equal work of equal value. In some instances, a certain task may not contribute as much value to the overall process as other tasks. Using a separate execution core to run that task may not be worth the cost. 3.Data splitting

task may not be worth the cost. 3.Data splitting .J u s ta sa p p l i c a t i o n sa r ed i v i d e di n t os e p a r a t et a s k s ,t h e data accessed and manipulated by the tasks must be divided to run on separate cores. 4.Data dependency .T h ed a t aa c c e s s e db yt h et a s k sm u s tb ee x a m i n e df o r dependencies between two or mor et a s k s .W h e no n et a s kd e p e n d so n data from another, programmers must ensure that the execution of the tasks is synchronized to

that the execution of the tasks is synchronized to accommodate the data dependency. We examine such strategies in Chapter 5. 5.Testing and debugging .W h e nap r o g r a mi sr u n n i n gi np a r a l l e lo n multiple cores, many different execution paths are possible. Testing and debugging such concurrent programs is inherently more difcult than testing and debugging singlethreaded applications. Because of these challenges, many software developers argue that the advent of multicore systems

argue that the advent of multicore systems will require an entirely new approach to designing software systems in the future. (Similarly, many computer science educators believe that software development must be taught with increased emphasis on parallel programming.) 4.2.2 Types of Parallelism In general, there are two types of parallelism: data parallelism and task parallelism. Data parallelism focuses on distributing subsets of the same data across multiple computing cores and performing the

across multiple computing cores and performing the same operation on each core. Consider, for example, summing the contents of an array of size N.O na singlecore system, one thread would simply sum the elements [0] . . . [ N1]. On a dualcore system, however, thread A,r u n n i n go nc o r e0 ,c o u l ds u mt h e elements [0] . . . [ N21] while thread B,r u n n i n go nc o r e1 ,c o u l ds u mt h e elements [ N2] . . . [ N1]. The two threads would be running in parallel on separate computing

would be running in parallel on separate computing cores. Task parallelism involves distributing not data but tasks (threads) across multiple computing cores. Each thread is performing a unique operation. Different threads may be operating on the same data, or they may be operating on different data. Consider again our example above. In contrast to that situation, an example of task parallelism might involve two threads, each performing a unique statistical operation on the array of elements.

statistical operation on the array of elements. The threads again are operating in parallel on separate computing cores, but each is performing a unique operation.4.3 Multithreading Models 169 Fundamentally, then, data parallelism involves the distribution of data across multiple cores and task parallelism on the distribution of tasks across multiple cores. In practice, however, f ew applications strictly follow either data or task parallelism. In most instances, applications use a hybrid of

In most instances, applications use a hybrid of these two strategies. 4.3 Multithreading Models Our discussion so far has treated threads in a generic sense. However, support for threads may be provided either at the user level, for user threads ,o rb yt h e kernel, for kernel threads .U s e rt h r e a d sa r es u p p o r t e da b o v et h ek e r n e la n d are managed without kernel support, whereas kernel threads are supported and managed directly by the operating system. Virtually all

directly by the operating system. Virtually all contemporary operating systemsincluding Windows, Linux, Mac OS X ,a n dS o l a r i s  support kernel threads. Ultimately, a relationship must exist betw een user threads and kernel threads. In this section, we look at three common ways of establishing such a relationship: the manytoone model, the onetoone model, and the manyto many model. 4.3.1 ManytoOne Model The manytoone model (Figure 4.5) maps many userlevel threads to one kernel thread. Thread

userlevel threads to one kernel thread. Thread management is d one by the thread library in user space, so it is efcient (we discuss thread libraries in Section 4.4). However, the entire process will block if a thread makes a blocking system call. Also, because only one thread can access the kernel at a time, multiple threads are unable to run in parallel on multicore systems. Green threads a thread library available for Solaris systems and adopted in early versions of Javaused the manytoone

in early versions of Javaused the manytoone model. However, very few systems continue to use the model because of its inability to take advantage of multiple processing cores. user thread kernel thread kFigure 4.5 Manytoone model.170 Chapter 4 Threads user thread kernel thread k k k k Figure 4.6 Onetoone model. 4.3.2 OnetoOne Model The onetoone model (Figure 4.6) maps each user thread to a kernel thread. It provides more concurrency than the manytoone model by allowing another thread to run when

model by allowing another thread to run when a thread makes a blocking system call. It also allows multiple threads to run in parallel on multiprocessors. The only drawback to this model is that creating a user thread requires creating the corresponding kernel thread. Because the overhead of creating kernel threads can burden the performance of an application, most implementations of this model restrict the number of threads supported by the system. Linux, along with the family of Windows

system. Linux, along with the family of Windows operating systems, implement the onetoone model. 4.3.3 ManytoMany Model The manytomany model (Figure 4.7) multiplexes many userlevel threads to as m a l l e ro re q u a ln u m b e ro fk e r n e lt h r e ads. The number of kernel threads may be specic to either a particular application or a particular machine (an application may be allocated more kernel threads on a multiprocessor than on as i n g l ep r o c e s s o r ) . Lets consider the effect of

ep r o c e s s o r ) . Lets consider the effect of this design on concurrency. Whereas the many toone model allows the developer to create as many user threads as she wishes, it does not result in true concurrency, because the kernel can schedule only one thread at a time. The onetoone model allows greater concurrency, but the developer has to be careful not to create too many threads within an application (and in some instances may be limited in the number of threads she can user thread kernel

the number of threads she can user thread kernel thread k k kFigure 4.7 Manytomany model.4.4 Thread Libraries 171 user thread kernel thread k k kk Figure 4.8 Twolevel model. create). The manytomany model suffers from neither of these shortcomings: developers can create as many user threads as necessary, and the corresponding kernel threads can run in parallel on a multiprocessor. Also, when a thread performs a blocking system call, the kernel can schedule another thread for execution. One

can schedule another thread for execution. One variation on the manytomany model still multiplexes many user level threads to a smaller or equal number of kernel threads but also allows a userlevel thread to be bound to a kernel thread. This variation is sometimes referred to as the twolevel model (Figure 4.8). The Solaris operating system supported the twolevel model in versions older than Solaris 9. However, beginning with Solaris 9, this system uses the onetoone model. 4.4 Thread Libraries

uses the onetoone model. 4.4 Thread Libraries Athread library provides the programmer with an API for creating and managing threads. There are two primary ways of implementing a thread library. The rst approach is to provide a library entirely in user space with no kernel support. All code and data structures for the library exist in user space. This means that invoking a function in the library results in a local function call in user space and not a system call. The second approach is t oi m p

not a system call. The second approach is t oi m p l e m e n tak e r n e l  l e v e ll i b r a r ys u p p o r t e d directly by the operating system. In this case, code and data structures for the library exist in kernel space. Invoking a function in the APIfor the library typically results in a system call to the kernel. Three main thread libraries are in use today: POSIX Pthreads, Windows, and Java. Pthreads, the threads extension of the POSIX standard, may be provided as either a userlevel or

standard, may be provided as either a userlevel or a kernellevel library. The Windows thread library is a kernellevel library available on Windows systems. The Java thread API allows threads to be created and managed directly in Java programs. However, because in most instances the JVM is running on top of a host operating system, the Java thread APIis generally implemented using a thread library available on the host system. This means that on Windows systems, Java threads are typically

on Windows systems, Java threads are typically implemented using the Windows API;UNIX and Linux systems often use Pthreads.172 Chapter 4 Threads For POSIX and Windows threading, any data declared globallythat is, declared outside of any functionare shared among all threads belonging to the same process. Because Java has no notion of global data, access to shared data must be explicitly arranged between threads. Data declared local to a function are typically stored on the stack. Since each

are typically stored on the stack. Since each thread has its own stack, each thread has its own copy of local data. In the remainder of this section, we describe basic thread creation using these three thread libraries. As an illustrative example, we design a multi threaded program that performs the summation of a nonnegative integer in a separate thread using the wellknown summation function: sum Nsummationdisplay i0i For example, if Nwere 5, this function would represent the summation of

5, this function would represent the summation of integers from 0 to 5, which is 15. Each of the three programs will be run with the upper bounds of the summation entered on the command line. Thus, if the user enters 8, the summation of the integer values from 0 to 8 will be output. Before we proceed with our examples of thread creation, we introduce two general strategies for creating multiple th reads: asynchronous threading and synchronous threading. With asynchronous threading, once the

threading. With asynchronous threading, once the parent creates a child thread, the parent resumes its execution, so that the parent and child execute concurrently. Each thr ead runs independently of every other thread, and the parent thread need no tk n o ww h e ni t sc h i l dt e r m i n a t e s .B e c a u s e the threads are independent, there is typically little data sharing between threads. Asynchronous threading is the strategy used in the multithreaded server illustrated in Figure 4.2.

multithreaded server illustrated in Figure 4.2. Synchronous threading occurs when the parent thread creates one or more children and then must wait for all of its children to terminate before it resumes the socalled forkjoin strategy. Here, the threads created by the parent perform work concurrently, but the parent cannot continue until this work has been completed. Once each thread has nished its work, it terminates and joins with its parent. Only after all of the children have joined can the

Only after all of the children have joined can the parent resume execution. Typically, synchronous threading involves signicant data sharing among threads. For example, the parent thread may combine the results calculated by its various children. All of the following examples use synchronous threading. 4.4.1 Pthreads Pthreads refers to the POSIX standard ( IEEE 1003.1c) dening an APIfor thread creation and synchronization. This is a specication for thread behavior, not an implementation .O p e r

thread behavior, not an implementation .O p e r a t i n g  s y s t e md e s i g n e r sm a yi m p l e m e n tt h e specication in any way they wish. Numerous systems implement the Pthreads specication; most are UNIX type systems, including Linux, Mac OS X ,a n d Solaris. Although Windows doesnt support Pthreads natively, some third party implementations for Windows are available. The C program shown in Figure 4.9 demonstrates the basic Pthreads APIfor constructing a multithreaded program that

APIfor constructing a multithreaded program that calculates the summation of a non negative integer in a separate thread. In a Pthreads program, separate threads4.4 Thread Libraries 173 include pthread.h  include stdio.h  int sum;  this data is shared by the thread(s)  void runner(void param);  threads call this function  int main(int argc, char argv[])  pthread tt i d ; t h et h r e a di d e n t i f i e r  pthread attr ta t t r ; s e to ft h r e a da t t r i b u t e s  if (argc ! 2)

to ft h r e a da t t r i b u t e s  if (argc ! 2)  fprintf(stderr,usage: a.out integer value n); return 1;  if (atoi(argv[1])  0)  fprintf(stderr,d must be  0 n,atoi(argv[1])); return 1;   get the default attributes  pthread attr init(attr);  create the thread  pthread create(tid,attr,runner,argv[1]);  wait for the thread to exit  pthread join(tid,NULL); printf(sum  d n,sum);   The thread will begin control in this function  void runner(void param)  int i, upper  atoi(param); sum  0; for (i  1;

int i, upper  atoi(param); sum  0; for (i  1; i  upper; i) sum  i; pthread exit(0);  Figure 4.9 Multithreaded C program using the Pthreads API. begin execution in a specied function. In Figure 4.9, this is the runner() function. When this program begins, a single thread of control begins in main() .A f t e rs o m ei n i t i a l i z a t i o n , main() creates a second thread that begins control in the runner() function. Both threads share the global data sum. Lets look more closely at this

global data sum. Lets look more closely at this program. All Pthreads programs must include the pthread.h header le. The statement pthread tt i d declares174 Chapter 4 Threads define NUM THREADS 10  an array of threads to be joined upon  pthread tw o r k e r s [ N U M THREADS]; for (int i  0; i  NUM THREADS; i) pthread join(workers[i], NULL); Figure 4.10 Pthread code for joining ten threads. the identier for the thread we will create. Each thread has a set of attributes, including stack size and

has a set of attributes, including stack size and scheduling information. The pthread attr ta t t r declaration represents the attributes for the thread. We set the attributes in the function call pthread attr init(attr) .B e c a u s ew ed i dn o te x p l i c i t l ys e t any attributes, we use the default attributes provided. (In Chapter 6, we discuss some of the scheduling attributes provided by the Pthreads API.) A separate thread is created with the pthread create() function call. In

with the pthread create() function call. In addition to passing the thread identier and the attributes for the thread, we also pass the name of the function where the new thread will begin executionin this case, therunner() function. Last, we pass the integer parameter that was provided on the command line, argv[1] . At this point, the program has two threads: the initial (or parent) thread inmain() and the summation (or child) thread performing the summation operation in the runner() function.

the summation operation in the runner() function. This program follows the forkjoin strategy described earlier: after creating the summation thread, the parent thread will wait for it to terminate by calling the pthread join() function. The summation thread will terminate when it calls the function pthread exit() . Once the summation thread has returned, the parent thread will output the value of the shared data sum. This example program creates only a single thread. With the growing dominance

only a single thread. With the growing dominance of multicore systems, writing programs containing several threads has become increasingly common. A simple method for waiting on several threads using the pthread join() function is to enclose the operation within as i m p l e forloop. For example, you can join on ten threads using the Pthread code shown in Figure 4.10. 4.4.2 Windows Threads The technique for creating threads using the Windows thread library is similar to the Pthreads technique in

library is similar to the Pthreads technique in several ways. We illustrate the Windows thread APIin the C program shown in Figure 4.11. Notice that we must include the windows.h header le when using the Windows API. Just as in the Pthreads version shown in Figure 4.9, data shared by the separate threadsin this case, Sumare declared globally (the DWORD data type is an unsigned 32bit integer). We also dene the Summation() function that is to be performed in a separate thread. This function is

performed in a separate thread. This function is passed a pointer to a void ,w h i c hW i n d o w sd e  n e sa s LPVOID .T h et h r e a dp e r f o r m i n gt h i s function sets the global data Sumto the value of the summation from 0 to the parameter passed to Summation() .4.4 Thread Libraries 175 include windows.h  include stdio.h  DWORD Sum;  data is shared by the thread(s)   the thread runs in this separate function  DWORD WINAPI Summation(LPVOID Param)  DWORD Upper  (DWORD)Param; for (DWORD

Param)  DWORD Upper  (DWORD)Param; for (DWORD i  0; i  Upper; i) Sum  i; return 0;  int main(int argc, char argv[])  DWORD ThreadId; HANDLE ThreadHandle; int Param; if (argc ! 2)  fprintf(stderr,An integer parameter is required n); return 1;  Param  atoi(argv[1]); if (Param  0)  fprintf(stderr,An integer  0 is required n); return 1;   create the thread  ThreadHandle  CreateThread( NULL,  default security attributes  0,  default stack size  Summation,  thread function  Param,  parameter to thread

thread function  Param,  parameter to thread function  0,  default creation flags  ThreadId);  returns the thread identifier  if (ThreadHandle ! NULL)   now wait for the thread to finish  WaitForSingleObject(ThreadHandle,INFINITE);  close the thread handle  CloseHandle(ThreadHandle); printf(sum  d n,Sum);   Figure 4.11 Multithreaded C program using the Windows API.176 Chapter 4 Threads Threads are created in the Windows API using the CreateThread() function, andjust as in Pthreadsa set of

function, andjust as in Pthreadsa set of attributes for the thread is passed to this function. These attributes include security information, the size of the stack, and a ag that can be set to indicate if the thread is to start in a suspended state. In this program, we use the default values for these attributes. (The default values do not initially set the thread to a suspended state and instead make it eligible to be run by the CPU scheduler.) Once the summation thread is created, the parent

Once the summation thread is created, the parent must wait for it to complete before outputting the value ofSum, as the value is set by the summation thread. Recall that the Pthread program (Figure 4.9) had the parent thread wait for the summation thread using the pthread join() statement. We perform the equivalent of this in the Windows APIusing the WaitForSingleObject() function, which causes the creating thread to block until the summation thread has exited. In situations that require waiting

has exited. In situations that require waiting for multiple threads to complete, the WaitForMultipleObjects() function is used. This function is passed four parameters: 1.The number of objects to wait for 2.Ap o i n t e rt ot h ea r r a yo fo b j e c t s 3.A a gi n d i c a t i n gw h e t h e ra l lo b j e c t sh a v eb e e ns i g n a l e d 4.At i m e o u td u r a t i o n( o r INFINITE ) For example, if THandles is an array of thread HANDLE objects of size N,t h e parent thread can wait for all

of size N,t h e parent thread can wait for all its child threads to complete with this statement: WaitForMultipleObjects(N, THandles, TRUE, INFINITE); 4.4.3 Java Threads Threads are the fundamental model of program execution in a Java program, and the Java language and its APIprovide a rich set of features for the creation and management of threads. All Java programs comprise at least a single thread of controleven a simple Java program consisting of only a main() method runs as a single thread

of only a main() method runs as a single thread in the JVM. Java threads are available on any system that provides a JVM including Windows, Linux, and Mac OS X .T h eJ a v at h r e a d API is available for Android applications as well. There are two techniques for creating threads in a Java program. One approach is to create a new class that is derived from the Thread class and to override its run() method. An alternativeand more commonly used technique is to dene a class that implements the

technique is to dene a class that implements the Runnable interface. The Runnable interface is dened as follows: public interface Runnable  public abstract void run();  When a class implements Runnable ,i tm u s td e  n ea run() method. The code implementing the run() method is what runs as a separate thread.4.5 Implicit Threading 177 Figure 4.12 shows the Java version of a multithreaded program that determines the summation of a nonnegative integer. The Summation class implements the Runnable

The Summation class implements the Runnable interface. Thread creation is performed by creating an object instance of the Thread class and passing the constructor a Runnable object. Creating a Thread object does not specically create the new thread; rather, thestart() method creates the new thread. Calling the start() method for the new object does two things: 1.It allocates memory and initializes a new thread in the JVM. 2.It calls the run() method, making the thread eligible to be run by the

making the thread eligible to be run by the JVM. (Note again that we never call the run() method directly. Rather, we call thestart() method, and it calls the run() method on our behalf.) When the summation program runs, the JVM creates two threads. The rst is the parent thread, which starts execution in the main() method. The second thread is created when the start() method on the Thread object is invoked. This child thread begins execution in the run() method of the Summation class. After

in the run() method of the Summation class. After outputting the value of the summation, this thread terminates when it exits from its run() method. Data sharing between threads occurs easily in Windows and Pthreads, since shared data are simply declared globally. As a pure objectoriented language, Java has no such notion of global data. If two or more threads are to share data in a Java program, the sharing occurs by passing references to the shared object to the appropriate threads. In the

shared object to the appropriate threads. In the Java program shown in Figure 4.12, the main thread and the summation thread share the object instance of the Sum class. This shared object is referenced through the appropriate getSum() and setSum() methods. (You might wonder why we dont use an Integer object rather than designing a new sumclass. The reason is that the Integer class is immutable that is, once its value is set, it cannot change.) Recall that the parent threads in the Pthreads and

Recall that the parent threads in the Pthreads and Windows libraries use pthread join() and WaitForSingleObject() (respectively) to wait for the summation threads to nish before proceeding. The join() method in Java provides similar functionality. (Notice that join() can throw an InterruptedException ,w h i c hw ec h o o s et oi g n o r e . )I ft h ep a r e n tm u s tw a i t for several threads to nish, the join() method can be enclosed in a forloop similar to that shown for Pthreads in Figure

similar to that shown for Pthreads in Figure 4.10. 4.5 Implicit Threading With the continued growth of multicore processing, applications containing hundredsor even thousandsof threads are looming on the horizon. Designing such applications is not a trivial undertaking: programmers must address not only the challenges outlined in Section 4.2 but additional difculties as well. These difculties, which relate to program correctness, are covered in Chapters 5 and 7. One way to address these

in Chapters 5 and 7. One way to address these difculties and better support the design of multithreaded applications is to transfer the creation and management of178 Chapter 4 Threads class Sum  private int sum; public int getSum()  return sum;  public void setSum(int sum)  this.sum  sum;   class Summation implements Runnable  private int upper; private Sum sumValue; public Summation(int upper, Sum sumValue)  this.upper  upper; this.sumValue  sumValue;  public void run()  int sum  0; for (int i

public void run()  int sum  0; for (int i  0; i  upper; i) sum  i; sumValue.setSum(sum);   public class Driver  public static void main(String[] args)  if (args.length  0)  if (Integer.parseInt(args[0])  0) System.err.println(args[0]   must be  0.); else Sum sumObject  new Sum(); int upper  Integer.parseInt(args[0]); Thread thrd  new Thread(new Summation(upper, sumObject)); thrd.start(); try thrd.join(); System.out.println (The sum of upper is sumObject.getSum()); catch (InterruptedException

sumObject.getSum()); catch (InterruptedException ie)    else System.err.println(Usage: Summation integer value);   Figure 4.12 Java program for the summation of a nonnegative integer.4.5 Implicit Threading 179 THE JVM AND THE HOST OPERATING SYSTEM The JVM is typically implemented on top of a host operating system (see Figure 16.10). This setup allows the JVM to hide the implementation details of the underlying operating system and to provide a consistent, abstract environment that allows Java

consistent, abstract environment that allows Java programs to operate on any platform that supports a JVM.T h es p e c i  c a t i o nf o rt h e JVM does not indicate how Java threads are to be mapped to the underlying operating system, instead leaving that decision to the particular implementation of the JVM.F o re x a m p l e ,t h e Windows XPoperating system uses the onetoone model; therefore, each Java thread for a JVM running on such a system maps to a kernel thread. On operating systems

maps to a kernel thread. On operating systems that use the manytomany model (such as Tru64 UNIX ), a Java thread is mapped according to the manytomany model. Solaris initially implemented the JVM using the manytoone model (the green threads library, mentioned earlier). Later releases of the JVM were implemented using the manytomany model. Beginning with Solaris 9, Java threads were mapped using the onetoone model. In addition, ther em a yb ear e l a t i o n s h i pb e t w e e n the Java thread

e l a t i o n s h i pb e t w e e n the Java thread library and the thread library on the host operating system. For example, implementations of a JVM for the Windows family of operating systems might use the Windows API when creating Java threads; Linux, Solaris, and Mac OS X systems might use the Pthreads API. threading from application developers to compilers and runtime libraries. This strategy, termed implicit threading , is a popular trend today. In this section, we explore three

trend today. In this section, we explore three alternative approaches for designing multithreaded programs that can take advantage of multicore processors through implicit threading. 4.5.1 Thread Pools In Section 4.1, we described a multithreaded web server. In this situation, whenever the server receives a request, it creates a separate thread to service the request. Whereas creating a separate th read is certainly superior to creating as e p a r a t ep r o c e s s ,am u l t i t h r e a d e ds

r a t ep r o c e s s ,am u l t i t h r e a d e ds e r v e rn o n e t h e l e s sh a sp o t e n t i a lp r o b l e m s . The rst issue concerns the amount of time required to create the thread, together with the fact that the thread will be discarded once it has completed its work. The second issue is more troublesome. If we allow all concurrent requests to be serviced in a new thread, we have not placed a bound on the number of threads concurrently active in the system. Unlimited threads could

active in the system. Unlimited threads could exhaust system resources, such as CPU time or memory. One solution to this problem is to use a thread pool . The general idea behind a thread pool is to create a number of threads at process startup and place them into a pool, where they sit and wait for work. When a server receives a request, it awakens a thread from this poolif one is availableand passes it the request for service. Once the thread completes its service, it returns to the pool and

completes its service, it returns to the pool and awaits more work. If the pool contains no available thread, the server wa its until one becomes free.180 Chapter 4 Threads Thread pools offer these benets: 1.Servicing a request with an existing thread is faster than waiting to create at h r e a d . 2.At h r e a dp o o ll i m i t st h en u m b e ro ft h r e a d st h a te x i s ta ta n yo n ep o i n t . This is particularly important on systems that cannot support a large number of concurrent

that cannot support a large number of concurrent threads. 3.Separating the task to be performed from the mechanics of creating the task allows us to use different strategies for running the task. For example, the task could be scheduled to execute after a time delay or to execute periodically. The number of threads in the pool can be set heuristically based on factors such as the number of CPUsi nt h es y s t e m ,t h ea m o u n to fp h y s i c a lm e m o r y , and the expected number of

s i c a lm e m o r y , and the expected number of concurrent client requests. More sophisticated threadpool architectures can dynamically adjust the number of threads in the pool according to usage patterns. Such architectures provide the further benet of having a smaller poolthereby consuming less memorywhen the load on the system is low. We discuss one such architecture, Apples Grand Central Dispatch, later in this section. The Windows APIprovides several functions related to thread pools.

several functions related to thread pools. Using the thread pool APIis similar to creating a thread with the Thread Create() function, as described in Section 4.4.2. Here, a function that is to run as a separate thread is dened. Such a function may appear as follows: DWORD WINAPI PoolFunction(AVOID Param)   t h i sf u n c t i o nr u n sa sas e p a r a t et h r e a d .   Ap o i n t e rt o PoolFunction() is passed to one of the functions in the thread pool API,a n dat h r e a df r o mt h ep o o le

pool API,a n dat h r e a df r o mt h ep o o le x e c u t e st h i sf u n c t i o n .O n es u c hm e m b e r in the thread pool APIis the QueueUserWorkItem() function, which is passed three parameters: LPTHREAD START ROUTINE Function apointertothefunctionthatisto run as a separate thread PVOID Param the parameter passed to Function ULONG Flags ags indicating how the thread pool is to create and manage execution of the thread An example of invoking a function is the following:

example of invoking a function is the following: QueueUserWorkItem(PoolFunction, NULL, 0); This causes a thread from the thread pool to invoke PoolFunction() on behalf of the programmer. In this instance, we pass no parameters to PoolFunc4.5 Implicit Threading 181 tion() .B e c a u s ew es p e c i f y 0as a ag, we provide the thread pool with no special instructions for thread creation. Other members in the Windows thread pool APIinclude utilities that invoke functions at periodic intervals or

that invoke functions at periodic intervals or when an asynchronous IOrequest completes. The java.util.concurrent package in the Java APIprovides a threadpool utility as well. 4.5.2 OpenMP Open MPis a set of compiler directives as well as an APIfor programs written in C, C, or FORTRAN that provides support for parallel programming in sharedmemory environments. Open MPidenties parallel regions as blocks of code that may run in parallel. Application developers insert compiler directives into their

developers insert compiler directives into their code at parallel region s, and these directives instruct the Open MPruntime library to execute the region in parallel. The following C program illustrates a compiler directive above the parallel region containing theprintf() statement: include omp.h  include stdio.h  int main(int argc, char argv[])   sequential code  pragma omp parallel  printf(I am a parallel region.);   sequential code  return 0;  When Open MPencounters the directive pragma omp

When Open MPencounters the directive pragma omp parallel it creates as many threads are there are processing cores in the system. Thus, for ad u a l  c o r es y s t e m ,t w ot h r e a d sa r ec r e a t e d ,f o raq u a d  c o r es y s t e m ,f o u ra r e created; and so forth. All the thr eads then simultaneously execute the parallel region. As each thread exits the parallel region, it is terminated. Open MPprovides several additional directives for running code regions in parallel, including

for running code regions in parallel, including parallelizing loops. For example, assume we have two arrays aand bof size N.W ew i s ht os u mt h e i rc o n t e n t sa n dp l a c et h er e s u l t s in array c.W ec a nh a v et h i st a s kr u ni np a r a l l e lb yu s i n gt h ef o l l o w i n gc o d e segment, which contains the compiler directive for parallelizing forloops:182 Chapter 4 Threads pragma omp parallel for for (i  0; i  N; i)  c[i]  a[i]  b[i];  Open MPdivides the work contained in

a[i]  b[i];  Open MPdivides the work contained in the forloop among the threads it has created in response to the directive pragma omp parallel for In addition to providing directives for parallelization, Open MPallows devel opers to choose among several levels of parallelism. For example, they can set the number of threads manually. I ta l s oa l l o w sd e v e l o p e r st oi d e n t i f yw h e t h e r data are shared between threads or are private to a thread. Open MPis available on several

to a thread. Open MPis available on several opensource and commercial c ompilers for Linux, Windows, and Mac OS X systems. We encourage readers interested in learning more about Open MPto consult the bibliography at the end of the chapter. 4.5.3 Grand Central Dispatch Grand Central Dispatch ( GCD )a technology for Apples Mac OS X and i OS operating systemsis a combination of extensions to the C language, an API, and a runtime library that allows application developers to identify sections of

application developers to identify sections of code to run in parallel. Like OpenMP , GCD manages most of the details of threading. GCD identies extensions to the C and C languages known as blocks .A block is simply a selfcontained unit of work. It is specied by a caret  inserted in front of a pair of braces .As i m p l ee x a m p l eo fab l o c ki ss h o w nb e l o w : printf(I am a block);  GCD schedules blocks for runtime execution by placing them on a dispatch queue . When it removes a block

them on a dispatch queue . When it removes a block from a queue, it assigns the block to an available thread from the thread pool it manages. GCD identies two types of dispatch queues: serial and concurrent . Blocks placed on a serial queue are removed in FIFO order. Once a block has been removed from the queue, it must complete execution before another block is removed. Each process has its own serial queue (known as its main queue ). Developers can create additional serial queues that are

can create additional serial queues that are local to particular processes. Serial queues are useful for ensuring the sequential execution of several tasks. Blocks placed on a concurrent queue are also removed in FIFO order, but several blocks may be removed at a time, thus allowing multiple blocks to execute in parallel. There are three systemwide concurrent dispatch queues, and they are distinguished according to priority: low, default, and high. Priorities represent an approximation of the

high. Priorities represent an approximation of the relative importance of blocks. Quite simply, blocks with a higher priority should be placed on the high priority dispatch queue. The following code segment illustrates obtaining the defaultpriority concurrent queue and submitting a block to the queue using the dispatch async()function:4.6 Threading Issues 183 dispatch queue tq u e u ed i s p a t c h get global queue (DISPATCH QUEUE PRIORITY DEFAULT, 0); dispatch async(queue,  printf(I am a

DEFAULT, 0); dispatch async(queue,  printf(I am a block.); ); Internally, GCD s thread pool is composed of POSIX threads. GCD actively manages the pool, allowing the number of threads to grow and shrink according to application demand and system capacity. 4.5.4 Other Approaches Thread pools, Open MP,a n dG r a n dC e n t r a lD i s p a t c ha r ej u s taf e wo fm a n y emerging technologies for managing m ultithreaded applications. Other com mercial approaches include parallel and concurrent

mercial approaches include parallel and concurrent libraries, such as Intels Threading Building Blocks ( TBB)a n ds e v e r a lp r o d u c t sf r o mM i c r o s o f t .T h eJ a v a language and APIhave seen signicant movement toward supporting concur rent programming as well. A notable example is the java.util.concurrent package, which supports implicit thread creation and management. 4.6 Threading Issues In this section, we discuss some of the issues to consider in designing multithreaded

the issues to consider in designing multithreaded programs. 4.6.1 The fork() and exec() System Calls In Chapter 3, we described how the fork() system call is used to create a separate, duplicate process. The semantics of the fork() and exec() system calls change in a multithreaded program. If one thread in a program calls fork() ,d o e st h en e wp r o c e s sd u p l i c a t e all threads, or is the new process singlethreaded? Some UNIX systems have chosen to have two versions of fork() , one

have chosen to have two versions of fork() , one that duplicates all threads and another that duplicates only the thread that invoked the fork() system call. The exec() system call typically works in the same way as described in Chapter 3. That is, if a thread invokes the exec() system call, the program specied in the parameter to exec() will replace the entire processincluding all threads. Which of the two versions of fork() to use depends on the application. Ifexec() is called immediately

on the application. Ifexec() is called immediately after forking, then duplicating all threads is unnecessary, as the program specied in the parameters to exec() will replace the process. In this instance, duplicating only the calling thread is appropriate. If, however, the separate process does not call exec() after forking, the separate process should duplicate all threads. 4.6.2 Signal Handling Asignal is used in UNIX systems to notify a process that a particular event has occurred. A signal

that a particular event has occurred. A signal may be received eith er synchronously or asynchronously,184 Chapter 4 Threads depending on the source of and the re ason for the event being signaled. All signals, whether synchronous or asynchronous, follow the same pattern: 1.As i g n a li sg e n e r a t e db yt h eo c c u r r e n c eo fap a r t i c u l a re v e n t . 2.The signal is delivered to a process. 3.Once delivered, the signal must be handled. Examples of synchronous signal include

be handled. Examples of synchronous signal include illegal memory access and divi sion by 0. If a running program performs either of these actions, a signal is generated. Synchronous signals are delivered to the same process that performed the operation that caused the signal (that is the reason they are considered synchronous). When a signal is generated by an event e xternal to a running process, that process receives the signal asynchronously. Examples of such signals include terminating a

Examples of such signals include terminating a process with specic keystrokes (such as control C)a n d having a timer expire. Typically, an asynchronous signal is sent to another process. As i g n a lm a yb e handled by one of two possible handlers: 1.Ad e f a u l ts i g n a lh a n d l e r 2.Au s e r  d e  n e ds i g n a lh a n d l e r Every signal has a default signal handler that the kernel runs when handling that signal. This default action can be overridden by a userdened signal handler that

be overridden by a userdened signal handler that is called to handle the signal. Signals are handled in different ways. Some signals (such as changing the size of a window) are simply ignored; others (such as an illegal memory access) are handled by terminating the program. Handling signals in singlethreaded programs is straightforward: signals are always delivered to a process. However, delivering signals is more complicated in multithreaded programs, where a process may have several threads.

where a process may have several threads. Where, then, should a signal be delivered? In general, the following options exist: 1.Deliver the signal to the thread to which the signal applies. 2.Deliver the signal to every thread in the process. 3.Deliver the signal to certain threads in the process. 4.Assign a specic thread to receive all signals for the process. The method for delivering a signal depends on the type of signal generated. For example, synchronous signals need to be delivered to the

synchronous signals need to be delivered to the thread causing the signal and not to other threads in the process. However, the situation with asynchronous signals is not as clear. Some asynchronous signalssuch as a signal that terminates a process ( control C,f o re x a m p l e )  s h o u l db e sent to all threads.4.6 Threading Issues 185 The standard UNIX function for delivering a signal is kill(pid tp i d ,i n ts i g n a l ) This function species the process ( pid)t ow h i c hap a r t i c u

the process ( pid)t ow h i c hap a r t i c u l a rs i g n a l( signal )i s to be delivered. Most multithreaded versions of UNIX allow a thread to specify which signals it will accept and which it will block. Therefore, in some cases, an asynchronous signal may be delivered only to those threads that are not blocking it. However, bec ause signals need to be handled only once, a signal is typically delivered only to the rst thread found that is not blocking it. POSIX Pthreads provides the

is not blocking it. POSIX Pthreads provides the following function, which allows a signal to be delivered to a specied thread ( tid): pthread kill(pthread tt i d ,i n ts i g n a l ) Although Windows does not explicitly provide support for signals, it allows us to emulate them using asynchronous procedure calls (APCs).T h e APC facility enables a user thread to specify a function that is to be called when the user thread receives noti cation of a particular event. As indicated by its name, an APC

particular event. As indicated by its name, an APC is roughly equivalent to an asynchronous signal in UNIX . However, whereas UNIX must contend with how to deal with signals in a multithreaded environment, the APC facility is more straightforward, since an APC is delivered to a particular thread rather than a process. 4.6.3 Thread Cancellation Thread cancellation involves terminating a thread before it has completed. For example, if multiple threads are concurrently searching through a database

are concurrently searching through a database and one thread returns the result, the remaining threads might be canceled. Another situation might occur when a user presses a button on a web browser that stops aw e bp a g ef r o ml o a d i n ga n yf u r t h e r .O f t e n ,aw e bp a g el o a d su s i n gs e v e r a l threadseach image is loaded in a separate thread. When a user presses the stop button on the browser, all threads loading the page are canceled. A thread that is to be canceled is

are canceled. A thread that is to be canceled is often referred to as the target thread . Cancellation of a target thread may occur in two different scenarios: 1.Asynchronous cancellation .O n et h r e a di m m e d i a t e l yt e r m i n a t e st h e target thread. 2.Deferred cancellation .T h et a r g e tt h r e a dp e r i o d i c a l l yc h e c k sw h e t h e ri t should terminate, allowing it an opportunity to terminate itself in an orderly fashion. The difculty with cancellation occurs in

fashion. The difculty with cancellation occurs in situations where resources have been allocated to a canceled thread or where a thread is canceled while in the midst of updating data it is sharing with other threads. This becomes especially troublesome with asynchronous cancellation. Often, the operating system will reclaim system resources from a canceled thread but will not reclaim all resources. Therefore, canceling a thread asynchronously may not free a necessary systemwide resource.186

may not free a necessary systemwide resource.186 Chapter 4 Threads With deferred cancellation, in contrast, one thread indicates that a target thread is to be canceled, but cancellation occurs only after the target thread has checked a ag to determine whether or no ti ts h o u l db ec a n c e l e d .T h et h r e a d can perform this check at a point at which it can be canceled safely. In Pthreads, thread cancellation is initiated using the pthread cancel() function. The identier of the target

cancel() function. The identier of the target thread is passed as a parameter to the function. The following code illustrates creatingand then canceling at h r e a d : pthread tt i d ; create the thread  pthread create(tid, 0, worker, NULL); ... cancel the thread  pthread cancel(tid); Invoking pthread cancel() indicates only a request to cancel the target thread, however; actual cancellation depends on how the target thread is set up to handle the request. Pthreads supports three cancellation

the request. Pthreads supports three cancellation modes. Each mode is dened as a state and a type, as illustrated in the table below. A thread may set its cancellation state and type using an API.Mode State Type Off Disabled  Deferred Enabled Deferred Asynchronous Enabled Asynchronous As the table illustrates, Pthreads allows threads to disable or enable cancellation. Obviously, a thread cannot be canceled if cancellation is disabled. However, cancellation requests remain pending, so the thread

requests remain pending, so the thread can later enable cancellation and respond to the request. The default cancellation type is deferred cancellation. Here, cancellation occurs only when a thread reaches a cancellation point .O n et e c h n i q u ef o r establishing a cancellation point is to invoke the pthread testcancel() function. If a cancellation request is found to be pending, a function known as a cleanup handler is invoked. This function allows any resources a thread may have acquired

allows any resources a thread may have acquired to be released before the thread is terminated. The following code illustrates how a thread may respond to a cancellation request using deferred cancellation: while (1)  do some work for awhile  ... check if there is a cancellation request  pthread testcancel(); 4.6 Threading Issues 187 Because of the issues described earlier, asynchronous cancellation is not recommended in Pthreads documentation. Thus, we do not cover it here. An interesting note

Thus, we do not cover it here. An interesting note is that on Linux systems, thread cancellation using the Pthreads APIis handled through signals (Section 4.6.2). 4.6.4 ThreadLocal Storage Threads belonging to a process share the data of the process. Indeed, this data sharing provides one of the benets of multithreaded programming. However, in some circumstances, each thread might need its own copy of certain data. We will call such data threadlocal storage (orTLS.) For example, in a

threadlocal storage (orTLS.) For example, in a transactionprocessing system, we might service each transaction in a separate thread. Furthermore, each transaction might be assigned a unique identier. To associate each thread with its unique identier, we could use threadlocal storage. It is easy to confuse TLS with local variables. However, local variables are visible only during a single function invocation, whereas TLS data are visible across function invocations. In some ways, TLSis similar to

invocations. In some ways, TLSis similar to static data. The difference is that TLSdata are unique to each thread. Most thread librariesincluding Windows and Pthreadsprovide some form of support for threadlocal storage; Java provides support as well. 4.6.5 Scheduler Activations A nal issue to be considered with multithreaded programs concerns com munication between the kernel and the thread library, which may be required by the manytomany and twolevel models discussed in Section 4.3.3. Such

twolevel models discussed in Section 4.3.3. Such coordination allows the number of kernel threads to be dynamically adjusted to help ensure the best performance. Many systems implementing either the manytomany or the twolevel model place an intermediate data structur eb e t w e e nt h eu s e ra n dk e r n e l threads. This data structuretypically known as a lightweight process ,o r LWP is shown in Figure 4.13. T o the userthread library , the LWP appears to be a virtual processor on which the

LWP appears to be a virtual processor on which the application can schedule a user thread to run. Each LWP is attached to a kernel thread, an d it is kernel threads that the LWPuser thread kernel thread klightweight process Figure 4.13 Lightweight process (LWP).188 Chapter 4 Threads operating system schedules to run on physical processors. If a kernel thread blocks (such as while waiting for an IOoperation to complete), the LWP blocks as well. Up the chain, the userlevel thread attached to the

Up the chain, the userlevel thread attached to the LWP also blocks. An application may require any number of LWPst or u ne f  c i e n t l y .C o n s i d e r aCPUbound application running on a single processor. In this scenario, only one thread can run at at a time, so one LWP is sufcient. An application that is IOintensive may require multiple LWPst oe x e c u t e ,h o w e v e r .T y p i c a l l y ,a n LWP is required for each concurrent blocking system call. Suppose, for example, that ve

system call. Suppose, for example, that ve different leread requests occur simultaneously. Five LWPs are needed, because all could be waiting for IOcompletion in the kernel. If a process has only four LWPs, then the fth request must wait for one of the LWPst or e t u r n from the kernel. One scheme for communication between the userthread library and the kernel is known as scheduler activation .I tw o r k sa sf o l l o w s :T h ek e r n e l provides an application with a set of virtual

e l provides an application with a set of virtual processors ( LWPs), and the application can schedule user threads onto an available virtual processor. Furthermore, the kernel must inform an application about certain events. This procedure is known as an upcall .U p c a l l sa r eh a n d l e db yt h et h r e a dl i b r a r y with an upcall handler ,a n du p c a l lh a n d l e r sm u s tr u no nav i r t u a lp r o c e s s o r . One event that triggers an upcall occurs when an application thread

an upcall occurs when an application thread is about to block. In this scenario, the kernel makes an upcall to the application informing it that a thread is about to block and identifying the specic thread. The kernel then allocates a new virtual processor to the application. The application runs an upcall handler on this new virtual processor, which saves the state of the blocking thread and relinquishes the virtual processor on which the blocking thread is running. The upcall handler then

thread is running. The upcall handler then schedules another thread that is eligible to run on the new virtual processor. When the event that the blocking thread was waiting for occurs, the kernel makes another upcall to the thread library informing it that the previously blocked thread is now eligible to run. The upcall handler for this event also r equires a virtual processor, and the kernel may allocate a new virtual processor or preempt one of the user threads and run the upcall handler on

of the user threads and run the upcall handler on its virtual processor. After marking the unblocked thread as eligible to run, the application schedules an eligible thread to run on an available virtual processor. 4.7 OperatingSystem Examples At this point, we have examined a number of concepts and issues related to threads. We conclude the chapter by exploring how threads are implemented in Windows and Linux systems. 4.7.1 Windows Threads Windows implements the Windows API, which is the

Windows implements the Windows API, which is the primary API for the family of Microsoft operating systems (Windows 98, NT,2 0 0 0 ,a n d XP,a sw e l l as Windows 7). Indeed, much of what is mentioned in this section applies to this entire family of operating systems. AW i n d o w sa p p l i c a t i o nr u n sa sas e p a r a t ep r o c e s s ,a n de a c hp r o c e s sm a y contain one or more threads. The Windows APIfor creating threads is covered in4.7 OperatingSystem Examples 189 Section

covered in4.7 OperatingSystem Examples 189 Section 4.4.2. Additionally, Windows uses the onetoone mapping described in Section 4.3.2, where each user level thread maps to an associated kernel thread. The general components of a thread include: At h r e a d IDuniquely identifying the thread Ar e g i s t e rs e tr e p r e s e n t i n gt h es t a t u so ft h ep r o c e s s o r A user stack, employed when the thread is running in user mode, and a kernel stack, employed when the thread is running in

stack, employed when the thread is running in kernel mode Ap r i v a t es t o r a g ea r e au s e db yv a r i o u sr u n  t i m el i b r a r i e sa n dd y n a m i cl i n k libraries ( DLLs) The register set, stacks, and private storage area are known as the context of the thread. The primary data structures of a thread include: ETHREAD executive thread block KTHREAD kernel thread block TEBthread environment block The key components of the ETHREAD include a pointer to the process to which the

include a pointer to the process to which the thread belongs and the address of the routine in which the thread starts control. The ETHREAD also contains a pointer to the corresponding KTHREAD . The KTHREAD includes scheduling and synchronization information for the thread. In addition, the KTHREAD includes the kernel stack (used when the thread is running in kernel mode) and a pointer to the TEB. The ETHREAD and the KTHREAD exist entirely in kernel space; this means that only the kernel can

kernel space; this means that only the kernel can access them. The TEB is a userspace data structure that is accessed when the thread is running in user mode. Among other elds, the TEB contains the thread identier, a usermode stack, and an array for threadlocal storage. The structure of a Windows thread is illustrated in Figure 4.14. 4.7.2 Linux Threads Linux provides the fork() system call with the traditional functionality of duplicating a process, as described in Chapter 3. Linux also

a process, as described in Chapter 3. Linux also provides the ability to create threads using the clone() system call. However, Linux does not distinguish between processes and threads. In fact, Linux uses the term task rather than process orthread w h e nr e f e r r i n gt oa o wo fc o n t r o lw i t h i na program. When clone() is invoked, it is passed a set of ags that determine how much sharing is to take place between the parent and child tasks. Some of these ags are listed in Figure 4.15.

Some of these ags are listed in Figure 4.15. For example, suppose that clone() is passed the ags CLONE FS,CLONE VM,CLONE SIGHAND ,a n d CLONE FILES .T h ep a r e n t and child tasks will then share the same lesystem information (such as the current working directory), the same memory space, the same signal handlers,190 Chapter 4 Threads user space kernel spacepointer to parent processthread start addressETHREAD KTHREAD   kernel stackscheduling and synchronization information   user stack

and synchronization information   user stack threadlocal storagethread identifierTEB   Figure 4.14 Data structures of a Windows thread. and the same set of open les. Using clone() in this fashion is equivalent to creating a thread as described in this chapter, since the parent task shares most of its resources with its child task. However, if none of these ags is set when clone() is invoked, no sharing takes place, resulting in functionality similar to that provided by the fork() system call.

to that provided by the fork() system call. The varying level of sharing is possible because of the way a task is represented in the Linux kernel. A unique kernel data structure (specically, struct task struct )e x i s t sf o re a c ht a s ki nt h es y s t e m .T h i sd a t as t r u c t u r e , instead of storing data for the task, contains pointers to other data structures where these data are storedfor example, data structures that represent the list of open les, signalhandling information,

the list of open les, signalhandling information, and virtual memory. When fork() is invoked, a new task is created, along with a copy of all the associated dataflag meaning CLONEFS CLONEVM CLONESIGHAND CLONEFILESFilesystem information is shared. The same memory space is shared. Signal handlers are shared. The set of open files is shared.Figure 4.15 Some of the ags passed when clone() is invoked.Practice Exercises 191 structures of the parent process. A new task is also created when the clone()

A new task is also created when the clone() system call is made. However, rather than copying all data structures, the new task points to the data structures of the parent task, depending on the set of ags passed to clone() . 4.8 Summary A thread is a ow of control within a process. A multithreaded process contains several different ows of control within the same address space. The benets of multithreading include increased responsiven ess to the user, resource sharing within the process,

to the user, resource sharing within the process, economy, and scalability factors, such as more efcient use of multiple processing cores. Userlevel threads are threads that are visible to the programmer and are unknown to the kernel. The operatingsystem kernel supports and manages kernellevel threads. In general, userlevel threads are faster to create and manage than are kernel threads, because no intervention from the kernel is required. Three different types of models relate user and kernel

different types of models relate user and kernel threads. The many toone model maps many user threads to a single kernel thread. The onetoone model maps each user thread to a corresponding kernel thread. The manyto many model multiplexes many user threads to a smaller or equal number of kernel threads. Most modern operating systems provide kernel support for threads. These include Windows, Mac OS X , Linux, and Solaris. Thread libraries provide the application programmer with an API for creating

application programmer with an API for creating and managing threads. Three primary thread libraries are in common use: POSIX Pthreads, Windows threads, and Java threads. In addition to explicitly creating threads using the API provided by a library, we can use implicit threading, in which the creation and management of threading is transferred to compilers and runtime libraries. Strategies for implicit threading include thread pools, Open MP,a n dG r a n dC e n t r a lD i s p a t c h .

MP,a n dG r a n dC e n t r a lD i s p a t c h . Multithreaded programs introduce many challenges for programmers, including the semantics of the fork() and exec() system calls. Other issues include signal handling, thread cancellation, threadlocal storage, and scheduler activations. Practice Exercises 4.1 Provide two programming examples in which multithreading provides better performance than a singlethreaded solution. 4.2 What are two differences between userlevel threads and kernellevel

between userlevel threads and kernellevel threads? Under what circumstances is one type better than the other? 4.3 Describe the actions taken by a kernel to contextswitch between kernel level threads. 4.4 What resources are used when a thread is created? How do they differ from those used when a process is created?192 Chapter 4 Threads 4.5 Assume that an operating system maps userlevel threads to the kernel using the manytomany model and that the mapping is done through LWPs. Furthermore, the

the mapping is done through LWPs. Furthermore, the system allows developers to create realtime threads for use in realtime systems. Is it necessary to bind a realtime thread to an LWP?E x p l a i n . Exercises 4.6 Provide two programming examples in which multithreading does not provide better performance than a singlethreaded solution. 4.7 Under what circumstances does a multithreaded solution using multi ple kernel threads provide b etter performance than a singlethreaded solution on a

performance than a singlethreaded solution on a singleprocessor system? 4.8 Which of the following components of program state are shared across threads in a multithreaded process? a. Register values b. Heap memory c. Global variables d. Stack memory 4.9 Can a multithreaded solution using multiple userlevel threads achieve better performance on a multiprocessor system than on a single processor system? Explain. 4.10 In Chapter 3, we discussed Googles Chrome browser and its practice of opening

Googles Chrome browser and its practice of opening each new website in a separate process. Would the same benets have been achieved if instead Chrome had been designed to open each new website in a separate thread? Explain. 4.11 Is it possible to have concurrency but not parallelism? Explain. 4.12 Using Amdahls Law, calculate the speedup gain of an application that has a 60 percent parallel component for (a) two processing cores and (b) four processing cores. 4.13 Determine if the following

processing cores. 4.13 Determine if the following problems exhibit task or data parallelism: The multithreaded statistical program described in Exercise 4.21 The multithreaded Sudoku validator described in Project 1 in this chapter The multithreaded sorting program described in Project 2 in this chapter The multithreaded web server described in Section 4.1 4.14 As y s t e mw i t ht w od u a l  c o r ep r o c e s s o r sh a sf o u rp r o c e s s o r sa v a i l a b l e for scheduling. A

o c e s s o r sa v a i l a b l e for scheduling. A CPUintensive application is running on this system. All input is performed at program startup, when a single le must be opened. Similarly, all output is performed just before the programExercises 193 terminates, when the program results must be written to a single le. Between startup and termination, the program is entirely CPU bound. Your task is to improve the performance of this application by multithreading it. The application runs on a

by multithreading it. The application runs on a system that uses the onetoone threading model (each user th read maps to a kernel thread). How many threads will you create to perform the input and output? Explain. How many threads will you create for the CPUintensive portion of the application? Explain. 4.15 Consider the following code segment: pid tp i d ; pid  fork(); if (pid  0) child process  fork(); thread create( . . .);  fork(); a. How many unique processes are created? b. How many unique

unique processes are created? b. How many unique threads are created? 4.16 As described in Section 4.7.2, Lin ux does not distinguish between processes and threads. Instead, Linux treats both in the same way, allowing a task to be more akin to a process or a thread depending on the set of ags passed to the clone() system call. However, other operating systems, such as Windows, treat processes and threads differently. Typically, such systems use a notation in which the data structure for ap r o c

notation in which the data structure for ap r o c e s sc o n t a i n sp o i n t e r st ot h es e p a r a t et h r e a d sb e l o n g i n gt ot h e process. Contrast these two approaches for modeling processes and threads within the kernel. 4.17 The program shown in Figure 4.16 uses the Pthreads API.W h a tw o u l d be the output from the program at LINE Cand LINE P? 4.18 Consider a multicore system and a multithreaded program written using the manytomany threading model. Let the number of

the manytomany threading model. Let the number of userlevel threads in the program be greater than the number of processing cores in the system. Discuss the performance implications of the following scenarios. a. The number of kernel threads allocated to the program is less than the number of processing cores. b. The number of kernel threads allocated to the program is equal to the number of processing cores. c. The number of kernel threads allocated to the program is greater than the number of

to the program is greater than the number of processing cores but less than the number of userlevel threads.194 Chapter 4 Threads include pthread.h  include stdio.h  include types.h  int value  0; void runner(void param);  the thread  int main(int argc, char argv[])  pid tp i d ; pthread tt i d ; pthread attr ta t t r ; pid  fork(); if (pid  0)  child process  pthread attr init(attr); pthread create(tid,attr,runner,NULL); pthread join(tid,NULL); printf(CHILD: value  d,value);  LINE C   else if

printf(CHILD: value  d,value);  LINE C   else if (pid  0)  parent process  wait(NULL); printf(PARENT: value  d,value);  LINE P    void runner(void param)  value  5; pthread exit(0);  Figure 4.16 Cp r o g r a mf o rE x e r c i s e4 . 1 7 . 4.19 Pthreads provides an API for managing thread cancellation. The pthread setcancelstate() function is used to set the cancellation state. Its prototype appears as follows: pthread setcancelstate(int state, int oldstate) The two possible values for the state

oldstate) The two possible values for the state are PTHREAD CANCEL ENABLE and PTHREAD CANCEL DISABLE . Using the code segment shown in Figure 4.17, provide examples of two operations that would be suitable to perform between the calls to disable and enable thread cancellation.Programming Problems 195 int oldstate; pthread setcancelstate(PTHREAD CANCEL DISABLE, oldstate);  What operations would be performed here?  pthread setcancelstate(PTHREAD CANCEL ENABLE, oldstate); Figure 4.17 Cp r o g r a

CANCEL ENABLE, oldstate); Figure 4.17 Cp r o g r a mf o rE x e r c i s e4 . 1 9 . Programming Problems 4.20 Modify programming problem Exercise 3.20 from Chapter 3, which asks you to design a pid manager. This modication will consist of writing am u l t i t h r e a d e dp r o g r a mt h a tt e s t sy o u rs o l u t i o nt oE x e r c i s e3 . 2 0 .Y o u will create a number of threadsfor example, 100and each thread will request a pid, sleep for a random period of time, and then release the pid.

a random period of time, and then release the pid. (Sleeping for a random period of time approximates the typical pid usage in which a pid is assigned to a new process, the process executes and then terminates, and the pid is released on the processs termination.) On UNIX and Linux systems, sleeping is accomplished through the sleep() function, which is passed an integer value representing the number of seconds to sleep. This problem will be modied in Chapter 5. 4.21 Write a multithreaded

be modied in Chapter 5. 4.21 Write a multithreaded program that calculates various statistical values for a list of numbers. This program will be passed a series of numbers on the command line and will then create three separate worker threads. One thread will determine the average of the numbers, the second will determine the maximum value, and the third will determine the minimum value. For example, suppose your program is passed the integers 90 81 78 95 79 72 85 The program will report The

90 81 78 95 79 72 85 The program will report The average value is 82 The minimum value is 72 The maximum value is 95 The variables representing t he average, minimum, and maximum values will be stored globally. The worker threads will set these values, and the parent thread will output the values once the workers have exited. (We could obviously expand this program by creating additional threads that determine other statistical values, such as median and standard deviation.) 4.22 An interesting

and standard deviation.) 4.22 An interesting way of calculating H9266is to use a technique known as Monte Carlo, which involves randomization. This technique works as follows: Suppose you have a circle inscribed within a square, as shown in Figure196 Chapter 4 Threads (1, 1) (1, 1)(1, 1) (1, 1)(0, 0) Figure 4.18 Monte Carlo technique for calculating pi. 4.18. (Assume that the radius of this circle is 1.) First, generate a series of random points as simple ( x,y)c o o r d i n a t e s .T h e s ep

as simple ( x,y)c o o r d i n a t e s .T h e s ep o i n t sm u s tf a l lw i t h i n the Cartesian coordinates that bound the square. Of the total number of random points that are generated, some will occur within the circle. Next, estimate H9266by performing the following calculation: H92664(number of points in circle) (total number of points) Write a multithreaded version of this algorithm that creates a separate thread to generate a number of random points. The thread will count the number of

random points. The thread will count the number of points that occur within the circle and store that result in a global variable. When this thread has exited, the parent thread will calculate and output the estimated value of H9266.I ti sw o r t he x p e r i m e n t i n g with the number of random points ge nerated. As a general rule, the greater the number of points, the closer the approximation to H9266. In the sourcecode download for this text, we provide a sample program that provides a

text, we provide a sample program that provides a technique for generating random numbers, as well as determining if the random ( x,y)p o i n to c c u r sw i t h i nt h ec i r c l e . Readers interested in the details of the Monte Carlo method for esti mating H9266should consult the bibliography at the end of this chapter. In Chapter 5, we modify this exercise using relevant material from that chapter. 4.23 Repeat Exercise 4.22, but instead of using a separate thread to generate random points,

using a separate thread to generate random points, use OpenMP to parallelize the generation of points. Be careful not to place the calculcation of H9266in the parallel region, since you want to calculcate H9266only once. 4.24 Write a multithreaded program that outputs prime numbers. This program should work as follows: The user will run the program and will enter a number on the command line. The program will then create a separate thread that outputs all the prime numbers less than or equal to

all the prime numbers less than or equal to the number entered by the user. 4.25 Modify the socketbased date server (Figure 3.21) in Chapter 3 so that the server services each client request in a separate thread.Programming Projects 197 4.26 The Fibonacci sequence is the series of numbers 0 ,1,1,2,3,5,8,. . .. Formally, it can be expressed as: fib00 fib11 fibnfibn1fibn2 Write a multithreaded program that generates the Fibonacci sequence. This program should work as follows: On the command line,

should work as follows: On the command line, the user will enter the number of Fibonacci numbers that the program is to generate. The program will then create a separate thread that will generate the Fibonacci numbers, placing the sequence in data that can be shared by the threads (an array is probably the most convenient data structure). When the thread nishes execution, the parent thread will output the sequence generated by th ec h i l dt h r e a d .B e c a u s et h e parent thread cannot

h r e a d .B e c a u s et h e parent thread cannot begin outputting the Fibonacci sequence until the child thread nishes, the parent thread will have to wait for the child thread to nish. Use the techniques described in Section 4.4 to meet this requirement. 4.27 Exercise 3.25 in Chapter 3 involves designing an echo server using the Java threading API.T h i ss e r v e ri ss i n g l e  t h r e a d e d ,m e a n i n gt h a tt h e server cannot respond to concurrent echo clients until the current

to concurrent echo clients until the current client exits. Modify the solution to Exercise 3.25 so that the echo server services each client in a separate request. Programming Projects Project 1Sudoku Solution Validator ASudoku puzzle uses a 9 9g r i di nw h i c he a c hc o l u m na n dr o w ,a sw e l la s each of the nine 3 3s u b g r i d s ,m u s tc o n t a i na l lo ft h ed i g i t s1 9. Figure 4.19 presents an example of a valid Sudoku puzzle. This project consists of designing a

puzzle. This project consists of designing a multithreaded application that determines whether the solution to aS u d o k up u z z l ei sv a l i d . There are several different ways of multithreading this application. One suggested strategy is to create threads that check the following criteria: At h r e a dt oc h e c kt h a te a c hc o l u m nc o n t a i n st h ed i g i t s1t h r o u g h9 At h r e a dt oc h e c kt h a te a c hr o wc o n t a i n st h ed i g i t s1t h r o u g h9 Nine threads to

n st h ed i g i t s1t h r o u g h9 Nine threads to check that each of the 3 3 subgrids contains the digits 1 through 9 This would result in a total of eleven separate threads for validating a Sudoku puzzle. However, you are welcome to create even more threads for this project. For example, rather than creating one thread that checks all nine198 Chapter 4 Threads624539187519728634837614295143865729958247361762391458371956842496182573285473916 Figure 4.19 Solution to a 9 9S u d o k up u z z l e .

4.19 Solution to a 9 9S u d o k up u z z l e . columns, you could create nine separate threads and have each of them check one column. Passing Parameters to Each Thread The parent thread will create the w orker threads, passing each worker the location that it must check in the Sudoku grid. This step will require passing several parameters to each thread. The easiest approach is to create a data structure using a struct .F o re x a m p l e ,as t r u c t u r et op a s st h er o wa n dc o l u m n

r u c t u r et op a s st h er o wa n dc o l u m n where a thread must begin validating would appear as follows:  structure for passing data to threads  typedef struct  int row; int column; parameters; Both Pthreads and Windows programs will create worker threads using a strategy similar to that shown below: parameters data  (parameters ) malloc(sizeof(parameters)); datarow  1; datacolumn  1;  Now create the thread passing it data as a parameter  The data pointer will be passed to either the

The data pointer will be passed to either the pthread create() (Pthreads) function or the CreateThread() (Windows) function, which in turn will pass it as a parameter to the function that is to run as a separate thread. Returning Results to the Parent Thread Each worker thread is assigned the task of determining the validity of a particular region of the Sudoku puzzle. Once a worker has performed thisBibliographical Notes 199 7, 12, 19, 3, 187, 12, 19, 3, 18, 4, 2, 6, 15, 8Original List 2, 3,

12, 19, 3, 18, 4, 2, 6, 15, 8Original List 2, 3, 4, 6, 7, 8, 12, 15, 18, 19Merge Thread Sorted ListSorting Thread 0Sorting Thread 1 4, 2, 6, 15, 8 Figure 4.20 Multithreaded sorting. check, it must pass its results back to the parent. One good way to handle this is to create an array of integer values that is visible to each thread. The ith index in this array corresponds to the ithworker thread. If a worker sets its corresponding value to 1, it is indicating that its region of the Sudoku puzzle

is indicating that its region of the Sudoku puzzle is valid. A value of 0 would indicate otherwise. When all worker threads have completed, the parent thread checks ea ch entry in the result array to determine if the Sudoku puzzle is valid. Project 2Multithreaded Sorting Application Write a multithreaded sorting program that works as follows: A list of integers is divided into two smaller lists of equal size. Two separate threads (which we will term sorting threads )s o r te a c hs u b l i s tu

term sorting threads )s o r te a c hs u b l i s tu s i n gas o r t i n ga l g o r i t h mo fy o u r choice. The two sublists are then merged by a third threada merging thread which merges the two sublists into a single sorted list. Because global data are shared cross all threads, perhaps the easiest way to set up the data is to create a global array. Each sorting thread will work on one half of this array. A second global array of the same size as the unsorted integer array will also be

size as the unsorted integer array will also be established. The merging thread will then merge the two sublists into this second array. Graphically, this program is structured according to Figure 4.20. This programming project will require passing parameters to each of the sorting threads. In particular, it will be necessary to identify the starting index from which each thread is to begin sorting. Refer to the instructions in Project 1f o rd e t a i l so np a s s i n gp a r a m e t e r st oat

t a i l so np a s s i n gp a r a m e t e r st oat h r e a d . The parent thread will output the sorted array once all sorting threads have exited. Bibliographical Notes Threads have had a long evolution, starting as cheap concurrency in programming languages and moving to lightweight processes, with early examples that included the Thoth system ([Cheriton et al. (1979)]) and the Pilot200 Chapter 4 Threads system ([Redell et al. (1980)]). [Binding (1985)] described moving threads into the UNIX

(1985)] described moving threads into the UNIX kernel. Mach ([Accetta et al. (1986)], [Tevanian et al. (1987)]), and V ([Cheriton (1988)]) made extensive use of threads, and eventually almost all major operating systems implemented them in some form or another. [Vahalia (1996)] covers threading in several versions of UNIX . [McDougall and Mauro (2007)] describes developments in threading the Solaris kernel. [Russinovich and Solomon (2009)] discuss threading in the Windows operating system

discuss threading in the Windows operating system family. [Mauerer (2008)] and [Love (2010)] explain how Linux handles threading, and [Singh (2007)] covers threads in Mac OS X . Information on Pthreads programming is given in [Lewis and Berg (1998)] and [Butenhof (1997)]. [Oaks and Wong (1999)] and [Lewis and Berg (2000)] discuss multithreading in Java. [Goetz et al. (2006)] present a detailed discussion of concurrent programming in Java. [Hart (2005)] describes multithreading using Windows.

(2005)] describes multithreading using Windows. Details on using Open MPcan be found at http:openmp.org . An analysis of an optimal threadpool size can be found in [Ling et al. (2000)]. Scheduler activations were rst presented in [Anderson et al. (1991)], and [Williams (2002)] discusses scheduler activations in the Net BSD system. [Breshears (2009)] and [Pacheco (2011)] cover parallel programming in detail. [Hill and Marty (2008)] examine Amdahls Law with respect to multicore systems. The Monte

Law with respect to multicore systems. The Monte Carlo technique for estimating H9266is further discussed in http:math.fullerton.edumathewsn2003montecarlopimod.html . Bibliography [Accetta et al. (1986)] M. Accetta, R. Baron, W. Bolosky, D. B. Golub, R. Rashid, A. Tevanian, and M. Young, Mach: A New Kernel Foundation for UNIX Development ,Proceedings of the Summer USENIX Conference (1986), pages 93112. [Anderson et al. (1991)] T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy,

B. N. Bershad, E. D. Lazowska, and H. M. Levy, Scheduler Activations: Effective Kernel Support for the UserLevel Management of Parallelism ,Proceedings of the ACM Symposium on Operating Systems Principles (1991), pages 95109. [Binding (1985)] C. Binding, Cheap Concurrency in C ,SIGPLAN Notices , Volume 20, Number 9 (1985), pages 2127. [Breshears (2009)] C. Breshears, The Art of Concurrency ,O  R e i l l yA s s o c i a t e s (2009). [Butenhof (1997)] D. Butenhof, Programming with POSIX Threads ,A

D. Butenhof, Programming with POSIX Threads ,A d d i s o n  Wesley (1997). [Cheriton (1988)] D. Cheriton, The V Distributed System ,Communications of the ACM ,V o l u m e3 1 ,N u m b e r3( 1 9 8 8 ) ,p a g e s3 1 4  3 3 3 . [Cheriton et al. (1979)] D. R. Cheriton, M. A. Malcolm, L. S. Melen, and G. R. Sager, Thoth, a Portable RealTime Operating System ,Communications of the ACM ,V o l u m e2 2 ,N u m b e r2( 1 9 7 9 ) ,p a g e s1 0 5  1 1 5 .Bibliography 201 [Goetz et al. (2006)] B. Goetz, T.

201 [Goetz et al. (2006)] B. Goetz, T. Peirls, J. Bloch, J. Bowbeer, D. Holmes, and D. Lea, Java Concurrency in Practice ,A d d i s o n  W e s l e y( 2 0 0 6 ) . [Hart (2005)] J. M. Hart, Windows System Programming, Third Edition, Addison Wesley (2005). [Hill and Marty (2008)] M. Hill and M. Marty, Amdahls Law in the Multicore Era,IEEE Computer ,V o l u m e4 1 ,N u m b e r7( 2 0 0 8 ) ,p a g e s3 3  3 8 . [Lewis and Berg (1998)] B. Lewis and D. Berg, Multithreaded Programming with Pthreads ,S u

Berg, Multithreaded Programming with Pthreads ,S u nM i c r o s y s t e m sP r e s s( 1 9 9 8 ) . [Lewis and Berg (2000)] B. Lewis and D. Berg, Multithreaded Programming with Java Technology ,S u nM i c r o s y s t e m sP r e s s( 2 0 0 0 ) . [Ling et al. (2000)] Y. Ling, T. Mullen, and X. Lin, Analysis of Optimal Thread Pool Size ,Operating System Review ,V o l u m e3 4 ,N u m b e r2( 2 0 0 0 ) ,p a g e s4 2  5 5 . [Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developers

Kernel Development, Third Edition, Developers Library (2010). [Mauerer (2008)] W. Mauerer, Professional Linux Kernel Architecture ,J o h nW i l e y and Sons (2008). [McDougall and Mauro (2007)] R. McDougall and J. Mauro, Solaris Internals, Second Edition, Prentice Hall (2007). [Oaks and Wong (1999)] S. Oaks and H. Wong, Java Threads, Second Edition, OReilly  Associates (1999). [Pacheco (2011)] P. S . P a c h e c o , An Introduction to Parallel Programming , Morgan Kaufmann (2011). [Redell et al.

, Morgan Kaufmann (2011). [Redell et al. (1980)] D. D. Redell, Y. K. Dalal, T. R. Horsley, H. C. Lauer, W. C. Lynch, P. R. McJones, H. G. Murray, and S. P. Purcell, Pilot: An Operating System for a Personal Computer ,Communications of the ACM ,V o l u m e2 3 ,N u m b e r2 (1980), pages 8192. [Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon, Win dows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition, Microsoft Press (2009). [Singh (2007)] A. Singh, Mac

Press (2009). [Singh (2007)] A. Singh, Mac OS X Internals: A Systems Approach ,A d d i s o n  Wesley (2007). [Tevanian et al. (1987)] A. Tevanian, Jr., R. F. Rashid, D. B. Golub, D. L. Black, E. Cooper, and M. W. Young, Mach Threads and the Unix Kernel: The Battle for Control ,Proceedings of the Summer USENIX Conference (1987). [Vahalia (1996)] U. Vahalia, Unix Internals: The New Frontiers ,P r e n t i c eH a l l (1996). [Williams (2002)] N. Williams, An Implementation of Scheduler Activations

An Implementation of Scheduler Activations on the NetBSD Operating System ,2002 USENIX Annual Technical Conference, FREENIX Track (2002).5CHAPTER Process Synchronization Acooperating process is one that can affect or be affected by other processes executing in the system. Cooperating processes can either directly share a logical address space (that is, both code and data) or be allowed to share data only through les or messages. The former case is achieved through the use of threads, discussed

is achieved through the use of threads, discussed in Chapter 4. Concurrent access to shared data may result in data inconsistency, however. In this chapter, we discuss various mechanisms to ensure the orderly execution of cooperating processes that share a logical address space, so that data consistency is maintained. CHAPTER OBJECTIVES To introduce the criticalsection problem, whose solutions can be used to ensure the consistency of shared data. To present both software and hardware solutions

To present both software and hardware solutions of the criticalsection problem. To examine several classical processsynchronization problems. To explore several tools that are used to solve process synchronization problems. 5.1 Background Weve already seen that processes can execute concurrently or in parallel. Section 3.2.2 introduced the role of process scheduling and described how the CPU scheduler switches rapidly between processes to provide concurrent execution. This means that one process

concurrent execution. This means that one process may only partially complete execution before another process is scheduled. In fact, a process may be interrupted at any point in its instruction stream, and the processing core may be assigned to execute instructions of another process. Additionally, Section 4.2 introduced parallel execution, in which two instruction streams (representing different processes) execute simultaneously on separate processing cores. In this chapter, 203204 Chapter 5

cores. In this chapter, 203204 Chapter 5 Process Synchronization we explain how concurrent or parallel execution can contribute to issues involving the integrity of data shared by several processes. Lets consider an example of how this can happen. In Chapter 3, we devel oped a model of a system consisting of cooperating sequential processes or threads, all running asynchronously and possibly sharing data. We illustrated this model with the producerconsumer problem, which is representative of

problem, which is representative of operating systems. Specically, in Section 3.4.1, we described how a bounded buffer could be used to enable processes to share memory. We now return to our consideration of the bounded buffer. As we pointed out, our original solution allowed at most BUFFER SIZE 1i t e m si nt h eb u f f e r at the same time. Suppose we want to modify the algorithm to remedy this deciency. One possibility is to add an integer variable counter ,i n i t i a l i z e dt o 0.counter

counter ,i n i t i a l i z e dt o 0.counter is incremented every time we add a new item to the buffer and is decremented every time we remove one item from the buffer. The code for the producer process can be modied as follows: while (true)   produce an item in next produced  while (counter  BUFFER SIZE) ; d on o t h i n g  buffer[in]  next produced; in  (in  1)  BUFFER SIZE; counter;  The code for the consumer process can be modied as follows: while (true)  while (counter  0) ; d on o t h i n g

(true)  while (counter  0) ; d on o t h i n g  next consumed  buffer[out]; out  (out  1)  BUFFER SIZE; counter;  consume the item in next consumed   Although the producer and consumer ro utines shown above are correct separately, they may not function correc tly when executed concurrently. As an illustration, suppose that the value of the variable counter is currently 5a n dt h a tt h ep r o d u c e ra n dc o n s u m e rp r o c e s s e sc o n c u r r e n t l ye x e c u t et h e statements

o n c u r r e n t l ye x e c u t et h e statements counter and counter .F o l l o w i n gt h ee x e c u t i o no ft h e s e two statements, the value of the variable counter may be 4, 5, or 6! The only correct result, though, is counter  5, which is generated correctly if the producer and consumer execute separately.5.1 Background 205 We can show that the value of counter may be incorrect as follows. Note that the statement counter may be implemented in machine language (on at y p i c a lm a c h

in machine language (on at y p i c a lm a c h i n e )a sf o l l o w s : register 1counter register 1register 11 counter register 1 where register 1is one of the local CPU registers. Similarly, the statement counter is implemented as follows: register 2counter register 2register 21 counter register 2 where again register 2is one of the local CPU registers. Even though register 1and register 2may be the same physical register (an accumulator, say), remember that the contents of this register will

remember that the contents of this register will be saved and restored by the interrupt handler (Section 1.2.3). The concurrent execution of counter and counter is equivalent to a sequential execution in which the lowerlevel statements presented previously are interleaved in some arbitrary order (but the order within each highlevel statement is preserved). One s uch interleaving is the following: T0:producer execute register 1counter register 15 T1:producer execute register 1register 11register

T1:producer execute register 1register 11register 16 T2:consumer execute register 2counter register 25 T3:consumer execute register 2register 21register 24 T4:producer execute counter register 1 counter 6 T5:consumer execute counter register 2 counter 4 Notice that we have arrived at the incorrect state counter  4 ,i n d i c a t i n g that four buffers are full, when, in fact, ve buffers are full. If we reversed the order of the statements at T4and T5, we would arrive at the incorrect state

T4and T5, we would arrive at the incorrect state counter  6 . We would arrive at this incorrect state because we allowed both processes to manipulate the variable counter concurrently. A situation like this, where several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place, is called a race condition .T og u a r da g a i n s tt h er a c ec o n d i t i o n above, we need to ensure that only one

d i t i o n above, we need to ensure that only one process at a time can be manipulating the variable counter .T om a k es u c hag u a r a n t e e ,w er e q u i r et h a tt h ep r o c e s s e s be synchronized in some way. Situations such as the one just described occur frequently in operating systems as different parts of the system manipulate resources. Furthermore, as we have emphasized in earlier chapters, the growing importance of multicore systems has brought an increased emphasis on

systems has brought an increased emphasis on developing multithreaded applications. In such applications, several threadswhich are quite possibly sharing dataare running in parallel on different processing cores. Clearly,206 Chapter 5 Process Synchronization do entry section critical section exit section remainder section while (true); Figure 5.1 General structure of a typical process Pi. we want any changes that result from such activities not to interfere with one another. Because of the

not to interfere with one another. Because of the importance of this issue, we devote a major portion of this chapter to process synchronization and coordination among cooperating processes. 5.2 The CriticalSection Problem We begin our consideration of process synchronization by discussing the so called criticalsection problem. Consider a system consisting of nprocesses P0,P1,. . . ,Pn1.E a c hp r o c e s sh a sas e g m e n to fc o d e ,c a l l e da critical section , in which the process may be

da critical section , in which the process may be changing common variables, updating a table, writing a le, and so on. The important feature of the system is that, when one process is executing in its critical section, no other process is allowed to execute in its critical section. That is, no two processes are executing in their critical sections at the same time. The criticalsection problem is to design a protocol that the processes can use to cooperate. Each process must request permission

to cooperate. Each process must request permission to enter its critical section. The section of code implementing this request is the entry section .T h ec r i t i c a ls e c t i o nm a yb ef o l l o w e db ya n exit section .T h er e m a i n i n gc o d ei st h e remainder section .T h eg e n e r a ls t r u c t u r eo f a typical process Piis shown in Figure 5.1. The entry section and exit section are enclosed in boxes to highlight these important segments of code. A solution to the

important segments of code. A solution to the criticalsection problem must satisfy the following three requirements: 1.Mutual exclusion .I fp r o c e s s Piis executing in its critical section, then no other processes can be executing in their critical sections. 2.Progress .I fn op r o c e s si se x e c u t i n gi ni t sc r i t i c a ls e c t i o na n ds o m e processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can

are not executing in their remainder sections can participate in deciding which will enter its critical section next, and this selection cannot be postponed indenitely. 3.Bounded waiting . There exists a bound, or limit, on the number of times that other processes are allowed to enter their critical sections after a5.3 Petersons Solution 207 process has made a request to enter its critical section and before that request is granted. We assume that each process is executing at a nonzero speed.

that each process is executing at a nonzero speed. However, we can make no assumption concerning the relative speed of the nprocesses. At a given point in time, many kernelmode processes may be active in the operating system. As a result, the code implementing an operating system (kernel code )i ss u b j e c tt os e v e r a lp o s s i b l er a c ec o n d i t i o n s .C o n s i d e ra sa n example a kernel data structure that maintains a list of all open les in the system. This list must be

of all open les in the system. This list must be modied when a new le is opened or closed (adding the le to the list or removing it from the list). If two processes were to open les simultaneously, the separate updates to this list could result in a race condition. Other kernel data structures that are pr one to possible race conditions include structures for maintaining memory allocation, for maintaining process lists, and for interrupt handling. It is up to kernel developers to ensure that the

It is up to kernel developers to ensure that the operating system is free from such race conditions. Two general approaches are used to ha ndle critical sections in operating systems: preemptive kernels and nonpreemptive kernels .Ap r e e m p t i v e kernel allows a process to be preempted while it is running in kernel mode. A nonpreemptive kernel does not allow a process running in kernel mode to be preempted; a kernelmode process will run u ntil it exits kernel mode, blocks, or voluntarily

ntil it exits kernel mode, blocks, or voluntarily yields control of the CPU. Obviously, a nonpreemptive kernel is essentially free from race conditions on kernel data structures, as only one pro cess is active in the kernel at a time. We cannot say the same about preemptive kernels, so they must be carefully designed to ensure that shared kernel data are free from race conditions. Preemptive kernels are especially difcult to design for SMP architectures, since in these environments it is

architectures, since in these environments it is possible for two kernelmode processes to run simultaneously on different processors. Why, then, would anyone favor a p reemptive kernel over a nonpreemptive one? A preemptive kernel may be more re sponsive, since there is less risk that a kernelmode process will run for an arbitrarily long period before relinquishing the processor to waiting processes. (Of course, this risk can also be minimized by designing kernel code that does not behave in

by designing kernel code that does not behave in this way.) Furthermore, a preemptive kernel is more suitable for realtime programming, as it will allow ar e a l  t i m ep r o c e s st op r e e m p tap r o c e s sc u r r e n t l yr u n n i n gi nt h ek e r n e l .L a t e r in this chapter, we explore how various operating systems manage preemption within the kernel. 5.3 Petersons Solution Next, we illustrate a classic softwarebased solution to the criticalsection problem known as Petersons

to the criticalsection problem known as Petersons solution .B e c a u s eo ft h ew a ym o d e r nc o m p u t e r architectures perform basic machinelanguage instructions, such as load and store , there are no guarantees that Petersons solution will work correctly on such architectures. However, we present the solution because it provides a good algorithmic description of solving the criticalsection problem and illustrates some of the complexities involved in designing software that addresses the

involved in designing software that addresses the requirements of mutual exclusion, progress, and bounded waiting.208 Chapter 5 Process Synchronization do flag[i]  true ; turn  j; while (flag[j]  turn  j); critical section flag[i]  false; remainder section while (true); Figure 5.2 The structure of process Piin Petersons solution. Petersons solution is restricted to two processes that alternate execution between their critical sections and re mainder sections. The processes are numbered P0and P1.

sections. The processes are numbered P0and P1. For convenience, when presenting Pi, we use Pjto denote the other process; that is, jequals 1 i. Petersons solution requires the two processes to share two data items: int turn; boolean flag[2]; The variable turn indicates whose turn it is to enter its critical section. That is, ifturn  i ,t h e np r o c e s s Piis allowed to execute in its critical section. The flag array is used to indicate if a process is ready to enter its critical section. For

is ready to enter its critical section. For example, if flag[i] istrue ,t h i sv a l u ei n d i c a t e st h a t Piis ready to enter its critical section. With an explanation of these data structures complete, we are now ready to describe the algorithm shown in Figure 5.2. To enter the critical section, process Pirst sets flag[i] to be true and then sets turn to the value j,t h e r e b ya s s e r t i n gt h a ti ft h eo t h e rp r o c e s sw i s h e s to enter the critical section, it can do so.

h e s to enter the critical section, it can do so. If both processes try to enter at the same time, turn will be set to both iandjat roughly the same time. Only one of these assignments will last; the other will occur but will be overwritten immediately. The eventual value of turn determines which of the two processes is allowed to enter its critical section rst. We now prove that this solution is correct. We need to show that: 1.Mutual exclusion is preserved. 2.The progress requirement is

is preserved. 2.The progress requirement is satised. 3.The boundedwaiting requirement is met. To prove property 1, we note that each Pienters its critical section only if either flag[j] false orturn i.A l s on o t et h a t ,i fb o t hp r o c e s s e s can be executing in their critical sections at the same time, then flag[0]  flag[1] true .T h e s et w oo b s e r v a t i o n si m p l yt h a t P0and P1could not have successfully executed their while statements at about the same time, since the5.4

statements at about the same time, since the5.4 Synchronization Hardware 209 value of turn can be either 0 or 1 but cannot be both. Hence, one of the processes say , Pjmust have successfully executed the while statement, whereas Pi had to execute at least one additional statement ( turn j). However, at that time, flag[j] true and turn j,a n dt h i sc o n d i t i o nw i l lp e r s i s ta s long as Pjis in its critical section; as a result, mutual exclusion is preserved. To prove properties 2 and

exclusion is preserved. To prove properties 2 and 3, we note that a process Pican be prevented from entering the critical section only if it is stuck in the while loop with the condition flag[j] true andturn j;t h i sl o o pi st h eo n l yo n ep o s s i b l e .I f Pjis not ready to enter the critical section, then flag[j] false ,a n d Pican enter its critical section. If Pjhas set flag[j] totrue and is also executing in its while statement, then either turn iorturn j.I fturn i,t h e n Piwill

either turn iorturn j.I fturn i,t h e n Piwill enter the critical section. If turn j,t h e n Pjwill enter the critic al section. However, once Pjexits its critical section, it will reset flag[j] tofalse ,a l l o w i n g Pito enter its critical section. If Pjresets flag[j] totrue ,i tm u s ta l s os e t turn toi. Thus, since Pidoes not change the value of the variable turn while executing thewhile statement, Piwill enter the critical section (progress) after at most one entry by Pj(bounded

(progress) after at most one entry by Pj(bounded waiting). 5.4 Synchronization Hardware We have just described one softwarebased solution to the criticalsection problem. However, as mentioned, softw arebased solutions such as Petersons are not guaranteed to work on modern computer architectures. In the following discussions, we explore several more solutions to the criticalsection problem using techniques ranging from hardware to softwarebased APIsa v a i l a b l et o both kernel developers and

v a i l a b l et o both kernel developers and application programmers. All these solutions are based on the premise of locking that is, protecting critical regions through the use of locks. As we shall see, the designs of such locks can be quite sophisticated. We start by presenting some simple hardware instructions that are available on many systems and showing how they can be used effectively in solving the criticalsection problem. Hardware features can make any programming task easier and

features can make any programming task easier and improve system efciency. The criticalsection problem could be solved simply in a singleprocessor environment if we could prevent interrupts from occurring while a shared variable was being modied. In this way, we could be sure that the current sequence of instructions would be allowed to execute in order without pre emption. No other instructions would be run, so no unexpected modications could be made to the shared variable. This is often the

be made to the shared variable. This is often the approach taken by nonpreemptive kernels. boolean test and set(boolean target)  boolean rv  target; target  true; return rv;  Figure 5.3 The denition of the test and set() instruction.210 Chapter 5 Process Synchronization do while (test and set(lock)) ; d on o t h i n g   critical section  lock  false;  remainder section  while (true); Figure 5.4 Mutualexclusion implementation with test and set() . Unfortunately, this solution is not as feasible

. Unfortunately, this solution is not as feasible in a multiprocessor environ ment. Disabling interrupts on a multiprocessor can be time consuming, since the message is passed to all the processors. This message passing delays entry into each critical section, and system efciency decreases. Also consider the effect on a systems clock if the clock is kept updated by interrupts. Many modern computer systems therefore provide special hardware instructions that allow us either to test and modify the

that allow us either to test and modify the content of a word or to swap the contents of two words atomically that is, as one uninterruptible unit. We can use these special instructions to solve the criticalsection problem in a relatively simple manner. Rather than discussing one specic instruction for one specic machine, we abstract the main concepts behind these types of instructions by describing the test and set() and compare and swap() instructions. The test and set() instruction can be

The test and set() instruction can be dened as shown in Figure 5.3. The important characteristic of this instruction is that it is executed atomically. Thus, if two test and set() instructions are executed simultaneously (each on a different CPU), they will be executed sequentially in some arbitrary order. If the machine supports the test and set() instruction, then we can implement mutual exclusion by declaring a boolean variable lock ,i n i t i a l i z e dt o false . The structure of process

i a l i z e dt o false . The structure of process Piis shown in Figure 5.4. The compare and swap() instruction, in contrast to the test and set() instruction, operates on three operands; it is dened in Figure 5.5. The operand value is set to new value only if the expression (value  exected) is true. Regardless, compare and swap() always returns the original value of the variable value .L i k et h e test and set() instruction, compare and swap() is int compare and swap(int value, int expected,

is int compare and swap(int value, int expected, int new value)  int temp  value; if (value  expected) value  new value; return temp;  Figure 5.5 The denition of the compare and swap() instruction.5.4 Synchronization Hardware 211 do while (compare and swap(lock, 0, 1) ! 0) ; d on o t h i n g   critical section  lock  0;  remainder section  while (true); Figure 5.6 Mutualexclusion implementation with the compare and swap() instruction. executed atomically. Mutual exclusion can be provided as

atomically. Mutual exclusion can be provided as follows: a global variable ( lock )i sd e c l a r e da n di si n i t i a l i z e dt o0 .T h e r s tp r o c e s st h a ti n v o k e s compare and swap() will set lock to 1. It will then enter its critical section, because the original value of lock was equal to the expected value of 0. Subsequent calls to compare and swap() will not succeed, because lock now is not equal to the expected value of 0. When a process exits its critical section, it sets

When a process exits its critical section, it sets lock back to 0, which allows another process to enter its critical section. The structure of process Piis shown in Figure 5.6. Although these algorithms satisfy the mutualexclusion requirement, they do not satisfy the boundedwaiting re quirement. In Figure 5.7, we present another algorithm using the test and set() instruction that satises all the criticalsection requirements. The common data structures are do waiting[i]  true; key  true; while

are do waiting[i]  true; key  true; while (waiting[i]  key) key  test and set(lock); waiting[i]  false;  critical section  j( i1 )n ; while ((j ! i)  !waiting[j]) j( j1 )n ; if (j  i) lock  false; else waiting[j]  false;  remainder section  while (true); Figure 5.7 Boundedwaiting mutual exclusion with test and set() .212 Chapter 5 Process Synchronization boolean waiting[n]; boolean lock; These data structures are initialized to false .T op r o v et h a tt h em u t u a l  exclusion requirement is

et h a tt h em u t u a l  exclusion requirement is met, we note that process Pican enter its critical section only if either waiting[i] false orkey false .T h ev a l u e ofkey can become false only if the test and set() is executed. The rst process to execute the test and set() will nd keyfalse ;a l lo t h e r sm u s t wait. The variable waiting[i] can become false only if another process leaves its critical section; only one waiting[i] is set to false ,m a i n t a i n i n gt h e mutualexclusion

false ,m a i n t a i n i n gt h e mutualexclusion requirement. To prove that the progress requirement is met, we note that the arguments presented for mutual exclusion also apply here, since a process exiting the critical section either sets lock tofalse or sets waiting[j] tofalse .B o t h allow a process that is waiting to enter its critical section to proceed. To prove that the boundedwaiting requirement is met, we note that, when ap r o c e s sl e a v e si t sc r i t i c a ls e c t i o n ,i

s sl e a v e si t sc r i t i c a ls e c t i o n ,i ts c a n st h ea r r a y waiting in the cyclic ordering ( i1 ,i2 ,. . . , n1, 0, ..., i1). It designates the rst process in this ordering that is in the entry section ( waiting[j] true )a st h en e x to n et o enter the critical section. Any process waiting to enter its critical section will thus do so within n1t u r n s . Details describing the implementation of the atomic test and set() and compare and swap() instructions are discussed more

compare and swap() instructions are discussed more fully in books on computer architecture. 5.5 Mutex Locks The hardwarebased solutions to the criticalsection problem presented in Section 5.4 are complicated as well as generally inaccessible to application programmers. Instead, operatingsystems designers build software tools to solve the criticalsection problem. The simplest of these tools is the mutex lock . (In fact, the term mutex is short for mutualexclusion.) We use the mutex lock to

for mutualexclusion.) We use the mutex lock to protect critical regions and th us prevent race conditions. That is, a process must acquire the lock before entering a critical section; it releases the lock when it exits the critical section. The acquire() function acquires the lock, and the release() function releases the lock, as illustrated in Figure 5.8. Am u t e xl o c kh a sab o o l e a nv a r i a b l e available whose value indicates if the lock is available or not. If the lock is

if the lock is available or not. If the lock is available, a call to acquire() succeeds, and the lock is then considered unavailable. A process that attempts to acquire an unavailable lock is blocked until the lock is released. The denition of acquire() is as follows: acquire()  while (!available) ; b u s yw a i t  available  false;; 5.6 Semaphores 213 do acquire lock critical section release lock remainder section while (true); Figure 5.8 Solution to the criticalsection problem using mutex

to the criticalsection problem using mutex locks. The denition of release() is as follows: release()  available  true;  Calls to either acquire() orrelease() must be performed atomically. Thus, mutex locks are often implemented using one of the hardware mecha nisms described in Section 5.4, and we leave the description of this technique as an exercise. The main disadvantage of the implementation given here is that it requires busy waiting .W h i l eap r o c e s si si ni t sc r i t i c a ls e c t

l eap r o c e s si si ni t sc r i t i c a ls e c t i o n ,a n yo t h e rp r o c e s st h a t tries to enter its critical section must loop continuously in the call to acquire() . In fact, this type of mutex lock is also called a spinlock because the process spins while waiting for the lock to become available. (We see the same issue with the code examples illustrating the test and set() instruction and the compare and swap() instruction.) This continual looping is clearly a problem in a real

continual looping is clearly a problem in a real multiprogramming system, where a single CPU is shared among many processes. Busy waiting wastes CPU cycles that some other process might be able to use productively. Spinlocks do have an advantage, how ever, in that no context switch is required when a process must wait on a lock, and a context switch may take considerable time. Thus, when locks are expected to be held for short times, spinlocks are useful. They are often employed on

spinlocks are useful. They are often employed on multiprocessor systems where one thread can spin on one processor while another thread performs its critical section on another processor. Later in this chapter (Section 5.7), we examine how mutex locks can be used to solve classical synchronization problems. We also discuss how these locks are used in several operating systems, as well as in Pthreads. 5.6 Semaphores Mutex locks, as we mentioned earlier, are generally considered the simplest of

earlier, are generally considered the simplest of synchronization tools. In this section, we examine a more robust tool that can214 Chapter 5 Process Synchronization behave similarly to a mutex lock but can also provide more sophisticated ways for processes to synchronize their activities. Asemaphore Sis an integer variable that, apart from initialization, is accessed only through two standard atomic operations: wait() andsignal() . The wait() operation was originally termed P(from the Dutch

operation was originally termed P(from the Dutch proberen, to test);signal() was originally called V(from verhogen, to increment ). The denition of wait() is as follows: wait(S)  while (S 0) ; b u s yw a i t S;  The denition of signal() is as follows: signal(S)  S;  All modications to the integer value of the semaphore in the wait() and signal() operations must be executed indivisibly. T hat is, when one process modies the semaphore value, no other process can simultaneously modify that same

other process can simultaneously modify that same semaphore value. In addition, in the case of wait(S) ,t h et e s t i n go f the integer value of S(S0), as well as its possible modication ( S), must be executed without interruption. We s hall see how these operations can be implemented in Section 5.6.2. First, lets see how semaphores can be used. 5.6.1 Semaphore Usage Operating systems often distinguish between counting and binary semaphores. The value of a counting semaphore can range over an

value of a counting semaphore can range over an unrestricted domain. The value of a binary semaphore can range only between 0 and 1. Thus, binary semaphores behave similarly to mutex locks. In fact, on systems that do not provide mutex locks, binary semaphores can be used instead for providing mutual exclusion. Counting semaphores can be used to control access to a given resource consisting of a nite number of instances. The semaphore is initialized to the number of resources available. Each

to the number of resources available. Each process that wishes to use a resource performs a wait() operation on the semaphore (thereby decrementing the count). When a process releases a resource, it performs a signal() operation (incrementing the count). When the count for the semaphore goes to 0, all resources are being used. After that, processes that wish to use a resource will block until the count becomes greater than 0. We can also use semaphores to solve various synchronization problems.

to solve various synchronization problems. For example, consider two concurrently running processes: P1with a statement S1and P2with a statement S2.S u p p o s ew er e q u i r et h a t S2be executed only after S1has completed. We can implement this scheme readily by letting P1 and P2share a common semaphore synch ,i n i t i a l i z e dt o0 .I np r o c e s s P1,w e insert the statements5.6 Semaphores 215 S1; signal(synch); In process P2,w ei n s e r tt h es t a t e m e n t s wait(synch); S2;

n s e r tt h es t a t e m e n t s wait(synch); S2; Because synch is initialized to 0, P2will execute S2only after P1has invoked signal(synch) ,w h i c hi sa f t e rs t a t e m e n t S1has been executed. 5.6.2 Semaphore Implementation Recall that the implementation of mutex locks discussed in Section 5.5 suffers from busy waiting. The denitions of the wait() and signal() semaphore operations just described present the s ame problem. To overcome the need for busy waiting, we can modify the

the need for busy waiting, we can modify the denition of the wait() and signal() operations as follows: When a process executes the wait() operation and nds that the semaphore value is not positive, it must wait. However, rather than engaging in busy waiting, the process can block itself. The block operation places a process into a waiting queue associated with the semaphore, and the state of the process is switched to the waiting state. Then control is transferred to the CPU scheduler, which

control is transferred to the CPU scheduler, which selects another process to execute. A process that is blocked, waiting on a semaphore S,should be restarted when some other process executes a signal() operation. The process is restarted by a wakeup() operation, which changes the process from the waiting state to the ready state. The process is then placed in the ready queue. (The CPU may or may not be switched from the running process to the newly ready process, depending on the CPUscheduling

ready process, depending on the CPUscheduling algorithm.) To implement semaphores under this denition, we dene a semaphore as follows: typedef struct  int value; struct process list; semaphore ; Each semaphore has an integer value and a list of processes list .W h e n ap r o c e s sm u s tw a i to nas e m a p h o r e ,i ti sa d d e dt ot h el i s to fp r o c e s s e s .A signal() operation removes one process from the list of waiting processes and awakens that process. Now, the wait() semaphore

awakens that process. Now, the wait() semaphore operation can be dened as wait(semaphore S)  Svalue; if (Svalue  0)  add this process to Slist ; block();  216 Chapter 5 Process Synchronization and the signal() semaphore operation can be dened as signal(semaphore S)  Svalue; if (Svalue  0)  remove a process Pfrom Slist; wakeup(P);   The block() operation suspends the proce ss that invokes it. The wakeup(P) operation resumes the execution of a blocked process P. These two operations are provided

process P. These two operations are provided by the operating system as basic system calls. Note that in this implementation, s emaphore values may be negative, whereas semaphore values are never negative under the classical denition of semaphores with busy waiting. If a s emaphore value is negative, its magnitude is the number of processes waiting on that semaphore. This fact results from switching the order of the decremen ta n dt h et e s ti nt h ei m p l e m e n t a t i o no ft h e wait()

nt h ei m p l e m e n t a t i o no ft h e wait() operation. The list of waiting processes can be easily implemented by a link eld in each process control block ( PCB). Each semaphore contains an integer value and ap o i n t e rt oal i s to f PCBs. One way to add and remove processes from the list so as to ensure bounded waiting is to use a FIFO queue, where the semaphore contains both head and tail pointers to the queue. In general, however, the list can use any queueing strategy. Correct usage

list can use any queueing strategy. Correct usage of semaphores does not depend on a particular queueing strategy for the semaphore lists. It is critical that semaphore operations be executed atomically. We must guarantee that no two processes can execute wait() andsignal() operations on the same semaphore at the same time. This is a criticalsection problem; and in a singleprocessor environment, we can solve it by simply inhibiting interrupts during the time the wait() andsignal() operations are

the time the wait() andsignal() operations are executing. This scheme works in a singleprocessor environment because, once interrupts are inhibited, instructions from different processes cannot be interleaved. Only the currently running process executes until interrupts are reenabled and the scheduler can regain control. In a multiprocessor environment, interr upts must be disabled on every pro cessor. Otherwise, instructions from different processes (running on different processors) may be

processes (running on different processors) may be interleaved in some arbitrary way. Disabling interrupts on every processor can be a difcult task and furthermore can seriously diminish performance. Therefore, SMP systems must provide alternative locking tech niquessuch as compare and swap() or spinlocksto ensure that wait() andsignal() are performed atomically. It is important to admit that we have not completely eliminated busy waiting with this denition of the wait() and signal() operations.

denition of the wait() and signal() operations. Rather, we have moved busy waiting from the entry section to the critical sections of application programs. Furthermore, we have limited busy waiting to the critical sections of the wait() andsignal() operations, and these sections are short (if properly coded, they should be no more than about ten instructions). Thus, the critical section is almost never occupied, and busy waiting occurs5.6 Semaphores 217 rarely, and then for only a short time. An

217 rarely, and then for only a short time. An entirely different situation exists with application programs whose critical sections may be long (minutes or even hours) or may almost always be occupied. In such cases, busy waiting is extremely inefcient. 5.6.3 Deadlocks and Starvation The implementation of a semaphore with a waiting queue may result in a situation where two or more processes are waiting indenitely for an event that can be caused only by one of the waiting processes. The event in

only by one of the waiting processes. The event in question is the execution of a signal() operation. When such a state is reached, these processes are said to be deadlocked . To illustrate this, consider a system consisting of two processes, P0and P1, each accessing two semaphores, SandQ,s e tt ot h ev a l u e1 : P0 P1 wait(S); wait(Q); wait(Q); wait(S); .. .. .. signal(S); signal(Q); signal(Q); signal(S); Suppose that P0executes wait(S) and then P1executes wait(Q) .W h e n P0 executes wait(Q)

P1executes wait(Q) .W h e n P0 executes wait(Q) ,i tm u s tw a i tu n t i l P1executes signal(Q) .S i m i l a r l y ,w h e n P1executes wait(S) ,i tm u s tw a i tu n t i l P0executes signal(S) .S i n c et h e s e signal() operations cannot be executed, P0and P1are deadlocked. We say that a set of processes is in a deadlocked state when every process in the set is waiting for an event that c an be caused only by another process in the set. The events with which we are mainly concerned here are

events with which we are mainly concerned here are resource acquisition and release. Other types of events may result in deadlocks, as we show in Chapter 7. In that chapter, we describe various mechanisms for dealing with the deadlock problem. Another problem related to deadlocks is indenite blocking orstarvation , as i t u a t i o ni nw h i c hp r o c e s s e sw a i ti n d e  n i t e l yw i t h i nt h es e m a p h o r e .I n d e   nite blocking may occur if we remove processes from the list

may occur if we remove processes from the list associated with a semaphore in LIFO (lastin, rstout) order. 5.6.4 Priority Inversion As c h e d u l i n gc h a l l e n g ea r i s e sw h e nah i g h e r  p r i o r i t yp r o c e s sn e e d st or e a d or modify kernel data that are currently being accessed by a lowerpriority processor a chain of lowerpriority processes. Since kernel data are typically protected with a lock, the higherpriority process will have to wait for a lowerpriority one to

will have to wait for a lowerpriority one to nish with the resource. The situation becomes more complicated if the lowerpriority process is preempted in favor of another process with a higher priority. As an example, assume we have three processes L,M,a n d Hwhose priorities follow the order LMH. Assume that process Hrequires218 Chapter 5 Process Synchronization PRIORITY INVERSION AND THE MARS PATHFINDER Priority inversion can be more than a scheduling inconvenience. On systems with tight time

inconvenience. On systems with tight time constraintssuch as realtime systemspriority inversion can cause a process to take longer than it should to accomplish a task. When that happens, other failures can cascade, resulting in system failure. Consider the Mars Pathnder, a NASA space probe that landed a robot, the Sojourner rover, on Mars in 1997 to conduct experiments. Shortly after the Sojourner began operating, it started to experience frequent computer resets. Each reset reinitialized all

computer resets. Each reset reinitialized all hardware and software, including communica tions. If the problem had not been solved, the Sojourner would have failed in its mission. The problem was caused by the fact that one highpriority task, bc dist,  was taking longer than expected to complete its work. This task was being forced to wait for a shared resource that was held by the lowerpriority ASIMET task, which in turn was preempted by multiple mediumpriority tasks. The bc disttask would

mediumpriority tasks. The bc disttask would stall waiting for the shared resource, and ultimately the bc sched task would discover the problem and perform the reset. The Sojourner was suffering from a typical case of priority inversion. The operating system on the Sojourner was the VxWorks realtime operat ing system, which had a global variable to enable priority inheritance on all semaphores. After testing, the variable was set on the Sojourner (on Mars!), and the problem was solved. Af u l ld

(on Mars!), and the problem was solved. Af u l ld e s c r i p t i o no ft h ep r o b l e m ,i t sd e t e c t i o n ,a n di t ss o l u  tion was written by the software team lead and is available at http:research.microsoft.comenusumpeoplembjmars pathnder authoritative account.html . resource R,w h i c hi sc u r r e n t l yb e i n ga c c e s s e db yp r o c e s s L.O r d i n a r i l y ,p r o c e s s Hwould wait for Lto nish using resource R.H o w e v e r ,n o ws u p p o s et h a t process Mbecomes

v e r ,n o ws u p p o s et h a t process Mbecomes runnable, thereby preempting process L.I n d i r e c t l y ,a process with a lower priorityprocess Mhas affected how long process Hmust wait for Lto relinquish resource R. This problem is known as priority inversion .I to c c u r so n l yi ns y s t e m sw i t h more than two priorities, so one solution is to have only two priorities. That is insufcient for most generalpurpose operating systems, however. Typically these systems solve the problem

however. Typically these systems solve the problem by implementing a priorityinheritance protocol .A c c o r d i n gt ot h i sp r o t o c o l ,a l lp r o c e s s e st h a ta r ea c c e s s i n gr e s o u r c e s needed by a higherpriority process inherit the higher priority until they are nished with the resources in question. When they are nished, their priorities revert to their original values. In the example above, a priorityinheritance protocol would allow process Lto temporarily inherit

would allow process Lto temporarily inherit the priority of process H, thereby preventing process Mfrom preempting its execution. When process Lhad nished using resource R,i tw o u l dr e l i n q u i s hi t si n h e r i t e dp r i o r i t yf r o m Hand assume its original priority. Because resource Rwould now be available, process Hnot Mwould run next.5.7 Classic Problems of Synchronization 219 do ...  produce an item in next produced  ... wait(empty); wait(mutex); ...  add next produced to the

wait(mutex); ...  add next produced to the buffer  ... signal(mutex); signal(full); while (true); Figure 5.9 The structure of the producer process. 5.7 Classic Problems of Synchronization In this section, we present a number of synchronization problems as examples of a large class of concurrencycontrol problems. These problems are used for testing nearly every newly proposed synch ronization scheme. In our solutions to the problems, we use semaphores for synchronization, since that is the

semaphores for synchronization, since that is the traditional way to present such solutions. However, actual implementations of these solutions could use mutex locks in place of binary semaphores. 5.7.1 The BoundedBuffer Problem The boundedbuffer problem was introduced in Section 5.1; it is commonly used to illustrate the power of synchronization primitives. Here, we present a general structure of this scheme without committing ourselves to any particular implementation. We provide a related

particular implementation. We provide a related programming project in the exercises at the end of the chapter. In our problem, the producer and consumer processes share the following data structures: int n; semaphore mutex  1; semaphore empty  n; semaphore full  0 We assume that the pool consists of nbuffers, each capable of holding one item. Themutex semaphore provides mutual exclusion for accesses to the buffer pool and is initialized to the value 1. The empty and full semaphores count the

value 1. The empty and full semaphores count the number of empty and full buffers. The semaphore empty is initialized to the value n;t h es e m a p h o r e full is initialized to the value 0. The code for the producer process is shown in Figure 5.9, and the code for the consumer process is shown i nF i g u r e5 . 1 0 .N o t et h es y m m e t r yb e t w e e n the producer and the consumer. We can interpret this code as the producer producing full buffers for the consumer or as the consumer

full buffers for the consumer or as the consumer producing empty buffers for the producer.220 Chapter 5 Process Synchronization do wait(full); wait(mutex); ...  remove an item from buffer to next consumed  ... signal(mutex); signal(empty); ...  consume the item in next consumed  ... while (true); Figure 5.10 The structure of the consumer process. 5.7.2 The ReadersWriters Problem Suppose that a database is to be shared among several concurrent processes. Some of these processes may want only to

Some of these processes may want only to read the database, whereas others may want to update (that is, to read and write) the database. We distinguish between these two types of processes by referring to the former as readers and to the latter as writers .O b v i o u s l y ,i ft w or e a d e r sa c c e s st h es h a r e dd a t a simultaneously, no adverse effects will result. However, if a writer and some other process (either a reader or a writer) access the database simultaneously, chaos may

access the database simultaneously, chaos may ensue. To ensure that these difculties do not arise, we require that the writers have exclusive access to the shared database while writing to the database. This synchronization problem is referred to as the readerswriters problem .S i n c ei t was originally stated, it has been used to test nearly every new synchronization primitive. The readerswriters problem has several variations, all involving priorities. The simplest one, referred to as the rst

The simplest one, referred to as the rst readerswriters problem, requires that no reader be kept waiting unless a writer has already obtained permission to use the shared object. In other words, no reader should wait for other readers to nish simply because a writer is waiting. The second readers writers problem requires that, once a writer is ready , that writer perform its write as soon as possible. In other words, if a writer is waiting to access the object, no new readers may start reading.

the object, no new readers may start reading. As o l u t i o nt oe i t h e rp r o b l e mm a yr e s u l ti ns t a r v a t i o n .I nt h e r s tc a s e , writers may starve; in the second case, readers may starve. For this reason, other variants of the problem h ave been proposed. Next, we present a solution to the rst readerswriters problem. See the bibliographical notes at the end of the chapter for references describ ing starvationfree solutions to the second readerswriters problem. In the

to the second readerswriters problem. In the solution to the rst readerswriters problem, the reader processes share the following data structures: semaphore rw mutex  1; semaphore mutex  1; int read count  0; The semaphores mutex and rw mutex are initialized to 1; read count is initialized to 0. The semaphore rw mutex is common to both reader and writer5.7 Classic Problems of Synchronization 221 do wait(rw mutex); ...  writing is performed  ... signal(rw mutex); while (true); Figure 5.11 The

signal(rw mutex); while (true); Figure 5.11 The structure of a writer process. processes. The mutex semaphore is used to ensure mutual exclusion when the variable read count is updated. The read count variable keeps track of how many processes are currently reading the object. The semaphore rw mutex functions as a mutual exclusion semaphore for the writers. It is also used by the rst or last reader that enters or exits the critical section. It is not used by readers who enter or exit while other

not used by readers who enter or exit while other readers are in their critical sections. The code for a writer process is shown in Figure 5.11; the code for a reader process is shown in Figure 5.12. Note that, if a writer is in the critical section and nreaders are waiting, then one reader is queued on rw mutex ,a n d n1r e a d e r sa r eq u e u e do n mutex .A l s oo b s e r v et h a t ,w h e naw r i t e re x e c u t e s signal(rw mutex) ,w em a yr e s u m et h ee x e c u t i o no fe i t h e

a yr e s u m et h ee x e c u t i o no fe i t h e rt h ew a i t i n gr e a d e r s or a single waiting writer. The selection is made by the scheduler. The readerswriters problem and its solutions have been generalized to provide readerwriter locks on some systems. Acquiring a readerwriter lock requires specifying the mode of the lock: either read orwrite access. When a process wishes only to read shar ed data, it requests the readerwriter lock in read mode. A process wishing to modify the shared

read mode. A process wishing to modify the shared data must request the lock in write mode. Multiple processes are permitted to concurrently acquire ar e a d e r  w r i t e rl o c ki nr e a dm o d e ,b u to n l yo n ep r o c e s sm a ya c q u i r et h el o c k for writing, as exclusive access is required for writers. Readerwriter locks are most useful in the following situations: do wait(mutex); read count; if (read count  1) wait(rw mutex); signal(mutex); ...  reading is performed  ...

signal(mutex); ...  reading is performed  ... wait(mutex); read count; if (read count  0) signal(rw mutex); signal(mutex); while (true); Figure 5.12 The structure of a reader process.222 Chapter 5 Process SynchronizationRICE Figure 5.13 The situation of the dining philosophers. In applications where it is easy to identify which processes only read shared data and which processes only write shared data. In applications that have more readers than writers. This is because reader writer locks

than writers. This is because reader writer locks generally require more overhead to establish than semaphores or mutualexclusion locks. The increased concurrency of allowing multiple readers compensates for the overhead involved in setting up the reader writer lock. 5.7.3 The DiningPhilosophers Problem Consider ve philosophers who spend their lives thinking and eating. The philosophers share a circular table surrounded by ve chairs, each belonging to one philosopher. In the center of the table

to one philosopher. In the center of the table is a bowl of rice, and the table is laid with ve single chopsticks (Figure 5.13). When a philosopher thinks, she does not interact with her colleagues. From time to time, a philosopher gets hungry and tries to pick up the two chopsticks that are closest to her (the chopsticks that are between her and her left an dr i g h tn e i g h b o r s ) .Ap h i l o s o p h e rm a yp i c k up only one chopstick at a time. Obviously, she cannot pick up a

at a time. Obviously, she cannot pick up a chopstick that is already in the hand of a neighbor. When a hungry philosopher has both her chopsticks at the same time, she eats without releasing the chopsticks. When she is nished eating, she puts down both chopsticks and starts thinking again. The diningphilosophers problem is considered a classic synchronization problem neither because of its practical importance nor because computer scientists dislike philosophers but because it is an example of a

philosophers but because it is an example of a large class of concurrencycontrol problems. It is a simple representation of the need to allocate several resources among several processes in a deadlockfree and starvationfree manner. One simple solution is to represent each chopstick with a semaphore. A philosopher tries to grab a chopstick by executing a wait() operation on that semaphore. She releases her chopsticks by executing the signal() operation on the appropriate semaphores. Thus, the

operation on the appropriate semaphores. Thus, the shared data are semaphore chopstick[5];5.8 Monitors 223 do wait(chopstick[i]); wait(chopstick[(i1)  5]); ...  eat for awhile  ... signal(chopstick[i]); signal(chopstick[(i1)  5]); ...  think for awhile  ... while (true); Figure 5.14 The structure of philosopher i. where all the elements of chopstick are initialized to 1. The structure of philosopher iis shown in Figure 5.14. Although this solution guarantees that no two neighbors are eating

guarantees that no two neighbors are eating simultaneously, it nevertheless must be rejected because it could create a deadlock. Suppose that all ve philosophers become hungry at the same time and each grabs her left chopstick. All the elements of chopstick will now be equal to 0. When each philosopher tries to grab her right chopstick, she will be delayed forever. Several possible remedies to the deadlock problem are replaced by: Allow at most four philosophers to be sitting simultaneously at

four philosophers to be sitting simultaneously at the table. Allow a philosopher to pick up her chopsticks only if both chopsticks are available (to do this, she must pick them up in a critical section). Use an asymmetric solutionthat is, an oddnumbered philosopher picks up rst her left chopstick and then he r right chopstick, whereas an even numbered philosopher picks up her right chopstick and then her left chopstick. In Section 5.8, we present a solution to the diningphilosophers problem that

a solution to the diningphilosophers problem that ensures freedom from deadlocks. Note, however, that any satisfactory solution to the diningphilosophers problem must guard against the possibility that one of the philosophers will starve to death. A deadlockfree solution does not necessarily eliminate the possibility of starvation. 5.8 Monitors Although semaphores provide a convenient a nd effective mechanism for process synchronization, using them incorrectly can result in timing errors that

them incorrectly can result in timing errors that are difcult to detect, since these errors happen only if particular execution sequences take place and these sequences do not always occur. We have seen an example of such errors in the use of counters in our solution to the producerconsumer problem (Section 5.1). In that example, the timing problem happened only ra rely, and even then the counter value224 Chapter 5 Process Synchronization appeared to be reasonableoff by only 1. Nevertheless, the

to be reasonableoff by only 1. Nevertheless, the solution is obviously not an acceptable one. It is for this reason that semaphores were introduced in the rst place. Unfortunately, such timing errors can still occur when semaphores are used. To illustrate how, we review the semaphore solution to the criticalsection problem. All processes share a semaphore variable mutex ,w h i c hi si n i t i a l i z e d to 1. Each process must execute wait(mutex) before entering the critical section

wait(mutex) before entering the critical section andsignal(mutex) afterward. If this sequence is not observed, two processes may be in their critical sections simultaneously. Next, we examine the various difculties that may result. Note that these difculties will arise even if a single process is not well behaved. This situation may be caused by an honest programming error or an uncooperative programmer. Suppose that a process interchanges the order in which the wait() and signal() operations on

in which the wait() and signal() operations on the semaphore mutex are executed, resulting in the following execution: signal(mutex); ... critical section ... wait(mutex); In this situation, several processes may be executing in their critical sections simultaneously, violating the mutualexclusion requirement. This error may be discovered only if several processes are simultaneously active in their critical sections. Note that this situation may not always be reproducible. Suppose that a process

not always be reproducible. Suppose that a process replaces signal(mutex) with wait(mutex) .T h a t is, it executes wait(mutex); ... critical section ... wait(mutex); In this case, a deadlock will occur. Suppose that a process omits the wait(mutex) ,o rt h e signal(mutex) ,o r both. In this case, either mutual exclusion is violated or a deadlock will occur. These examples illustrate that various types of errors can be generated easily when programmers use semaphores incorrectly to solve the

use semaphores incorrectly to solve the criticalsection problem. Similar problems may arise in the other synchronization models discussed in Section 5.7. To deal with such errors, resear chers have developed highlevel language constructs. In this section, we describ eo n ef u n d a m e n t a lh i g h  l e v e ls y n c h r o  nization constructthe monitor type.5.8 Monitors 225 monitor monitor name   shared variable declarations  function P 1(...)  ...  function P 2(...)  ...  . . . function P

...  function P 2(...)  ...  . . . function P n(...)  ...  initialization code (...)  ...   Figure 5.15 Syntax of a monitor. 5.8.1 Monitor Usage Anabstract data type or ADT encapsulates data with a set of functions to operate on that data that are independent of any specic implementation of the ADT.A monitor type is an ADT that includes a set of programmer dened operations that are provided with mutual exclusion within the monitor. The monitor type also declares the variables whose values dene

type also declares the variables whose values dene the state of an instance of that type, along with the bodies of functions that operate on those variables. The syntax of a monitor type is shown in Figure 5.15. The representation of a monitor type cannot be used directly by the various processes. Thus, a function dened within a monitor can access only those variables declared locally within the monitor and its formal parameters. Similarly, the local variables of a monitor can be accessed by

local variables of a monitor can be accessed by only the local functions. The monitor construct ensures that only one process at a time is active within the monitor. Consequently, the programmer does not need to code this synchronization constraint explicitly (Figure 5.16). However, the monitor construct, as dened so far, is not sufciently powerful for modeling some synchronization schemes. For this purpose, we need to dene additional syn chronization mechanisms. These mechanisms are provided by

mechanisms. These mechanisms are provided by the condition construct. A programmer who needs to write a tailormade synchronization scheme can dene one or more variables of type condition : condition x, y;226 Chapter 5 Process Synchronization entry queue shared data operations initialization code. . . Figure 5.16 Schematic view of a monitor. The only operations that can be invoked on a condition variable are wait() andsignal() .T h eo p e r a t i o n x.wait(); means that the process invoking this

o n x.wait(); means that the process invoking this operation is suspended until another process invokes x.signal(); Thex.signal() operation resumes exactly one suspended process. If no process is suspended, then the signal() operation has no effect; that is, the state of xis the same as if the operation had never been executed (Figure 5.17). Contrast this operation with the signal() operation associated with semaphores, which always affects the state of the semaphore. Now suppose that, when the

state of the semaphore. Now suppose that, when the x.signal() operation is invoked by a process P,t h e r ee x i s t sas u s p e n d e dp r o c e s s Qassociated with condition x.C l e a r l y ,i ft h e suspended process Qis allowed to resume its execution, the signaling process Pmust wait. Otherwise, both Pand Qwould be active simultaneously within the monitor. Note, however, that conceptually both processes can continue with their execution. Two possibilities exist: 1.Signal and wait .Peither

possibilities exist: 1.Signal and wait .Peither waits until Qleaves the monitor or waits for another condition. 2.Signal and continue .Qeither waits until Pleaves the monitor or waits for another condition.5.8 Monitors 227 operationsqueues associated with x, y conditionsentry queueshared datax y initialization code   Figure 5.17 Monitor with condition variables. There are reasonable arguments in favor of adopting either option. On the one hand, since Pwas already executing in the monitor, the

since Pwas already executing in the monitor, the signaland continue method seems more reasonable. On the other, if we allow thread P to continue, then by the time Qis resumed, the logical condition for which Q was waiting may no longer hold. A compromise between these two choices was adopted in the language Concurrent Pascal. When thread Pexecutes the signal operation, it immediately leaves the monitor. Hence, Qis immediately resumed. Many programming languages have incorporated the idea of the

languages have incorporated the idea of the monitor as described in this section, includ ing Java and C (pronounced Csharp ). Other languagessuch as Erlangprovide some type of concurrency support using a similar mechanism. 5.8.2 DiningPhilosophers Solution Using Monitors Next, we illustrate monitor concepts by presenting a deadlockfree solution to the diningphilosophers problem. This solution imposes the restriction that a philosopher may pick up her chopsticks only if both of them are

pick up her chopsticks only if both of them are available. To code this solution, we need to distinguish among three states in which we may nd a philosopher. For this purpose, we introduce the following data structure: enum THINKING, HUNGRY, EATING state[5]; Philosopher ican set the variable state[i]  EATING only if her two neighbors are not eating: ( state[(i4)  5] ! EATING )a n d( state[(i1) 5 ]! E A T I N G ).228 Chapter 5 Process Synchronization monitor DiningPhilosophers  enum THINKING,

monitor DiningPhilosophers  enum THINKING, HUNGRY, EATING state[5]; condition self[5]; void pickup(int i)  state[i]  HUNGRY; test(i); if (state[i] ! EATING) self[i].wait();  void putdown(int i)  state[i]  THINKING; test((i  4)  5); test((i  1)  5);  void test(int i)  if ((state[(i  4)  5] ! EATING)  (state[i]  HUNGRY)  (state[(i  1)  5] ! EATING))  state[i]  EATING; self[i].signal();   initialization code()  for (int i  0; i  5; i) state[i]  THINKING;   Figure 5.18 Am o n i t o rs o l u t i o nt

Figure 5.18 Am o n i t o rs o l u t i o nt ot h ed i n i n g  p h i l o s o p h e rp r o b l e m . We also need to declare condition self[5]; This allows philosopher ito delay herself when she is hungry but is unable to obtain the chopsticks she needs. We are now in a position to describe our solution to the diningphilosophers problem. The distribution of the chopsticks is controlled by the monitor Din ingPhilosophers , whose denition is shown in Figure 5.18. Each philosopher, before starting

in Figure 5.18. Each philosopher, before starting to eat, must invoke the operation pickup() .T h i sa c tm a yr e s u l t in the suspension of the philosopher process. After the successful completion of the operation, the philosopher may eat. Following this, the philosopher invokes theputdown() operation. Thus, philosopher imust invoke the operations pickup() andputdown() in the following sequence:5.8 Monitors 229 DiningPhilosophers.pickup(i); ... eat ... DiningPhilosophers.putdown(i); It is

... eat ... DiningPhilosophers.putdown(i); It is easy to show that this solution ensures that no two neighbors are eating simultaneously and that no deadlocks will occur. We note, however, that it is possible for a philosopher to starve to death. We do not present a solution to this problem but rather leave it as an exercise for you. 5.8.3 Implementing a Monitor Using Semaphores We now consider a possible implementation of the monitor mechanism using semaphores. For each monitor, a semaphore

using semaphores. For each monitor, a semaphore mutex (initialized to 1) is provided. Ap r o c e s sm u s te x e c u t e wait(mutex) before entering the monitor and must execute signal(mutex) after leaving the monitor. Since a signaling process must wait until the resumed process either leaves or waits, an additional semaphore, next ,i si n t r o d u c e d ,i n i t i a l i z e dt o0 .T h e signaling processes can use next to suspend themselves. An integer variable next count is also provided to

An integer variable next count is also provided to count the number of processes suspended on next .T h u s ,e a c he x t e r n a lf u n c t i o n Fis replaced by wait(mutex); ... body of F ... if (next count  0) signal(next); else signal(mutex); Mutual exclusion within a monitor is ensured. We can now describe how condition variables are implemented as well. For each condition x,w ei n t r o d u c eas e m a p h o r e x sem and an integer variable x count ,b o t hi n i t i a l i z e dt o0 .T h

x count ,b o t hi n i t i a l i z e dt o0 .T h eo p e r a t i o n x.wait() can now be implemented as x count; if (next count  0) signal(next); else signal(mutex); wait(x sem); x count; The operation x.signal() can be implemented as230 Chapter 5 Process Synchronization if (x count  0)  next count; signal(x sem); wait(next); next count;  This implementation is applicable to the denitions of monitors given by both Hoare and BrinchHansen (see the bibliographical notes at the end of the chapter). In

notes at the end of the chapter). In some cases, however, the generality of the implementation is unnecessary, and a signicant improvement in efciency is possible. We leave this problem to you in Exercise 5.30. 5.8.4 Resuming Processes within a Monitor We turn now to the subject of processresumption order within a monitor. If several processes are suspended on condition x,a n da n x.signal() operation is executed by some process, then how do we determine which of the suspended processes should

determine which of the suspended processes should be resumed next? One simple solution is to use a rstcome, rstserved ( FCFS ) ordering, so that the process that has been waiting the longest is resumed rst. In many circumstances, however, such a simple scheduling scheme is not adequate. For this purpose, the conditionalwait construct can be used. This construct has the form x.wait(c); where cis an integer expression that is evaluated when the wait() operation is executed. The value of c, which

operation is executed. The value of c, which is called a priority number ,i st h e ns t o r e d with the name of the process that is suspended. When x.signal() is executed, the process with the smallest priority number is resumed next. To illustrate this new mechanism, consider the ResourceAllocator mon itor shown in Figure 5.19, which controls the allocation of a single resource among competing processes. Each process, when requesting an allocation of this resource, species the maximum time it

of this resource, species the maximum time it plans to use the resource. The mon itor allocates the resource to the process that has the shortest timeallocation request. A process that needs to access the resource in question must observe the following sequence: R.acquire(t); ... access the resource; ... R.release(); where Ris an instance of type ResourceAllocator . Unfortunately, the monitor concept cannot guarantee that the preceding access sequence will be observed. In particular, the

sequence will be observed. In particular, the following problems can occur: A process might access a resource without rst gaining access permission to the resource.5.8 Monitors 231 monitor ResourceAllocator  boolean busy; condition x; void acquire(int time)  if (busy) x.wait(time); busy  true;  void release()  busy  false; x.signal();  initialization code()  busy  false;   Figure 5.19 Am o n i t o rt oa l l o c a t eas i n g l er e s o u r c e . Ap r o c e s sm i g h tn e v e rr e l e a s ear e

Ap r o c e s sm i g h tn e v e rr e l e a s ear e s o u r c eo n c ei th a sb e e ng r a n t e da c c e s s to the resource. Ap r o c e s sm i g h ta t t e m p tt or e l e a s ear e s o u r c et h a ti tn e v e rr e q u e s t e d . A process might request the same resource twice (without rst releasing the resource). The same difculties are encountered with the use of semaphores, and these difculties are similar in natur et ot h o s et h a te n c o u r a g e du st od e v e l o p the monitor

n c o u r a g e du st od e v e l o p the monitor constructs in the rst place. Previously, we had to worry about the correct use of semaphores. Now, we have to worry about the correct use of higherlevel programmerdened operations, with which the compiler can no longer assist us. One possible solution to the current problem is to include the resource access operations within the ResourceAllocator monitor. However, using this solution will mean that scheduling is done according to the builtin

that scheduling is done according to the builtin monitorscheduling algorithm rather than the one we have coded. To ensure that the processes observe the appropriate sequences, we must inspect all the programs that make use of the ResourceAllocator monitor and its managed resource. We must ch eck two conditions to establish the correctness of this system. First, user processes must always make their calls on the monitor in a correct sequence. Second, we must be sure that an uncooperative process

we must be sure that an uncooperative process does not simply ignore the mutualexclusion gateway provided by the monitor and try to access the shared resource directly, without using the access protocols. Only if these two conditions can be ensured can we guarantee that no timedependent errors will occur and that the scheduling algorithm will not be defeated.232 Chapter 5 Process Synchronization JAVA MONITORS Java provides a monitorlike concurrency mechanism for thread synchro nization. Every

mechanism for thread synchro nization. Every object in Java has associated with it a single lock. When a method is declared to be synchronized ,c a l l i n gt h em e t h o dr e q u i r e so w n i n g the lock for the object. We declare a synchronized method by placing the synchronized keyword in the method denition. The following denes safeMethod() assynchronized ,f o re x a m p l e : public class SimpleClass  ... public synchronized void safeMethod()  ...  Implementation of safeMethod()  ...

...  Implementation of safeMethod()  ...   Next, we create an object instance of SimpleClass ,s u c ha st h ef o l l o w i n g : SimpleClass sc  new SimpleClass(); Invoking sc.safeMethod() method requires owning the lock on the object instance sc.I ft h el o c ki sa l r e a d yo w n e db ya n o t h e rt h r e a d ,t h et h r e a dc a l l i n g thesynchronized method blocks and is placed in the entry set for the objects lock. The entry set represents the set of threads waiting for the lock to

the set of threads waiting for the lock to become available. If the lock is available when a synchronized method is called, the calling thread becomes the owner of the objects lock and can enter the method. The lock is released when the thread exits the method. A thread from the entry set is then selected as the new owner of the lock. Java also provides wait() andnotify() methods, which are similar in function to the wait() andsignal() statements for a monitor. The Java APIprovides support for

for a monitor. The Java APIprovides support for semaphores, condition variables, and mutex locks (among other concurrency mechanisms) in the java.util.concurrent package. Although this inspection may be possible for a small, static system, it is not reasonable for a large system or a dynamic system. This accesscontrol problem can be solved only through the use of the additional mechanisms that are described in Chapter 14. 5.9 Synchronization Examples We next describe the synchronization

Examples We next describe the synchronization mechanisms provided by the Windows, Linux, and Solaris operating systems, as well as the Pthreads API.W eh a v e chosen these three operating systems because they provide good examples of different approaches to synchronizing the kernel, and we have included the5.9 Synchronization Examples 233 Pthreads APIbecause it is widely used for thread creation and synchronization by developers on UNIX and Linux systems. As you will see in this section, the

systems. As you will see in this section, the synchronization methods available in these differing systems vary in subtle and signicant ways. 5.9.1 Synchronization in Windows The Windows operating system is a multithreaded kernel that provides support for realtime applications and multiple processors. When the Windows kernel accesses a global resource on a singleprocessor system, it temporarily masks interrupts for all interrupt handlers that may also access the global resource. On a

that may also access the global resource. On a multiprocessor system, Windows protects access to global resources using spinlocks, although the kernel uses spinlocks only to protect short code segments. Furthermore, for reasons of ef ciency, the kernel ensures that a thread will never be pr eempted while holding a spinlock. For thread synchronization outside the kernel, Windows provides dis patcher objects .U s i n gad i s p a t c h e ro b j e c t ,t h r e a d ss y n c h r o n i z ea c c o r d i

t ,t h r e a d ss y n c h r o n i z ea c c o r d i n gt o several different mechanisms, includin gm u t e xl o c k s ,s e m a p h o r e s ,e v e n t s ,a n d timers. The system protects shared data by requiring a thread to gain ownership of a mutex to access the data and to release ownership when it is nished. Semaphores behave as described in Section 5.6. Events are similar to condition variables; that is, they may notify a waiting thread when a desired condition occurs. Finally, timers are

a desired condition occurs. Finally, timers are used to notify one (or more than one) thread that a specied amount of time has expired. Dispatcher objects may be in either as i g n a l e ds t a t eo ran o n s i g n a l e ds t a t e . An object in a signaled state is available, and a thread will not block when acquiring the object. An object in a nonsignaled state is not available, and a thread will block when attempting to acquire the object. We illustrate the state transitions of a mutex lock

illustrate the state transitions of a mutex lock dispatcher object in Figure 5.20. Ar e l a t i o n s h i pe x i s t sb e t w e e nt h es t a t eo fad i s p a t c h e ro b j e c ta n dt h es t a t e of a thread. When a thread blocks on a nonsignaled dispatcher object, its state changes from ready to waiting, and the thread is placed in a waiting queue for that object. When the state for th ed i s p a t c h e ro b j e c tm o v e st os i g n a l e d ,t h e kernel checks whether any threads ar ew a

d ,t h e kernel checks whether any threads ar ew a i t i n go nt h eo b j e c t .I fs o ,t h ek e r n e l moves one threador possibly morefrom the waiting state to the ready state, where they can resume executing. The number of threads the kernel selects from the waiting queue depends on the type of dispatcher object for which it is waiting. The kernel will select only one thread from the waiting queue for a mutex, since a mutex object may be owned by only a singlenonsignaled signaledowner

be owned by only a singlenonsignaled signaledowner thread releases mutex lock thread acquires mutex lock Figure 5.20 Mutex dispatcher object.234 Chapter 5 Process Synchronization thread. For an event object, the kernel will select all threads that are waiting for the event. We can use a mutex lock as an illustration of dispatcher objects and thread states. If a thread tries to acquire a mutex dispatcher object that is in a nonsignaled state, that thread will be suspended and placed in a waiting

thread will be suspended and placed in a waiting queue for the mutex object. When the mutex moves to the signaled state (because another thread has released the lock on the mutex), the thread waiting at the front of the queue will be moved from the waiting state to the ready state and will acquire the mutex lock. Acriticalsection object is a usermode mutex that can often be acquired and released without kernel interven tion. On a multiprocessor system, a criticalsection object rst uses a

system, a criticalsection object rst uses a spinlock while waiting for the other thread to release the object. If it spins too long, the acquiring thread will then allocate a kernel mutex and yield its CPU.C r i t i c a l  s e c t i o no b j e c t sa r ep a r t i c u l a r l ye f  c i e n t because the kernel mutex is allocated only when there is contention for the object. In practice, there is very little contention, so the savings are signicant. We provide a programming project at the end of

We provide a programming project at the end of this chapter that uses mutex locks and semaphores in the Windows API. 5.9.2 Synchronization in Linux Prior to Version 2.6, Linux was a nonpreemptive kernel, meaning that a process running in kernel mode could not be preemptedeven if a higherpriority process became available to run. Now, however, the Linux kernel is fully preemptive, so a task can be preempted when it is running in the kernel. Linux provides several different mechanisms for

Linux provides several different mechanisms for synchronization in the kernel. As most computer architectures provide instructions for atomic ver sions of simple math operations, the simplest synchronization technique within the Linux kernel is an atomic integer, which is represented using the opaque data type atomic t.A st h en a m ei m p l i e s ,a l lm a t ho p e r a t i o n su s i n ga t o m i c integers are performed without interruption. The following code illustrates declaring an atomic

The following code illustrates declaring an atomic integer counter and then performing various atomic operations: atomic tc o u n t e r ; int value; atomic set(counter,5);  counter  5  atomic add(10, counter);  counter  counter  10  atomic sub(4, counter);  counter  counter  4  atomic inc(counter);  counter  counter  1  value  atomic read(counter);  value  12  Atomic integers are particularly efcient in situations where an integer variable such as a counterneeds to be updated, since atomic

such as a counterneeds to be updated, since atomic operations do not require the overhead of locking mechanisms. However, their usage is limited to these sorts of scenarios. In situations where there are several variables contributing to a possible race condition, more sophisticated locking tools must be used. Mutex locks are available in Linux for protecting critical sections within the kernel. Here, a task must invoke the mutex lock() function prior to entering5.9 Synchronization Examples 235

prior to entering5.9 Synchronization Examples 235 ac r i t i c a ls e c t i o na n dt h e mutex unlock() function after exiting the critical section. If the mutex lock is unavailable, a task calling mutex lock() is put into as l e e ps t a t ea n di sa w a k e n e dw h e nt h el o c k  so w n e ri n v o k e s mutex unlock() . Linux also provides spinlocks and semaphores (as well as readerwriter versions of these two locks) for locking in the kernel. On SMP machines, the fundamental locking

kernel. On SMP machines, the fundamental locking mechanism is a spinlock, and the kernel is designed so that the spinlock is held only for short durations. On singleprocessor machines, such as embedded systems with only a single processing core, spinlocks are inappropriate for use and are replaced by enabling and disabling kernel preemption. That is, on singleprocessor systems, rather than holding a spinlock, the kernel disables kernel pr eemption; and rather than releasing the spinlock, it

and rather than releasing the spinlock, it enables kernel preemption. This is summarized below:single processor multiple processors Acquire spin lock. Release spin lock.Disable kernel preemption. Enable kernel preemption.Linux uses an interesting approach t od i s a b l ea n de n a b l ek e r n e lp r e e m p  tion. It provides two simple system calls preempt disable() and pre empt enable() for disabling and enabling k ernel preemption. The kernel is not preemptible, however, if a task ru nning

is not preemptible, however, if a task ru nning in the kernel is holding a lock. To enforce this rule, each task in the system has a threadinfo structure containing a counter, preempt count ,t oi n d i c a t et h en u m b e ro fl o c k sb e i n g held by the task. When a lock is acquired, preempt count is incremented. It is decremented when a lock is released. If the value of preempt count for the task currently running in the kernel is greater than 0, it is not safe to preempt the kernel, as

than 0, it is not safe to preempt the kernel, as this task currently holds a lock. If the count is 0, the kernel can safely be interrupted (assuming there are no outstanding calls to preempt disable() ). Spinlocksalong with enabling and disabling kernel preemptionare used in the kernel only when a lock (or disabling kernel preemption) is held for a short duration. When a lock must be held for a longer period, semaphores or mutex locks are appropriate for use. 5.9.3 Synchronization in Solaris To

for use. 5.9.3 Synchronization in Solaris To control access to critical sections, Solaris provides adaptive mutex locks, condition variables, semaphores, readerwriter locks, and turnstiles. Solaris implements semaphores and condition variables essentially as they are pre sented in Sections 5.6 and 5.7 In this section, we describe adaptive mutex locks, readerwriter locks, and turnstiles. An adaptive mutex protects access to every critical data item. On a multiprocessor system, an adaptive mutex

On a multiprocessor system, an adaptive mutex starts as a standard semaphore implemented as a spinlock. If the data are locked and therefore already in use, the adaptive mutex does one of two things. If the lock is held by a thread that is currently running on another CPU,t h et h r e a ds p i n sw h i l ew a i t i n gf o rt h e lock to become available, because the thread holding the lock is likely to nish soon. If the thread holding the lock is not currently in run state, the thread236 Chapter

not currently in run state, the thread236 Chapter 5 Process Synchronization blocks, going to sleep until it is awakened by the release of the lock. It is put to sleep so that it will not spin while waiting, since the lock will not be freed very soon. A lock held by a sleeping thread is likely to be in this category. On as i n g l e  p r o c e s s o rs y s t e m ,t h et h r e a dh o l d i n gt h el o c ki sn e v e rr u n n i n gi ft h e lock is being tested by another thread, because only one

being tested by another thread, because only one thread can run at a time. Therefore, on this type of system, threads always sleep rather than spin if they encounter a lock. Solaris uses the adaptivemutex method to protect only data that are accessed by short code segments. That is, a mutex is used if a lock will be held for less than a few hundred instructions. If the code segment is longer than that, the spinwaiting method is exceedingly inefcient. For these longer code segments, condition

For these longer code segments, condition variables and semaphores are used. If the desired lock is already held, the thread issues a wait and sleeps. When a thread frees the lock, it issues a signal to the nex ts l e e p i n gt h r e a di nt h eq u e u e .T h ee x t r a cost of putting a thread to sleep and waking it, and of the associated context switches, is less than the cost of wa sting several hundred instructions waiting in a spinlock. Readerwriter locks are used to protect data that are

locks are used to protect data that are accessed frequently but are usually accessed in a readonly manner. In these circumstances, readerwriter locks are more efcient than semaphores, because multiple threads can read data concurrently, whereas semaphores always serialize access to the data. Readerwriter locks are relatively expensive to implement, so again they are used only on long sections of code. Solaris uses turnstiles to order the list of threads waiting to acquire either an adaptive

of threads waiting to acquire either an adaptive mutex or a readerwriter lock. A turnstile is a queue structure containing threads blocked on a lock. For example, if one thread currently owns the lock for a synchronized object, all other threads trying to acquire the lock will block and enter the turnstile for that lock. When the lock is released, the kernel selects a thread from the turnstile as the next owner of the lock. Each synchronized object with at least on et h r e a db l o c k e do nt

with at least on et h r e a db l o c k e do nt h eo b j e c t  sl o c k requires a separate turnstile. However, rath er than associating a turnstile with each synchronized object, Solaris gives each kernel thread its own turnstile. Because a thread can be blocked only on one object at a time, this is more efcient than having a turnstile for each object. The turnstile for the rst thread to block on a synchronized object becomes the turnstile for the object itself. Threads subsequently blocking on

object itself. Threads subsequently blocking on the lock will be added to this turnstile. When the initial thread ultimately releases the lock, it gains a new turnstile from a list of free turnstiles maintained by the kernel. To prevent a priority inversion, turnstiles ar e organized according to a priority inheritance protocol .T h i sm e a n st h a ti fal o w e r  p r i o r i t yt h r e a dc u r r e n t l yh o l d s al o c ko nw h i c hah i g h e r  p r i o r i t yt h r e a di sb l o c k e d

h e r  p r i o r i t yt h r e a di sb l o c k e d ,t h et h r e a dw i t ht h el o w e r priority will temporarily inherit the priority of the higherpriority thread. Upon releasing the lock, the thread will revert to its original priority. Note that the locking mechanisms used by the kernel are implemented for userlevel threads as well, so the same types of locks are available inside and outside the kernel. A crucial implementation difference is the priority inheritance protocol. Kernellocking

the priority inheritance protocol. Kernellocking routines adhere to the kernel priority inheritance methods used by the scheduler, as described in Section 5.6.4. Userlevel threadlocking mechanisms do not provide this functionality.5.9 Synchronization Examples 237 To optimize Solaris performance, developers have rened and netuned the locking methods. Because locks are used frequently and typically are used for crucial kernel functions, tuning their implementation and use can produce great

their implementation and use can produce great performance gains. 5.9.4 Pthreads Synchronization Although the locking mechanisms used in Solaris are available to userlevel threads as well as kernel threads, basically the synchronization methods discussed thus far pertain to synchronization within the kernel. In contrast, the Pthreads APIis available for programmers at the user level and is not part of any particular kernel. This APIprovides mutex locks, condition variables, and readwrite locks

locks, condition variables, and readwrite locks for thread synchronization. Mutex locks represent the fundamental synchronization technique used with Pthreads. A mutex lock is used to protect critical sections of codethat is, a thread acquires the lock before entering a critical section and releases it upon exiting the critical section. Pthreads uses the pthread mutex tdata type for mutex locks. A mutex is created with the pthread mutex init() function. The rst parameter is a pointer to the

function. The rst parameter is a pointer to the mutex. By passing NULL as a second parameter, we initialize the mutex to its default attributes. This is illustrated below: include pthread.h  pthread mutex tm u t e x ;  create the mutex lock  pthread mutex init(mutex,NULL); The mutex is acquired and released with the pthread mutex lock() and pthread mutex unlock() functions. If the mutex lock is unavailable when pthread mutex lock() is invoked, the calling thread is blocked until the owner

the calling thread is blocked until the owner invokes pthread mutex unlock() . The following code illustrates protecting a critical section with mutex locks:  acquire the mutex lock  pthread mutex lock(mutex);  critical section   release the mutex lock  pthread mutex unlock(mutex); All mutex functions return a value of 0 with correct operation; if an error occurs, these functions return a nonzero error code. Condition variables and readwrite locks behave similarly to the way they are described

behave similarly to the way they are described in Sections 5.8 and 5.7.2, respectively. Many systems that implement Pthreads also provide semaphores, although semaphores are not part of the Pthreads standard and instead belong to the POSIX SEM extension. POSIX species two types of semaphores named and238 Chapter 5 Process Synchronization unnamed . The fundamental distinction between the two is that a named semaphore has an actual name in the le system and can be shared by multiple unrelated

le system and can be shared by multiple unrelated processes. Unnamed semaphores can be used only by threads belonging to the same process. In this section, we describe unnamed semaphores. The code below illustrates the sem init() function for creating and initializing an unnamed semaphore: include semaphore.h  sem ts e m ;  Create the semaphore and initialize it to 1  sem init(sem, 0, 1); Thesem init() function is passed three parameters: 1.Ap o i n t e rt ot h es e m a p h o r e 2.A ag

1.Ap o i n t e rt ot h es e m a p h o r e 2.A ag indicating the level of sharing 3.The semaphores initial value In this example, by passing the ag 0, we are indicating that this semaphore can be shared only by threads belonging to the process that created the semaphore. An o n z e r ov a l u ew o u l da l l o wo t h e rp r o c e s s e st oa c c e s st h es e m a p h o r ea sw e l l . In addition, we initialize the semaphore to the value 1. In Section 5.6, we described the classical wait()

In Section 5.6, we described the classical wait() andsignal() semaphore operations. Pthreads names these operations sem wait() and sem post() , respectively. The following code sample illustrates protecting a critical section using the semaphore created above:  acquire the semaphore  sem wait(sem);  critical section   release the semaphore  sem post(sem); Just like mutex locks, all semaphore functions return 0 when successful, and nonzero when an error condition occurs. There are other

when an error condition occurs. There are other extensions to the Pthreads API i n c l u d i n gs p i n l o c k s but it is important to note that not all extensions are considered portable from one implementation to another. We provide several programming problems and projects at the end of this chapter that use Pthreads mutex locks and condition variables as well as POSIX semaphores. 5.10 Alternative Approaches With the emergence of multicore systems has come increased pressure to develop

systems has come increased pressure to develop multithreaded applications that take advantage of multiple processing5.10 Alternative Approaches 239 cores. However, multithreaded applications p resent an increased risk of race conditions and deadlocks. Traditionally, techniques such as mutex locks, semaphores, and monitors have been used to address these issues, but as the number of processing cores increases, it becomes increasingly difcult to design multithreaded applications that are free from

multithreaded applications that are free from race conditions and deadlocks. In this section, we explore various features provided in both program ming languages and hardware that support designing threadsafe concurrent applications. 5.10.1 Transactional Memory Quite often in computer science, ideas from one area of study can be used to solve problems in other areas. The concept of transactional memory originated in database theory, for example, yet it provides a strategy for process

example, yet it provides a strategy for process synchronization. A memory transaction is a sequence of memory readwrite operations that are atomic. If all operations in a transaction are completed, the memory transaction is committed. Otherwise, the operations must be aborted and rolled back. The benets of transactional memory can be obtained through features added to a programming language. Consider an example. Suppose we have a function update() that modies shared data. Traditionally, this

that modies shared data. Traditionally, this function would be written using mutex locks (or semaphores) such as the following: void update ()  acquire();  modify shared data  release();  However, using synchronization mech anisms such as mutex locks and semaphores involves many potential pr oblems, including deadlock. Addition ally, as the number of threads increases, traditional locking scales less well, because the level of contention among t hreads for lock ownership becomes very high. As an

hreads for lock ownership becomes very high. As an alternative to traditional locking methods, new features that take advantage of transactional memory can be added to a programming language. In our example, suppose we add the construct atomicS,w h i c he n s u r e s that the operations in Sexecute as a transaction. This allows us to rewrite the update() function as follows: void update ()  atomic   modify shared data    The advantage of using such a mechanism rather than locks is that the

such a mechanism rather than locks is that the transactional memory systemnot the developeris responsible for240 Chapter 5 Process Synchronization guaranteeing atomicity. Additionally, because no locks are involved, deadlock is not possible. Furthermore, a transactional memory system can identify which statements in atomic blocks can be executed concurrently, such as concurrent read access to a shared variable. It is, of course, possible for a programmer to identify these situations and use

a programmer to identify these situations and use readerwriter locks, but the task becomes increasingly difcult as the number of threads within an application grows. Transactional memory can be implemented in either software or hardware. Software transactional memory (STM), as the name suggests, implements transactional memory exclusively in softwar enospecialhardwareisneeded. STM works by inserting instrumentation code inside transaction blocks. The code is inserted by a compiler and manages

The code is inserted by a compiler and manages each transaction by examining where statements may run concurrently and where specic lowlevel locking is required. Hardware transactional memory (HTM )u s e sh a r d w a r ec a c h e hierarchies and cache coherency protocols to manage and resolve conicts involving shared data residing in separate processors caches. HTM requires no special code instrumentation and thus has less overhead than STM.H o w e v e r , HTM does require that existing cache

o w e v e r , HTM does require that existing cache hierarchies and cache coherency protocols be modied to support transactional memory. Transactional memory has existed for several years without widespread implementation. However, the growth of multicore systems and the associ ated emphasis on concurrent and parallel programming have prompted a signicant amount of research in this area on the part of both academics and commercial software and hardware vendors. 5.10.2 OpenMP In Section 4.5.2, we

vendors. 5.10.2 OpenMP In Section 4.5.2, we provided an overview of Open MPand its support of parallel programming in a sharedmemory environment. Recall that Open MPincludes as e to fc o m p i l e rd i r e c t i v e sa n da n API.A n yc o d ef o l l o w i n gt h ec o m p i l e r directive pragma omp parallel is identied as a parallel region and is performed by a number of threads equal to the number of processing cores in the system. The advantage of Open MP(and similar tools) is that thread

of Open MP(and similar tools) is that thread creation and management are handled by the Open MPlibrary and are not the responsibility of application developers. Along with its pragma omp parallel compiler directive, Open MPpro vides the compiler directive pragma omp critical , which species the code region following the directive as a critical section in which only one thread may be active at a time. In this way, Open MPprovides support for ensuring that threads do not generate race conditions.

that threads do not generate race conditions. As an example of the use of the criticalsection compiler directive, rst assume that the shared variable counter can be modied in the update() function as follows: void update(int value)  counter  value;  If the update() function can be part ofor invoked froma parallel region, a race condition is possible on the variable counter .5.10 Alternative Approaches 241 The criticalsection compiler directive can be used to remedy this race condition and is

can be used to remedy this race condition and is coded as follows: void update(int value)  pragma omp critical  counter  value;   The criticalsection compiler directive behaves much like a binary semaphore or mutex lock, ensuring that only one thread at a time is active in the critical section. If a thread attempts to enter a critical section when another thread is currently active in that section (that is, owns the section), the calling thread is blocked until the owner thread exits. If

thread is blocked until the owner thread exits. If multiple critical sections must be used, each critical section can be assigned a separate name, and a rule can specify that no more than one thread may be active in a critical section of the same name simultaneously. An advantage of using the criticalsection compiler directive in Open MP is that it is generally considered easier to use than standard mutex locks. However, a disadvantage is that application developers must still identify possible

developers must still identify possible race conditions and adequately protect shared data using the compiler directive. Additionally, because the criticalsection compiler directive behaves much like a mutex lock, deadlock is still possible when two or more critical sections are identied. 5.10.3 Functional Programming Languages Most wellknown programming languagessuch as C, C, Java, and C are known as imperative (orprocedural )l a n g u a g e s .I m p e r a t i v el a n g u a g e sa r e used for

m p e r a t i v el a n g u a g e sa r e used for implementing algorithms that are statebased. In these languages, the ow of the algorithm is crucial to its correct operation, and state is represented with variables and other data structures. Of course, program state is mutable, as variables may be assigned different values over time. With the current emphasis on concurrent and parallel programming for multicore systems, there has been greater focus on functional programming languages, which

focus on functional programming languages, which follow a programming paradigm much different from that offered by imperative languages. The fundamental difference between imperative and functional languages is that functional languages do not maintain state. That is, once a variable has been dened and assigned a value, its value is immutableit cannot change. Because functional languages disallow mutable state, they need not be concerned with issues such as race conditions and deadlocks.

with issues such as race conditions and deadlocks. Essentially, most of the problems addressed in this chapter are nonexistent in functional languages. Several functional languages are pr esently in use, and we briey mention two of them here: Erlang and Scala. The Erlang language has gained signicant attention because of its support for concurrency and the ease with which it can be used to develop applications that run on parallel systems. Scala is a functional language that is also

Scala is a functional language that is also objectoriented. In fact, much of the syntax of Scala is similar to the popular objectoriented languages Java and C. Readers242 Chapter 5 Process Synchronization interested in Erlang and Scala, and in further details about functional languages in general, are encouraged to consult the bibliography at the end of this chapter for additional references. 5.11 Summary Given a collection of cooperating sequential processes that share data, mutual exclusion

processes that share data, mutual exclusion must be provided to ensure that a critical section of code is used by only one process or thread at a time. Typically, computer hardware provides several operations that ensure mutual exclusion. However, such hardware based solutions are too complicated for most developers to use. Mutex locks and semaphores overcome this obstacle. Both tools can be used to solve various synchronization problems and can be implemented efciently, especially if hardware

be implemented efciently, especially if hardware support for atomic operations is available. Various synchronization problems (such as the boundedbuffer problem, the readerswriters problem, and the diningphilosophers problem) are impor tant mainly because they are examples of a large class of concurrencycontrol problems. These problems are used to test nearly every newly proposed synchronization scheme. The operating system must provide the means to guard against timing errors, and several

means to guard against timing errors, and several language constructs have been proposed to deal with these problems. Monitors provide a synchronization mechanism for sharing abstract data types. A condition variable provides a method by which a monitor function can block its execution until it is signaled to continue. Operating systems also provide support for synchronization. For example, Windows, Linux, and Solaris provide mechanisms such as semaphores, mutex locks, spinlocks, and condition

semaphores, mutex locks, spinlocks, and condition variables to control access to shared data. The Pthreads APIprovides support for mutex locks and semaphores, as well as condition variables. Several alternative approaches focus on synchronization for multicore systems. One approach uses transactional memory, which may address syn chronization issues using either software or hardware techniques. Another approach uses the compiler extensions offered by Open MP.F i n a l l y ,f u n c  tional

offered by Open MP.F i n a l l y ,f u n c  tional programming languages address synchronization issues by disallowing mutability. Practice Exercises 5.1 In Section 5.4, we mentioned that disab ling interrupts frequently can affect the systems clock. Explain why this can occur and how such effects can be minimized. 5.2 Explain why Windows, Linux, and Solaris implement multiple locking mechanisms. Describe the circumstances under which they use spin locks, mutex locks, semaphores, adaptive mutex

locks, mutex locks, semaphores, adaptive mutex locks, and condition variables. In each case, explain why the mechanism is needed.Exercises 243 5.3 What is the meaning of the term busy waiting ?W h a to t h e rk i n d so f waiting are there in an operating system? Can busy waiting be avoided altogether? Explain your answer. 5.4 Explain why spinlocks are not appropriate for singleprocessor systems yet are often used in multiprocessor systems. 5.5 Show that, if the wait() and signal() semaphore

Show that, if the wait() and signal() semaphore operations are not executed atomically, then mutual exclusion may be violated. 5.6 Illustrate how a binary semaphore can be used to implement mutual exclusion among nprocesses. Exercises 5.7 Race conditions are possible in many computer systems. Consider a banking system that maintains an account balance with two functions: deposit(amount) and withdraw(amount) .T h e s et w of u n c t i o n sa r e passed the amount that is to be deposited or

r e passed the amount that is to be deposited or withdrawn from the bank account balance. Assume that a husband and wife share a bank account. Concurrently, the husband calls the withdraw() function and the wife calls deposit() .D e s c r i b eh o war a c ec o n d i t i o ni sp o s s i b l ea n dw h a t might be done to prevent the race condition from occurring. 5.8 The rst known correct software solution to the criticalsection problem for two processes was developed by Dekker. The two

for two processes was developed by Dekker. The two processes, P0and P1,s h a r et h ef o l l o w i n gv a r i a b l e s : boolean flag[2];  initially false  int turn ; The structure of process Pi(i 0 or 1) is shown in Figure 5.21. The other process is Pj(j 1 or 0). Prove that the algorithm satises all three requirements for the criticalsection problem. 5.9 The rst known correct software solution to the criticalsection problem fornprocesses with a lower bound on waiting of n1t u r n sw a s

with a lower bound on waiting of n1t u r n sw a s presented by Eisenberg and McGuire. The processes share the following variables: enum pstate idle, want in, in cs; pstate flag[n]; int turn; All the elements of flag are initially idle .T h ei n i t i a lv a l u eo f turn is immaterial (between 0 and n1). The structure of process Piis shown in Figure 5.22. Prove that the algorithm satises all three requirements for the criticalsection problem. 5.10 Explain why implementing synch ronization

5.10 Explain why implementing synch ronization primitives by disabling interrupts is not appropriate in a singleprocessor system if the syn chronization primitives are to be used in userlevel programs.244 Chapter 5 Process Synchronization do flag[i]  true; while (flag[j])  if (turn  j)  flag[i]  false; while (turn  j) ; d on o t h i n g  flag[i]  true;    critical section  turn  j; flag[i]  false;  remainder section  while (true); Figure 5.21 The structure of process Piin Dekkers algorithm. 5.11

structure of process Piin Dekkers algorithm. 5.11 Explain why interrupts are not appropriate for implementing synchro nization primitives in multiprocessor systems. 5.12 The Linux kernel has a policy that a process cannot hold a spinlock while attempting to acquire a semaphore. Explain why this policy is in place. 5.13 Describe two kernel data structures in which race conditions are possible. Be sure to include a description of how a race condition can occur. 5.14 Describe how the compare and

can occur. 5.14 Describe how the compare and swap() instruction can be used to pro vide mutual exclusion that satises the boundedwaiting requirement. 5.15 Consider how to implement a mutex lock using an atomic hardware instruction. Assume that the following structure dening the mutex lock is available: typedef struct  int available; lock ; (available  0) indicates that the lock is available, and a value of 1 indicates that the lock is unavailable. Using this struct ,i l l u s t r a t eh o w the

Using this struct ,i l l u s t r a t eh o w the following functions can be implemented using the test and set() andcompare and swap() instructions: void acquire(lock mutex) void release(lock mutex) Be sure to include any initialization that may be necessary.Exercises 245 do while (true)  flag[i]  want in; jt u r n ; while (j ! i)  if (flag[j] ! idle)  jt u r n ; else j( j1 )n ;  flag[i]  in cs; j0 ; while ( (j  n)  (j  i  flag[j] ! in cs)) j; if ( (j  n)  (turn  i  flag[turn]  idle)) break;

if ( (j  n)  (turn  i  flag[turn]  idle)) break;   critical section  j( t u r n1 )n ; while (flag[j]  idle) j( j1 )n ; turn  j; flag[i]  idle;  remainder section  while (true); Figure 5.22 The structure of process Piin Eisenberg and McGuires algorithm. 5.16 The implementation of mutex locks provided in Section 5.5 suffers from busy waiting. Describe what changes would be necessary so that a process waiting to acquire a mutex lock would be blocked and placed into a waiting queue until th el o c

and placed into a waiting queue until th el o c kb e c a m ea v a i l a b l e . 5.17 Assume that a system has multiple processing cores. For each of the following scenarios, describe which is a better locking mechanisma spinlock or a mutex lock where waiting processes sleep while waiting for the lock to become available: The lock is to be held for a short duration. The lock is to be held for a long duration. At h r e a dm a yb ep u tt os l e e pw h i l eh o l d i n gt h el o c k .246 Chapter 5

pw h i l eh o l d i n gt h el o c k .246 Chapter 5 Process Synchronization define MAX PROCESSES 255 int number of processes  0;  the implementation of fork() calls this function  int allocate process()  int new pid; if (number of processes  MAX PROCESSES) return 1; else   allocate necessary process resources  number of processes; return new pid;    the implementation of exit() calls this function  void release process()   release process resources  number of processes;  Figure 5.23 Allocating

number of processes;  Figure 5.23 Allocating and releasing processes. 5.18 Assume that a context switch takes Ttime. Suggest an upper bound (in terms of T)f o rh o l d i n gas p i n l o c k .I ft h es p i n l o c ki sh e l df o ra n y longer, a mutex lock (where waiting threads are put to sleep) is a better alternative. 5.19 Am u l t i t h r e a d e dw e bs e r v e rw i s h e st ok e e pt r a c ko ft h en u m b e r of requests it services (known as hits). Consider the two following strategies

as hits). Consider the two following strategies to prevent a race condition on the variable hits .T h e r s t strategy is to use a basic mutex lock when updating hits : int hits; mutex lock hit lock; hit lock.acquire(); hits; hit lock.release(); As e c o n ds t r a t e g yi st ou s ea na t o m i ci n t e g e r : atomic th i t s ; atomic inc(hits); Explain which of these two strategies is more efcient. 5.20 Consider the code example for alloca ting and releasing processes shown in Figure

ting and releasing processes shown in Figure 5.23.Exercises 247 a. Identify the race condition(s). b. Assume you have a mutex lock named mutexwith the operations acquire() and release() .I n d i c a t ew h e r et h el o c k i n gn e e d st o be placed to prevent the race condition(s). c. Could we replace the integer variable int number of processes  0 with the atomic integer atomic tn u m b e r of processes  0 to prevent the race condition(s)? 5.21 Servers can be designed to limit the number of

Servers can be designed to limit the number of open connections. For example, a server may wish to have only Nsocket connections at any point in time. As soon as Nconnections are made, the server will not accept another incoming connection until an existing connection is released. Explain how semaphores can be used by a server to limit the number of concurrent connections. 5.22 Windows Vista provides a lightweight synchronization tool called slim readerwriter locks. Whereas most implementations

readerwriter locks. Whereas most implementations of readerwriter locks favor either readers or writers, or perhaps order waiting threads using a FIFO policy, slim readerwriter locks favor neither readers nor writers, nor are waiting threads ordered in a FIFO queue. Explain the benets of providing such a synchronization tool. 5.23 Show how to implement the wait() and signal() semaphore oper ations in multiprocessor environments using the test and set() instruction. The solution should exhibit

and set() instruction. The solution should exhibit minimal busy waiting. 5.24 Exercise 4.26 requires the parent thread to wait for the child thread to nish its execution before printing out the computed values. If we let the parent thread access the Fibonacci numbers as soon as they have been computed by the child threadrather than waiting for the child thread to terminatewhat changes would be necessary to the solution for this exercise? Implement your modied solution. 5.25 Demonstrate that

your modied solution. 5.25 Demonstrate that monitors and semaphores are equivalent insofar as they can be used to implement solutions to the same types of synchronization problems. 5.26 Design an algorithm for a boundedbuffer monitor in which the buffers (portions) are embedded within the monitor itself. 5.27 The strict mutual exclusion within a monitor makes the boundedbuffer monitor of Exercise 5.26 mainly suitable for small portions. a. Explain why this is true. b. Design a new scheme that is

why this is true. b. Design a new scheme that is suitable for larger portions. 5.28 Discuss the tradeoff between fairness and throughput of operations in the readerswriters problem. Propose a method for solving the readerswriters problem without causing starvation.248 Chapter 5 Process Synchronization 5.29 How does the signal() operation associated with monitors differ from the corresponding operation dened for semaphores? 5.30 Suppose the signal() statement can appear only as the last statement

statement can appear only as the last statement in a monitor function. Suggest how the implementation described in Section 5.8 can be simplied in this situation. 5.31 Consider a system consisting of processes P1,P2,. . . ,Pn,e a c ho fw h i c hh a s au n i q u ep r i o r i t yn u m b e r .W r i t eam o n i t o rt h a ta l l o c a t e st h r e ei d e n t i c a l printers to these processes, using the priority numbers for deciding the order of allocation. 5.32 A le is to be shared among different

5.32 A le is to be shared among different processes, each of which has a unique number. The le can be accessed simultaneously by several processes, subject to the following constraint: the sum of all unique numbers associated with all the processes currently accessing the le must be less than n. Write a monitor to coordinate access to the le. 5.33 When a signal is performed on a condition inside a monitor, the signaling process can either continue its execution or transfer control to the process

its execution or transfer control to the process that is signaled. How would the solution to the preceding exercise differ with these two different ways in which signaling can be performed? 5.34 Suppose we replace the wait() and signal() operations of moni tors with a single construct await(B) ,w h e r e Bis a general Boolean expression that causes the process executing it to wait until Bbecomes true . a. Write a monitor using this scheme to implement the readers writers problem. b. Explain why,

the readers writers problem. b. Explain why, in general, t his construct cannot be implemented efciently. c. What restrictions need to be put on the await statement so that it can be implemented efciently? (Hint: Restrict the generality of B; see [Kessels (1977)].) 5.35 Design an algorithm for a monitor that implements an alarm clock that enables a calling program to delay itself for a specied number of time units ( ticks). You may assume the existence of a real hardware clock that invokes a

existence of a real hardware clock that invokes a function tick() in your monitor at regular intervals. Programming Problems 5.36 Programming Exercise 3.20 required you to design a PIDmanager that allocated a unique process identi er to each process. Exercise 4.20 required you to modify your solution to Exercise 3.20 by writing a program that created a number of threads that requested and released process identiers. Now modify your solution to Exercise 4.20 by ensuring that the data structure

Exercise 4.20 by ensuring that the data structure used to represent the availability of process identiers is safe from race conditions. Use Pthreads mutex locks, described in Section 5.9.4.Programming Problems 249 5.37 Assume that a nite number of resources of a single resource type must be managed. Processes may ask for a number of these resources and will return them once nished. As an example, many commercial software packages provide a given number of licenses ,i n d i c a t i n gt h en u m

number of licenses ,i n d i c a t i n gt h en u m b e ro f applications that may run concurrently. When the application is started, the license count is decremented. When the application is terminated, the license count is incremented. If all licenses are in use, requests to start the application are denied. Such requests will only be granted when an existing license holder terminates the application and a license is returned. The following program segment is used to manage a nite number of

program segment is used to manage a nite number of instances of an available resource. The maximum number of resources and the number of available resources are declared as follows: define MAX RESOURCES 5 int available resources  MAX RESOURCES; When a process wishes to obtain a number of resources, it invokes the decrease count() function:  decrease available resources by count resources   return 0 if sufficient resources available,   otherwise return 1  int decrease count(int count)  if

return 1  int decrease count(int count)  if (available resources count) return 1; else  available resources  count; return 0;   When a process wants to return a number of resources, it calls the increase count() function:  increase available resources by count  int increase count(int count)  available resources  count; return 0;  The preceding program segment produces a race condition. Do the following: a. Identify the data involved in the race condition. b. Identify the location (or locations)

condition. b. Identify the location (or locations) in the code where the race condition occurs.250 Chapter 5 Process Synchronization c. Using a semaphore or mutex lock, x the race condition. It is permissible to modify the decrease count() function so that the calling process is blocked until sufcient resources are available. 5.38 The decrease count() function in the previous exercise currently returns 0 if sufcient resources are available and 1 otherwise. This leads to awkward programming for a

otherwise. This leads to awkward programming for a process that wishes to obtain a number of resources: while (decrease count(count)  1) ; Rewrite the resourcemanager code segment using a monitor and condition variables so that the decrease count() function suspends the process until sufcient resources are available. This will allow a process to invoke decrease count() by simply calling decrease count(count); The process will return from this function call only when sufcient resources are

function call only when sufcient resources are available. 5.39 Exercise 4.22 asked you to design a multithreaded program that esti mated H9266using the Monte Carlo technique. In that exercise, you were asked to create a single thread that generated random points, storing the result in a global variable. Once that thread exited, the parent thread performed the calcuation that estimated the value of H9266.M o d i f yt h a t program so that you create several threads, each of which generates random

several threads, each of which generates random points and determines if the points fall within the circle. Each thread will have to update the global count of all points that fall within the circle. Protect against race conditions on updates to the shared global variable by using mutex locks. 5.40 Exercise 4.23 asked you to design a program using Open MP that estimated H9266using the Monte Carlo technique. Examine your solution to that program looking for any possible race conditions. If you

looking for any possible race conditions. If you identify a race condition, protect against it using the strategy outlined in Section 5.10.2. 5.41 Abarrier is a tool for synchronizing the activity of a number of threads. When a thread reaches a barrier point ,i tc a n n o tp r o c e e du n t i la l lo t h e r threads have reached this point as w ell. When the last thread reaches the barrier point, all threads are released and can resume concurrent execution. Assume that the barrier is

concurrent execution. Assume that the barrier is initialized to Nthe number of threads that must wait at the barrier point: init(N); Each thread then performs some work until it reaches the barrier point:Programming Projects 251  do some work for awhile  barrier point();  do some work for awhile  Using synchronization tools described in this chapter, construct a barrier that implements the following API: int init(int n) Initializes the barrier to the specied size. int barrier point(void)

to the specied size. int barrier point(void) Identies the barrier point. All threads are released from the barrier when the last thread reaches this point. The return value of each function is used to identify error conditions. Each function will return 0 under normal operation and will return 1i fa ne r r o ro c c u r s .At e s t i n gh a r n e s si sp r o v i d e di nt h es o u r c ec o d e download to test your implementation of the barrier. Programming Projects Project 1The Sleeping Teaching

Projects Project 1The Sleeping Teaching Assistant A university computer science department has a teaching assistant ( TA)w h o helps undergraduate students with their programming assignments during regular ofce hours. The TAs ofce is rather small and has room for only one desk with a chair and computer. There are three chairs in the hallway outside the ofce where students can sit and wait if the TAis currently helping another student. When there are no students wh on e e dh e l pd u r i n go f

are no students wh on e e dh e l pd u r i n go f  c eh o u r s ,t h e TAsits at the desk and takes a nap. If a student arrives during ofce hours and nds the TAsleeping, the student must awaken the TAto ask for help. If a student arrives and nds the TAcurrently helping another student, the student sits on one of the chairs in the hallway and waits. If no chairs are available, the student will come back at a later time. Using POSIX threads, mutex locks, and semaphores, implement a solution that

locks, and semaphores, implement a solution that coordinates the activities of the TAand the students. Details for this assignment are provided below. The Students and the TA Using Pthreads (Section 4.4.1), begin by creating nstudents. Each will run as a separate thread. The TAwill run as a separate thread as well. Student threads will alternate between programming for a period of time and seeking help from the TA.I ft h e TAis available, they will obtain help. Otherwise, they will either sit in

obtain help. Otherwise, they will either sit in a chair in the hallway or, if no chairs are available, will resume programming and will seek help at a later time. If a student arrives and notices that the TAis sleeping, the student must notify the TAusing a semaphore. When the TAnishes helping a student, the TAmust check to see if there are students waiting for help in the hallway. If so, the TAmust help each of these students in turn. If no students are present, the TAmay return to napping.252

are present, the TAmay return to napping.252 Chapter 5 Process Synchronization Perhaps the best option for simulating students programmingas well as the TAproviding help to a studentis to have the appropriate threads sleep for a random period of time. POSIX Synchronization Coverage of POSIX mutex locks and semaphores is provided in Section 5.9.4. Consult that section for details. Project 2The Dining Philosophers Problem In Section 5.7.3, we provide an outline of a solution to the

5.7.3, we provide an outline of a solution to the diningphilosophers problem using monitors. This problem will require implementing a solution using Pthreads mutex locks and condition variables. The Philosophers Begin by creating ve philosophers, each identied by a number 0 . . 4. Each philosopher will run as a separate thread. Thread creation using Pthreads is covered in Section 4.4.1. Philosophers alternate between thinking and eating. To simulate both activities, have the thread sleep for a

both activities, have the thread sleep for a random period between one and three seconds. When a ph ilosopher wishes to eat, she invokes the function pickup forks(int philosopher number) where philosopher number identies the number of the philosopher wishing to eat. When a philosopher n ishes eating, she invokes return forks(int philosopher number) Pthreads Condition Variables Condition variables in Pthreads behave similarly to those described in Section 5.8. However, in that section, condition

Section 5.8. However, in that section, condition variables are used within the context of a monitor, which provides a locking mechanism to ensure data integrity. Since Pthreads is typically used in C programsand since C does not have am o n i t o r w ea c c o m p l i s hl o c k i n gb ya s s o c i a t i n gac o n d i t i o nv a r i a b l ew i t h am u t e xl o c k .P t h r e a d sm u t e xl o c k sa r ec o v e r e di nS e c t i o n5 . 9 . 4 .W ec o v e r Pthreads condition variables here.

4 .W ec o v e r Pthreads condition variables here. Condition variables in Pthreads use the pthread cond tdata type and are initialized using the pthread cond init() function. The following code creates and initializes a condition variable as well as its associated mutex lock: pthread mutex tm u t e x ; pthread cond tc o n d var; pthread mutex init(mutex,NULL); pthread cond init(cond var,NULL);Programming Projects 253 The pthread cond wait() function is used for waiting on a condition variable.

is used for waiting on a condition variable. The following code illustrates how a thread can wait for the condition a b to become true using a Pthread condition variable: pthread mutex lock(mutex); while (a ! b) pthread cond wait(mutex, cond var); pthread mutex unlock(mutex); The mutex lock associated with the condition variable must be locked before the pthread cond wait() function is called, since it is used to protect the data in the conditional clause from a possible race condition. Once

clause from a possible race condition. Once this lock is acquired, the thread can check the condition. If the condition is not true, the thread then invokes pthread cond wait() ,p a s s i n gt h em u t e xl o c ka n d the condition variable as parameters. Calling pthread cond wait() releases the mutex lock, thereby allowing another thread to access the shared data and possibly update its value so that the condition clause evaluates to true. (To protect against program errors, it is important to

protect against program errors, it is important to place the conditional clause within a loop so that the condition is rechecked after being signaled.) A thread that modies the shared data can invoke the pthread cond signal() function, thereby signaling one thread waiting on the condition variable. This is illustrated below: pthread mutex lock(mutex); ab ; pthread cond signal(cond var); pthread mutex unlock(mutex); It is important to note that the call to pthread cond signal() does not release

the call to pthread cond signal() does not release the mutex lock. It is the subsequent call to pthread mutex unlock() that releases the mutex. Once the mutex lock is released, the signaled thread becomes the owner of the mutex lock and returns control from the call to pthread cond wait() . Project 3ProducerConsumer Problem In Section 5.7.1, we presented a semaph orebased solution to the producer consumer problem using a bounded buffer. In this project, you will design a programming solution to

project, you will design a programming solution to the boundedbuffer problem using the producer and consumer processes shown in Figures 5.9 and 5.10. The solution presented in Section 5.7.1 uses three semaphores: empty andfull ,w h i c hc o u n tt h en u m b e r of empty and full slots in the buffer, and mutex ,w h i c hi sab i n a r y( o rm u t u a l  exclusion) semaphore that protects the actual insertion or removal of items in the buffer. For this project, you will use standard counting

For this project, you will use standard counting semaphores for empty andfull and a mutex lock, rather than a binary semaphore, to represent mutex .T h ep r o d u c e ra n dc o n s u m e rr u n n i n ga ss e p a r a t et h r e a d sw i l lm o v e items to and from a buffer that is synchronized with the empty ,full ,a n d mutex structures. You can solve this problem using either Pthreads or the Windows API.254 Chapter 5 Process Synchronization include buffer.h  the buffer  buffer item

include buffer.h  the buffer  buffer item buffer[BUFFER SIZE]; int insert item(buffer item item)   insert item into buffer return 0 if successful, otherwise return 1 indicating an error condition   int remove item(buffer item item)   remove an object from buffer placing it in item return 0 if successful, otherwise return 1 indicating an error condition   Figure 5.24 Outline of buffer operations. The Buffer Internally, the buffer will consist of a xedsize array of type buffer item (which will be

a xedsize array of type buffer item (which will be dened using a typedef ). The array of buffer item objects will be manipulated as a circular queue. The denition of buffer item ,a l o n g with the size of the buffer, can be stored in a header le such as the following:  buffer.h  typedef int buffer item; define BUFFER SIZE 5 The buffer will be manipulated with two functions, insert item() and remove item() ,w h i c ha r ec a l l e db yt h ep r o d u c e ra n dc o n s u m e rt h r e a d s ,

r o d u c e ra n dc o n s u m e rt h r e a d s , respectively. A skeleton outlining these functions appears in Figure 5.24. The insert item() and remove item() functions will synchronize the producer and consumer using the algorithms outlined in Figures 5.9 and 5.10. The buffer will also require an initialization function that initializes the mutualexclusion object mutex along with the empty andfull semaphores. The main() function will initialize the buffer and create the separate producer and

the buffer and create the separate producer and consumer threads. Once it has created the producer and consumer threads, the main() function will sleep for a period of time and, upon awakening, will terminate the application. The main() function will be passed three parameters on the command line: 1.How long to sleep before terminating 2.The number of producer threads 3.The number of consumer threadsProgramming Projects 255 include buffer.h int main(int argc, char argv[])   1. Get command line

main(int argc, char argv[])   1. Get command line arguments argv[1],argv[2],argv[3]   2. Initialize buffer   3. Create producer thread(s)   4. Create consumer thread(s)   5. Sleep   6. Exit   Figure 5.25 Outline of skeleton program. As k e l e t o nf o rt h i sf u n c t i o na p p e a r si nF i g u r e5 . 2 5 . The Producer and Consumer Threads The producer thread will alternate between sleeping for a random period of time and inserting a random integer into the buffer. Random numbers will be

integer into the buffer. Random numbers will be produced using the rand() function, which produces random integers between 0 and RAND MAX .T h ec o n s u m e rw i l la l s os l e e pf o rar a n d o mp e r i o d of time and, upon awakening, will attempt to remove an item from the buffer. An outline of the producer and consumer threads appears in Figure 5.26. As noted earlier, you can solve this problem using either Pthreads or the Windows API.I nt h ef o l l o w i n gs e c t i o n s ,w es u p p l

h ef o l l o w i n gs e c t i o n s ,w es u p p l ym o r ei n f o r m a t i o no ne a c h of these choices. Pthreads Thread Creation and Synchronization Creating threads using the Pthreads APIis discussed in Section 4.4.1. Coverage of mutex locks and semaphores using Pthreads is provided in Section 5.9.4. Refer to those sections for specic instructions on Pthreads thread creation and synchronization. Windows Section 4.4.2 discusses thread creation using the Windows API.R e f e rt ot h a t

creation using the Windows API.R e f e rt ot h a t section for specic instructions on creating threads. Windows Mutex Locks Mutex locks are a type of dispatcher object, as described in Section 5.9.1. The following illustrates how to create a mutex lock using the CreateMutex() function: include windows.h  HANDLE Mutex; Mutex  CreateMutex(NULL, FALSE, NULL);256 Chapter 5 Process Synchronization include stdlib.h  required for rand()  include buffer.h void producer(void param)  buffer item item;

void producer(void param)  buffer item item; while (true)   sleep for a random period of time  sleep(...);  generate a random number  item  rand(); if (insert item(item)) fprintf(report error condition); else printf(producer produced d n,item);  void consumer(void param)  buffer item item; while (true)   sleep for a random period of time  sleep(...); if (remove item(item)) fprintf(report error condition); else printf(consumer consumed d n,item);  Figure 5.26 An outline of the producer and

Figure 5.26 An outline of the producer and consumer threads. The rst parameter refers to a security attribute for the mutex lock. By setting this attribute to NULL ,w ed i s a l l o wa n yc h i l d r e no ft h ep r o c e s sc r e a t i n gt h i s mutex lock to inherit the handle of the l ock. The second parameter indicates whether the creator of the mutex lock is the locks initial owner. Passing a value ofFALSE indicates that the thread creating the mutex is not the initial owner. (We shall

the mutex is not the initial owner. (We shall soon see how mutex locks are acquired.) The third parameter allows us to name the mutex. However, because we provide a value of NULL ,w ed o not name the mutex. If successful, CreateMutex() returns a HANDLE to the mutex lock; otherwise, it returns NULL . In Section 5.9.1, we identied dispat cher objects as being either signaled or nonsignaled .As i g n a l e dd i s p a t c h e ro b j e c t( s u c ha sam u t e xl o c k )i sa v a i l a b l e for

c ha sam u t e xl o c k )i sa v a i l a b l e for ownership. Once it is acquired , it moves to the nonsignaled state. When it is released, it returns to signaled. Mutex locks are acquired by invoking the WaitForSingleObject() func tion. The function is passed the HANDLE to the lock along with a ag indicating how long to wait. The following code demonstrates how the mutex lock created above can be acquired: WaitForSingleObject(Mutex, INFINITE);Programming Projects 257 The parameter value INFINITE

Projects 257 The parameter value INFINITE indicates that we will wait an innite amount of time for the lock to become available. Other values could be used that would allow the calling thread to time out if the lock did not become available within as p e c i  e dt i m e .I ft h el o c ki si nas i g n a l e ds t a t e , WaitForSingleObject() returns immediately, and the lock becomes nonsignaled. A lock is released (moves to the signaled state) by invoking ReleaseMutex() for example, as follows:

invoking ReleaseMutex() for example, as follows: ReleaseMutex(Mutex); Windows Semaphores Semaphores in the Windows APIare dispatcher objects and thus use the same signaling mechanism as mutex locks. Semaphores are created as follows: include windows.h  HANDLE Sem; Sem  CreateSemaphore(NULL, 1, 5, NULL); The rst and last parameters identify a security attribute and a name for the semaphore, similar to what we described for mutex locks. The second and third parameters indicate the initial value

and third parameters indicate the initial value and maximum value of the semaphore. In this instance, the initial value of the semaphore is 1, and its maximum value is 5. If successful, CreateSemaphore() returns a HANDLE to the mutex lock; otherwise, it returns NULL . Semaphores are acquired with the same WaitForSingleObject() func tion as mutex locks. We acquire the semaphore Semcreated in this example by using the following statement: WaitForSingleObject(Semaphore, INFINITE); If the value of

INFINITE); If the value of the semaphore is 0, the semaphore is in the signaled state and thus is acquired by the calling thread. Otherwise, the calling thread blocks indenitelyas we are specifying INFINITE until the semaphore returns to the signaled state. The equivalent of the signal() operation for Windows semaphores is the ReleaseSemaphore() function. This function is passed three parameters: 1.The HANDLE of the semaphore 2.How much to increase the value of the semaphore 3.Ap o i n t e rt ot

the value of the semaphore 3.Ap o i n t e rt ot h ep r e v i o u sv a l u eo ft h es e m a p h o r e We can use the following statement to increase Semby 1: ReleaseSemaphore(Sem, 1, NULL ); Both ReleaseSemaphore() and ReleaseMutex() return a nonzero value if successful and 0 otherwise.258 Chapter 5 Process Synchronization Bibliographical Notes The mutualexclusion problem was rst discussed in a classic paper by [Dijkstra (1965)]. Dekkers algorithm (Exercise 5.8)the rst correct software solution

(Exercise 5.8)the rst correct software solution to the twoprocess mutualexclusion problemwas developed by the Dutch mathematician T. Dekker. This algorithm also was discussed by [Dijkstra (1965)]. A simpler solution to the twoprocess mutualexclusion problem has since been presented by [Peterson (1981)] (Figure 5.2). The semaphore concept was suggested by [Dijkstra (1965)]. The classic processcoordination problems that we have described are paradigms for a large class of concurrencycontrol

paradigms for a large class of concurrencycontrol problems. The bounded buffer problem and the diningphilosophers problem were suggested in [Dijkstra (1965)] and [Dijkstra (1971)]. The readerswriters problem was suggested by [Courtois et al. (1971)]. The criticalregion concept was suggested by [Hoare (1972)] and by [BrinchHansen (1972)]. The monitor concept was developed by [BrinchHansen (1973)]. [Hoare (1974)] gave a complete description of the monitor. Some details of the locking mechanisms

monitor. Some details of the locking mechanisms used in Solaris were presented in [Mauro and McDougall (2007)]. As noted earlier, the locking mechanisms used by the kernel are implemented for userlevel threads as well, so the same types of locks are available inside and outside the kernel. Details of Windows 2000 synchronization can be found in [Solomon and Russinovich (2000)]. [Love (2010)] describes synchronization in the Linux kernel. Information on Pthreads programming can be found in [Lewis

on Pthreads programming can be found in [Lewis and Berg (1998)] and [Butenhof (1997)]. [Hart (2005)] describes thread synchronization using Windows. [Goetz et al. (2006)] present a detailed discussion of concur rent programming in Java as well as the java.util.concurrent package. [Breshears (2009)] and [Pacheco (2011)] provide detailed coverage of synchro nization issues in relation to parallel programming. [Lu et al. (2008)] provide a study of concurrency bugs in realworld applications.

of concurrency bugs in realworld applications. [AdlTabatabai et al. (2007)] discuss transactional memory. Details on using Open MPcan be found at http:openmp.org .F u n c t i o n a lp r o g r a m m i n gu s i n g Erlang and Scala is covered in [Armstrong (2007)] and [Odersky et al. ()] respectively. Bibliography [AdlTabatabai et al. (2007)] A.R. AdlTabatabai, C. Kozyrakis, and B. Saha, Unlocking Concurrency ,Queue ,V o l u m e4 ,N u m b e r1 0( 2 0 0 7 ) ,p a g e s2 4  3 3 . [Armstrong (2007)]

2 0 0 7 ) ,p a g e s2 4  3 3 . [Armstrong (2007)] J. Armstrong, Programming Erlang Software for a Concurrent World , The Pragmatic Bookshelf (2007). [Breshears (2009)] C. Breshears, The Art of Concurrency ,O  R e i l l yA s s o c i a t e s (2009). [BrinchHansen (1972)] P. B r i n c h  H a n s e n , Structured Multiprogramming , Communications of the ACM ,V o l u m e1 5 ,N u m b e r7( 1 9 7 2 ) ,p a g e s5 7 4  5 7 8 .Bibliography 259 [BrinchHansen (1973)] P. B r i n c h  H a n s e n , Operating

(1973)] P. B r i n c h  H a n s e n , Operating System Principles ,P r e n t i c e Hall (1973). [Butenhof (1997)] D. Butenhof, Programming with POSIX Threads ,A d d i s o n  Wesley (1997). [Courtois et al. (1971)] P. J . C o u r t o i s , F. H e y m a n s , a n d D . L . P a r n a s , Concurrent Control with Readers and Writers ,Communications of the ACM ,V o l u m e1 4 , Number 10 (1971), pages 667668. [Dijkstra (1965)] E. W. Dijkstra, Cooperating Sequential Processes ,T e c h n i c a l report,

Sequential Processes ,T e c h n i c a l report, Technological University, Eindhoven, the Netherlands (1965). [Dijkstra (1971)] E. W. Dijkstra, Hierarchical Ordering of Sequential Processes , Acta Informatica ,V o l u m e1 ,N u m b e r2( 1 9 7 1 ) ,p a g e s1 1 5  1 3 8 . [Goetz et al. (2006)] B. Goetz, T. Peirls, J. Bloch, J. Bowbeer, D. Holmes, and D. Lea, Java Concurrency in Practice ,A d d i s o n  W e s l e y( 2 0 0 6 ) . [Hart (2005)] J. M. Hart, Windows System Programming, Third Edition,

Hart, Windows System Programming, Third Edition, Addison Wesley (2005). [Hoare (1972)] C. A. R. Hoare, Towards a Theory of Parallel Programming ,i n [Hoare and Perrott 1972] (1972), pages 6171. [Hoare (1974)] C. A. R. Hoare, Monitors: An Operating System Structuring Concept ,Communications of the ACM ,V o l u m e1 7 ,N u m b e r1 0( 1 9 7 4 ) ,p a g e s 549557. [Kessels (1977)] J. L. W. Kessels, An Alternative to Event Queues for Synchro nization in Monitors ,Communications of the ACM ,V o l u m

in Monitors ,Communications of the ACM ,V o l u m e2 0 ,N u m b e r7( 1 9 7 7 ) , pages 500503. [Lewis and Berg (1998)] B. Lewis and D. Berg, Multithreaded Programming with Pthreads ,S u nM i c r o s y s t e m sP r e s s( 1 9 9 8 ) . [Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developers Library (2010). [Lu et al. (2008)] S. Lu, S. Park, E. Seo, and Y. Zhou, Learning from mistakes: a comprehensive study on real world concurrency bug characteristics ,SIGPLAN Notices ,V o l u m

bug characteristics ,SIGPLAN Notices ,V o l u m e4 3 ,N u m b e r3( 2 0 0 8 ) ,p a g e s3 2 9  3 3 9 . [Mauro and McDougall (2007)] J. Mauro and R. McDougall, Solaris Internals: Core Kernel Architecture ,P r e n t i c eH a l l( 2 0 0 7 ) . [Odersky et al. ()] M. Odersky, V . Cremet, I. Dragos, G. Dubochet, B. Emir, S. Mcdirmid, S. Micheloud, N. Mihaylov, M. Schinz, E. Stenman, L. Spoon, and M. Zenger. [Pacheco (2011)] P. S . P a c h e c o , An Introduction to Parallel Programming , Morgan

, An Introduction to Parallel Programming , Morgan Kaufmann (2011). [Peterson (1981)] G. L. Peterson, Myths About the Mutual Exclusion Problem , Information Processing Letters ,V o l u m e1 2 ,N u m b e r3( 1 9 8 1 ) . [Solomon and Russinovich (2000)] D. A. Solomon and M. E. Russinovich, Inside Microsoft Windows 2000, Third Edition, Microsoft Press (2000).6CHAPTER CPU Scheduling CPU scheduling is the basis of multiprogrammed operating systems. By switching the CPU among processes, the operating

switching the CPU among processes, the operating system can make the computer more productive. In this chapter, we introduce basic CPUscheduling concepts and present several CPUscheduling algorithms. We also consider the problem of selecting an algorithm for a particular system. In Chapter 4, we introduced threads to the process model. On operating systems that support them, it is kernellevel threadsnot processesthat are in fact being scheduled by the operating system. However, the terms process

the operating system. However, the terms process scheduling and thread scheduling are often used interchangeably. In this chapter, we use process scheduling when discussing general scheduling concepts and thread scheduling to refer to threadspecic ideas. CHAPTER OBJECTIVES To introduce CPU scheduling, which is the basis for multiprogrammed operating systems. To describe various CPUscheduling algorithms. To discuss evaluation criteria for selecting a CPUscheduling algorithm for ap a r t i c u l a

a CPUscheduling algorithm for ap a r t i c u l a rs y s t e m . To examine the scheduling algorithms of several operating systems. 6.1 Basic Concepts In a singleprocessor system, only one process can run at a time. Others must wait until the CPU is free and can be rescheduled. The objective of multiprogramming is to have some process running at all times, to maximize CPU utilization. The idea is relatively simple. A process is executed until it must wait, typically for the completion of some IO

must wait, typically for the completion of some IO request. In a simple computer system, the CPU then just sits idle. All this waiting time is wasted; no useful work is accomplished. With multiprogramming, we try to use this time productively. Several processes are kept in memory at one time. When 261262 Chapter 6 CPU SchedulingCPU burstload store add store read from file store increment index write to file load store add store read from filewait for IO wait for IO wait for IOIO burst IO burst

for IO wait for IO wait for IOIO burst IO burst IO burstCPU burst CPU burst      Figure 6.1 Alternating sequence of CPU and IO bursts. one process has to wait, the operating system takes the CPU away from that process and gives the CPU to another process. This pattern continues. Every time one process has to wait, another process can take over use of the CPU. Scheduling of this kind is a fundamental operatingsystem function. Almost all computer resources are scheduled before use. The CPU is, of

resources are scheduled before use. The CPU is, of course, one of the primary computer resources. Thus, its scheduling is central to operatingsystem design. 6.1.1 CPUIO Burst Cycle The success of CPU scheduling depends on an observed property of processes: process execution consists of a cycle ofCPU execution and IOwait. Processes alternate between these two states. Pr ocess execution begins with a CPU burst . That is followed by an IOburst ,w h i c hi sf o l l o w e db ya n o t h e r CPU burst,

i c hi sf o l l o w e db ya n o t h e r CPU burst, then another IOburst, and so on. Eventually, the nal CPU burst ends with a system request to terminate execution (Figure 6.1). The durations of CPU bursts have been measured extensively. Although they vary greatly from process to process and from computer to computer, they tend to have a frequency curve similar to that shown in Figure 6.2. The curve is generally characterized as exponential or hyperexponential, with a large number of short CPU

hyperexponential, with a large number of short CPU bursts and a small number of long CPU bursts.6.1 Basic Concepts 263frequency160 140 120 100 80 60 40 20 0 8 16 24 32 40 burst duration (milliseconds) Figure 6.2 Histogram of CPUburst durations. An IObound program typically has many short CPU bursts. A CPUbound program might have a few long CPU bursts. This distribution can be important in the selection of an appropriate CPUscheduling algorithm. 6.1.2 CPU Scheduler Whenever the CPU becomes idle,

6.1.2 CPU Scheduler Whenever the CPU becomes idle, the operating system must select one of the processes in the ready queue to be executed. The selection process is carried out by the shortterm scheduler ,o r CPU scheduler. The scheduler selects a process from the processes in memory that are ready to execute and allocates the CPU to that process. Note that the ready queue is not necessarily a rstin, rstout ( FIFO )q u e u e . As we shall see when we consider the various scheduling algorithms, a

we consider the various scheduling algorithms, a ready queue can be implemented as a FIFO queue, a priority queue, a tree, or simply an unordered linked list. Conceptually, how ever, all the processes in the ready queue are lined up waiting for a chance to run on the CPU.T h er e c o r d si nt h e queues are generally process control blocks ( PCBs) of the processes. 6.1.3 Preemptive Scheduling CPUscheduling decisions may take place under the following four circum stances: 1.When a process

following four circum stances: 1.When a process switches from the running state to the waiting state (for example, as the result of an IOrequest or an invocation of wait() for the termination of a child process)264 Chapter 6 CPU Scheduling 2.When a process switches from the running state to the ready state (for example, when an interrupt occurs) 3.When a process switches from the waiting state to the ready state (for example, at completion of IO) 4.When a process terminates For situations 1 and

4.When a process terminates For situations 1 and 4, there is no choice in terms of scheduling. A new process (if one exists in the ready queue) must be selected for execution. There is a choice, however, for situations 2 and 3. When scheduling takes place only under circumstances 1 and 4, we say that the scheduling scheme is nonpreemptive orcooperative .O t h e r w i s e , it is preemptive . Under nonpreemptive scheduling, once the CPU has been allocated to a process, the process keeps the CPU

allocated to a process, the process keeps the CPU until it releases the CPU either by terminating or by switching to the waiting state. This scheduling method was used by Microsoft Windows 3.x. Windows 95 introduced preemptive scheduling, and all subsequent versions of Windows operating systems have used preemptive scheduling. The Mac OS X operating system for the Macintosh also uses preemptive scheduling; previous versions of the Macintosh operating system relied on cooperative scheduling.

operating system relied on cooperative scheduling. Cooperative scheduling is the only method that can be used on certain hardware platforms, because it does not require the special hardware (for example, a timer) needed for preemptive scheduling. Unfortunately, preemptive scheduling can result in race conditions when data are shared among several processes. Consider the case of two processes that share data. While one process is updating the data, it is preempted so that the second process can

it is preempted so that the second process can run. The second process then tries to read the data, which are in an inconsistent state. This issue was explored in detail in Chapter 5. Preemption also affects the design of the operatingsystem kernel. During the processing of a system call, the kernel may be busy with an activity on behalf of a process. Such activities may involve changing important kernel data (for instance, IOqueues). What happens if the process is preempted in the middle of

if the process is preempted in the middle of these changes and the kernel (or the device driver) needs to read or modify the same structure? Chaos ensues. Certain operating systems, including most versions of UNIX ,d e a lw i t ht h i sp r o b l e mb yw a i t i n ge i t h e rf o ras y s t e mc a l l to complete or for an IOblock to take place before doing a context switch. This scheme ensures that the kernel structure is simple, since the kernel will not preempt a process while the kernel data

will not preempt a process while the kernel data structures are in an inconsistent state. Unfortunately, this kernelexecution model is a poor one for supporting realtime computing where tasks must complete execution within a given time frame. In Section 6.6, we explore scheduling demands of realtime systems. Because interrupts can, by de nition, occur at any time, and because they cannot always be ignored by the k ernel, the sections of code affected by interrupts must be guarded from

code affected by interrupts must be guarded from simultaneous use. The operating system needs to accept interrupts at almost all times. Otherwise, input might be lost or output overwritten. So that these sections of code are not accessed concurrently by several processes, they disable interru pts at entry and reenable interrupts at exit. It is important to note that sections of code that disable interrupts do not occur very often and typically contain few instructions.6.2 Scheduling Criteria 265

few instructions.6.2 Scheduling Criteria 265 6.1.4 Dispatcher Another component involved in the CPUscheduling function is the dispatcher . The dispatcher is the module that gives control of the CPU to the process selected by the shortterm scheduler. This f unction involves the following: Switching context Switching to user mode Jumping to the proper location in the user program to restart that program The dispatcher should be as fast as possible, since it is invoked during every process switch.

since it is invoked during every process switch. The time it takes for the dispatcher to stop one process and start another running is known as the dispatch latency . 6.2 Scheduling Criteria Different CPUscheduling algorithms have different properties, and the choice of a particular algorithm may favor one class of processes over another. In choosing which algorithm to use in a particular situation, we must consider the properties of the various algorithms. Many criteria have been suggested for

algorithms. Many criteria have been suggested for comparing CPUscheduling algo rithms. Which characteristics are used for comparison can make a substantial difference in which algorithm is judged to be best. The criteria include the following: CPU utilization .W ew a n tt ok e e pt h e CPU as busy as possible. Concep tually, CPU utilization can range from 0 to 100 percent. In a real system, it should range from 40 percent (for a lightly loaded system) to 90 percent (for a heavily loaded system).

to 90 percent (for a heavily loaded system). Throughput .I ft h e CPU is busy executing processes, then work is being done. One measure of work is the number of processes that are completed per time unit, called throughput .F o rl o n gp r o c e s s e s ,t h i sr a t em a yb eo n e process per hour; for short transactions, it may be ten processes per second. Tu r n a r o u n d t i m e . From the point of view of a particular process, the important criterion is how long it takes to exec ute that

criterion is how long it takes to exec ute that process. The interval from the time of submission of a process to the time of completion is the turnaround time. Turnaround time is the sum of the periods spent waiting to get into memory, waiting in the ready queue, executing on the CPU,a n d doing IO. Waiting time .T h e CPUscheduling algorithm does not affect the amount of time during which a process executes or does IO. It affects only the amount of time that a process spends waiting in the

of time that a process spends waiting in the ready queue. Waiting time is the sum of the periods spent waiting in the ready queue. Response time . In an interactive system, turnaround time may not be the best criterion. Often, a process can produce some output fairly early and can continue computing new results while previous results are being266 Chapter 6 CPU Scheduling output to the user. Thus, another measure is the time from the submission of a request until the rst response is produced.

of a request until the rst response is produced. This measure, called response time, is the time it takes to start responding, not the time it takes to output the response. The turnaround time is generally limited by the speed of the output device. It is desirable to maximize CPU utilization and throughput and to minimize turnaround time, waiting time, and response time. In most cases, we optimize the average measure. However, under some circumstances, we prefer to optimize the minimum or

we prefer to optimize the minimum or maximum values rather than the average. For example, to guarantee that all users get good service, we may want to minimize the maximum response time. Investigators have suggested that, for interactive systems (such as desktop systems), it is more important to minimize the variance in the response time than to minimize the average response time. A system with reasonable and predictable response time may be considered more desirable than a system that is faster

more desirable than a system that is faster on the average but is highly variable. However, little work has been done on CPUscheduling algorithms that minimize variance. As we discuss various CPUscheduling algorithms in the following section, we illustrate their operation. An accurate illustration should involve many processes, each a sequence of several hundred CPU bursts and IO bursts. For simplicity, though, we consider only one CPU burst (in milliseconds) per process in our examples. Our

(in milliseconds) per process in our examples. Our measure of comparison is the average waiting time. More elaborate evaluation mechanisms are discussed in Section 6.8. 6.3 Scheduling Algorithms CPU scheduling deals with the problem of decid ing which of the processes in the ready queue is to be allocated the CPU. There are many different CPUscheduling algorithms. In this section, w ed e s c r i b es e v e r a lo ft h e m . 6.3.1 FirstCome, FirstServed Scheduling By far the simplest

FirstServed Scheduling By far the simplest CPUscheduling algorithm is the rstcome, rstserved (FCFS )scheduling algorithm. With this scheme, the process that requests the CPU rst is allocated the CPU rst. The implementation of the FCFS policy is easily managed with a FIFO queue. When a process enters the ready queue, its PCB is linked onto the tail of the queue. When the CPU is free, it is allocated to the process at the head of the queue. The running process is then removed from the queue. The

process is then removed from the queue. The code for FCFS scheduling is simple to write and understand. On the negative side, the average waiting time under the FCFS policy is often quite long. Consider the following set of processes that arrive at time 0, with the length of the CPU burst given in milliseconds: Process Burst Time P1 24 P2 3 P3 36.3 Scheduling Algorithms 267 If the processes arrive in the order P1,P2,P3,a n da r es e r v e di n FCFS order, we get the result shown in the following

order, we get the result shown in the following Gantt chart ,w h i c hi sab a rc h a r tt h a t illustrates a particular schedule, including the start and nish times of each of the participating processes: P1 P2 P3 30 27 24 0 The waiting time is 0 milliseconds for process P1,2 4m i l l i s e c o n d sf o rp r o c e s s P2,a n d2 7m i l l i s e c o n d sf o rp r o c e s s P3.T h u s ,t h ea v e r a g ew a i t i n gt i m ei s( 0 2 42 7 )  31 7m i l l i s e c o n d s .I ft h ep r o c e s s e sa r r

l i s e c o n d s .I ft h ep r o c e s s e sa r r i v ei nt h eo r d e r P2,P3,P1, however, the results will be as shown in the following Gantt chart: P1 P2 P3 30 036 The average waiting time is now (6  0  3)3  3 milliseconds. This reduction is substantial. Thus, the average waiting time under an FCFS policy is generally not minimal and may vary substantially if the processes CPU burst times vary greatly. In addition, consider the performance of FCFS scheduling in a dynamic situation. Assume we

FCFS scheduling in a dynamic situation. Assume we have one CPUbound process and many IObound processes. As the processes ow around the system, the following scenario may result. The CPUbound process will get and hold the CPU.D u r i n gt h i s time, all the other processes will nish their IOand will move into the ready queue, waiting for the CPU.W h i l et h ep r o c e s s e sw a i ti nt h er e a d yq u e u e ,t h e IOdevices are idle. Eventually, the CPUbound process nishes its CPU burst and

the CPUbound process nishes its CPU burst and moves to an IOdevice. All the IObound processes, which have short CPU bursts, execute quickly and move back to the IOqueues. At this point, the CPU sits idle. The CPUbound process will then move back to the ready queue and be allocated the CPU.A g a i n ,a l lt h e IOprocesses end up waiting in the ready queue until the CPUbound process is done. There is a convoy effect as all the other processes wait for the one big process to get off the CPU.T h i

for the one big process to get off the CPU.T h i s effect results in lower CPU and device utilization than might be possible if the shorter processes were allowed to go rst. Note also that the FCFS scheduling algorithm is nonpreemptive. Once the CPU has been allocated to a process, that process keeps the CPU until it releases theCPU,e i t h e rb yt e r m i n a t i n go rb yr e q u e s t i n g IO.T h e FCFS algorithm is thus particularly troublesome for timesharing systems, where it is important

for timesharing systems, where it is important that each user get a share of the CPU at regular intervals. It would be disastrous to allow one process to keep the CPU for an extended period. 6.3.2 ShortestJobFirst Scheduling Ad i f f e r e n ta p p r o a c ht o CPU scheduling is the shortestjobrst (SJF)scheduling algorithm. This algorithm associates with each process the length of the processs next CPU burst. When the CPU is available, it is assigned to the268 Chapter 6 CPU Scheduling process

to the268 Chapter 6 CPU Scheduling process that has the smallest next CPU burst. If the next CPU bursts of two processes are the same, FCFS scheduling is used to break the tie. Note that a more appropriate term for this scheduling method would be the shortestnext CPUburst algorithm, because scheduling depends on the length of the next CPU burst of a process, rather than its total length. We use the term SJFbecause most people and textbooks use this term to refer to this type of scheduling. As an

term to refer to this type of scheduling. As an example of SJFscheduling, consider the following set of processes, with the length of the CPU burst given in milliseconds: Process Burst Time P1 6 P2 8 P3 7 P4 3 Using SJFscheduling, we would schedule these processes according to the following Gantt chart: P3 P2 P4 P1 24 16 9 03 The waiting time is 3 milliseconds for process P1,1 6m i l l i s e c o n d sf o rp r o c e s s P2,9m i l l i s e c o n d sf o rp r o c e s s P3,a n d0m i l l i s e c o n d

sf o rp r o c e s s P3,a n d0m i l l i s e c o n d sf o rp r o c e s s P4.T h u s ,t h e average waiting time is (3  16  9  0)4  7 milliseconds. By comparison, if we were using the FCFS scheduling scheme, the average waiting time would be 10.25 milliseconds. The SJFscheduling algorithm is provably optimal, in that it gives the minimum average waiting time for a given set of processes. Moving a short process before a long one decreases the waiting time of the short process more than it increases

time of the short process more than it increases the waiting time of the long process. Consequently, the average waiting time decreases. The real difculty with the SJFalgorithm is knowing the length of the next CPU request. For longterm (job) scheduling in a batch system, we can use the process time limit that a user species when he submits the job. In this situation, users are motivated to estimate the process time limit accurately, since a lower value may mean faster response but too low a

lower value may mean faster response but too low a value will cause at i m e  l i m i t  e x c e e d e de r r o ra n dr e q u i r er e s u b m i s s i o n . SJFscheduling is used frequently in longterm scheduling. Although the SJFalgorithm is optimal, it cannot be implemented at the level of shortterm CPU scheduling. With shortterm scheduling, there is no way to know the length of the next CPU burst. One approach to this problem is to try to approximate SJFscheduling. We may not know the length

SJFscheduling. We may not know the length of the next CPU burst, but we may be able to predict its value. We expect that the next CPU burst will be similar in length to the previous ones. By computing an approximation of the length of the next CPU burst, we can pick the process with the shortest predicted CPU burst. The next CPU burst is generally predicted as an exponential average of the measured lengths of previous CPU bursts. We can dene the exponential6.3 Scheduling Algorithms 269 6 4 6 4

exponential6.3 Scheduling Algorithms 269 6 4 6 4 13 13 13  8 10 6 6 5 9 11 12 CPU burst ( ti) guess ( i)tii 2 time4681012 Figure 6.3 Prediction of the length of the next CPU burst. average with the following formula. Let tnbe the length of the nthCPU burst, and let H9270n1be our predicted value for the next CPU burst. Then, for H9251,0H9251 1, dene H9270n1H9251tn( 1 H9251)H9270n. The value of tncontains our most recent information, while H9270nstores the past history. The parameter H9251controls

the past history. The parameter H9251controls the relative weight of recent and past history in our prediction. If H92510 ,t h e n H9270n1H9270n,a n dr e c e n th i s t o r yh a sn oe f f e c t( c u r r e n t conditions are assumed to be transient). If H92511 ,t h e n H9270n1tn,a n do n l yt h em o s t recent CPU burst matters (history is assumed to be old and irrelevant). More commonly, H92511  2 ,s or e c e n th i s t o r ya n dp a s th i s t o r ya r ee q u a l l yw e i g h t e d . The

i s t o r ya r ee q u a l l yw e i g h t e d . The initial H92700can be dened as a constant or as an overall system average. Figure 6.3 shows an exponential average with H92511  2a n d H927001 0 . To understand the behavior of the ex ponential average, we can expand the formula for H9270n1by substituting for H9270nto nd H9270n1H9251tn(1H9251)H9251tn1 (1H9251)jH9251tnj (1H9251)n1H92700. Typically, H9251is less than 1. As a result, (1 H9251)i sa l s ol e s st h a n1 ,a n de a c h successive term

l s ol e s st h a n1 ,a n de a c h successive term has less weight than its predecessor. The SJFalgorithm can be either preemptive or nonpreemptive. The choice arises when a new process arrives at the ready queue while a previous process is still executing. The next CPU burst of the newly arrived process may be shorter than what is left of the currently executing process. A preemptive SJFalgorithm will preempt the currently executing process, whereas a nonpreemptive SJF algorithm will allow the

a nonpreemptive SJF algorithm will allow the currently running process to nish its CPU burst. Preemptive SJFscheduling is sometimes called shortestremainingtimerst scheduling.270 Chapter 6 CPU Scheduling As an example, consider the following four processes, with the length of the CPU burst given in milliseconds: Process Arrival Time Burst Time P1 08 P2 14 P3 29 P4 35 If the processes arrive at the ready queue at the times shown and need the indicated burst times, then the resulting preemptive

burst times, then the resulting preemptive SJFschedule is as depicted in the following Gantt chart: P1P3P1P2P4 26 17 10 01 5 Process P1is started at time 0, since it is the only process in the queue. Process P2arrives at time 1. The remaining time for process P1(7 milliseconds) is larger than the time required by process P2(4 milliseconds), so process P1is preempted, and process P2is scheduled. The average waiting time for this example is [(10 1)  (1 1)  (17 2)  (5 3)]4  264  6.5 milliseconds.

1)  (1 1)  (17 2)  (5 3)]4  264  6.5 milliseconds. Nonpreemptive SJFscheduling would result in an average waiting time of 7.75 milliseconds. 6.3.3 Priority Scheduling The SJFalgorithm is a special case of the general priorityscheduling algorithm. A priority is associated with each process, and the CPU is allocated to the process with the highest priority. Equalpriority processes are scheduled in FCFS order. An SJFalgorithm is simply a priority algorithm where the priority ( p)i st h e inverse of

where the priority ( p)i st h e inverse of the (predicted) next CPU burst. The larger the CPU burst, the lower the priority, and vice versa. Note that we discuss scheduling in terms of high priority and low priority. Priorities are generally indicated by some xed range of numbers, such as 0 to 7 or 0 to 4,095. However, there is no general agreement on whether 0 is the highest or lowest priority. Some systems use low numbers to represent low priority; others use low numbers for high priority.

others use low numbers for high priority. This difference can lead to confusion. In this text, we assume that low numbers represent high priority. As an example, consider the following set of processes, assumed to have arrived at time 0 in the order P1,P2,,P5,w i t ht h el e n g t ho ft h e CPU burst given in milliseconds: Process Burst Time Priority P1 10 3 P2 11 P3 24 P4 15 P5 526.3 Scheduling Algorithms 271 Using priority scheduling, we would schedule these processes according to the

we would schedule these processes according to the following Gantt chart: P1P4P3P2P5 1918 16 6 01 The average waiting time is 8.2 milliseconds. Priorities can be dened either internally or externally. Internally dened priorities use some measurable quantity or quantities to compute the priority of a process. For example, time limits, memory requirements, the number of open les, and the ratio of average IOburst to average CPU burst have been used in computing priorities. External priorities are

in computing priorities. External priorities are set by criteria outside the operating system, such as the importance of the process, the type and amount of funds being paid for computer use, the department sponsoring the work, and other, often political, factors. Priority scheduling can be either preemptive or nonpreemptive. When a process arrives at the ready queue, its priority is compared with the priority of the currently running process. A preemptive priority scheduling algorithm will

A preemptive priority scheduling algorithm will preempt the CPU if the priority of the newly arrived process is higher than the priority of the currently running process. A nonpreemptive priority scheduling algorithm will simply put the new process at the head of the ready queue. Am a j o rp r o b l e mw i t hp r i o r i t ys c h e d u l i n ga l g o r i t h m si s indenite block ing,o rstarvation .Ap r o c e s st h a ti sr e a d yt or u nb u tw a i t i n gf o rt h e CPU can be considered

u tw a i t i n gf o rt h e CPU can be considered blocked. A priority scheduling algorithm can leave some low priority processes waiting indenitely. In a heavily loaded computer system, a steady stream of higherpriority processes can prevent a lowpriority process from ever getting the CPU.G e n e r a l l y ,o n eo ft w ot h i n g sw i l lh a p p e n .E i t h e rt h e process will eventually be run (at 2 A.M. Sunday, when the system is nally lightly loaded), or the computer system will eventually

loaded), or the computer system will eventually crash and lose all unnished lowpriority processes. (Rumor has it that when they shut down the IBM 7094 atMIT in 1973, they found a lowpriority process that had been submitted in 1967 and had not yet been run.) As o l u t i o nt ot h ep r o b l e mo fi n d e  n i t eb l o c k a g eo fl o w  p r i o r i t yp r o c e s s e si s aging .A g i n gi n v o l v e sg r a d u a l l yi n c r e a s i n gt h ep r i o r i t yo fp r o c e s s e st h a tw a i t in

r i o r i t yo fp r o c e s s e st h a tw a i t in the system for a long time. For example, if priorities range from 127 (low) to 0 (high), we could increase the priority of a waiting process by 1 every 15 minutes. Eventually, even a process with an initial priority of 127 would have the highest priority in the system and would be executed. In fact, it would take no more than 32 hours for a priority127 process to age to a priority0 process. 6.3.4 RoundRobin Scheduling The roundrobin

6.3.4 RoundRobin Scheduling The roundrobin (RR)scheduling algorithm is designed especially for time sharing systems. It is similar to FCFS scheduling, but preemption is added to enable the system to switch between processes. A small unit of time, called a time quantum ortime slice ,i sd e  n e d .At i m eq u a n t u mi sg e n e r a l l yf r o m1 0 to 100 milliseconds in length. The ready queue is treated as a circular queue.272 Chapter 6 CPU Scheduling The CPU scheduler goes around the ready

Scheduling The CPU scheduler goes around the ready queue, allocating the CPU to each process for a time interval of up to 1 time quantum. To implement RRscheduling, we again treat the ready queue as a FIFO queue of processes. New processes are added to the tail of the ready queue. The CPU scheduler picks the rst process from the ready queue, sets a timer to interrupt after 1 time quantum, and dispatches the process. One of two things will then happen. The process may have a CPU burst of less

happen. The process may have a CPU burst of less than 1 time quantum. In this case, the process itself will release the CPU voluntarily. The scheduler will then proceed to the next process in the ready queue. If the CPU burst of the currently running process is longer than 1 time quantum, the timer will go off and will cause an interrupt to the operating system. A context switch will be executed, and the process will be put at the tail of the ready queue. The CPU scheduler will then select the

queue. The CPU scheduler will then select the next process in the ready queue. The average waiting time under the RRpolicy is often long. Consider the following set of processes that arrive at time 0, with the length of the CPU burst given in milliseconds: Process Burst Time P1 24 P2 3 P3 3 If we use a time quantum of 4 milliseconds, then process P1gets the rst 4 milliseconds. Since it requires another 20 milliseconds, it is preempted after the rst time quantum, and the CPU is given to the next

rst time quantum, and the CPU is given to the next process in the queue, process P2.P r o c e s s P2does not need 4 milliseconds, so it quits before its time quantum expires. The CPU is then given to the next process, process P3.O n c e each process has received 1 time quantum, the CPU is returned to process P1 for an additional time quantum. The resulting RRschedule is as follows: P1P1P1P1P1P1P2 30 18 14 26 22 10 7 04P3 Lets calculate the average waiting time for this schedule. P1waits for 6

waiting time for this schedule. P1waits for 6 milliseconds (10  4), P2waits for 4 milliseconds, and P3waits for 7 milliseconds. Thus, the average waiting time is 173  5.66 milliseconds. In the RRscheduling algorithm, no process is allocated the CPU for more than 1 time quantum in a row (unless it is the only runnable process). If a processs CPU burst exceeds 1 time quantum, that process is preempted and is put back in the ready queue. The RRscheduling algorithm is thus preemptive. If there are

algorithm is thus preemptive. If there are nprocesses in the ready queue and the time quantum is q, then each process gets 1 nof the CPU time in chunks of at most qtime units. Each process must wait no longer than ( n1)qtime units until its next time quantum. For example, with ve processes and a time quantum of 20 milliseconds, each process will get up to 20 milliseconds every 100 milliseconds. The performance of the RRalgorithm depends heavily on the size of the time quantum. At one extreme, if

the size of the time quantum. At one extreme, if the time quantum is extremely large, the RRpolicy6.3 Scheduling Algorithms 273 process time H11005 10 quantum context switches 12 0 61 1901 0 01 0 0123456789 1 06 Figure 6.4 How a smaller time quantum increases context switches. is the same as the FCFS policy. In contrast, if the time quantum is extremely small (say, 1 millisecond), the RRapproach can result in a large number of context switches. Assume, for example, that we have only one process

Assume, for example, that we have only one process of 10 time units. If the quantum is 12 time units, the process nishes in less than 1 time quantum, with no overhead. If the quantum is 6 time units, however, the process requires 2 quanta, resulting in a context switch. If the time quantum is 1t i m eu n i t ,t h e nn i n ec o n t e x ts w i t c h e sw i l lo c c u r ,s l o w i n gt h ee x e c u t i o no ft h e process accordingly (Figure 6.4). Thus, we want the time quantum to be large with

Thus, we want the time quantum to be large with respect to the context switch time. If the contextswitch time is approximately 10 percent of the time quantum, then about 10 percent of the CPU time will be spent in context switching. In practice, most modern systems have time quanta ranging from 10 to 100 milliseconds. The time required for a context switch is typically less than 10 microseconds; thus, the contextswitch time is a small fraction of the time quantum. Turnaround time also depends on

the time quantum. Turnaround time also depends on the size of the time quantum. As we can see from Figure 6.5, the average turnaround time of a set of processes does not necessarily improve as the timequantum size increases. In general, the average turnaround time can be improved if most processes nish their next CPU burst in a single time quantum. For example, given three processes of 10 time units each and a quantum of 1 time unit, the average turnaround time is 29. If the time quantum is 10,

turnaround time is 29. If the time quantum is 10, however, the average turnaround time drops to 20. If contextswitch time is added in, the average turnaround time increases even more for a smaller time quantum, since more context switches are required. Although the time quantum should be large compared with the context switch time, it should not be too large. As we pointed out earlier, if the time quantum is too large, RRscheduling degenerates to an FCFS policy. A rule of thumb is that 80

to an FCFS policy. A rule of thumb is that 80 percent of the CPU bursts should be shorter than the time quantum. 6.3.5 Multilevel Queue Scheduling Another class of scheduling algorithms has been created for situations in which processes are easily classied into different groups. For example, a274 Chapter 6 CPU Schedulingaverage turnaround time 112.5 12.0 11.5 11.0 10.5 10.0 9.5 9.0 234 time quantum567P1 P2 P3 P46 3 1 7process time Figure 6.5 How turnaround time varies with the time quantum.

How turnaround time varies with the time quantum. common division is made between foreground (interactive) processes and background (batch) processes. These two types of processes have different responsetime requirements and so may have different scheduling needs. In addition, foreground processes may have priority (externally dened) over background processes. Amultilevel queue scheduling algorithm partitions the ready queue into several separate queues (Figure 6.6). The processes are

separate queues (Figure 6.6). The processes are permanently assigned to one queue, generally based on some property of the process, such as memory size, process priority, or process type. Each queue has its own scheduling algorithm. For example, separate queues might be used for foreground and background processes. The foreground queue might be scheduled by an RR algorithm, while the background queue is scheduled by an FCFS algorithm. In addition, there must be scheduling among the queues, which

there must be scheduling among the queues, which is com monly implemented as xedpriority preemptive scheduling. For example, the foreground queue may have absolute priority over the background queue. Lets look at an example of a multilevel queue scheduling algorithm with ve queues, listed below in order of priority: 1.System processes 2.Interactive processes 3.Interactive editing processes 4.Batch processes 5.Student processes6.3 Scheduling Algorithms 275system processeshighest priority lowest

275system processeshighest priority lowest priorityinteractive processes interactive editing processes batch processes student processes Figure 6.6 Multilevel queue scheduling. Each queue has absolute priority over lowerpriority queues. No process in the batch queue, for example, could run unless the queues for system processes, interactive processes, and interactive editing processes were all empty. If an interactive editing process entered th er e a d yq u e u ew h i l eab a t c hp r o c e s

er e a d yq u e u ew h i l eab a t c hp r o c e s sw a s running, the batch process would be preempted. Another possibility is to timeslice among the queues. Here, each queue gets ac e r t a i np o r t i o no ft h e CPU time, which it can then schedule among its various processes. For instance, in the foregroundbackground queue example, the foreground queue can be given 80 percent of the CPU time for RRscheduling among its processes, while the backgroun d queue receives 20 percent of the CPU to

d queue receives 20 percent of the CPU to give to its processes on an FCFS basis. 6.3.6 Multilevel Feedback Queue Scheduling Normally, when the multilevel queue scheduling algorithm is used, processes are permanently assigned to a queue when they enter the system. If there are separate queues for foreground and background processes, for example, processes do not move from one queue to the other, since processes do not change their foreground or background nature. This setup has the advantage of

background nature. This setup has the advantage of low scheduling overhead, but it is inexible. The multilevel feedback queue scheduling algorithm, in contrast, allows ap r o c e s st om o v eb e t w e e nq u e u e s .T h ei d e ai st os e p a r a t ep r o c e s s e sa c c o r d i n g to the characteristics of their CPU bursts. If a process uses too much CPU time, it will be moved to a lowerpriority queue. This scheme leaves IObound and interactive processes in the higherpriority queues. In

processes in the higherpriority queues. In addition, a process that waits too long in a lowerpriority queue may be moved to a higherpriority queue. This form of aging prevents starvation. For example, consider a multilevel feedbac kq u e u es c h e d u l e rw i t ht h r e e queues, numbered from 0 to 2 (Figure 6.7). The scheduler rst executes all276 Chapter 6 CPU Schedulingquantum H11005 8 quantum H11005 16 FCFS Figure 6.7 Multilevel feedback queues. processes in queue 0. Only when queue 0 is

queues. processes in queue 0. Only when queue 0 is empty will it execute processes in queue 1. Similarly, processes in queue 2 will be executed only if queues 0 and 1 are empty. A process that arrives for queue 1 will preempt a process in queue 2. A process in queue 1 will in turn be preempted by a process arriving for queue 0. A process entering the ready queue is put in queue 0. A process in queue 0 is given a time quantum of 8 milliseconds. If it does not nish within this time, it is moved to

it does not nish within this time, it is moved to the tail of queue 1. If queue 0 is empty, the process at the head of queue 1 is given a quantum of 16 milliseconds. If it does not complete, it is preempted and is put into queue 2. Processes in queue 2 are run on an FCFS basis but are run only when queues 0 and 1 are empty. This scheduling algorithm gives highest priority to any process with a CPU burst of 8 milliseconds or less. Such a process will quickly get the CPU, n i s h itsCPU burst, and

quickly get the CPU, n i s h itsCPU burst, and go off to its next IOburst. Processes that need more than 8b u tl e s st h a n2 4m i l l i s e c o n d sa r ea l s os e r v e dq u i c k l y ,a l t h o u g hw i t hl o w e r priority than shorter processes. Long processes automatically sink to queue 2a n da r es e r v e di n FCFS order with any CPU cycles left over from queues 0 and 1. In general, a multilevel feedback queue scheduler is dened by the following parameters: The number of queues The

the following parameters: The number of queues The scheduling algorithm for each queue The method used to determine when to upgrade a process to a higher priority queue The method used to determine when to demote a process to a lower priority queue The method used to determine which queue a process will enter when that process needs service The denition of a multilevel feedbac kq u e u es c h e d u l e rm a k e si tt h em o s t general CPUscheduling algorithm. It can be congured to match a

algorithm. It can be congured to match a specic system under design. Unfortunately, it is also the most complex algorithm,6.4 Thread Scheduling 277 since dening the best scheduler requires some means by which to select values for all the parameters. 6.4 Thread Scheduling In Chapter 4, we introduced thr eads to the process model, distinguishing between userlevel andkernellevel threads. On operating systems that support them, it is kernellevel threadsnot pr ocessesthat are being scheduled by the

pr ocessesthat are being scheduled by the operating system. Userlevel threads are managed by a thread library, and the kernel is unaware of them. To run on a CPU,u s e r  l e v e lt h r e a d s must ultimately be mapped to an associated kernellevel thread, although this mapping may be indirect and may use a lightweight process ( LWP). In this section, we explore scheduling issues involving userlevel and kernellevel threads and offer specic examples of scheduling for Pthreads. 6.4.1 Contention

of scheduling for Pthreads. 6.4.1 Contention Scope One distinction between userlevel and kernellevel threads lies in how they are scheduled. On systems implementing the manytoone (Section 4.3.1) and manytomany (Section 4.3.3) models, the thread library schedules userlevel threads to run on an available LWP.T h i ss c h e m ei sk n o w na s process contention scope (PCS),s i n c ec o m p e t i t i o nf o rt h e CPU takes place among threads belonging to the same process. (When we say the thread

to the same process. (When we say the thread library schedules user threads onto available LWPs, we do not mean that the threads are actually running on a CPU.T h a tw o u l dr e q u i r et h eo p e r a t i n gs y s t e mt o schedule the kernel thread onto a physical CPU.) To decide which kernellevel thread to schedule onto a CPU,t h ek e r n e lu s e s systemcontention scope (SCS). Competition for the CPU with SCSscheduling takes place among all threads in the system. Systems using the onetoone

threads in the system. Systems using the onetoone model (Section 4.3.2), such as Windows, Linux, and Solaris, schedule threads using only SCS. Typically, PCS is done according to prioritythe scheduler selects the runnable thread with the highest priority to run. Userlevel thread priorities are set by the programmer and are not adjusted by the thread library, although some thread libraries may allow the programmer to change the priority of at h r e a d .I ti si m p o r t a n tt on o t et h a t

r e a d .I ti si m p o r t a n tt on o t et h a t PCS will typically preempt the thread currently running in favor of a higherpriority thread; however, there is no guarantee of time slicing (Section 6.3.4) among threads of equal priority. 6.4.2 Pthread Scheduling We provided a sample POSIX Pthread program in Section 4.4.1, along with an introduction to thread creation with Pthreads. Now, we highlight the POSIX Pthread APIthat allows specifying PCSorSCSduring thread creation. Pthreads identies

PCSorSCSduring thread creation. Pthreads identies the following contention scope values: PTHREAD SCOPE PROCESS schedules threads using PCSscheduling. PTHREAD SCOPE SYSTEM schedules threads using SCSscheduling.278 Chapter 6 CPU Scheduling On systems implementing the manytomany model, the PTHREAD SCOPE PROCESS policy schedules userlevel threads onto available LWPs. The number of LWPsi sm a i n t a i n e db yt h et h r e a dl i b r a r y ,p e r h a p su s i n g scheduler activations (Section

r h a p su s i n g scheduler activations (Section 4.6.5). The PTHREAD SCOPE SYSTEM scheduling policy will create and bind an LWP for each userlevel thread on manytomany systems, effectively mapping threads using the onetoone policy. The Pthread IPCprovides two functions for gettingand settingthe contention scope policy: pthread attr setscope(pthread attr t a t t r ,i n ts c o p e ) pthread attr getscope(pthread attr t a t t r ,i n t s c o p e ) The rst parameter for both functions contains a

) The rst parameter for both functions contains a pointer to the attribute set for the thread. The second parameter for the pthread attr setscope() function is passed either the PTHREAD SCOPE SYSTEM or the PTHREAD SCOPE PROCESS value, indicating how the contention scope is to be set. In the case of pthread attr getscope() ,t h i ss e c o n dp a r a m e t e rc o n t a i n sap o i n t e rt oa n int value that is set to the current value of the contention scope. If an error occurs, each of these

scope. If an error occurs, each of these functions returns a nonzero value. In Figure 6.8, we illustrate a Pthread scheduling API.T h ep r o  gram rst determines the existing contention scope and sets it to PTHREAD SCOPE SYSTEM .I tt h e nc r e a t e s v es e p a r a t et h r e a d st h a tw i l l run using the SCSscheduling policy. Note that on some systems, only certain contention scope values are allowed. For example, Linux and Mac OS X systems allow only PTHREAD SCOPE SYSTEM . 6.5

OS X systems allow only PTHREAD SCOPE SYSTEM . 6.5 MultipleProcessor Scheduling Our discussion thus far has focused on the problems of scheduling the CPU in as y s t e mw i t has i n g l ep r o c e s s o r .I fm u l t i p l e CPUsa r ea v a i l a b l e , load sharing becomes possiblebut scheduling proble ms become correspondingly more complex. Many possibilities have been tried; and as we saw with single processor CPU scheduling, there is no one best solution. Here, we discuss several concerns

best solution. Here, we discuss several concerns in multiprocessor scheduling. We concentrate on systems in which the processors are identicalhomogeneous in terms of their functionality . W e can then use any available processor to run any process in the queue. Note, however, that even with homogeneous multiprocessors, there are sometimes limitations on scheduling. Consider a system with an IOdevice attached to a private bus of one processor. Processes that wish to use that device must be

Processes that wish to use that device must be scheduled to run on that processor. 6.5.1 Approaches to MultipleProcessor Scheduling One approach to CPU scheduling in a multiprocessor system has all scheduling decisions, IO processing, and other system activities handled by a single processorthe master server. The other processors execute only user code. This asymmetric multiprocessing is simple because only one processor accesses the system data structures, reducing the need for data sharing.6.5

structures, reducing the need for data sharing.6.5 MultipleProcessor Scheduling 279 include pthread.h  include stdio.h  define NUM THREADS 5 int main(int argc, char argv[])  int i, scope; pthread tt i d [ N U M THREADS]; pthread attr ta t t r ;  get the default attributes  pthread attr init(attr);  first inquire on the current scope  if (pthread attr getscope(attr, scope) ! 0) fprintf(stderr, Unable to get scheduling scope n); else  if (scope  PTHREAD SCOPE PROCESS) printf(PTHREAD SCOPE

PTHREAD SCOPE PROCESS) printf(PTHREAD SCOPE PROCESS); else if (scope  PTHREAD SCOPE SYSTEM) printf(PTHREAD SCOPE SYSTEM); else fprintf(stderr, Illegal scope value. n);   set the scheduling algorithm to PCS or SCS  pthread attr setscope(attr, PTHREAD SCOPE SYSTEM);  create the threads  for (i  0; i  NUM THREADS; i) pthread create(tid[i],attr,runner,NULL);  now join on each thread  for (i  0; i  NUM THREADS; i) pthread join(tid[i], NULL);   Each thread will begin control in this function  void

thread will begin control in this function  void runner(void param)   do some work ...  pthread exit(0);  Figure 6.8 Pthread scheduling API. As e c o n da p p r o a c hu s e s symmetric multiprocessing (SMP ),w h e r ee a c h processor is selfscheduling. All processes may be in a common ready queue, or each processor may have its own private queue of ready processes. Regardless,280 Chapter 6 CPU Scheduling scheduling proceeds by having the sch eduler for each processor examine the ready queue

eduler for each processor examine the ready queue and select a process to execute. As we saw in Chapter 5, if we have multiple processors trying to access and update a common data structure, the scheduler must be programmed carefully. We must ensure that two separate processors do not choose to schedule the same process and that processes are not lost from the queue. Virtually all modern operating systems support SMP, including Windows, Linux, and Mac OS X .I nt h er e m a i n d e ro ft h i ss e

Mac OS X .I nt h er e m a i n d e ro ft h i ss e c t i o n ,w e discuss issues concerning SMP systems. 6.5.2 Processor Afnity Consider what happens to cache memory when a process has been running on a specic processor. The data most recently accessed by the process populate the cache for the processor. As a result, successive memory accesses by the process are often satised in cache memory. Now consider what happens if the process migrates to another processor. The contents of cache memory must

processor. The contents of cache memory must be invalidated for the rst processor, and the cache for the second processor must be repopulated. Because of the high cost of invalidating and repopulating caches, most SMP systems try to avoid migration of processes from one processor to another and instead attempt to keep a process running on the same processor. This is known as processor afnity that is, a process has an afnity for the processor on which it is currently running. Processor afnity

on which it is currently running. Processor afnity takes several forms. When an operating system has a policy of attempting to keep a process running on the same processorbut not guaranteeing that it will do sowe have a situation known as soft afnity . Here, the operating system will attempt to keep a process on a single processor, but it is possible for a process to migrate between processors. In contrast, some systems provide system calls that support hard afnity ,t h e r e b ya l l o w i n ga

support hard afnity ,t h e r e b ya l l o w i n ga process to specify a subset of processors on which it may run. Many systems provide both soft and hard afnity. For example, Linux implements soft afnity, but it also provides the sched setaffinity() system call, which supports hard afnity. The mainmemory architecture of a system can affect processor afnity issues. Figure 6.9 illustrates an architecture featuring nonuniform memory access ( NUMA ), in which a CPU has faster access to some parts of

in which a CPU has faster access to some parts of main memory than to other parts. Typically, this occurs in systems containing combined CPU and memory boards. The CPUso nab o a r dc a na c c e s st h em e m o r yo nt h a t board faster than they can access memory on other boards in the system. If the operating systems CPU scheduler and memoryplacement algorithms work together, then a process that is assigned afnity to a particular CPU can be allocated memory on the board where that CPU resides.

memory on the board where that CPU resides. This example also shows that operating systems are frequently not as cleanly dened and implemented as described in operatingsystem textbooks. Rather, the solid lines between sections of an operating system are frequently only dotted lines, with algorithms creating connections in ways aimed at optimizing performance and reliability. 6.5.3 Load Balancing On SMP systems, it is important to keep the workload balanced among all processors to fully utilize

balanced among all processors to fully utilize the benets of having more than one processor.6.5 MultipleProcessor Scheduling 281CPU fast access memoryCPU fast accessslow access memory computer Figure 6.9 NUMA and CPU scheduling. Otherwise, one or more processors may sit idle while other processors have high workloads, along with lists of processes awaiting the CPU.Load balancing attempts to keep the workload evenly distributed across all processors in an SMP system. It is important to note that

in an SMP system. It is important to note that load balancing is typically necessary only on systems where each processor has its own private queue of eligible processes to execute. On systems with a common run queue, load balancing is often unnecessary, because once a processor becomes idle, it immediately extracts a runnable process from the common run queue. It is also important to note, however, that in most contemporary operating systems supporting SMP, each processor does have a private

supporting SMP, each processor does have a private queue of eligible processes. There are two general approaches to load balancing: push migration and pull migration .W i t hp u s hm i g r a t i o n ,as p e c i  ct a s kp e r i o d i c a l l yc h e c k st h e load on each processor andif it nds an imbalanceevenly distributes the load by moving (or pushing) processes from overloaded to idle or lessbusy processors. Pull migration occurs when an idle processor pulls a waiting task from a busy

an idle processor pulls a waiting task from a busy processor. Push and pull migration need not be mutually exclusive and are in fact often implemented in parallel on loadbalancing systems. For example, the Linux scheduler (described in Section 6.7.1) and the ULE scheduler available for FreeBSD systems implement both techniques. Interestingly, load balancing often counteracts the benets of processor afnity, discussed in Section 6.5.2. That is, the benet of keeping a process running on the same

the benet of keeping a process running on the same processor is that the process can take advantage of its data being in that processors cache memory. Either pulling or pushing a process from one processor to another removes this benet. As is often the case in systems engineering, there is no absolute rule concerning what policy is best. Thus, in some systems, an idle processor always pulls a process from a nonidle processor. In other systems, processes are moved only if the imbalance exceeds ac

are moved only if the imbalance exceeds ac e r t a i nt h r e s h o l d . 6.5.4 Multicore Processors Traditionally, SMP systems have allowed several threads to run concurrently by providing multiple physical processors. However, a recent practice in computer282 Chapter 6 CPU Scheduling timecompute cycle memory stall cycle threadCC M C M C MM CM Figure 6.10 Memory stall. hardware has been to place multiple processor cores on the same physical chip, resulting in a multicore processor .E a c hc o r

resulting in a multicore processor .E a c hc o r em a i n t a i n si t sa r c h i t e c t u r a ls t a t e and thus appears to the operating system to be a separate physical processor. SMP systems that use multicore processors are faster and consume less power than systems in which each processor has its own physical chip. Multicore processors may complicate scheduling issues. Lets consider how this can happen. Researchers have d iscovered that when a processor accesses memory, it spends a

that when a processor accesses memory, it spends a signicant amount of time waiting for the data to become available. This situation, known as a memory stall ,m a yo c c u rf o rv a r i o u s reasons, such as a cache miss (accessing data that are not in cache memory). Figure 6.10 illustrates a memory stall. In this scenario, the processor can spend up to 50 percent of its time waiting for data to become available from memory. To remedy this situation, many recent hardware designs have

this situation, many recent hardware designs have implemented multithreaded processor cores in which two (or more) hardware threads are assigned to each core. That way, if one thread stalls while waiting for memory, the core can switch to another thread. Figure 6.11 illustrates a dualthreaded processor core on which the execution of thread 0 and the execution of thread 1 are interleaved. From an operatingsystem perspective, each hardware thread appears as a logical processor that is available to

as a logical processor that is available to run a software thread. Thus, on a dualthreaded, dualcore system, four logical processors are presented to the operating system. The Ultra SPARC T3 CPU has sixteen cores per chip and eight hardware threads per core. From the perspective of the operating system, there appear to be 128 logical processors. In general, there are two ways to multithread a processing core: coarse grained andnegrained multithreading. With coarsegrained multithreading, at h r e

With coarsegrained multithreading, at h r e a de x e c u t e so nap r o c e s s o ru n t i lal o n g  l a t e n c ye v e n ts u c ha sam e m o r y stall occurs. Because of the delay ca used by the longlatency event, the processor must switch to another thread to begin execution. However, the cost of switching between threads is high, since the instruction pipeline must timethread 0thread 1C M C M C M CC M C M C M C Figure 6.11 Multithreaded multicore system.6.6 RealTime CPU Scheduling 283 be

system.6.6 RealTime CPU Scheduling 283 be ushed before the other thread can begin execution on the processor core. Once this new thread begins execution, it begins lling the pipeline with its instructions. Finegrained (or in terleaved) multithreading switches between threads at a much ner level of granularitytypically at the boundary of an instruction cycle. However, the architectural design of negrained systems includes logic for thread switching. As a result, the cost of switching between

As a result, the cost of switching between threads is small. Notice that a multithreaded multicore processor actually requires two different levels of scheduling. On one level are the scheduling decisions that must be made by the operating system as it chooses which software thread to run on each hardware thread (logical processor). For this level of scheduling, the operating system may choose any scheduling algorithm, such as those described in Section 6.3. A second level of scheduling species

Section 6.3. A second level of scheduling species how each core decides which hardware thread to run. There are several strategies to adopt in this situation. The Ultra SPARC T3 ,m e n t i o n e de a r l i e r ,u s e sas i m p l er o u n d  robin algorithm to schedule the eight hardware threads to each core. Another example, the Intel Itanium, is a dualcore processor with two hardware managed threads per core. Assigned to each hardware thread is a dynamic urgency value ranging from 0 to 7, with

a dynamic urgency value ranging from 0 to 7, with 0 representing the lowest urgency and 7 the highest. The Itanium identi es ve different events that may trigger at h r e a ds w i t c h .W h e no n eo ft h e s ee v e n t so c c u r s ,t h et h r e a d  s w i t c h i n gl o g i c compares the urgency of the two threads and selects the thread with the highest urgency value to execute on the processor core. 6.6 RealTime CPU Scheduling CPU scheduling for realtime operating systems involves special

for realtime operating systems involves special issues. In general, we can distinguish between soft realtime systems and hard realtime systems. Soft realtime systems provide no guarantee as to when a critical realtime process will be scheduled. They guarantee only that the process will be given preference over noncritical processes. Hard realtime systems have stricter requirements. A task must be servic ed by its deadline; service after the deadline has expired is the same as no service at all.

has expired is the same as no service at all. In this section, we explore several issues related to process scheduling in both soft and hard realtime operating systems. 6.6.1 Minimizing Latency Consider the eventdriven nature of a realtime system. The system is typically waiting for an event in real time to occu r. Events may arise either in software as when a timer expiresor in hardwareas when a remotecontrolled vehicle detects that it is approaching an obstruction. When an event occurs, the

an obstruction. When an event occurs, the system must respond to and service it as quickly as possible. We refer to event latency as the amount of time that elapses fr om when an event occurs to when it is serviced (Figure 6.12). Usually, different events have different latency requirements. For example, the latency requirement for an antilock brake system might be 3 to 5 millisec onds. That is, from the time a wheel rst detects that it is sliding, the system controlling the antilock brakes has

the system controlling the antilock brakes has 3 to 5 milliseconds to respond to and control284 Chapter 6 CPU Scheduling t1 t0event latencyevent E first occurs realtime system responds to E Time Figure 6.12 Event latency. the situation. Any response that takes longer might result in the automobiles veering out of control. In contrast, an embedded system controlling radar in an airliner might tolerate a latency period of several seconds. Two types of latencies affect the performance of realtime

of latencies affect the performance of realtime systems: 1.Interrupt latency 2.Dispatch latency Interrupt latency refers to the period of time from the arrival of an interrupt at the CPU to the start of the routine that services the interrupt. When an interrupt occurs, the operating system must rst complete the instruction it is executing and determine the type of interrupt that occurred. It must then save the state of the current process b efore servicing the interrupt using the specic

b efore servicing the interrupt using the specic interrupt service routine ( ISR). The total time required to perform these tasks is the interrupt latency (Figure 6.13). Obviously, it is crucial for realtask T running ISRdetermine interrupt typeinterrupt interrupt latencycontext switch time Figure 6.13 Interrupt latency.6.6 RealTime CPU Scheduling 285 response to event realtime process executionevent conflicts timedispatch response interval dispatch latencyprocess made available interrupt

dispatch latencyprocess made available interrupt processing Figure 6.14 Dispatch latency. time operating systems to minimize interrupt latency to ensure that realtime tasks receive immediate attention. Indeed, for hard realtime systems, interrupt latency must not simply be minimized, it must be bounded to meet the strict requirements of these systems. One important factor contributing to interrupt latency is the amount of time interrupts may be disabled while kernel data structures are being

be disabled while kernel data structures are being updated. Realtime operating systems require that interrupts be disabled for only very short periods of time. The amount of time required for the scheduling dispatcher to stop one process and start another is known as dispatch latency. Providing realtime tasks with immediate access to the CPU mandates that realtime operating systems minimize this latency as well. The most effective technique for keeping dispatch latency low is to provide

for keeping dispatch latency low is to provide preemptive kernels. In Figure 6.14, we diagram the makeup of dispatch latency. The conict phase of dispatch latency has two components: 1.Preemption of any process running in the kernel 2.Release by lowpriority processes of resources needed by a highpriority process As an example, in Solaris, the dispatch latency with preemption disabled is over a hundred milliseconds. With preemption enabled, it is reduced to less than a millisecond. 6.6.2

it is reduced to less than a millisecond. 6.6.2 PriorityBased Scheduling The most important feature of a realtime operating system is to respond immediately to a realtime process as soon as that process requires the CPU.286 Chapter 6 CPU Scheduling As a result, the scheduler for a realtime operating system must support a prioritybased algorithm with preemption. Recall that prioritybased schedul ing algorithms assign each process a priority based on its importance; more important tasks are

based on its importance; more important tasks are assigned higher priorities than those deemed less impor tant. If the scheduler also supports preemption, a process currently running on the CPU will be preempted if a higherpriority process becomes available to run. Preemptive, prioritybased scheduling algorithms are discussed in detail in Section 6.3.3, and Section 6.7 presen ts examples of the soft realtime scheduling features of the Linux, Windows, and Solaris operating systems. Each of these

and Solaris operating systems. Each of these systems assigns realtime processes the highest scheduling priority. For example, Windows has 32 different priority levels. The highest levelspriority values 16 to 31are reserved for realtime processes. Solaris and Linux have similar prioritization schemes. Note that providing a preemptive, prioritybased scheduler only guaran tees soft realtime functionality. Hard realtime systems must further guarantee that realtime tasks will be serviced in accord

that realtime tasks will be serviced in accord with their deadline requirements, and making such guarantees requires additional scheduling features. In the remainder of this section, we cover scheduling algorithms appropriate for hard realtime systems. Before we proceed with the details of the individual schedulers, however, we must dene certain characteristics of the processes that are to be scheduled. First, the processes are considered periodic . That is, they require the CPU at constant

. That is, they require the CPU at constant intervals (periods). Once a periodic process has acquired the CPU,i t has a xed processing time t,ad e a d l i n e dby which it must be serviced by the CPU,a n dap e r i o d p.T h er e l a t i o n s h i po ft h ep r o c e s s i n gt i m e ,t h ed e a d l i n e ,a n d the period can be expressed as 0 tdp.T h e rate of a periodic task is 1 p. Figure 6.15 illustrates the execution of a periodic process over time. Schedulers can take advantage of these

over time. Schedulers can take advantage of these characteristics and assign priorities according to a processs deadline or rate requirements. What is unusual about this form of scheduling is that a process may have to announce its deadline requirements to the scheduler. Then, using a technique known as an admissioncontrol algorithm, the scheduler does one of two things. It either admits the process, guaranteeing that the process will complete on time, or rejects the request as impossible if it

time, or rejects the request as impossible if it cannot guarantee that the task will be serviced by its deadline. period 1 period 2 period 3Timeppp d d d tt t Figure 6.15 Periodic task.6.6 RealTime CPU Scheduling 287 01 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 1 2 0 90 100 110P1 P1P1, P2 P2deadlines Figure 6.16 Scheduling of tasks when P2has a higher priority than P1. 6.6.3 RateMonotonic Scheduling The ratemonotonic scheduling algorithm schedules periodic tasks using a static priority policy with

periodic tasks using a static priority policy with preemption. If a lowerpriority process is running and a higherpriority process becomes available to run, it will preempt the lowerpriority process. Upon entering the system, each periodic task is assigned ap r i o r i t yi n v e r s e l yb a s e do ni t sp e r i o d .T h es h o r t e rt h ep e r i o d ,t h eh i g h e rt h e priority; the longer the period, the lower the priority. The rationale behind this policy is to assign a higher priority to

this policy is to assign a higher priority to tasks that require the CPU more often. Furthermore, ratemonotonic scheduling assumes that the processing time of ap e r i o d i cp r o c e s si st h es a m ef o re a c h CPU burst. That is, every time a process acquires the CPU,t h ed u r a t i o no fi t s CPU burst is the same. Lets consider an example. We have two processes, P1and P2.T h ep e r i o d s forP1and P2are 50 and 100, respectivelythat is, p150 and p2100. The processing times are t120 for

p150 and p2100. The processing times are t120 for P1and t235 for P2.T h ed e a d l i n ef o re a c h process requires that it complete its CPU burst by the start of its next period. We must rst ask ourselves whether it is possible to schedule these tasks so that each meets its deadlines. If we measure the CPU utilization of a process Pias the ratio of its burst to its period tipithe CPU utilization of P1is 20500.40 and that of P2is 35 1000.35, for a total CPU utilization of 75 percent.

for a total CPU utilization of 75 percent. Therefore, it seems we can schedule these tasks in such a way that both meet their deadlines and still leave the CPU with available cycles. Suppose we assign P2ah i g h e rp r i o r i t yt h a n P1.T h ee x e c u t i o no f P1and P2 in this situation is shown in Figure 6.16. As we can see, P2starts execution rst and completes at time 35. At this point, P1starts; it completes its CPU burst at time 55. However, the rst deadline for P1was at time 50, so

However, the rst deadline for P1was at time 50, so the scheduler has caused P1to miss its deadline. Now suppose we use ratemonotonic scheduling, in which we assign P1 ah i g h e rp r i o r i t yt h a n P2because the period of P1is shorter than that of P2. The execution of these processes in this situation is shown in Figure 6.17. P1starts rst and completes its CPU burst at time 20, thereby meeting its rst deadline. P2starts running at this point and runs until time 50. At this time, it is

point and runs until time 50. At this time, it is preempted by P1,a l t h o u g hi ts t i l lh a s5m i l l i s e c o n d sr e m a i n i n gi ni t s CPU burst. P1completes its CPU burst at time 70, at which point the scheduler resumes 01 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 1 2 0 130140150160170180190200 90 100 110P1 P1P1, P2 P1 P2deadlines P1, P2 P1 P2 P1 P2 P1P2 Figure 6.17 Ratemonotonic scheduling.288 Chapter 6 CPU Scheduling P2.P2completes its CPU burst at time 75, also meeting its rst deadline. The

at time 75, also meeting its rst deadline. The system is idle until time 100, when P1is scheduled again. Ratemonotonic scheduling is considered optimal in that if a set of processes cannot be scheduled by this algorithm, it cannot be scheduled by any other algorithm that assigns static priorities. Lets next examine a set of processes that cannot be scheduled using the ratemonotonic algorithm. Assume that process P1has a period of p150 and a CPU burst of t125. For P2,t h ec o r r e s p o n d i n

burst of t125. For P2,t h ec o r r e s p o n d i n gv a l u e sa r e p280 and t235. Ratemonotonic scheduling would assign process P1a higher priority, as it has the shorter period. The total CPU utilization of the two processes is (25 50)(3580)0.94, and it therefore seems logical that the two processes could be scheduled and still leave the CPU with 6 percent available time. Figure 6.18 shows the scheduling of processes P1and P2.I n i t i a l l y , P1runs until it completes its CPU burst at time

, P1runs until it completes its CPU burst at time 25. Process P2then begins running and runs until time 50, when it is preempted by P1.A tt h i sp o i n t , P2still has 10 milliseconds remaining in its CPU burst. Process P1runs until time 75; consequently, P2misses the deadline for completion of its CPU burst at time 80. Despite being optimal, then, ratemonotonic scheduling has a limitation: CPU utilization is bounded, and it is not always possible fully to maximize CPU resources. The worstcase

fully to maximize CPU resources. The worstcase CPU utilization for scheduling Nprocesses is N(21N1). With one process in the system, CPU utilization is 100 percent, but it falls to approximately 69 percent as the number of processes approaches innity. With two processes, CPU utilization is bounded at about 83 percent. Combined CPU utilization for the two processes scheduled in Figure 6.16 and Figure 6.17 is 75 percent; therefore, th er a t e  m o n o t o n i cs c h e d u l i n ga l g o r i t h

o n o t o n i cs c h e d u l i n ga l g o r i t h mi s guaranteed to schedule them so that they can meet their deadlines. For the two processes scheduled in Figure 6.18, combined CPU utilization is approximately 94 percent; therefore, ratemonotonic sc heduling cannot guarantee that they can be scheduled so that they meet their deadlines. 6.6.4 EarliestDeadlineFirst Scheduling Earliestdeadlinerst (EDF)scheduling dynamically assigns priorities accord ing to deadline. The earlier the deadline, the

ing to deadline. The earlier the deadline, the higher the priority; the later the deadline, the lower the priority. Under the EDFpolicy, when a process becomes runnable, it must announce its deadline requirements to the system. Priorities may have to be adjusted to reect the deadline of the newly runnable process. Note how this differs from ratemonotonic scheduling, where priorities are xed. 0 10 20 30 40 50 60 70 80 120 130 140 150 160 90 100 110P1 P1P2 P1 P2deadlines P1 P2P1, P2 Figure 6.18

110P1 P1P2 P1 P2deadlines P1 P2P1, P2 Figure 6.18 Missing deadlines with ratemonotonic scheduling.6.6 RealTime CPU Scheduling 289 0 10 20 30 40 50 60 70 80 120 130 140 150 160 90 100 110P1 P1 P1P2 P1 P2deadlines P2 P1 P1 P2 P2 Figure 6.19 Earliestdeadlinerst scheduling. To illustrate EDF scheduling, we again schedule the processes shown in Figure 6.18, which failed to meet deadline requirements under ratemonotonic scheduling. Recall that P1has values of p150 and t125 and that P2has values of

values of p150 and t125 and that P2has values of p280 and t235. The EDF scheduling of these processes is shown in Figure 6.19. Process P1has the earliest deadline, so its initial priority is higher than that of process P2. Process P2begins running at the end of the CPU burst forP1. However, whereas ratemonotonic scheduling allows P1to preempt P2 at the beginning of its next period at time 50, EDF scheduling allows process P2to continue running. P2now has a higher priority than P1because its next

has a higher priority than P1because its next deadline (at time 80) is earlier than that of P1(at time 100). Thus, both P1and P2meet their rst deadlines. Process P1again begins running at time 60 and completes its second CPU burst at time 85, also meeting its second deadline at time 100. P2begins running at this point, only to be preempted by P1at the start of its next period at time 100. P2is preempted because P1has an earlier deadline (time 150) than P2(time 160). At time 125, P1completes its

than P2(time 160). At time 125, P1completes its CPU burst and P2resumes execution, nishing at time 145 and meeting its deadline as well. The system is idle until time 150, when P1is scheduled to run once again. Unlike the ratemonotonic algorithm, EDFscheduling does not require that processes be periodic, nor must a process require a constant amount of CPU time per burst. The only requirement is that a process announce its deadline to the scheduler when it becomes runnable. The appeal of EDF

when it becomes runnable. The appeal of EDF scheduling is that it is theoretically optimaltheoretically, it can schedule processes so that each process can meet its deadline requirements and CPU utilization will be 100 percent. In practice, however, i ti si m p o s s i b l et oa c h i e v et h i sl e v e lo f CPU utilization due to the cost of context switching between processes and interrupt handling. 6.6.5 Proportional Share Scheduling Proportional share schedulers operate by allocating

share schedulers operate by allocating Tshares among all applications. An application can receive Nshares of time, thus ensuring that the application will have NTof the total processor time. As an example, assume that a total of T100 shares is to be divided among three processes, A,B,a n d C.Ais assigned 50 shares, Bis assigned 15 shares, and Cis assigned 20 shares. This scheme ensures that Awill have 50 percent of total processor time, Bwill have 15 percent, and Cwill have 20 percent.

Bwill have 15 percent, and Cwill have 20 percent. Proportional share schedulers must work in conjunction with an admissioncontrol policy to guarantee that an application receives its allocated shares of time. An admissioncontrol policy will admit a client requesting ap a r t i c u l a rn u m b e ro fs h a r e so n l yi fs u f  c i e n ts h a r e sa r ea v a i l a b l e .I no u r current example, we have allocated 50 152085 shares of the total of290 Chapter 6 CPU Scheduling 100 shares. If a new

Chapter 6 CPU Scheduling 100 shares. If a new process Drequested 30 shares, the admission controller would deny Dentry into the system. 6.6.6 POSIX RealTime Scheduling The POSIX standard also provides extensions for realtime computing POSIX .1b. Here, we cover some of the POSIX API related to scheduling realtime threads. POSIX denes two scheduling classes for realtime threads: SCHED FIFO SCHED RR SCHED FIFO schedules threads accordin gt oa r s t  c o m e , r s t  s e r v e dp o l i c y using a

t  c o m e , r s t  s e r v e dp o l i c y using a FIFO queue as outlined in Section 6.3.1. However, there is no time slicing among threads of equal priority. Therefore, the highestpriority realtime thread at the front of the FIFO queue will be granted the CPU until it terminates or blocks. SCHED RRuses a roundrobin policy. It is similar to SCHED FIFO except that it provides time slicing among threads of equal priority. POSIX provides an additional scheduling class SCHED OTHER but its

an additional scheduling class SCHED OTHER but its implementation is undened and system specic; it may behave differently on different systems. The POSIX API species the following two functions for getting and setting the scheduling policy: pthread attr getsched policy(pthread attr t a t t r ,i n t policy) pthread attr setsched policy(pthread attr t a t t r ,i n t policy) The rst parameter to both functions is a pointer to the set of attributes for the thread. The second parameter is eith er (1)

the thread. The second parameter is eith er (1) a pointer to an integer that is set to the current scheduling policy (for pthread attr getsched policy() ) or (2) an integer value ( SCHED FIFO ,SCHED RR,o r SCHED OTHER )f o rt h e pthread attr setsched policy() function. Both functions return nonzero values if an error occurs. In Figure 6.20, we illustrate a POSIX Pthread program using this API.T h i s program rst determines the current scheduling policy and then sets the scheduling algorithm to

policy and then sets the scheduling algorithm to SCHED FIFO . 6.7 OperatingSystem Examples We turn next to a description of the scheduling policies of the Linux, Windows, and Solaris operating systems. It is important to note that we use the term process scheduling in a general sense here. In fact, we are describing the scheduling of kernel threads with Solaris and Windows systems and of tasks with the Linux scheduler. 6.7.1 Example: Linux Scheduling Process scheduling in Linux has had an

Scheduling Process scheduling in Linux has had an interesting history. Prior to Version 2.5, the Linux kernel ran a variation of the traditional UNIX scheduling algorithm.6.7 OperatingSystem Examples 291 include pthread.h  include stdio.h  define NUM THREADS 5 int main(int argc, char argv[])  int i, policy; pthread tt i d [ N U M THREADS]; pthread attr ta t t r ;  get the default attributes  pthread attr init(attr);  get the current scheduling policy  if (pthread attr getschedpolicy(attr,

policy  if (pthread attr getschedpolicy(attr, policy) ! 0) fprintf(stderr, Unable to get policy. n); else  if (policy  SCHED OTHER) printf(SCHED OTHER n); else if (policy  SCHED RR) printf(SCHED RRn); else if (policy  SCHED FIFO) printf(SCHED FIFO n);   set the scheduling policy  FIFO, RR, or OTHER  if (pthread attr setschedpolicy(attr, SCHED FIFO) ! 0) fprintf(stderr, Unable to set policy. n);  create the threads  for (i  0; i  NUM THREADS; i) pthread create(tid[i],attr,runner,NULL);  now join

pthread create(tid[i],attr,runner,NULL);  now join on each thread  for (i  0; i  NUM THREADS; i) pthread join(tid[i], NULL);   Each thread will begin control in this function  void runner(void param)   do some work ...  pthread exit(0);  Figure 6.20 POSIX realtime scheduling API.292 Chapter 6 CPU Scheduling However, as this algorithm was not designed with SMP systems in mind, it did not adequately support systems with multiple processors. In addition, it resulted in poor performance for systems

it resulted in poor performance for systems with a large number of runnable processes. With Version 2.5 of the kernel, the scheduler was overhauled to include a scheduling algorithmknown as O(1)that ran in constant time regardless of the number of tasks in the system. The O(1) scheduler also provided increased support for SMP systems, including processor afnity and load balancing between processors. Ho wever, in practice, although the O(1) scheduler delivered excellent performance on SMP

scheduler delivered excellent performance on SMP systems, it led to poor response times for the interactive processes that are common on many desktop computer systems. During development of the 2.6 kernel, the scheduler was again revised; and in release 2.6.23 of the kernel, the Completely Fair Scheduler (CFS) became the default Linux scheduling algorithm. Scheduling in the Linux system is based on scheduling classes .E a c hc l a s si s assigned a specic priority. By using different scheduling

a specic priority. By using different scheduling classes, the kernel can accommodate different scheduling algorithms based on the needs of the system and its processes. The scheduling criteria for a Linux server, for example, may be different from those for a mobile device running Linux. To decide which task to run next, the scheduler selects the highestpriority task belonging to the highestpriority scheduling class. Standard Linux kernels implement two scheduling classes: (1) a default

implement two scheduling classes: (1) a default scheduling class using the CFS scheduling algorithm and (2) a realtime scheduling class. We discuss each of these classes here. New scheduling classes can, of course, be added. Rather than using strict rules that associate a relative priority value with the length of a time quantum, the CFSscheduler assigns a proportion of CPU processing time to each task. This proportion is calculated based on the nice value assigned to each task. Nice values

the nice value assigned to each task. Nice values range from 20 to 19, where a numerically lower nice value indicates a higher relative priority. Tasks with lower nice values receive a higher proportion of CPU processing time than tasks with higher nice values. The default nice value is 0. (The term nice comes from the idea that if a task increases its nice value from, say, 0 to 10, it is being nice to other tasks in the system by lowering its relative priority.) CFSdoesnt use discrete values of

priority.) CFSdoesnt use discrete values of time slices and instead identies a targeted latency , which is an interval of time during which every runnable task should run at least once. Proportions of CPU time are allocated from the value of targeted latency. In addition to having default and minimum values, targeted latency can increase if the number of active tasks in the system grows beyond a certain threshold. The CFSscheduler doesnt directly assign priorities. Rather, it records how long

assign priorities. Rather, it records how long each task has run by maintaining the virtual run time of each task using the pertask variable vruntime .T h ev i r t u a lr u nt i m ei sa s s o c i a t e dw i t had e c a y factor based on the priority of a task: lowerpriority tasks have higher rates of decay than higherpriority tasks. For tasks at normal priority (nice values of 0), virtual run time is identical to actual physical run time. Thus, if a task with default priority runs for 200

Thus, if a task with default priority runs for 200 milliseconds, its vruntime will also be 200 milliseconds. However, if a lowerpriority task runs for 200 milliseconds, its vruntime will be higher than 200 milliseconds. Similarly, if a higherpriority task runs for 200 milliseconds, its vruntime will be less than 200 milliseconds. To decide which task to run next, the sch eduler simply selects the task that has the smallest vruntime value. In addition, a higherpriority task that becomes available

a higherpriority task that becomes available to run can preempt a lowerpriority task.6.7 OperatingSystem Examples 293 CFS PERFORMANCE The Linux CFS scheduler provides an efcient algorithm for selecting which task to run next. Each runnable task is placed in a redblack treea balanced binary search tree whose key is based on the value of vruntime .T h i st r e ei s shown below: T0 T2 T3 T5 T6T1 T4 T9 T7 T8 smaller largerTask with the smallest value of vruntime Value of vruntime When a task becomes

of vruntime Value of vruntime When a task becomes runnable, it is added to the tree. If a task on the tree is not runnable (for example, if it is blocked while waiting for IO), it is removed. Generally speaking, tasks that have been given less processing time (smaller values of vruntime ) are toward the left side of the tree, and tasks that have been given more processing time are on the right side. According to the properties of a binary search tree, the leftmost node has the smallest key

tree, the leftmost node has the smallest key value, which for the sake of the CFS scheduler means that it is the task with the highest priority. Because the redblack tree is balanced, navigating it to discover the leftmost node will require O(lgN)o p e r a t i o n s( w h e r e N is the number of nodes in the tree). However, for efciency reasons, the Linux scheduler caches this value in the variable rb leftmost ,a n dt h u s determining which task to run next requires only retrieving the cached

to run next requires only retrieving the cached value. Lets examine the CFSscheduler in action: Assume that two tasks have the same nice values. One task is IObound and the other is CPUbound. Typically, theIObound task will run only for short periods before blocking for additional IO,a n dt h e CPUbound task will exhaust its time period whenever it has an opportunity to run on a processor. Therefore, the value of vruntime will eventually be lower for the IObound task than for the CPUbound task,

for the IObound task than for the CPUbound task, giving the IObound task higher priority than the CPUbound task. At that point, if the CPUbound task is executing when the IObound task becomes eligible to run (for example, when IOthe task is waiting for becomes available), the IObound task will preempt the CPUbound task. Linux also implements realtime scheduling using the POSIX standard as described in Section 6.6.6. Any task scheduled using either the SCHED FIFO or the SCHED RRrealtime policy

the SCHED FIFO or the SCHED RRrealtime policy runs at a higher priority than normal (nonreal294 Chapter 6 CPU Scheduling 0 100 139 99RealT ime Normal PriorityHigher Lower Figure 6.21 Scheduling priorities on a Linux system. time) tasks. Linux uses two separate priority ranges, one for realtime tasks and a second for normal tasks. Realtime tasks are assigned static priorities within the range of 0 to 99, and normal (i.e. non realtime) tasks are assigned priorities from 100 to 139. These two

are assigned priorities from 100 to 139. These two ranges map into a global priority scheme wherein numerically lower values indicate higher relative priorities. Normal tasks are assigned a priority based on their nice values, where a value of 20 maps to priority 100 and a nice value of 19 maps to 139. This scheme is shown in Figure 6.21. 6.7.2 Example: Windows Scheduling Windows schedules threads usin gap r i o r i t y  b a s e d ,p r e e m p t i v es c h e d u l i n g algorithm. The Windows

t i v es c h e d u l i n g algorithm. The Windows scheduler ensures that the highestpriority thread will always run. The portion of the Windows kernel that handles scheduling is called the dispatcher . A thread selected to run by the dispatcher will run until it is preempted by a higherpriority thread, until it terminates, until its time quantum ends, or until it calls a blocking system call, such as for IO.I fa higherpriority realtime thread becomes ready while a lowerpriority thread is

becomes ready while a lowerpriority thread is running, the lowerpriority thread will be preempted. This preemption gives ar e a l  t i m et h r e a dp r e f e r e n t i a la c c e s st ot h e CPU when the thread needs such access. The dispatcher uses a 32level priority scheme to determine the order of thread execution. Priorities are divided into two classes. The variable class contains threads having priorities from 1 to 15, and the realtime class contains threads with priorities ranging from

contains threads with priorities ranging from 16 to 31. (There is also a thread running at priority 0 that is used for memory management.) The dispatcher uses a queue for each scheduling priority and traverses the set of queues from highest to lowest until it nds a thread that is ready to run. If no ready thread is found, the dispatcher will execute a special thread called the idle thread . There is a relationship between the numeric priorities of the Windows kernel and the Windows API.T h eW i

of the Windows kernel and the Windows API.T h eW i n d o w s APIidenties the following six priority classes to which a process can belong: IDLE PRIORITY CLASS BELOW NORMAL PRIORITY CLASS NORMAL PRIORITY CLASS ABOVE NORMAL PRIORITY CLASS6.7 OperatingSystem Examples 295 HIGH PRIORITY CLASS REALTIME PRIORITY CLASS Processes are typically members of the NORMAL PRIORITY CLASS .Ap r o c e s s belongs to this class unless the parent of the process was a member of the IDLE PRIORITY CLASS or unless

was a member of the IDLE PRIORITY CLASS or unless another class was specied when the process was created. Additionally, the priority class of a process can be altered with theSetPriorityClass() function in the Windows API.P r i o r i t i e si na l lc l a s s e s except the REALTIME PRIORITY CLASS are variable, meaning that the priority of at h r e a db e l o n g i n gt oo n eo ft h e s ec l a s s e sc a nc h a n g e . At h r e a dw i t h i nag i v e np r i o r i t yc l a s s e sa l s oh a sar e

v e np r i o r i t yc l a s s e sa l s oh a sar e l a t i v ep r i o r i t y .T h e values for relative priorities include: IDLE LOWEST BELOW NORMAL NORMAL ABOVE NORMAL HIGHEST TIME CRITICAL The priority of each thread is based on both the priority class it belongs to and its relative priority within that class. This relationship is shown in Figure 6.22. The values of the priority classes appear in the top row. The left column contains the values for the relative priorities. For example, if the

for the relative priorities. For example, if the relative priority of a thread in the ABOVE NORMAL PRIORITY CLASS isNORMAL ,t h en u m e r i c priority of that thread is 10. Furthermore, each thread has a base priority representing a value in the priority range for the class to which the thread belongs. By default, the basehighabove normalnormalbelow normalidle priority timecr iticalreal time 31 26 25 24 23 22 1615 15 14 13 12 11 115 12 11 10 9 8 115 10 9 8 7 6 115 8 7 6 5 4 115 6 5 4 3 2

10 9 8 115 10 9 8 7 6 115 8 7 6 5 4 115 6 5 4 3 2 1highest above normal normal lowest idlebelow normal Figure 6.22 Windows thread priorities.296 Chapter 6 CPU Scheduling priority is the value of the NORMAL relative priority for that class. The base priorities for each priority class are as follows: REALTIME PRIORITY CLASS 24 HIGH PRIORITY CLASS 13 ABOVE NORMAL PRIORITY CLASS 10 NORMAL PRIORITY CLASS 8 BELOW NORMAL PRIORITY CLASS 6 IDLE PRIORITY CLASS 4 The initial priority of a thread is

CLASS 4 The initial priority of a thread is typically the base priority of the process the thread belongs to, although the SetThreadPriority() function in the Windows APIcan also be used to modify a threads the base priority. When a threads time quantum runs out, that thread is interrupted. If the thread is in the variablepriority class, its priority is lowered. The priority is never lowered below the base priority ,h o w e v e r .L o w e r i n gt h ep r i o r i t yt e n d s to limit the CPU

n gt h ep r i o r i t yt e n d s to limit the CPU consumption of computebound threads. When a variable priority thread is released from a wait operation, the dispatcher boosts the priority. The amount of the boost depends on what the thread was waiting for. For example, a thread waiting for keyboard IOwould get a large increase, whereas a thread waiting for a disk operation would get a moderate one. This strategy tends to give good response times to interactive threads that are using the mouse

to interactive threads that are using the mouse and windows. It also enables IObound threads to keep the IOdevices busy while permitting computebound threads to use spare CPU cycles in the background. This strategy is used by several timesharing operating systems, including UNIX .I na d d i t i o n ,t h ew i n d o ww i t hw h i c ht h e user is currently interacting receives ap r i o r i t yb o o s tt oe n h a n c ei t sr e s p o n s e time. When a user is running an interactive program, the

When a user is running an interactive program, the system needs to provide especially good performance. For this reason, Windows has a special schedul ing rule for processes in the NORMAL PRIORITY CLASS .W i n d o w sd i s t i n g u i s h e s between the foreground process that is currently selected on the screen and thebackground processes that are not currently selected. When a process moves into the foreground, Windows increases the scheduling quantum by some factortypically by 3. This

quantum by some factortypically by 3. This increase gives the foreground process three times longer to run before a timesharing preemption occurs. Windows 7 introduced usermode scheduling (UMS ),w h i c ha l l o w sa p p l i  cations to create and manage threads independently of the kernel. Thus, an application can create and schedule multiple threads without involving the Windows kernel scheduler. For applications that create a large number of threads, scheduling threads in user mode is much

threads, scheduling threads in user mode is much more efcient than kernelmode thread scheduling, as no kernel intervention is necessary. Earlier versions of Windows provided a similar feature known as bers , which allowed several usermode threads (bers) to be mapped to a single kernel thread. However, bers were of limited practical use. A ber was unable to make calls to the Windows APIbecause all bers had to share the thread environment block ( TEB)o ft h et h r e a do nw h i c ht h e yw e r er

TEB)o ft h et h r e a do nw h i c ht h e yw e r er u n n i n g .T h i s6.7 OperatingSystem Examples 297 presented a problem if a Windows APIfunction placed state information into the TEB for one ber, only to have the information overwritten by a different ber. UMS overcomes this obstacle by pro viding each usermode thread with its own thread context. In addition, unlike bers, UMS is not intended to be used directly by the programmer. The details of writing usermode schedulers can be very

details of writing usermode schedulers can be very challenging, and UMS does not include such a scheduler. Rather, the schedulers come from programming language libraries that build on top of UMS .F o r example, Microsoft provides Concurrency Runtime (ConcRT), a concurrent programming framework for C that is designed for taskbased parallelism (Section 4.2) on multicore processors. ConcRTprovides a usermode scheduler together with facilities for decomposing programs into tasks, which can then be

decomposing programs into tasks, which can then be scheduled on the available processing cores. Further details on UMS can be found in Section 19.7.3.7. 6.7.3 Example: Solaris Scheduling Solaris uses prioritybased thread scheduling. Each thread belongs to one of six classes: 1.Time sharing ( TS) 2.Interactive ( IA) 3.Real time ( RT) 4.System ( SYS) 5.Fair share ( FSS) 6.Fixed priority ( FP) Within each class there are different priorities and different scheduling algo rithms. The default

and different scheduling algo rithms. The default scheduling class for a process is time sharing. The scheduling policy for the timesharing class dynamically alters priorities and assigns time slices of different lengths usin gam u l t i l e v e lf e e d b a c kq u e u e .B yd e f a u l t ,t h e r e is an inverse relationship between priorities and time slices. The higher the priority, the smaller the time slice; and the lower the priority, the larger the time slice. Interactive processes

the larger the time slice. Interactive processes typically have a higher priority; CPUbound processes, a lower priority. This scheduling policy gives good response time for interactive processes and good throughput for CPUbound processes. The interactive class uses the same scheduling policy as the timesharing class, but it gives windowing applicationssuch as those created by the KDE orGNOME window managersa higher priority for better performance. Figure 6.23 shows the dispatch table for

Figure 6.23 shows the dispatch table for scheduling timesharing and interactive threads. These two s cheduling classes include 60 priority levels, but for brevity, we display only a handful. The dispatch table shown in Figure 6.23 contains the following elds: Priority .T h ec l a s s  d e p e n d e n tp r i o r i t yf o rt h et i m e  s h a r i n ga n di n t e r a c t i v e classes. A higher number indicates a higher priority.298 Chapter 6 CPU Schedulingtime quantum priorityreturn from sleeptime

quantum priorityreturn from sleeptime quantum expired 0 5 10 15 20 25 30 35 40 45 50 55 59200 200 160 160 120 120 80 80 40 40 40 40 200 0 0 5 10 15 20 25 30 35 40 45 4950 50 51 51 52 52 53 54 55 56 58 58 59Figure 6.23 Solaris dispatch table for timesharing and interactive threads. Time quantum .T h et i m eq u a n t u mf o rt h ea s s o c i a t e dp r i o r i t y .T h i si l l u s  trates the inverse relationship between priorities and time quanta: the lowest priority (priority 0) has the

quanta: the lowest priority (priority 0) has the highest time quantum (200 millisec onds), and the highest priority (priority 59) has the lowest time quantum (20 milliseconds). Time quantum expired .T h en e wp r i o r i t yo fat h r e a dt h a th a su s e d its entire time quantum without blocking. Such threads are considered CPUintensive. As shown in the tab le, these threads have their priorities lowered. Return from sleep .T h ep r i o r i t yo fat h r e a dt h a ti sr e t u r n i n gf r o

t yo fat h r e a dt h a ti sr e t u r n i n gf r o ms l e e p i n g (such as from waiting for IO). As the table illustrates, when IOis available for a waiting thread, its priority is boosted to between 50 and 59, supporting the scheduling policy of providin gg o o dr e s p o n s et i m ef o ri n t e r a c t i v e processes. Threads in the realtime class are given the highest priority. A realtime process will run before a process in any other class. This assignment allows ar e a l  t i m ep r o c

This assignment allows ar e a l  t i m ep r o c e s st oh a v eag u a r a n t e e dr e s p o n s ef r o mt h es y s t e mw i t h i n ab o u n d e dp e r i o do ft i m e .I ng e n e r a l ,h o w e v e r ,f e wp r o c e s s e sb e l o n gt ot h e realtime class. Solaris uses the system class to run kernel threads, such as the scheduler and paging daemon. Once the priority of a system thread is established, it does not change. The system class is reserved for kernel use (user processes running in

reserved for kernel use (user processes running in kernel mode are not in the system class).6.7 OperatingSystem Examples 299 The xedpriority and fairshare classes were introduced with Solaris 9. Threads in the xedpriority class have the same priority range as those in the timesharing class; however, the ir priorities are not dynamically adjusted. The fairshare scheduling class uses CPU shares instead of priorities to make scheduling decisions. CPU shares indicate entitlement to available CPU

CPU shares indicate entitlement to available CPU resources and are allocated to a set of processes (known as a project ). Each scheduling class includes a set of priorities. However, the scheduler converts the classspecic priorities into global priorities and selects the thread with the highest global priority to run. The selected thread runs on the CPU until it (1) blocks, (2) uses its time slice, or (3) is preempted by a higherpriority thread. If there are multiple threads with the same

If there are multiple threads with the same priority, the scheduler uses ar o u n d  r o b i nq u e u e .F i g u r e6 . 2 4i l l u s t r a t e sh o wt h es i xs c h e d u l i n gc l a s s e s relate to one another and how they map to global priorities. Notice that the kernel maintains ten threads for servicin gi n t e r r u p t s .T h e s et h r e a d sd on o t belong to any scheduling class and execute at the highest priority (160169). As mentioned, Solaris has traditionally used the manytomany

Solaris has traditionally used the manytomany model (Section 4.3.3) but switched to the onetoone model (Section 4.3.2) beginning with Solaris 9. interrupt threads169highest lowestfirstscheduling orderglobal priority last160 159 100 60 59 099realtime (RT) threads system (SYS) threads fair share (FSS) threads fixed priority (FX) threads timeshare (TS) threads interactive (IA) threads Figure 6.24 Solaris scheduling.300 Chapter 6 CPU Scheduling 6.8 Algorithm Evaluation How do we select a

6.8 Algorithm Evaluation How do we select a CPUscheduling algorithm for a particular system? As we saw in Section 6.3, there are many scheduling algorithms, each with its own parameters. As a result, selecting an algorithm can be difcult. The rst problem is dening the criteria to be used in selecting an algorithm. As we saw in Section 6.2, criteria are often dened in terms of CPU utilization, response time, or throughput. To select an algorithm, we must rst dene the relative importance of these

we must rst dene the relative importance of these elements. Our criteria may include several measures, such as these: Maximizing CPU utilization under the constraint that the maximum response time is 1 second Maximizing throughput such that turnaround time is (on average) linearly proportional to total execution time Once the selection criteria have been dened, we want to evaluate the algorithms under consideration. We next describe the various evaluation methods we can use. 6.8.1 Deterministic

evaluation methods we can use. 6.8.1 Deterministic Modeling One major class of evaluation methods is analytic evaluation .A n a l y t i c evaluation uses the given algorithm and the system workload to produce af o r m u l ao rn u m b e rt oe v a l u a t et h ep e r f o r m a n c eo ft h ea l g o r i t h mf o rt h a t workload. Deterministic modeling is one type of analytic evaluation. This method takes a particular predetermined workload and denes the performance of each algorithm for that

denes the performance of each algorithm for that workload. For example, assume that we have the workload shown below. All ve processes arrive at time 0, in the order given, with the length of the CPU burst given in milliseconds: Process Burst Time P1 10 P2 29 P3 3 P4 7 P5 12 Consider the FCFS ,SJF,a n d RR(quantum  10 milliseconds) scheduling algorithms for this set of processes. Which algorithm would give the minimum average waiting time? For the FCFS algorithm, we would execute the processes

the FCFS algorithm, we would execute the processes as P2P5P3P4P1 61 39 49 42 0 106.8 Algorithm Evaluation 301 The waiting time is 0 milliseconds for process P1,1 0m i l l i s e c o n d sf o rp r o c e s s P2,3 9m i l l i s e c o n d sf o rp r o c e s s P3,4 2m i l l i s e c o n d sf o rp r o c e s s P4,a n d4 9 milliseconds for process P5. Thus, the average waiting time is (0  10  39 4 24 9 )  52 8m i l l i s e c o n d s . With nonpreemptive SJFscheduling, we execute the processes as P5P2P3P4 61

we execute the processes as P5P2P3P4 61 32 20 10 03P1 The waiting time is 10 milliseconds for process P1,3 2m i l l i s e c o n d sf o rp r o c e s s P2,0m i l l i s e c o n d sf o rp r o c e s s P3,3m i l l i s e c o n d sf o rp r o c e s s P4,a n d2 0 milliseconds for process P5. Thus, the average waiting time is (10  32  0 32 0 )  51 3m i l l i s e c o n d s . With the RRalgorithm, we execute the processes as P5P5P2P2P2P3P4 61 30 40 50 52 20 23 10 0P1 The waiting time is 0 milliseconds for

23 10 0P1 The waiting time is 0 milliseconds for process P1,3 2m i l l i s e c o n d sf o rp r o c e s s P2,2 0m i l l i s e c o n d sf o rp r o c e s s P3,2 3m i l l i s e c o n d sf o rp r o c e s s P4,a n d4 0 milliseconds for process P5. Thus, the average waiting time is (0  32  20 2 34 0 )  52 3m i l l i s e c o n d s . We can see that, in this case, the average waiting time obtained with the SJF policy is less than half that obtained with FCFS scheduling; the RRalgorithm gives us an

with FCFS scheduling; the RRalgorithm gives us an intermediate value. Deterministic modeling is simple and fast. It gives us exact numbers, allowing us to compare the algorithms. However, it requires exact numbers for input, and its answers apply only to those cases. The main uses of deterministic modeling are in describing scheduling algorithms and providing examples. In cases where we are running the same program over and over again and can measure the programs processing requirements exactly,

the programs processing requirements exactly, we may be able to use deterministic modeling to select a scheduling algorithm. Furthermore, over a set of examples, deterministic modeling may indicate trends that can then be analyzed and proved separately. For example, it can be shown that, for the environment described (all proc esses and their times available at time 0), the SJFpolicy will always result in the minimum waiting time. 6.8.2 Queueing Models On many systems, the processes that are run

Models On many systems, the processes that are run vary from day to day, so there is no static set of processes (or times) to use for deterministic modeling. What can be determined, however, is the distribution of CPU and IObursts. These distributions can be measured and then approximated or simply estimated. The result is a mathematical formula describing the probability of a particular CPU burst. Commonly, this distribution is exponential and is described by its mean. Similarly, we can

and is described by its mean. Similarly, we can describe the distribution of times when processes arrive in the system (the arrivaltime distribution). From these two distributions, it is302 Chapter 6 CPU Scheduling possible to compute the average throughput, utilization, waiting time, and so on for most algorithms. The computer system is described as a network of servers. Each server has a queue of waiting processes. The CPU is a server with its ready queue, as is theIOsystem with its device

its ready queue, as is theIOsystem with its device queues. Knowing arrival rates and service rates, we can compute utilization, average queue length, average wait time, and so on. This area of study is called queueingnetwork analysis . As an example, let nbe the average queue length (excluding the process being serviced), let Wbe the average waiting time in the queue, and let H9261be the average arrival rate for new processes in the queue (such as three processes per second). We expect that

as three processes per second). We expect that during the time Wthat a process waits, H9261W new processes will arrive in the queue. If the system is in a steady state, then the number of processes leaving the queue must be equal to the number of processes that arrive. Thus, nH9261W. This equation, known as Littles formula ,i sp a r t i c u l a r l yu s e f u lb e c a u s ei ti s valid for any scheduling algorithm and arrival distribution. We can use Littles formula to compute one of the three

use Littles formula to compute one of the three variables if we know the other two. For example, if we know that 7 processes arrive every second (on average) and that there are normally 14 processes in the queue, then we can compute the average waiting time per process as 2 seconds. Queueing analysis can be useful in comparing scheduling algorithms, but it also has limitations. At the moment, the classes of algorithms and distributions that can be handled are fairly limited. The mathematics of

be handled are fairly limited. The mathematics of complicated algorithms and distributions can be difcult to work with. Thus, arrival and service distributions are often dened in mathematically tractable but unrealisticways. It is also generally necessary to make a number of independent assumptions, which may not be accurate. As a result of these difculties, queueing models are often only approximations of real systems, and the accuracy of the computed results may be questionable. 6.8.3

of the computed results may be questionable. 6.8.3 Simulations To get a more accurate evaluation of scheduling algorithms, we can use simulations. Running simulations involves programming a model of the computer system. Software data structures represent the major components of the system. The simulator has a variable representing a clock. As this variables value is increased, the simulator modies the system state to reect the activities of the devices, the processes, and the scheduler. As the

devices, the processes, and the scheduler. As the simulation executes, statistics that indicate a lgorithm performance are gathered and printed. The data to drive the simulation can be generated in several ways. The most common method uses a randomnumber generator that is programmed to generate processes, CPU burst times, arrivals, departures, and so on, according to probability distributions. The distributions can be dened mathematically (uniform, exponential, Poisson) or empirically. If a

exponential, Poisson) or empirically. If a distribution is to be dened empirically, measurements of the actual system under study are taken. The results dene the distribution of events in the real system; this distribution can then be used to drive the simulation.6.8 Algorithm Evaluation 303 actual process executionperformance statistics for FCFSsimulation FCFS performance statistics for SJF performance statistics for RR ( q H11005 14)trace tapesimulation SJF simulation RR (q H11005 14)   CPU 10

SJF simulation RR (q H11005 14)   CPU 10 IO 213 CPU 12 IO 112 CPU 2 IO 147 CPU 173    Figure 6.25 Evaluation of CPU schedulers by simulation. Ad i s t r i b u t i o n  d r i v e ns i m u l a t i o nm a yb ei n a c c u r a t e ,h o w e v e r ,b e c a u s eo f relationships between successive events in the real system. The frequency distribution indicates only how many in stances of each event occur; it does not indicate anything about the order of their occurrence. To correct this problem, we can

their occurrence. To correct this problem, we can use trace tapes .W ec r e a t eat r a c et a p eb ym o n i t o r i n gt h er e a ls y s t e ma n d recording the sequence of actual events (Figure 6.25). We then use this sequence to drive the simulation. Trace tapes provide an excellent way to compare two algorithms on exactly the same set of real inputs. This method can produce accurate results for its inputs. Simulations can be expensive, often requiring hours of computer time. A more detailed

requiring hours of computer time. A more detailed simulation provides more accurate results, but it also takes more computer time. In addition, trace tapes can require large amounts of storage space. Finally, the design, coding, and debugging of the simulator can be a major task. 6.8.4 Implementation Even a simulation is of limited accuracy. The only completely accurate way to evaluate a scheduling algorithm is to code it up, put it in the operating system, and see how it works. This approach

system, and see how it works. This approach puts the actual algorithm in the real system for evaluation under real operating conditions. The major difculty with this approach is the high cost. The expense is incurred not only in coding the algorithm and modifying the operating system to support it (along with its required data structures) but also in the reaction of the users to a constantly changing operating system. Most users are not interested in building a better operating system; they

in building a better operating system; they merely want to get their processes executed and use their results. A constantly changing operating system does not help the users to get their work done. Another difculty is that the environment in which the algorithm is used will change. The environment will change not only in the usual way, as new304 Chapter 6 CPU Scheduling programs are written and the types of problems change, but also as a result of the performance of the scheduler. If short

of the performance of the scheduler. If short processes are given priority, then users may break larger processes into sets of smaller processes. If interactive processes are given priority over noninteractive processes, then users may switch to interactive use. For example, researchers designed one system that classied interactive and noninteractive processes automatically by looking at the amount of terminal IO. If a process did not input or output to the terminal in a 1second interval, the

output to the terminal in a 1second interval, the process was classied as noninteractive and was moved to a lowerpriority queue. In response to this policy, one programmer modied his programs to write an arbitrary character to the terminal at regular intervals of less than 1 second. The system gave his programs a high priority, even though the terminal output was completely meaningless. The most exible scheduling algorithms are those that can be altered by the system managers or by the users so

altered by the system managers or by the users so that they can be tuned for as p e c i  ca p p l i c a t i o no rs e to fa p p l i c a t i o n s .Aw o r k s t a t i o nt h a tp e r f o r m s highend graphical applications, for instance, may have scheduling needs different from those of a Web server or le server. Some operating systems particularly several versions of UNIX allow the system manager to netune the scheduling parameters for a particular system conguration. For example, Solaris

system conguration. For example, Solaris provides the dispadmin command to allow the system administrator to modify the parameters of the scheduling classes described in Section 6.7.3. Another approach is to use APIs that can modify the priority of a process or thread. The Java, POSIX ,a n dW i n d o w s API provide such functions. The downfall of this approach is that performancetuning a system or application most often does not result in improved performance in more general situations. 6.9

performance in more general situations. 6.9 Summary CPU scheduling is the task of selecting a waiting process from the ready queue and allocating the CPU to it. The CPU is allocated to the selected process by the dispatcher. Firstcome, rstserved ( FCFS )s c h e d u l i n gi st h es i m p l e s ts c h e d u l i n ga l g o  rithm, but it can cause short processes to wait for very long processes. Shortest jobrst ( SJF)s c h e d u l i n gi sp r o v a b l yo p t i m a l ,p r o v i d i n gt h es h o r

b l yo p t i m a l ,p r o v i d i n gt h es h o r t e s ta v e r a g e waiting time. Implementing SJFscheduling is difcult, however, because pre dicting the length of the next CPU burst is difcult. The SJFalgorithm is a special case of the general priority scheduling algorithm, which simply allocates the CPU to the highestpriority process. Both priority and SJFscheduling may suffer from starvation. Aging is a technique to prevent starvation. Roundrobin ( RR)s c h e d u l i n gi sm o r ea p p r o

( RR)s c h e d u l i n gi sm o r ea p p r o p r i a t ef o rat i m e  s h a r e d( i n t e r  active) system. RRscheduling allocates the CPU to the rst process in the ready queue for qtime units, where qis the time quantum. After qtime units, if the process has not relinquished the CPU,i ti sp r e e m p t e d ,a n dt h ep r o c e s si s put at the tail of the ready queue. The major problem is the selection of the time quantum. If the quantum is too large, RRscheduling degenerates to FCFS

is too large, RRscheduling degenerates to FCFS scheduling. If the quantum is too small, scheduling overhead in the form of contextswitch time becomes excessive.Practice Exercises 305 The FCFS algorithm is nonpreemptive; the RRalgorithm is preemptive. The SJFand priority algorithms may be either preemptive or nonpreemptive. Multilevel queue algorithms allow different algorithms to be used for different classes of processes. The most common model includes a foreground interactive queue that uses

includes a foreground interactive queue that uses RRscheduling and a background batch queue that uses FCFS scheduling. Multilevel feedback queues allow processes to move from one queue to another. Many contemporary computer systems support multiple processors and allow each processor to schedule itself independently. Typically, each processor maintains its own private queue of processes (or threads), all of which are available to run. Additional issues related to multiprocessor scheduling

issues related to multiprocessor scheduling include processor afnity, load balancing, and multicore processing. Ar e a l  t i m ec o m p u t e rs y s t e mr e q u i r e st h a tr e s u l t sa r r i v ew i t h i nad e a d l i n e period; results arriving after the deadline has passed are useless. Hard realtime systems must guarantee that realtime tasks are serviced within their deadline periods. Soft realtime systems are less restrictive, assigning realtime tasks higher scheduling priority than

realtime tasks higher scheduling priority than other tasks. Realtime scheduling algorithms include ratemonotonic and earliest deadlinerst scheduling. Ratemonotonic scheduling assigns tasks that require the CPU more often a higher priority than tasks that require the CPU less often. Earliestdeadlinerst scheduling assigns priority according to upcoming deadlinesthe earlier the deadline, the higher the priority. Proportional share scheduling divides up processor time into shares and assigning each

up processor time into shares and assigning each process a number of shares, thus guaranteeing each process ap r o p o r t i o n a ls h a r eo f CPU time. The POSIX Pthread API provides various features for scheduling realtime threads as well. Operating systems supporting threads at the kernel level must schedule threadsnot processesfor execution. This is the case with Solaris and Windows. Both of these systems schedule threads using preemptive, priority based scheduling algorithms, including

priority based scheduling algorithms, including support for realtime threads. The Linux process scheduler uses a prioritybased algorithm with realtime support as well. The scheduling algorithms for these three operating systems typically favor interactive over CPUbound processes. The wide variety of scheduling algorithms demands that we have methods to select among algorithms. Analytic methods use mathematical analysis to determine the performance of an algorithm. Simulation methods determine

of an algorithm. Simulation methods determine performance by imitating the scheduling algorithm on a representative  sample of processes and computing the resulting performance. However, simulation can at best provide an approximation of actual system performance. The only reliable technique for evaluating a scheduling algorithm is to implement the algorithm on an actual system and monitor its performance in a realworld environment. Practice Exercises 6.1 ACPUscheduling algorithm determines an

6.1 ACPUscheduling algorithm determines an order for the execution of its scheduled processes. Given nprocesses to be scheduled on one processor, how many different schedules are possible? Give a formula in terms of n.306 Chapter 6 CPU Scheduling 6.2 Explain the difference between preemptive and nonpreemptive schedul ing. 6.3 Suppose that the following processes arrive for execution at the times indicated. Each process will run for the amount of time listed. In answering the questions, use nonpr

time listed. In answering the questions, use nonpr eemptive scheduling, and base all decisions on the information you have at the time the decision must be made. Process Arrival Time Burst Time P1 0.0 8 P2 0.4 4 P3 1.0 1 a. What is the average turnaround time for these processes with the FCFS scheduling algorithm? b. What is the average turnaround time for these processes with the SJFscheduling algorithm? c. The SJFalgorithm is supposed to improve performance, but notice that we chose to run

performance, but notice that we chose to run process P1at time 0 because we did not know that two shorter processes would arrive soon. Compute what the average turnaround time will be if the CPU is left idle for the rst 1u n i ta n dt h e n SJFscheduling is used. Remember that processes P1and P2are waiting during this idle time, so their waiting time may increase. This algorithm could be called futureknowledge scheduling. 6.4 What advantage is there in having different timequantum sizes at

is there in having different timequantum sizes at different levels of a multilevel queueing system? 6.5 Many CPUscheduling algorithms are parameterized. For example, the RRalgorithm requires a parameter to indicate the time slice. Multilevel feedback queues require parameters to dene the number of queues, the scheduling algorithm for each queue, the criteria used to move processes between queues, and so on. These algorithms are thus really sets of algorithms (for example, the set of RRalgorithms

algorithms (for example, the set of RRalgorithms for all time slices, and so on). One set of algorithms may include another (for example, the FCFS algorithm is the RRalgorithm with an innite time quantum). What (if any) relation holds between the following pairs of algorithm sets? a. Priority and SJF b. Multilevel feedback queues and FCFS c. Priority and FCFS d. RRand SJF 6.6 Suppose that a scheduling algorith m( a tt h el e v e lo fs h o r t  t e r m CPU scheduling) favors those processes that

e r m CPU scheduling) favors those processes that have used the least processorExercises 307 time in the recent past. Why will this algorithm favor IObound programs and yet not permanently starve CPUbound programs? 6.7 Distinguish between PCSand SCSscheduling. 6.8 Assume that an operating system maps userlevel threads to the kernel using the manytomany model and that the mapping is done through the use of LWPs. Furthermore, the system allows program developers to create realtime threads. Is it

developers to create realtime threads. Is it necessary to bind a realtime thread to an LWP? 6.9 The traditional UNIX scheduler enforces an inverse relationship between priority numbers and priorities: the higher the number, the lower the priority. The scheduler recalculates process priorities once per second using the following function: Priority  (recent CPU usage  2)  base where base  60 and recent CPU usage refers to a value indicating how often a process has used the CPU since priorities

often a process has used the CPU since priorities were last recalculated. Assume that recent CPU usage is 40 for process P1, 18 for process P2, and 10 for process P3.W h a tw i l lb et h en e wp r i o r i t i e sf o rt h e s et h r e e processes when priorities are recalculated? Based on this information, does the traditional UNIX scheduler raise or lower the relative priority of a CPUbound process? Exercises 6.10 Why is it important for the scheduler to distinguish IObound programs from

the scheduler to distinguish IObound programs from CPUbound programs? 6.11 Discuss how the following pairs of scheduling criteria conict in certain settings. a. CPU utilization and response time b. Average turnaround time and maximum waiting time c. IOdevice utilization and CPU utilization 6.12 One technique for implementing lottery scheduling works by assigning processes lottery tickets, which are used for allocating CPU time. When ever a scheduling decision has to be made, a lottery ticket is

decision has to be made, a lottery ticket is chosen at random, and the process holding that ticket gets the CPU.T h e BTV operating system implements lottery scheduling by holding a lottery 50 times each second, with each lottery winner getting 20 milliseconds ofCPU time (20 milliseconds 501s e c o n d ) .D e s c r i b eh o wt h e BTV scheduler can ensure that higherpriority threads receive more attention from the CPU than lowerpriority threads. 6.13 In Chapter 5, we discussed possible race

6.13 In Chapter 5, we discussed possible race conditions on various kernel data structures. Most scheduling algorithms maintain a run queue , which lists processes eligible to run on a processor. On multicore systems, there are two general options: (1) each processing core has its own run308 Chapter 6 CPU Scheduling queue, or (2) a single run queue is shared by all processing cores. What are the advantages and disadvantages of each of these approaches? 6.14 Consider the exponential average

approaches? 6.14 Consider the exponential average formula used to predict the length of the next CPU burst. What are the implications of assigning the following values to the parameters used by the algorithm? a.H92510a n d H92700100 milliseconds b. H92510.99 and H9270010 milliseconds 6.15 Av a r i a t i o no ft h er o u n d  r o b i ns c h e d u l e ri st h e regressive roundrobin scheduler. This scheduler assigns each process a time quantum and a priority. The initial value of a time quantum is

a priority. The initial value of a time quantum is 50 milliseconds. However, every time a process has been allocated the CPU and uses its entire time quantum (does not block for IO), 10 milliseconds is added to its time quantum, and its priority level is boosted. (The time quantum for a process can be increased to a maximum of 100 milliseconds.) When a process blocks before using its entire time quantum, its time quantum is reduced by 5 milliseconds, but its priority remains the same. What type

but its priority remains the same. What type of process ( CPUbound or IObound) does the regressive roundrobin scheduler favor? Explain. 6.16 Consider the following set of processes, with the length of the CPU burst given in milliseconds: Process Burst Time Priority P1 22 P2 11 P3 84 P4 42 P5 53 The processes are assumed to have arrived in the order P1,P2,P3,P4,P5, all at time 0. a. Draw four Gantt charts that illustrate the execution of these processes using the following scheduling algorithms:

using the following scheduling algorithms: FCFS ,SJF, nonpreemptive priority (a larger priority number implies a higher priority), and RR(quantum  2). b. What is the turnaround time of each process for each of the scheduling algorithms in part a? c. What is the waiting time of each process for each of these schedul ing algorithms? d. Which of the algorithms results in the minimum average waiting time (over all processes)? 6.17 The following processes are bei ng scheduled using a preemptive,

processes are bei ng scheduled using a preemptive, round robin scheduling algorithm. Each process is assigned a numerical priority, with a higher number indicating a higher relative priority. In addition to the processes listed below, the system also has an idleExercises 309 task (which consumes no CPU resources and is identied as Pidle). This task has priority 0 and is scheduled whenever the system has no other available processes to run. The length of a time quantum is 10 units. If a process

length of a time quantum is 10 units. If a process is preempted by a higherpriority process, the preempted process is placed at the end of the queue. Thread Priority Burst Arrival P1 40 20 0 P2 30 25 25 P3 30 25 30 P4 35 15 60 P5 5 10 100 P6 10 10 105 a. Show the scheduling order of the processes using a Gantt chart. b. What is the turnaround time for each process? c. What is the waiting time for each process? d. What is the CPU utilization rate? 6.18 Thenice command is used to set the nice

rate? 6.18 Thenice command is used to set the nice value of a process on Linux, as well as on other UNIX systems. Explain why some systems may allow any user to assign a process a nice value 0y e ta l l o wo n l yt h er o o t user to assign nice values 0. 6.19 Which of the following scheduling algorithms could result in starvation? a. Firstcome, rstserved b. Shortest job rst c. Round robin d. Priority 6.20 Consider a variant of the RRscheduling algorithm in which the entries in the ready queue

algorithm in which the entries in the ready queue are pointers to the PCBs. a. What would be the effect of putting two pointers to the same process in the ready queue? b. What would be two major advantages and two disadvantages of this scheme? c. How would you modify the basic RRalgorithm to achieve the same effect without the duplicate pointers? 6.21 Consider a system running ten IObound tasks and one CPUbound task. Assume that the IObound tasks issue an IOoperation once for every millisecond

issue an IOoperation once for every millisecond of CPU computing and that each IOoperation takes 10 milliseconds to complete. Also assume that the contextswitching overhead is 0.1 millisecond and that all processes are longrunning tasks. Describe the CPU utilization for a roundrobin scheduler when:310 Chapter 6 CPU Scheduling a. The time quantum is 1 millisecond b. The time quantum is 10 milliseconds 6.22 Consider a system implementing multilevel queue scheduling. What strategy can a computer

queue scheduling. What strategy can a computer user employ to maximize the amount of CPU time allocated to the users process? 6.23 Consider a preemptive priority scheduling algorithm based on dynami cally changing priorities. Larger priority numbers imply higher priority. When a process is waiting for the CPU (in the ready queue, but not running), its priority changes at a rate H9251.W h e ni ti sr u n n i n g ,i t sp r i o r i t y changes at a rate H9252.A l lp r o c e s s e sa r eg i v e nap r

H9252.A l lp r o c e s s e sa r eg i v e nap r i o r i t yo f0w h e nt h e y enter the ready queue. The parameters H9251and H9252can be set to give many different scheduling algorithms. a. What is the algorithm that results from H9252H92510? b. What is the algorithm that results from H9251H92520? 6.24 Explain the differences in how much the following scheduling algo rithms discriminate in favor of short processes: a. FCFS b. RR c. Multilevel feedback queues 6.25 Using the Windows scheduling

feedback queues 6.25 Using the Windows scheduling algorithm, determine the numeric priority of each of the following threads. a. A thread in the REALTIME PRIORITY CLASS with a relative priority ofNORMAL b. A thread in the ABOVE NORMAL PRIORITY CLASS with a relative priority of HIGHEST c. A thread in the BELOW NORMAL PRIORITY CLASS with a relative priority of ABOVE NORMAL 6.26 Assuming that no threads belong to the REALTIME PRIORITY CLASS and that none may be assigned a TIME CRITICAL priority,

none may be assigned a TIME CRITICAL priority, what combination of priority class and priority corresponds to the highest possible relative priority in Windows scheduling? 6.27 Consider the scheduling algorithm in the Solaris operating system for timesharing threads. a. What is the time quantum (in milliseconds) for a thread with priority 15? With priority 40? b. Assume that a thread with priority 50 has used its entire time quantum without blocking. What new priority will the scheduler assign

What new priority will the scheduler assign this thread? c. Assume that a thread with priority 20 blocks for IObefore its time quantum has expired. What new priority will the scheduler assign this thread?Bibliographical Notes 311 6.28 Assume that two tasks Aand Bare running on a Linux system. The nice values of Aand Bare5a n d 5, respectively. Using the CFSscheduler as ag u i d e ,d e s c r i b eh o wt h er e s p e c t i v ev a l u e so f vruntime vary between the two processes given each of the

vary between the two processes given each of the following scenarios: Both Aand BareCPUbound. AisIObound, and BisCPUbound. AisCPUbound, and BisIObound. 6.29 Discuss ways in which the priority inversion problem could be addressed in a realtime system. Also discuss whether the solutions could be implemented within the context of a proportional share sched uler. 6.30 Under what circumstances is ratemonotonic scheduling inferior to earliestdeadlinerst scheduling in meeting the deadlines associated

scheduling in meeting the deadlines associated with processes? 6.31 Consider two processes, P1and P2,w h e r e p150,t125,p275, and t230. a. Can these two processes be scheduled using ratemonotonic scheduling? Illustrate your answer using a Gantt chart such as the ones in Figure 6.16Figure 6.19. b. Illustrate the scheduling of these two processes using earliest deadlinerst ( EDF)s c h e d u l i n g . 6.32 Explain why interrupt and dispatch latency times must be bounded in ah a r dr e a l  t i m

times must be bounded in ah a r dr e a l  t i m es y s t e m . Bibliographical Notes Feedback queues were originally implemented on the CTSS system described in [Corbato et al. (1962)]. This feedback queue scheduling system was analyzed by [Schrage (1967)]. The preemptive priority scheduling algorithm of Exercise 6.23 was suggested by [Kleinrock (1975)]. The scheduling algorithms for hard real time systems, such as rate monotonic scheduling and earliestdeadlinerst scheduling, are presented in

earliestdeadlinerst scheduling, are presented in [Liu and Layland (1973)]. [Anderson et al. (1989)], [Lewis and Berg (1998)], and [Philbin et al. (1996)] discuss thread scheduling. Multicore scheduling is examined in [McNairy and Bhatia (2005)] and [Kongetira et al. (2005)]. [Fisher (1981)], [Hall et al. (1996)], and [Lowney et al. (1993)] describe scheduling techniques that take into account information regarding process execution times from previous runs. Fairshare schedulers are covered by

previous runs. Fairshare schedulers are covered by [Henry (1984)], [Woodside (1986)], and [Kay and Lauder (1988)]. Scheduling policies used in the UNIX V operating system are described by [Bach (1987)]; those for UNIX F reeBSD 5.2 are presented by [McKusick and NevilleNeil (2005)]; and those for the Mach operating system are discussed by [Black (1990)]. [Love (2010)] and [Mauerer (2008)] cover scheduling in312 Chapter 6 CPU Scheduling Linux. [Faggioli et al. (2009)] discuss adding an EDF

[Faggioli et al. (2009)] discuss adding an EDF scheduler to the Linux kernel. Details of the ULE scheduler can be found in [Roberson (2003)]. Solaris scheduling is described by [Mauro and McDougall (2007)]. [Russinovich and Solomon (2009)] discusses scheduling in Windows internals. [Butenhof (1997)] and [Lewis and Berg (1998)] describe scheduling in Pthreads systems. [Siddha et al. (2007)] discuss scheduling challenges on multicore systems. Bibliography [Anderson et al. (1989)] T. E. Anderson,

[Anderson et al. (1989)] T. E. Anderson, E. D. Lazowska, and H. M. Levy, The Performance Implications of Thread Management Alternatives for SharedMemory Multiprocessors ,IEEE Transactions on Computers ,V o l u m e3 8 , Number 12 (1989), pages 16311644. [Bach (1987)] M. J. Bach, The Design of the UNIX Operating System ,P r e n t i c eH a l l (1987). [Black (1990)] D. L. Black, Scheduling Support for Concurrency and Parallelism in the Mach Operating System ,IEEE Computer ,V o l u m e2 3 ,N u m b e

System ,IEEE Computer ,V o l u m e2 3 ,N u m b e r5( 1 9 9 0 ) , pages 3543. [Butenhof (1997)] D. Butenhof, Programming with POSIX Threads ,A d d i s o n  Wesley (1997). [Corbato et al. (1962)] F. J. Corbato, M. MerwinDaggett, and R. C. Daley, An Experimental TimeSharing System ,Proceedings of the AFIPS Fall Joint Computer Conference (1962), pages 335344. [Faggioli et al. (2009)] D. Faggioli, F. Checconi, M. Trimarchi, and C. Scordino, An EDF scheduling class for the Linux kernel ,Proceedings of

class for the Linux kernel ,Proceedings of the 11th RealTime Linux Workshop (2009). [Fisher (1981)] J. A. Fisher, Trace Scheduling: A Technique for Global Microcode Compaction ,IEEE Transactions on Computers ,V o l u m e3 0 ,N u m b e r7( 1 9 8 1 ) , pages 478490. [Hall et al. (1996)] L. Hall, D. Shmoys, and J. Wein, Scheduling To Minimize Average Completion Time: Offline and Online Algorithms ,SODA: ACM SIAM Symposium on Discrete Algorithms (1996). [Henry (1984)] G. Henry, The Fair Share

(1996). [Henry (1984)] G. Henry, The Fair Share Scheduler ,ATT Bell Laboratories Technical Journal (1984). [Kay and Lauder (1988)] J. Kay and P . Lauder, AF a i rS h a r eS c h e d u l e r ,Com munications of the ACM ,V o l u m e3 1 ,N u m b e r1( 1 9 8 8 ) ,p a g e s4 4  5 5 . [Kleinrock (1975)] L. Kleinrock, Queueing Systems, Volume II: Computer Applica tions,W i l e y  I n t e r s c i e n c e( 1 9 7 5 ) . [Kongetira et al. (2005)] P. K o n g e t i r a , K . A i n g a r a n , a n d K . O l u k

t i r a , K . A i n g a r a n , a n d K . O l u k o t u n , Niagara: A3 2  W a yM u l t i t h r e a d e dS P A R CP r o c e s s o r ,IEEE Micro Magazine ,V o l u m e2 5 , Number 2 (2005), pages 2129.Bibliography 313 [Lewis and Berg (1998)] B. Lewis and D. Berg, Multithreaded Programming with Pthreads ,S u nM i c r o s y s t e m sP r e s s( 1 9 9 8 ) . [Liu and Layland (1973)] C. L. Liu and J. W. Layland, Scheduling Algorithms for Multiprogramming in a Hard RealTime Environment ,Communications of

in a Hard RealTime Environment ,Communications of the ACM ,V o l u m e2 0 ,N u m b e r1( 1 9 7 3 ) ,p a g e s4 6  6 1 . [Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developers Library (2010). [Lowney et al. (1993)] P. G . L o w n e y, S . M . F re u d e n b e rg e r, T. J . K a r z e s , W. D . Lichtenstein, R. P . Nix, J. S. ODonnell, and J. C. Ruttenberg, The Multiow Trace Scheduling Compiler ,Journal of Supercomputing , Volume 7, Number 12 (1993), pages 51142. [Mauerer

Volume 7, Number 12 (1993), pages 51142. [Mauerer (2008)] W. Mauerer, Professional Linux Kernel Architecture ,J o h nW i l e y and Sons (2008). [Mauro and McDougall (2007)] J. Mauro and R. McDougall, Solaris Internals: Core Kernel Architecture ,P r e n t i c eH a l l( 2 0 0 7 ) . [McKusick and NevilleNeil (2005)] M. K. McKusick and G. V . NevilleNeil, The Design and Implementation of the FreeBSD UNIX Operating System ,A d d i s o n Wesley (2005). [McNairy and Bhatia (2005)] C. McNairy and R.

[McNairy and Bhatia (2005)] C. McNairy and R. Bhatia, Montecito: A Dual Core, DualThreaded Itanium Processor ,IEEE Micro Magazine ,V o l u m e2 5 , Number 2 (2005), pages 1020. [Philbin et al. (1996)] J. Philbin, J. Edler, O. J. Anshus, C. C. Douglas, and K. Li, Thread Scheduling for Cache Locality ,Architectural Support for Programming Languages and Operating Systems (1996), pages 6071. [Roberson (2003)] J. Roberson, ULE: A Modern Scheduler For FreeBSD , Proceedings of the USENIX BSDCon

For FreeBSD , Proceedings of the USENIX BSDCon Conference (2003), pages 1728. [Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon, Win dows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition, Microsoft Press (2009). [Schrage (1967)] L. E. Schrage, The Queue MGI with Feedback to Lower Priority Queues ,Management Science , Volume 13, (1967), pages 466474. [Siddha et al. (2007)] S. Siddha, V . Pallipadi, and A. Mallick, Process Schedul ing Challenges in the

A. Mallick, Process Schedul ing Challenges in the Era of MultiCore Processors ,Intel Technology Journal , Volume 11, Number 4 (2007). [Woodside (1986)] C. Woodside, Controllability of Computer Performance Tradeoffs Obtained Using ControlledShare Queue Schedulers ,IEEE Transac tions on Software Engineering ,V o l u m eS E  1 2 ,N u m b e r1 0( 1 9 8 6 ) ,p a g e s1 0 4 1  1 0 4 8 .7CHAPTER Deadlocks In a multiprogramming environment, several processes may compete for a nite number of resources. A

may compete for a nite number of resources. A process requests resources; if the resources are not available at that time, the process enters a waiting state. Sometimes, a waiting process is never again able to ch ange state, because the resources it has requested are held by other waiting processes. This situation is called adeadlock .W ed i s c u s s e dt h i si s s u eb r i e  yi nC h a p t e r5i nc o n n e c t i o nw i t h semaphores. Perhaps the best illustration of a deadlock can be drawn

the best illustration of a deadlock can be drawn from a law passed by the Kansas legislature early in the 20th century. It said, in part: When two trains approach each other at a crossing, both shall come to a full stop and neither shall start up again until the other has gone.  In this chapter, we describe methods that an operating system can use to prevent or deal with deadlocks. Although some applications can identify programs that may deadlock, operating systems typically do not provide

operating systems typically do not provide deadlockprevention facilities, and it remains the responsibility of program mers to ensure that they design deadlockfree programs. Deadlock problems can only become more common, given curr ent trends, including larger num bers of processes, multithreaded programs, many more resources within a system, and an emphasis on longlived le and database servers rather than batch systems. CHAPTER OBJECTIVES To develop a description of deadlocks, which prevent

develop a description of deadlocks, which prevent sets of concurrent processes from completing their tasks. To present a number of different methods for preventing or avoiding deadlocks in a computer system. 7.1 System Model A system consists of a nite number of resources to be distributed among a number of competing processes. The resources may be partitioned into several 315316 Chapter 7 Deadlocks types (or classes), each consisting of some number of identical instances. CPU cycles, les, and

of identical instances. CPU cycles, les, and IOdevices (such as printers and DVD drives) are examples of resource types. If a system has two CPUs, then the resource type CPU has two instances. Similarly, the resource type printer may have ve instances. If a process requests an instance of a resource type, the allocation of any instance of the type should satisfy the request. If it does not, then the instances are not identical, and the resource type classes have not been dened properly. For

type classes have not been dened properly. For example, a system may have two printers. These two printers may be dened to be in the same resource class if no one cares which printer prints which output. However, if one printer is on the ninth oor and the other is in the basement, then people on the ninth oor may not see both printers as equivalent, and separate resource classes may need to be dened for each printer. Chapter 5 discussed various synchronization tools, such as mutex locks and

synchronization tools, such as mutex locks and semaphores. These tools are also considered system resources, and they are a common source of deadlock. However, a lock is typically associated with protecting a specic data structurethat is, one lock may be used to protect access to a queue, another to protect access to a linked list, and so forth. For that reason, each lock is typically assigned its own resource class, and denition is not a problem. Ap r o c e s sm u s tr e q u e s tar e s o u r c

Ap r o c e s sm u s tr e q u e s tar e s o u r c eb e f o r eu s i n gi ta n dm u s tr e l e a s et h e resource after using it. A process may request as many resources as it requires to carry out its designated task. Obviously, the number of resources requested may not exceed the total number of resources available in the system. In other words, a process cannot request three printers if the system has only two. Under the normal mode of operation, a process may utilize a resource in only the

a process may utilize a resource in only the following sequence: 1.Request .T h ep r o c e s sr e q u e s t st h er e s o u r c e .I ft h er e q u e s tc a n n o tb e granted immediately (for example, if the resource is being used by another process), then the requesting process must wait until it can acquire the resource. 2.Use.T h ep r o c e s sc a no p e r a t eo nt h er e s o u r c e( f o re x a m p l e ,i ft h er e s o u r c e is a printer, the process can print on the printer). 3.Release

the process can print on the printer). 3.Release .T h ep r o c e s sr e l e a s e st h er e s o u r c e . The request and release of resources may be system calls, as explained in Chapter 2. Examples are the request() and release() device, open() and close() le, and allocate() and free() memory system calls. Similarly, as we saw in Chapter 5, the request and release of semaphores can be accomplished through the wait() and signal() operations on semaphores or through acquire() and release() of a

semaphores or through acquire() and release() of a mutex lock. For each use of a kernelmanaged resource by a process or thread, the operating system checks to make sure that the process has requested and has been allocated the resource. As y s t e mt a b l er e c o r d sw h e t h e re a c hr e s o u r c ei sf r e eo ra l l o c a t e d .F o re a c h resource that is allocated, the table also records the process to which it is allocated. If a process requests a resource that is currently allocated

requests a resource that is currently allocated to another process, it can be added to a queue of processes waiting for this resource. As e to fp r o c e s s e si si nad e a d l o c k e ds t a t ew h e ne v e r yp r o c e s si nt h es e ti s waiting for an event that can be caused only by another process in the set. The7.2 Deadlock Characterization 317 events with which we are mainly conc erned here are resource acquisition and release. The resources may be either physical resources (for

resources may be either physical resources (for example, printers, tape drives, memory space, and CPU cycles) or logical resources (for example, semaphores, mutex locks, and les). However, other types of events may result in deadlocks (for example, the IPCfacilities discussed in Chapter 3). To illustrate a deadlocked state, consider a system with three CD RW drives. Suppose each of three processes holds one of these CD RW drives. If each process now requests another drive, the three processes

now requests another drive, the three processes will be in a deadlocked state. Each is waiting for the event CD RW is released, which can be caused only by one of the other waiting processes. This example illustrates a deadlock involving the same resource type. Deadlocks may also involve different resource types. For example, consider as y s t e mw i t ho n ep r i n t e ra n do n e DVD drive. Suppose that process Piis holding the DVD and process Pjis holding the printer. If Pirequests the

Pjis holding the printer. If Pirequests the printer and Pj requests the DVD drive, a deadlock occurs. Developers of multithreaded applications must remain aware of the possibility of deadlocks. The locking tools presented in Chapter 5 are designed to avoid race conditions. However, in using these tools, developers must pay careful attention to how locks are acquired and released. Otherwise, deadlock can occur, as illustrated in the diningphilosophers problem in Section 5.7.3. 7.2 Deadlock

problem in Section 5.7.3. 7.2 Deadlock Characterization In a deadlock, processes never nish executing, and system resources are tied up, preventing other jobs from starting. Before we discuss the various methods for dealing with the deadlock problem, we look more closely at features that characterize deadlocks. DEADLOCK WITH MUTEX LOCKS Lets see how deadlock can occur in a multithreaded Pthread program using mutex locks. The pthread mutex init() function initializes an unlocked mutex. Mutex

function initializes an unlocked mutex. Mutex locks are acquired and released using pthread mutex lock() and pthread mutex unlock() ,r e s p e c  tively. If a thread attempts to acquire a locked mutex, the call to pthread mutex lock() blocks the thread until the owner of the mutex lock invokes pthread mutex unlock() . Two mutex locks are created in the following code example:  Create and initialize the mutex locks  pthread mutex tf i r s t mutex; pthread mutex ts e c o n d mutex; pthread mutex

pthread mutex ts e c o n d mutex; pthread mutex init(first mutex,NULL); pthread mutex init(second mutex,NULL); Next, two threads thread one andthread twoare created, and both these threads have access to both mutex locks. thread oneandthread two318 Chapter 7 Deadlocks DEADLOCK WITH MUTEX LOCKS (Continued) run in the functions do work one() and do work two() ,r e s p e c t i v e l y ,a s shown below:  thread one runs in this function  void do work one(void param)  pthread mutex lock(first mutex);

one(void param)  pthread mutex lock(first mutex); pthread mutex lock(second mutex);  D os o m ew o r k  pthread mutex unlock(second mutex); pthread mutex unlock(first mutex); pthread exit(0);   thread two runs in this function  void do work two(void param)  pthread mutex lock(second mutex); pthread mutex lock(first mutex);  D os o m ew o r k  pthread mutex unlock(first mutex); pthread mutex unlock(second mutex); pthread exit(0);  In this example, thread oneattempts to acquire the mutex locks in

thread oneattempts to acquire the mutex locks in the order (1)first mutex ,( 2 ) second mutex ,w h i l e thread two attempts to acquire the mutex locks in the order (1) second mutex , (2) first mutex .D e a d l o c k is possible if thread oneacquires first mutex while thread twoacquires second mutex . Note that, even though deadlock is possible, it will not occur if thread one can acquire and release the mutex locks for first mutex andsecond mutex before thread two attempts to acquire the locks.

before thread two attempts to acquire the locks. And, of course, the order in which the threads run depends on how they are scheduled by the CPU scheduler. This example illustrates a problem with handling deadlocks: it is difcult to identify and test for deadlocks that may occur only under certain scheduling circumstances. 7.2.1 Necessary Conditions Ad e a d l o c ks i t u a t i o nc a na r i s ei ft h ef o l l o w i n gf o u rc o n d i t i o n sh o l ds i m u l t a n e  ously in a system:7.2

n sh o l ds i m u l t a n e  ously in a system:7.2 Deadlock Characterization 319 1.Mutual exclusion .A tl e a s to n er e s o u r c em u s tb eh e l di nan o n s h a r a b l e mode; that is, only one process at a time can use the resource. If another process requests that resource, the requesting process must be delayed until the resource has been released. 2.Hold and wait .Ap r o c e s sm u s tb eh o l d i n ga tl e a s to n er e s o u r c ea n d waiting to acquire additional resources that are

d waiting to acquire additional resources that are currently being held by other processes. 3.No preemption . Resources cannot be preempted; that is, a resource can be released only voluntarily by the process holding it, after that process has completed its task. 4.Circular wait .As e t P0,P1,. . . , Pnof waiting processes must exist such that P0is waiting for a resource held by P1,P1is waiting for a resource held by P2,. . . ,Pn1is waiting for a resource held by Pn,a n d Pnis waiting for a

for a resource held by Pn,a n d Pnis waiting for a resource held by P0. We emphasize that all four conditions must hold for a deadlock to occur. The circularwait condition implies the holdandwait condition, so the four conditions are not completely independent. We shall see in Section 7.4, however, that it is useful to consider each condition separately. 7.2.2 ResourceAllocation Graph Deadlocks can be described more precisely in terms of a directed graph called asystem resourceallocation graph

graph called asystem resourceallocation graph .T h i sg r a p hc o n s i s t so fas e to fv e r t i c e s V and a set of edges E.T h es e to fv e r t i c e s Vis partitioned into two different types of nodes: PP1,P2,. . . ,Pn,t h es e tc o n s i s t i n go fa l lt h ea c t i v ep r o c e s s e si nt h e system, and RR1,R2,. . . ,Rm,t h es e tc o n s i s t i n go fa l lr e s o u r c et y p e si nt h e system. Ad i r e c t e de d g ef r o mp r o c e s s Pito resource type Rjis denoted by PiRj; it

e s s Pito resource type Rjis denoted by PiRj; it signies that process Pihas requested an instance of resource type Rjand is currently waiting for that resource. A directed edge from resource type Rj to process Piis denoted by RjPi;i ts i g n i  e st h a ta ni n s t a n c eo fr e s o u r c e type Rjhas been allocated to process Pi.Ad i r e c t e de d g e PiRjis called a request edge ;ad i r e c t e de d g e RjPiis called an assignment edge . Pictorially, we represent each process Pias a circle

we represent each process Pias a circle and each resource type Rjas a rectangle. Since resource type Rjmay have more than one instance, we represent each such instance as a dot within the rectangle. Note that a request edge points to only the rectangle Rj, whereas an assignment edge must also designate one of the dots in the rectangle. When process Pirequests an instance of resource type Rj,ar e q u e s te d g e is inserted in the resourceallocation graph. When this request can be fullled, the

graph. When this request can be fullled, the request edge is instantaneously transformed to an assignment edge. When the process no longer needs access to the resource, it releases the resource. As ar e s u l t ,t h ea s s i g n m e n te d g ei sd e l e t e d . The resourceallocation graph shown in Figure 7.1 depicts the following situation. The sets P, R, and E: PP1,P2,P3320 Chapter 7 DeadlocksR1 R3 R2 R4P3 P2 P1 Figure 7.1 Resourceallocation graph. RR1,R2,R3,R4 EP1R1,P2R3,R1P2,R2P2,R2P1,R3P3

graph. RR1,R2,R3,R4 EP1R1,P2R3,R1P2,R2P2,R2P1,R3P3 Resource instances: One instance of resource type R1 Two instances of resource type R2 One instance of resource type R3 Three instances of resource type R4 Process states: Process P1is holding an instance of resource type R2and is waiting for an instance of resource type R1. Process P2is holding an instance of R1and an instance of R2and is waiting for an instance of R3. Process P3is holding an instance of R3. Given the denition of a

holding an instance of R3. Given the denition of a resourceallocation graph, it can be shown that, if the graph contains no cycles, then no process in the system is deadlocked. If the graph does contain a cycle, then a deadlock may exist. If each resource type has exactly one instance, then a cycle implies that a deadlock has occurred. If the cycle involves only a set of resource types, each of which has only a single instance, then a deadlock has occurred. Each process involved in the cycle is

occurred. Each process involved in the cycle is deadlocked. In this case, a cycle in the graph is both a necessary and a sufcient condition for the existence of deadlock. If each resource type has several instances, then a cycle does not necessarily imply that a deadlock has occurred. In this case, a cycle in the graph is a necessary but not a sufcient condition for the existence of deadlock. To illustrate this concept, we return to the resourceallocation graph depicted in Figure 7.1. Suppose

graph depicted in Figure 7.1. Suppose that process P3requests an instance of resource7.2 Deadlock Characterization 321R1 R3 R2 R4P3 P2 P1 Figure 7.2 Resourceallocation graph with a deadlock. type R2.S i n c en or e s o u r c ei n s t a n c ei sc u r r e n t l ya v a i l a b l e ,w ea d dar e q u e s te d g e P3R2to the graph (Figure 7.2). At this point, two minimal cycles exist in the system: P1R1P2R3P3R2P1 P2R3P3R2P2 Processes P1,P2,a n d P3are deadlocked. Process P2is waiting for the resource

deadlocked. Process P2is waiting for the resource R3,w h i c hi sh e l db yp r o c e s s P3.P r o c e s s P3is waiting for either process P1or process P2to release resource R2.I na d d i t i o n ,p r o c e s s P1is waiting for process P2to release resource R1. Now consider the resourceallocation graph in Figure 7.3. In this example, we also have a cycle: P1R1P3R2P1 R2R1 P3 P4P2P1 Figure 7.3 Resourceallocation graph with a cycle but no deadlock.322 Chapter 7 Deadlocks However, there is no

Chapter 7 Deadlocks However, there is no deadlock. Observe that process P4may release its instance of resource type R2.T h a tr e s o u r c ec a nt h e nb ea l l o c a t e dt o P3,b r e a k i n gt h ec y c l e . In summary, if a resourceallocation graph does not have a cycle, then the system is notin a deadlocked state. If there is a cycle, then the system may or may not be in a deadlocked state. This observation is important when we deal with the deadlock problem. 7.3 Methods for Handling

the deadlock problem. 7.3 Methods for Handling Deadlocks Generally speaking, we can deal with the deadlock problem in one of three ways: We can use a protocol to prevent or avoid deadlocks, ensuring that the system will never enter a deadlocked state. We can allow the system to enter a deadlocked state, detect it, and recover. We can ignore the problem altogeth er and pretend that deadlocks never occur in the system. The third solution is the one used by most operating systems, including Linux

used by most operating systems, including Linux and Windows. It is then up to the application developer to write programs that handle deadlocks. Next, we elaborate briey on each of the three methods for handling deadlocks. Then, in Sections 7.4 through 7.7, we present detailed algorithms. Before proceeding, we should mention that some researchers have argued that none of the basic approaches alone is appropriate for the entire spectrum of resourceallocation problems in operating systems. The

problems in operating systems. The basic approaches can be combined, however, allowing us to select an optimal approach for each class of resources in a system. To ensure that deadlocks never occur, the system can use either a deadlock prevention or a deadlockavoidance scheme. Deadlock prevention provides a set of methods to ensure that at least one of the necessary conditions (Section 7.2.1) cannot hold. These methods prevent deadlocks by constraining how requests for resources can be made. We

how requests for resources can be made. We discuss these methods in Section 7.4. Deadlock avoidance requires that the operating system be given additional information in advance concerning which resources a process will request and use during its lifetime. With this additional knowledge, the operating system can decide for each request whether or not the process should wait. To decide whether the current request can be satised or must be delayed, the system must consider the resources currently

the system must consider the resources currently available, the resources currently allocated to each process, and the future requests and releases of each process. We discuss these schemes in Section 7.5. If a system does not employ either a deadlockprevention or a deadlock avoidance algorithm, then a deadlock situation may arise. In this environment, the system can provide an algorithm that examines the state of the system to determine whether a deadlock has occurr ed and an algorithm to

a deadlock has occurr ed and an algorithm to recover from the deadlock (if a deadlock has indeed occurred). We discuss these issues in Section 7.6 and Section 7.7.7.4 Deadlock Prevention 323 In the absence of algorithms to detect and recover from deadlocks, we may arrive at a situation in which the system is in a deadlocked state yet has no way of recognizing what has happened. In this case, the undetected deadlock will cause the systems performance to deteriorate, because resources are being

to deteriorate, because resources are being held by processes that cannot run and because more and more processes, as they make requests for resources, will enter a deadlocked state. Eventually, the system will stop functioning and will need to be restarted manually. Although this method may not seem to be a viable approach to the deadlock problem, it is nevertheless used in most operating systems, as mentioned earlier. Expense is one important consideration. Ignoring the possibility of

consideration. Ignoring the possibility of deadlocks is cheaper than the other approaches. Since in many systems, deadlocks occur infrequently (say, onc ep e ry e a r ) ,t h ee x t r ae x p e n s eo ft h e other methods may not seem worthwh ile. In addition, methods used to recover from other conditions may be put to use to recover from deadlock. In some circumstances, a system is in a frozen state but not in a deadlocked state. We see this situation, for example, with a realtime process running

for example, with a realtime process running at the highest priority (or any process running on a nonpreemptive scheduler) and never returning control to the operating system. The system must have manual recovery methods for such conditions and may simply use those techniques for deadlock recovery. 7.4 Deadlock Prevention As we noted in Section 7.2.1, for a deadlock to occur, each of the four necessary conditions must hold. By ensuring that at least one of these conditions cannot hold, we can

least one of these conditions cannot hold, we can prevent the occurrence of a deadlock. We elaborate on this approach by examining each of the four necessary conditions separately. 7.4.1 Mutual Exclusion The mutual exclusion condition must hold. That is, at least one resource must be nonsharable. Sharable resources, in contrast, do not require mutually exclusive access and thus cannot be involved in a deadlock. Readonly les are a good example of a sharable resource. If several processes attempt

a sharable resource. If several processes attempt to open a readonly le at the same time, they can be granted simultaneous access to the le. A process never needs to wait for a sharab le resource. In general, however, we cannot prevent deadlocks by denyin g the mutualexclusion condition, because some resources are intrinsically nonsharable. For example, a mutex lock cannot be simultaneously shared by several processes. 7.4.2 Hold and Wait To ensure that the holdandwait condition never occurs in

that the holdandwait condition never occurs in the system, we must guarantee that, whenever a process requests a resource, it does not hold any other resources. One protocol that we can use requires each process to request and be allocated all its resources before it begins execution. We can implement this provision by requiring that system calls requesting resources for a process precede all other system calls.324 Chapter 7 Deadlocks An alternative protocol allows a process to request resources

protocol allows a process to request resources only when it has none. A process may request some resources and use them. Before it can request any additional resources, it must release all the resources that it is currently allocated. To illustrate the difference between these two protocols, we consider a process that copies data from a DVD drive to a le on disk, sorts the le, and then prints the results to a printer. If all resources must be requested at the beginning of the process, then the

at the beginning of the process, then the process must initially request the DVD drive, disk le, and printer. It will hold the printer for its entire execution, even though it needs the printer only at the end. The second method allows the process to request initially only the DVD drive and disk le. It copies from the DVD drive to the disk and then releases both the DVD drive and the disk le. The process must then request the disk le and the printer. After copying the disk le to the printer, it

After copying the disk le to the printer, it releases these two resources and terminates. Both these protocols have two main disadvantages. First, resource utiliza tion may be low, since resources may be allocated but unused for a long period. In the example given, for instance, we can release the DVD drive and disk le, and then request the disk le and printer, only if we can be sure that our data will remain on the disk le. Otherwise, we must request all resources at the beginning for both

request all resources at the beginning for both protocols. Second, starvation is possible. A process that needs several popular resources may have to wait indenitely, because at least one of the resources that it needs is always allocated to some other process. 7.4.3 No Preemption The third necessary condition for deadlocks is that there be no preemption of resources that have already been allocated. To ensure that this condition does not hold, we can use the following protocol. If a process is

we can use the following protocol. If a process is holding some resources and requests another resource that cannot be immediately allocated to it (that is, the process must wait), then all resources the process is currently holding are preempted. In other words, these resources are implicitly released. The preempted resources are added to the list of resources for which the process is waiting. The process will be restarted only when it can regain its old resources, as well as the new ones that

its old resources, as well as the new ones that it is requesting. Alternatively, if a process requests some resources, we rst check whether they are available. If they are, we allocate them. If they are not, we check whether they are allocated to some other process that is waiting for additional resources. If so, we preempt the desired resources from the waiting process and allocate them to the requesting process. If the resources are neither available nor held by a waiting process, the

available nor held by a waiting process, the requesting process must wait. While it is waiting, some of its resources may be preempted, but only if another process requests them. A process can be restarted only when it is allocated the new resources it is requesting and recovers any resources that were preempted while it was waiting. This protocol is often applied to resources whose state can be easily saved and restored later, such as CPU registers and memory space. It cannot generally be

registers and memory space. It cannot generally be applied to such resources as mutex locks and semaphores.7.4 Deadlock Prevention 325 7.4.4 Circular Wait The fourth and nal condition for deadlocks is the circularwait condition. One way to ensure that this condition never holds is to impose a total ordering of all resource types and to require that each process requests resources in an increasing order of enumeration. To illustrate, we let RR1,R2,. . . , Rmbe the set of resource types. We assign

. . , Rmbe the set of resource types. We assign to each resource type a unique integer number, which allows us to compare two resources and to determine whether one precedes another in our ordering. Formally, we dene a onetoone function F:RN,where Nis the set of natural numbers. For example, if the set of resource types Rincludes tape drives, disk drives, and printers, then the function Fmight be dened as follows: F(tape drive)  1 F(disk drive)  5 F(printer)  12 We can now consider the following

5 F(printer)  12 We can now consider the following protocol to prevent deadlocks: Each process can request resources only in an increasing order of enumeration. That is, a process can initially request any number of instances of a resource type say , Ri.A f t e rt h a t ,t h ep r o c e s sc a nr e q u e s ti n s t a n c e so fr e s o u r c et y p e Rjif and only if F(Rj)F(Ri). For example, using the function dened previously, a process that wants to use the tape drive and printer at the same

to use the tape drive and printer at the same time must rst request the tape drive and then request the printer. Alternatively, we can require that a process requesting an instance of resource type Rjmust have released any resources Risuch that F(Ri)F(Rj). Note also that if several instances of the same resource type are needed, a single request for all of them must be issued. If these two protocols are used, then the circularwait condition cannot hold. We can demonstrate this fact by assuming

hold. We can demonstrate this fact by assuming that a circular wait exists (proof by contradiction). Let the set of processes involved in the circular wait be P0,P1,. . . ,Pn,w h e r e Piis waiting for a resource Ri,w h i c hi sh e l db yp r o c e s s Pi1.( M o d u l oa r i t h m e t i ci su s e do nt h ei n d e x e s ,s ot h a t Pnis waiting for ar e s o u r c e Rnheld by P0.) Then, since process Pi1is holding resource Ri while requesting resource Ri1,w em u s th a v e F(Ri)F(Ri1)f o ra l l

Ri1,w em u s th a v e F(Ri)F(Ri1)f o ra l l i.But this condition means that F(R0)F(R1)...F(Rn)F(R0). By transitivity, F(R0)F(R0), which is impossible. Therefore, there can be no circular wait. We can accomplish this scheme in an application program by developing an ordering among all synchronization objects in the system. All requests for synchronization objects must be made in increasing order. For example, if the lock ordering in the Pthread program shown in Figure 7.4 was F(first mutex )1

program shown in Figure 7.4 was F(first mutex )1 F(second mutex )5 then thread twocould not request the locks out of order. Keep in mind that developing an ordering, or hierarchy, does not in itself prevent deadlock. It is up to application developers to write programs that follow the ordering. Also note that the function Fshould be dened according to the normal order of usage of the resources in a system. For example, because326 Chapter 7 Deadlocks  thread one runs in this function  void do

thread one runs in this function  void do work one(void param)  pthread mutex lock(first mutex); pthread mutex lock(second mutex);  D os o m ew o r k  pthread mutex unlock(second mutex); pthread mutex unlock(first mutex); pthread exit(0);   thread two runs in this function  void do work two(void param)  pthread mutex lock(second mutex); pthread mutex lock(first mutex);  D os o m ew o r k  pthread mutex unlock(first mutex); pthread mutex unlock(second mutex); pthread exit(0);  Figure 7.4

unlock(second mutex); pthread exit(0);  Figure 7.4 Deadlock example. the tape drive is usually needed before the printer, it would be reasonable to dene F(tape drive) F(printer). Although ensuring that resources are acquired in the proper order is the responsibility of application developers, certain software can be used to verify that locks are acquired in the proper order and to give appropriate warnings when locks are acquired out of order and deadlock is possible. One lockorder verier, which

deadlock is possible. One lockorder verier, which works on BSD versions of UNIX such as FreeBSD, is known as witness .W i t n e s su s e sm u t u a l  e x c l u s i o nl o c k st op r o t e c tc r i t i c a ls e c t i o n s ,a s described in Chapter 5. It works by dynamically maintaining the relationship of lock orders in a system. Lets use the program shown in Figure 7.4 as an example. Assume that thread oneis the rst to acquire the locks and does so in the order (1) first mutex ,( 2 ) second

does so in the order (1) first mutex ,( 2 ) second mutex .W i t n e s sr e c o r d st h er e l a t i o n s h i p that first mutex must be acquired before second mutex .I fthread twolater acquires the locks out of order, witness generates a warning message on the system console. It is also important to note that imposing a lock ordering does not guarantee deadlock prevention if locks can be acquired dynamically. For example, assume we have a function that transfers fun ds between two accounts. To

that transfers fun ds between two accounts. To prevent a race condition, each account has an associated mutex lock that is obtained from aget lock() function such as shown in Figure 7.5:7.5 Deadlock Avoidance 327 void transaction(Account from, Account to, double amount)  mutex lock1, lock2; lock1  get lock(from); lock2  get lock(to); acquire(lock1); acquire(lock2); withdraw(from, amount); deposit(to, amount); release(lock2); release(lock1);  Figure 7.5 Deadlock example with lock ordering.

Figure 7.5 Deadlock example with lock ordering. Deadlock is possible if two threads simultaneously invoke the transaction() function, transposing different accounts. That is, one thread might invoke transaction(checking account, savings account, 25); and another might invoke transaction(savings account, checking account, 50); We leave it as an exercise for students to x this situation. 7.5 Deadlock Avoidance Deadlockprevention algorithms, as discu ssed in Section 7.4, prevent deadlocks by

as discu ssed in Section 7.4, prevent deadlocks by limiting how requests can be made. The limits ensure that at least one of the necessary conditions for deadlock cannot occur. Possible side effects of preventing deadlocks by this method, however, are low device utilization and reduced system throughput. An alternative method for avoiding deadlocks is to require additional information about how resources are to be requested. For example, in a system with one tape drive and one printer, the

a system with one tape drive and one printer, the system might need to know that process Pwill request rst the tape drive and then the printer before releasing both resources, whereas process Qwill request rst the printer and then the tape drive. With this knowledge of the complete sequence of requests and releases for each process, the system can decide for each request whether or not the process should wait in order to avoid a possible future deadlock. Each request requires that in making this

Each request requires that in making this decision the system consider the resources currently available, the resources curren tly allocated to each process, and the future requests and releases of each process. The various algorithms that use this approach differ in the amount and type of information required. The simplest and most useful model requires that each process declare the maximum number of resources of each type that it may need. Given this a priori information, it is possible to

Given this a priori information, it is possible to construct an328 Chapter 7 Deadlocks algorithm that ensures that the system will never enter a deadlocked state. A deadlockavoidance algorithm dynamically examines the resourceallocation state to ensure that a circularwait condition can never exist. The resource allocation state is dened by the number of a vailable and allocated resources and the maximum demands of the processes. In the following sections, we explore two deadlockavoidance

sections, we explore two deadlockavoidance algorithms. 7.5.1 Safe State As t a t ei s safeif the system can allocate resources to each process (up to its maximum) in some order and still avoid a deadlock. More formally, a system is in a safe state only if there exists a safe sequence .As e q u e n c eo fp r o c e s s e s P1,P2,. . . , Pnis a safe sequence for the current allocation state if, for each Pi,t h er e s o u r c er e q u e s t st h a t Pican still make can be satised by the currently

t Pican still make can be satised by the currently available resources plus the resources held by all Pj,w i t h ji.In this situation, if the resources that Pineeds are not immediately available, then Pican wait until all Pjhave nished. When they have nished, Pican obtain all of its needed resources, complete its designated task, return its allocated resources, and terminate. When Piterminates, Pi1can obtain its needed resources, and so on. If no such sequence exists, then the system state is

no such sequence exists, then the system state is said to be unsafe. As a f es t a t ei sn o tad e a d l o c k e ds t a t e .C o n v e r s e l y ,ad e a d l o c k e ds t a t ei s an unsafe state. Not all unsafe states are deadlocks, however (Figure 7.6). An unsafe state may lead to a deadlock. As long as the state is safe, the operating system can avoid unsafe (and deadlocked) states. In an unsafe state, the operating system cannot prevent processes from requesting resources in such a way that a

from requesting resources in such a way that a deadlock occurs. The behavior of the processes controls unsafe states. To illustrate, we consider a system with twelve magnetic tape drives and three processes: P0,P1,a n d P2.P r o c e s s P0requires ten tape drives, process P1 may need as many as four tape drives, and process P2may need up to nine tape drives. Suppose that, at time t0,p r o c e s s P0is holding ve tape drives, process P1is holding two tape drives, and process P2is holding two tape

two tape drives, and process P2is holding two tape drives. (Thus, there are three free tape drives.)deadlockunsafe safe Figure 7.6 Safe, unsafe, and deadlocked state spaces.7.5 Deadlock Avoidance 329 Maximum Needs Current Needs P0 10 5 P1 42 P2 92 At time t0,t h es y s t e mi si nas a f es t a t e .T h es e q u e n c e P1,P0,P2satises the safety condition. Process P1can immediately be allocated all its tape drives and then return them (the system will then have ve available tape drives); then

will then have ve available tape drives); then process P0can get all its tape drives and return them (the system will then have ten available tape drives); and nally process P2can get all its tape drives and return them (the system will then have all twelve tape drives available). As y s t e mc a ng of r o mas a f es t a t et oa nu n s a f es t a t e .S u p p o s et h a t ,a tt i m e t1,p r o c e s s P2requests and is allocated one more tape drive. The system is no longer in a safe state. At

drive. The system is no longer in a safe state. At this point, only process P1can be allocated all its tape drives. When it returns them, the system will have only four available tape drives. Since process P0is allocated ve tape drives but has a maximum of ten, it may request ve more tape drives. If it does so, it will have to wait, because they are unavailable. Similarly, process P2may request six additional tape drives and have to wait, resulting in a deadlock. Our mistake was in granting the

in a deadlock. Our mistake was in granting the request from process P2for one more tape drive. If we had made P2wait until either of the other processes had nished and released its resources, then we could have avoided the deadlock. Given the concept of a safe state, we can dene avoidance algorithms that ensure that the system will never deadlock. The idea is simply to ensure that the system will always remain in a safe state. Initially, the system is in a safe state. Whenever a process requests

is in a safe state. Whenever a process requests a resource that is currently available, the system must decide whether the resource can be allocated immediately or whether the process must wait. The request is granted only if the allocation leaves the system in a safe state. In this scheme, if a process requests a r esource that is currently available, it may still have to wait. Thus, resource utilization may be lower than it would otherwise be. 7.5.2 ResourceAllocationGraph Algorithm If we have

7.5.2 ResourceAllocationGraph Algorithm If we have a resourceallocation system with only one instance of each resource type, we can use a variant of the resourceallocation graph dened in Section 7.2.2 for deadlock avoidance. In addition to the request and assignment edges already described, we introduce a new type of edge, called a claim edge . Ac l a i me d g e PiRjindicates that process Pimay request resource Rjat some time in the future. This edge resembles a request edge in direction but is

edge resembles a request edge in direction but is represented in the graph by a dashed line. When process Pirequests resource Rj,t h ec l a i me d g e PiRjis converted to a request edge. Similarly, when a resource Rjis released by Pi,t h ea s s i g n m e n te d g e RjPiis reconverted to a claim edge PiRj. Note that the resources must be claimed a priori in the system. That is, before process Pistarts executing, all its claim edges must already appear in the resourceallocation graph. We can relax

in the resourceallocation graph. We can relax this condition by allowing a claim edge PiRjto be added to the graph only if all the edges associated with process Piare claim edges.330 Chapter 7 DeadlocksR1 R2P2 P1 Figure 7.7 Resourceallocation graph for deadlock avoidance. Now suppose that process Pirequests resource Rj.T h er e q u e s tc a nb e granted only if converting the request edge PiRjto an assignment edge RjPidoes not result in the formation of a cycle in the resourceallocation graph.

of a cycle in the resourceallocation graph. We check for safety by using a cycledetection algorithm. An algorithm for detecting a cycle in this graph requires an order of n2operations, where n is the number of processes in the system. If no cycle exists, then the allocation of the resource will leave the system in a safe state. If a cycle is found, then the allocation will put the system in an unsafe state. In that case, process Piwill have to wait for its requests to be satised. To illustrate

wait for its requests to be satised. To illustrate this algorithm, we consider the resourceallocation graph of Figure 7.7. Suppose that P2requests R2.A l t h o u g h R2is currently free, we cannot allocate it to P2, since this action will create a cycle in the graph (Figure 7.8). A cycle, as mentioned, indicates that the system is in an unsafe state. If P1 requests R2,a n d P2requests R1,t h e nad e a d l o c kw i l lo c c u r . 7.5.3 Bankers Algorithm The resourceallocationgraph algorithm is

Algorithm The resourceallocationgraph algorithm is not applicable to a resource allocation system with multiple instances of each resource type. The deadlock avoidance algorithm that we describe next is applicable to such a system but is less efcient than the resourceallocation graph scheme. This algorithm is commonly known as the bankers algorithm. The name was chosen because the algorithm could be used in a banking system to ensure that the bank neverR1 R2P2 P1 Figure 7.8 An unsafe state in a

neverR1 R2P2 P1 Figure 7.8 An unsafe state in a resourceallocation graph.7.5 Deadlock Avoidance 331 allocated its available cash in such a way that it could no longer satisfy the needs of all its customers. When a new process enters the system, it must declare the maximum number of instances of each resource type that it may need. This number may not exceed the total number of resources in the system. When a user requests as e to fr e s o u r c e s ,t h es y s t e mm u s td e t e r m i n ew h e

e s ,t h es y s t e mm u s td e t e r m i n ew h e t h e rt h ea l l o c a t i o no ft h e s e resources will leave the system in a safe state. If it will, the resources are allocated; otherwise, the process must wa it until some other process releases enough resources. Several data structures must be maintained to implement the bankers algorithm. These data structures encode the state of the resourceallocation system. We need the following data structures, where nis the number of processes in

structures, where nis the number of processes in the system and mis the number of resource types: Available .Av e c t o ro fl e n g t h mindicates the number of available resources of each type. If Available [j]e q u a l s k,then kinstances of resource type Rj are available. Max .A n nmmatrix denes the maximum demand of each process. IfMax [i][j]e q u a l s k,then process Pimay request at most kinstances of resource type Rj. Allocation .A n nmmatrix denes the number of resources of each type

denes the number of resources of each type currently allocated to each process. If Allocation [i][j] equals k,then process Piis currently allocated kinstances of resource type Rj. Need .A n nmmatrix indicates the remaining resource need of each process. If Need [i][j]e q u a l s k,then process Pimay need kmore instances of resource type Rjto complete its task. Note that Need [i][j]e q u a l s Max [i][j] Allocation [i][j]. These data structures vary over time in both size and value. To simplify

vary over time in both size and value. To simplify the presentation of the banker s algorithm, we next establish some notation. Let Xand Ybe vectors of length n.We say that XYif and only if X[i]Y[i]f o ra l l i1 ,2 ,. . . , n.For example, if X( 1 , 7 , 3 , 2 )a n d Y (0,3,2,1), then YX.In addition, YXifYXand YX. We can treat each row in the matrices Allocation and Need as vectors and refer to them as Allocation iand Need i.The vector Allocation ispecies the resources currently allocated to

ispecies the resources currently allocated to process Pi;t h ev e c t o r Need ispecies the additional resources that process Pimay still request to complete its task. 7.5.3.1 Safety Algorithm We can now present the algorithm for nding out whether or not a system is in a safe state. This algorithm can be described as follows: 1.LetWork and Finish be vectors of length mand n,respectively. Initialize Work Available and Finish [i]false fori0 ,1 ,. . . , n1. 2.Find an index isuch that both a.Finish

. . , n1. 2.Find an index isuch that both a.Finish [i]  false b.Need iWork332 Chapter 7 Deadlocks If no such iexists, go to step 4. 3.Work Work Allocation i Finish [i]true Go to step 2. 4.IfFinish [i]  true for all i,then the system is in a safe state. This algorithm may require an order of mn2operations to determine whether as t a t ei ss a f e . 7.5.3.2 ResourceRequest Algorithm Next, we describe the algorithm for determining whether requests can be safely granted. LetRequest ibe the request

can be safely granted. LetRequest ibe the request vector for process Pi.I fRequest i[j] k,t h e n process Piwants kinstances of resource type Rj.W h e nar e q u e s tf o rr e s o u r c e s is made by process Pi,t h ef o l l o w i n ga c t i o n sa r et a k e n : 1.IfRequest iNeed i, go to step 2. Otherwise, raise an error condition, since the process has exceeded its maximum claim. 2.IfRequest iAvailable, go to step 3. Otherwise, Pimust wait, since the resources are not available. 3.Have the

since the resources are not available. 3.Have the system pretend to have allocated the requested resources to process Piby modifying the state as follows: Available Available Request i; Allocation iAllocation iRequest i; Need iNeed iRequest i; If the resulting resourceallocation state is safe, the transaction is com pleted, and process Piis allocated its resources. However, if the new state is unsafe, then Pimust wait for Request i,and the old resourceallocation state is restored. 7.5.3.3 An

resourceallocation state is restored. 7.5.3.3 An Illustrative Example To illustrate the use of the banker s algorithm, consider a system with ve processes P0through P4and three resource types A, B, and C.Resource type A has ten instances, resource type Bhas ve instances, and resource type Chas seven instances. Suppose that, at time T0,t h ef o l l o w i n gs n a p s h o to ft h es y s t e m has been taken: Allocation Max Available ABC ABC ABC P0 010 753 332 P1 200 322 P2 302 902 P3 211 222 P4

P0 010 753 332 P1 200 322 P2 302 902 P3 211 222 P4 002 4337.6 Deadlock Detection 333 The content of the matrix Need is dened to be Max Allocation and is as follows: Need ABC P0 743 P1 122 P2 600 P3 011 P4 431 We claim that the system is currently in a safe state. Indeed, the sequence P1,P3,P4,P2,P0satises the safety criteria. Suppose now that process P1requests one additional instance of resource type Aand two instances of resource type C,soRequest 1( 1 , 0 , 2 ) .T od e c i d ew h e t h e rt h

1( 1 , 0 , 2 ) .T od e c i d ew h e t h e rt h i sr e q u e s tc a nb e immediately granted, we rst check that Request 1Available that is, that (1,0,2) (3,3,2), which is true. We then pretend that this request has been fullled, and we arrive at the following new state: Allocation Need Available ABC ABC ABC P0 010 743 230 P1 302 020 P2 302 600 P3 211 011 P4 002 431 We must determine whether this new system state is safe. To do so, we execute our safety algorithm and nd that the sequence

our safety algorithm and nd that the sequence P1,P3,P4,P0,P2 satises the safety requirement. Hence, we can immediately grant the request of process P1. You should be able to see, however, that when the system is in this state, a request for (3,3,0) by P4cannot be granted, since the resources are not available. Furthermore, a request for (0,2,0) by P0cannot be granted, even though the resources are available, since the resulting state is unsafe. We leave it as a programming exercise for students

We leave it as a programming exercise for students to implement the bankers algorithm. 7.6 Deadlock Detection If a system does not employ either a deadlockprevention or a deadlock avoidance algorithm, then a deadlock situation may occur. In this environment, the system may provide: An algorithm that examines the state of the system to determine whether a deadlock has occurred An algorithm to recover from the deadlock334 Chapter 7 Deadlocks P3P5 P4P2 P1R2R1 R3 R4 R5P3P5 P4P2P1 (b) (a) Figure 7.9

P4P2 P1R2R1 R3 R4 R5P3P5 P4P2P1 (b) (a) Figure 7.9 (a) Resourceallocation graph. (b) Corresponding waitfor graph. In the following discussion, we elaborate on these two requirements as they pertain to systems with only a single instance of each resource type, as well as to systems with several instances of each resource type. At this point, however, we note that a detectionandrecovery scheme requires overhead that includes not only the runtime costs of maintaining the necessary information and

costs of maintaining the necessary information and executing the detection algorithm but also the potential losses inherent in recovering from ad e a d l o c k . 7.6.1 Single Instance of Each Resource Type If all resources have only a single instance, then we can dene a deadlock detection algorithm that uses a variant of the resourceallocation graph, called awaitfor graph. We obtain this graph from the resourceallocation graph by removing the resource nodes and collapsing the appropriate edges.

nodes and collapsing the appropriate edges. More precisely, an edge from PitoPjin a waitfor graph implies that process Piis waiting for process Pjto release a resource that Pineeds. An edge PiPjexists in a waitfor graph if and only if the corresponding resource allocation graph contains two edges PiRqand RqPjfor some resource Rq.I nF i g u r e7 . 9 ,w ep r e s e n tar e s o u r c e  a l l o c a t i o ng r a p ha n dt h ec o r r e s p o n d i n g waitfor graph. As before, a deadlock exists in the

waitfor graph. As before, a deadlock exists in the system if and only if the waitfor graph contains a cycle. To detect deadlocks, the system needs to maintain the wait for graph and periodically invoke an algorithm that searches for a cycle in the graph. An algorithm to detect a cycle in a graph requires an order of n2 operations, where nis the number of vertices in the graph. 7.6.2 Several Instances of a Resource Type The waitfor graph scheme is not applicable to a resourceallocation system

is not applicable to a resourceallocation system with multiple instances of each resource type. We turn now to a deadlock7.6 Deadlock Detection 335 detection algorithm that is applicable to such a system. The algorithm employs several timevarying data structures that are similar to those used in the bankers algorithm (Section 7.5.3): Available .Av e c t o ro fl e n g t h mindicates the number of available resources of each type. Allocation .A n nmmatrix denes the number of resources of each type

denes the number of resources of each type currently allocated to each process. Request .A n nmmatrix indicates the current request of each process. IfRequest [i][j]e q u a l s k,then process Piis requesting kmore instances of resource type Rj. Therelation between two vectors is dened as in Section 7.5.3. To simplify notation, we again treat the rows in the matrices Allocation and Request as vectors; we refer to them as Allocation iand Request i.The detection algorithm described here simply

i.The detection algorithm described here simply investigates every possib le allocation sequence for the processes that remain to be completed. Compare this algorithm with the bankers algorithm of Section 7.5.3. 1.LetWork and Finish be vectors of length mand n,respectively. Initialize Work Available. Fori0 ,1 ,. . . , n1, ifAllocation i0, then Finish [i] false. Otherwise, Finish [i]true. 2.Find an index isuch that both a.Finish [i]  false b.Request iWork If no such iexists, go to step 4. 3.Work

iWork If no such iexists, go to step 4. 3.Work Work Allocation i Finish [i]true Go to step 2. 4.IfFinish [i]  false for some i,0in,then the system is in a deadlocked state. Moreover, if Finish [i]  false, then process Piis deadlocked. This algorithm requires an order of mn2operations to detect whether the system is in a deadlocked state. You may wonder why we reclaim the resources of process Pi(in step 3) as soon as we determine that Request iWork (in step 2b). We know that Piis currently

iWork (in step 2b). We know that Piis currently notinvolved in a deadlock (since Request iWork ). Thus, we take an optimistic attitude and assume that Piwill require no more resources to complete its task; it will thus soon return all currently allocated resources to the system. If our assumption is incorrect, a deadlock may occur later. That deadlock will be detected the next time the deadlockdetection algorithm is invoked. To illustrate this algorithm, we consider a system with ve processes P0

we consider a system with ve processes P0 through P4and three resource types A, B, and C.Resource type Ahas seven instances, resource type Bhas two instances, and resource type Chas six336 Chapter 7 Deadlocks instances. Suppose that, at time T0,w eh a v et h ef o l l o w i n gr e s o u r c e  a l l o c a t i o n state: Allocation Request Available ABC ABC ABC P0 010 000 000 P1 200 202 P2 303 000 P3 211 100 P4 002 002 We claim that the system is not in a deadlocked state. Indeed, if we execute

not in a deadlocked state. Indeed, if we execute our algorithm, we will nd that the sequence P0,P2,P3,P1,P4results in Finish [i]  truefor all i. Suppose now that process P2makes one additional request for an instance of type C.The Request matrix is modied as follows: Request ABC P0 000 P1 202 P2 001 P3 100 P4 002 We claim that the system is now deadlocked. Although we can reclaim the resources held by process P0,t h en u m b e ro fa v a i l a b l er e s o u r c e si sn o ts u f  c i e n t to

b l er e s o u r c e si sn o ts u f  c i e n t to fulll the requests of the other processes. Thus, a deadlock exists, consisting of processes P1,P2,P3,a n d P4. 7.6.3 DetectionAlgorithm Usage When should we invoke the detection algorith m? The answer depends on two factors: 1.How often is a deadlock likely to occur? 2.How many processes will be affected by deadlock when it happens? If deadlocks occur frequently, then the detection algorithm should be invoked frequently. Resources allocated to

be invoked frequently. Resources allocated to deadlocked processes will be idle until the deadlock can be broken. In addition, the number of processes involved in the deadlock cycle may grow. Deadlocks occur only when some process makes a request that cannot be granted immediately. This request may be the nal request that completes a chain of waiting processes. In the extreme, then, we can invoke the deadlock detection algorithm every time a request for allocation cannot be granted immediately.

for allocation cannot be granted immediately. In this case, we can identify not only the deadlocked set of7.7 Recovery from Deadlock 337 processes but also the specic process that caused the deadlock. (In reality, each of the deadlocked processes is a link in the cycle in the resource graph, so all of them, jointly, caused the deadlock.) If there are many different resource types, one request may create many cycles in the resource graph, each cycle completed by the most recent request and caused

completed by the most recent request and caused by the one identiable process. Of course, invoking the deadlockdetection algorithm for every resource request will incur considerable overhead in computation time. A less expensive alternative is simply to invoke the algorithm at dened intervalsfor example, once per hour or whenever CPU utilization drops below 40 percent. (A deadlock eventually cripples system throughput and causes CPU utilization to drop.) If the detection algorithm is invoked at

to drop.) If the detection algorithm is invoked at arbitrary points in time, the resource graph may contain many cycles. In this case, we generally cannot tell which of the many deadlocked processes caused the deadlock. 7.7 Recovery from Deadlock When a detection algorithm determines th at a deadlock exists, several alter natives are available. One possibility is to inform the operator that a deadlock has occurred and to let the operator deal with the deadlock manually. Another possibility is to

the deadlock manually. Another possibility is to let the system recover from the deadlock automatically. There are two options for breaking a deadlock. One is simply to abort one or more processes to break the circular wait. The other is to preempt some resources from one or more of the deadlocked processes. 7.7.1 Process Termination To eliminate deadlocks by aborting a process, we use one of two methods. In both methods, the system reclaims all resources allocated to the terminated processes.

resources allocated to the terminated processes. Abort all deadlocked processes . This method clearly will break the deadlock cycle, but at great expense. The deadlocked processes may have computed for a long time, and the results of these partial computations must be discarded and probably will have to be recomputed later. Abort one process at a time until the deadlock cycle is eliminated .T h i s method incurs considerable overhead, since after each process is aborted, a deadlockdetection

after each process is aborted, a deadlockdetection algorithm must be invoked to determine whether any processes are still deadlocked. Aborting a process may not be easy. If the process was in the midst of updating a le, terminating it will leave that le in an incorrect state. Similarly, if the process was in the midst of printing data on a printer, the system must reset the printer to a correct state before printing the next job. If the partial termination method is used, then we must determine

termination method is used, then we must determine which deadlocked process (or processes) sh ould be terminated. This determination is ap o l i c yd e c i s i o n ,s i m i l a rt o CPUscheduling decisions. The question is basically an economic one; we should abort those processes whose termination will incur338 Chapter 7 Deadlocks the minimum cost. Unfortunately, the term minimum cost is not a precise one. Many factors may affect which process is chosen, including: 1.What the priority of the

is chosen, including: 1.What the priority of the process is 2.How long the process has computed and how much longer the process will compute before completing its designated task 3.How many and what types of resources the process has used (for example, whether the resources are simple to preempt) 4.How many more resources the process needs in order to complete 5.How many processes will need to be terminated 6.Whether the process is interactive or batch 7.7.2 Resource Preemption To eliminate

or batch 7.7.2 Resource Preemption To eliminate deadlocks using resource preemption, we successively preempt some resources from processes and give these resources to other processes until the deadlock cycle is broken. If preemption is required to deal with deadlocks, then three issues need to be addressed: 1.Selecting a victim .W h i c hr e s o u r c e sa n dw h i c hp r o c e s s e sa r et ob e preempted? As in process termination, we must determine the order of preemption to minimize cost.

the order of preemption to minimize cost. Cost factors may include such parameters as the number of resources a deadlocked process is holding and the amount of time the process has thus far consumed. 2.Rollback .I fw ep r e e m p tar e s o u r c ef r o map r o c e s s ,w h a ts h o u l db ed o n e with that process? Clearly, it cannot continue with its normal execution; it is missing some needed resource. We must roll back the process to some safe state and restart it from that state. Since, in

state and restart it from that state. Since, in general, it is difcult to determine what a safe state is, the simplest solution is a total rollback: abort the process and then restart it. Although it is more effective to roll back the process only as far as necessary to break the deadlock, this method requires the system to keep more information about the state of all running processes. 3.Starvation .H o wd ow ee n s u r et h a ts t a r v a t i o nw i l ln o to c c u r ?T h a ti s , how can we

o nw i l ln o to c c u r ?T h a ti s , how can we guarantee that resources will not always be preempted from the same process? In a system where victim selection is based primarily on cost factors, it may happen that the same process is always picked as a victim. As ar e s u l t ,t h i sp r o c e s sn e v e rc o m p l e t e si t sd e s i g n a t e dt a s k ,as t a r v a t i o n situation any practical system must address. Clearly, we must ensure that a process can be picked as a victim only a

that a process can be picked as a victim only a (small) nite number of times. The most common solution is to include the number of rollbacks in the cost factor.Practice Exercises 339 7.8 Summary Ad e a d l o c k e ds t a t eo c c u r sw h e nt w oo rm o r ep r o c e s s e sa r ew a i t i n gi n d e  n i t e l y for an event that can be caused only by one of the waiting processes. There are three principal methods for dealing with deadlocks: Use some protocol to prevent or avoid deadlocks,

Use some protocol to prevent or avoid deadlocks, ensuring that the system will never enter a deadlocked state. Allow the system to enter a deadlocked state, detect it, and then recover. Ignore the problem altogether and pre tend that deadlocks never occur in the system. The third solution is the one used by most operating systems, including Linux and Windows. Ad e a d l o c kc a no c c u ro n l yi ff o u rn e c e s s a r yc o n d i t i o n sh o l ds i m u l t a n e o u s l y in the system:

sh o l ds i m u l t a n e o u s l y in the system: mutual exclusion, hold and wait, no preemption, and circular wait. To prevent deadlocks, we can ensure that at least one of the necessary conditions never holds. Am e t h o df o ra v o i d i n gd e a d l o c k s ,r a t h e r than preventing them, requires that the operating system have a priori information about how each process will utilize system resources. The bankers algorithm, for example, requires ap r i o r ii n f o r m a t i o na b o u

requires ap r i o r ii n f o r m a t i o na b o u tt h em a x i m u mn u m b e ro fe a c hr e s o u r c ec l a s st h a t each process may request. Using this information, we can dene a deadlock avoidance algorithm. If a system does not employ a protocol to ensure that deadlocks will never occur, then a detectionandrec overy scheme may be employed. A deadlock detection algorithm must be invoked to determine whether a deadlock has occurred. If a deadlock is detected, the system must recover

If a deadlock is detected, the system must recover either by terminating some of the deadlocked processes or by preempting resources from some of the deadlocked processes. Where preemption is used to deal with deadlocks, three issues must be addressed: selecting a victim, rollback, and starvation. In a system that selects victims for rollback primarily on the basis of cost factors, starvation may occur, and the selected process can never complete its designated task. Researchers have argued that

its designated task. Researchers have argued that none of the basic approaches alone is appro priate for the entire spectrum of resourceallocation problems in operating systems. The basic approaches can be combined, however, allowing us to select an optimal approach for each class of resources in a system. Practice Exercises 7.1 List three examples of deadlocks that are not related to a computer system environment. 7.2 Suppose that a system is in an unsafe state. Show that it is possible for the

an unsafe state. Show that it is possible for the processes to complete their exec ution without entering a deadlocked state.340 Chapter 7 Deadlocks 7.3 Consider the following snapshot of a system: Allocation Max Available ABCD ABCD ABCD P0 0012 0012 1520 P1 1000 1750 P2 1354 2356 P3 0632 0652 P4 0014 0656 Answer the following questions using the bankers algorithm: a. What is the content of the matrix Need ? b. Is the system in a safe state? c. If a request from process P1arrives for (0,4,2,0),

If a request from process P1arrives for (0,4,2,0), can the request be granted immediately? 7.4 Ap o s s i b l em e t h o df o rp r e v e n t i n gd e a d l o c ks is to have a single, higher order resource that must be requested before any other resource. For example, if multiple threads attempt to access the synchronization objects AE, deadlock is possible. (Such synchronization objects may include mutexes, semaphores, condition variables, and the like.) We can prevent the deadlock by adding a

the like.) We can prevent the deadlock by adding a sixth object F.W h e n e v e rat h r e a d wants to acquire the synchronization lock for any object AE,i tm u s t rst acquire the lock for object F.T h i ss o l u t i o ni sk n o w na s containment : the locks for objects AEare contained within the lock for object F. Compare this scheme with the circularwait scheme of Section 7.4.4. 7.5 Prove that the safety algorithm presented in Section 7.5.3 requires an order of mn2operations. 7.6 Consider a

requires an order of mn2operations. 7.6 Consider a computer system that runs 5,000 jobs per month and has no deadlockprevention or deadlockavoidance scheme. Deadlocks occur about twice per month, and the operator must terminate and rerun about ten jobs per deadlock. Each job is worth about two dollars (in CPU time), and the jobs terminated tend to be about half done when they are aborted. As y s t e m sp r o g r a m m e rh a se s t i m a t e dt h a tad e a d l o c k  a v o i d a n c e algorithm

h a tad e a d l o c k  a v o i d a n c e algorithm (like the bankers algorithm) could be installed in the system with an increase of about 10 percent in the average execution time per job. Since the machine currently has 30 percent idle time, all 5,000 jobs per month could still be run, although turnaround time would increase by about 20 percent on average. a. What are the arguments for installing the deadlockavoidance algorithm? b. What are the arguments against installing the deadlockavoidance

arguments against installing the deadlockavoidance algorithm?Exercises 341 7.7 Can a system detect that some of its processes are starving? If you answer yes,explain how it can. If you answer no,explain how the system can deal with the starvation problem. 7.8 Consider the following resourceallocation policy. Requests for and releases of resources are allowed at any time. If a request for resources cannot be satised because the resources are not available, then we check any processes that are

available, then we check any processes that are blocked waiting for resources. If a blocked process has the desired resources, then these resources are taken away from it and are given to the requesting process. Th ev e c t o ro fr e s o u r c e sf o rw h i c h the blocked process is waiting is inc reased to include the resources that were taken away. For example, a system has three resource types, and the vector Available is initialized to (4,2,2). If process P0asks for (2,2,1), it gets them.

If process P0asks for (2,2,1), it gets them. If P1asks for (1,0,1), it gets them. Then, if P0asks for (0,0,1), it is blocked (resource not available). If P2now asks for (2,0,0), it gets the available one (1,0,0), as well as one that was allocated to P0(since P0is blocked). P0sAllocation vector goes down to (1,2,1), and its Need vector goes up to (1,0,1). a. Can deadlock occur? If you answer yes,give an example. If you answer no,specify which necessary condition cannot occur. b. Can indenite

necessary condition cannot occur. b. Can indenite blocking occur? Explain your answer. 7.9 Suppose that you have coded the deadlockavoidance safety algorithm and now have been asked to implement the deadlockdetection algo rithm. Can you do so by simply using the safety algorithm code and redening Max iWaiting iAllocation i,w h e r e Waiting iis a vector specifying the resources for which process iis waiting and Allocation i is as dened in Section 7.5? Explain your answer. 7.10 Is it possible to

7.5? Explain your answer. 7.10 Is it possible to have a deadlock involving only one singlethreaded process? Explain your answer. Exercises 7.11 Consider the trafc deadlock depicted in Figure 7.10. a. Show that the four necessary conditions for deadlock hold in this example. b. State a simple rule for avoiding deadlocks in this system. 7.12 Assume a multithreaded application uses only readerwriter locks for synchronization. Applying the four necessary conditions for deadlock, is deadlock still

conditions for deadlock, is deadlock still possible if multiple readerwriter locks are used? 7.13 The program example shown in Figure 7.4 doesnt always lead to deadlock. Describe what role the CPU scheduler plays and how it can contribute to deadlock in this program.342 Chapter 7 Deadlocks           Figure 7.10 Trafc deadlock for Exercise 7.11. 7.14 In Section 7.4.4, we describe a situation in which we prevent deadlock by ensuring that all locks are acquired in a certain order. However, we also

are acquired in a certain order. However, we also point out that deadlock is possible in this situation if two threads simultaneously invoke the transaction() function. Fix the transaction() function to prevent deadlocks. 7.15 Compare the circularwait scheme with the various deadlockavoidance schemes (like the bankers algorithm) with respect to the following issues: a. Runtime overheads b. System throughput 7.16 In a real computer system, neither the resources available nor the demands of

neither the resources available nor the demands of processes for resour ces are consistent over long periods (months). Resources break or are replaced, new processes come and go, and new resources are bought and added to the system. If deadlock is controlled by the bankers algorithm, which of the following changes can be made safely (without introducing the possibility of deadlock), and under what circumstances? a. Increase Available (new resources added). b. Decrease Available (resource

resources added). b. Decrease Available (resource permanently removed from system). c. Increase Max for one process (the process needs or wants more resources than allowed). d. Decrease Max for one process (the process decides it does not need that many resources).Exercises 343 e. Increase the number of processes. f. Decrease the number of processes. 7.17 Consider a system consisting of four resources of the same type that are shared by three processes, each of which needs at most two resources.

each of which needs at most two resources. Show that the system is deadlock free. 7.18 Consider a system consisting of mresources of the same type being shared by nprocesses. A process can request or release only one resource at a time. Show that the system is deadlock free if the following two conditions hold: a. The maximum need of each process is between one resource and mresources. b. The sum of all maximum needs is less than mn. 7.19 Consider the version of the diningphilosophers problem in

the version of the diningphilosophers problem in which the chopsticks are placed at the center of the table and any two of them can be used by a philosopher. Assume that requests for chopsticks are made one at a time. Describe a simple rule for determining whether a particular request can be satised without causing deadlock given the current allocation of chopsticks to philosophers. 7.20 Consider again the setting in the preceding question. Assume now that each philosopher requires three

Assume now that each philosopher requires three chopsticks to eat. Resource requests are still issued one at a time. Describe some simple rules for determining whether a particular request can be satised without causing deadlock given the current allocation of chopsticks to philosophers. 7.21 We can obtain the banker s algorithm for a single resource type from the general bankers algorithm simply by reducing the dimensionality of the various arrays by 1. Show through an example that we cannot

by 1. Show through an example that we cannot implement the multipleresourcetype bankers scheme by applying the singleresourcetype scheme to each resource type individually. 7.22 Consider the following snapshot of a system: Allocation Max ABCD ABCD P0 3014 5117 P1 2210 3211 P2 3121 3321 P3 0510 4612 P4 4212 6325 Using the bankers algorithm, determine whether or not each of the following states is unsafe. If the state is safe, illustrate the order in which the processes may complete. Otherwise,

in which the processes may complete. Otherwise, illustrate why the state is unsafe. a.Available (0,3,0,1) b. Available (1,0,0,2)344 Chapter 7 Deadlocks 7.23 Consider the following snapshot of a system: Allocation Max Available ABCD ABCD ABCD P0 2001 4212 3321 P1 3121 5252 P2 2103 2316 P3 1312 1424 P4 1432 3665 Answer the following questions using the bankers algorithm: a. Illustrate that the system is in a safe state by demonstrating an order in which the processes may complete. b. If a request

which the processes may complete. b. If a request from process P1arrives for (1 ,1,0,0), can the request be granted immediately? c. If a request from process P4arrives for (0 ,0,2,0), can the request be granted immediately? 7.24 What is the optimistic assumption made in the deadlockdetection algorithm? How can this assumption be violated? 7.25 As i n g l e  l a n eb r i d g ec o n n e c t st h et w oV e r m o n tv i l l a g e so fN o r t h Tunbridge and South Tunbridge. Farmers in the two

Tunbridge and South Tunbridge. Farmers in the two villages use this bridge to deliver their produce to the neighboring town. The bridge can become deadlocked if a northbound and a southbound farmer get on the bridge at the same time. (Vermont farmers are stubborn and are unable to back up.) Using semaphores andor mutex locks, design an algorithm in pseudocode that prevents deadlock. Initially, do not be concerned about starvation (the situation in which northbound farmers prevent southbound

in which northbound farmers prevent southbound farmers from using the bridge, or vice versa). 7.26 Modify your solution to Exercise 7.25 so that it is starvationfree. Programming Problems 7.27 Implement your solution to Exercise 7.25 using POSIX synchronization. In particular, represent northbound and southbound farmers as separate threads. Once a farmer is on the bridge, the associated thread will sleep for a random period of time, representing traveling across the bridge. Design your program

traveling across the bridge. Design your program so that you can create several threads representing the northbound and southbound farmers.Programming Projects 345 Programming Projects Bankers Algorithm For this project, you will write a multithreaded program that implements the bankers algorithm discussed in Section 7.5.3. Several customers request and release resources from the bank. The banker will grant a request only if it leaves the system in a safe state. A request that leaves the system

in a safe state. A request that leaves the system in an unsafe state will be denied. This programming assignment combines three separate topics: (1) multithreading, (2) preventing race con ditions, and (3) deadlock avoidance. The Banker The banker will consider requests from ncustomers for mresources types. as outlined in Section 7.5.3. The banker will keep track of the resources using the following data structures:  these may be any values  0  define NUMBER OF CUSTOMERS 5 define NUMBER OF

0  define NUMBER OF CUSTOMERS 5 define NUMBER OF RESOURCES 3  the available amount of each resource  int available[NUMBER OF RESOURCES]; the maximum demand of each customer  int maximum[NUMBER OF CUSTOMERS][NUMBER OF RESOURCES];  the amount currently allocated to each customer  int allocation[NUMBER OF CUSTOMERS][NUMBER OF RESOURCES];  the remaining need of each customer  int need[NUMBER OF CUSTOMERS][NUMBER OF RESOURCES]; The Customers Create ncustomer threads that request and release

Create ncustomer threads that request and release resources from the bank. The customers will continually loop, requesting and then releasing random numbers of resources. The customers requests for resources will be bounded by their respective values in the need array. The banker will grant a request if it satises the safety algorithm outlined in Section 7.5.3.1. If a request does not leave the system in a safe state, the banker will deny it. Function prototypes for requesting and releasing

Function prototypes for requesting and releasing resources are as follows: int request resources(int customer num, int request[]); int release resources(int customer num, int release[]); These two functions should return 0 if successful (the request has been granted) and 1 if unsuccessful. Multiple threads (customers) will concurrently346 Chapter 7 Deadlocks access shared data through these two functions. Therefore, access must be controlled through mutex locks to prevent race conditions. Both

mutex locks to prevent race conditions. Both the Pthreads and Windows APIsp r o v i d em u t e xl o c k s .T h eu s eo fP t h r e a d sm u t e xl o c k si s covered in Section 5.9.4; mutex locks for Windows systems are described in the project entitled ProducerConsumer Problem at the end of Chapter 5. Implementation You should invoke your program by passing the number of resources of each type on the command line. For example, if there were three resource types, with ten instances of the rst

resource types, with ten instances of the rst type, ve of the second type, and seven of the third type, you would invoke your program follows: .a.out 10 5 7 Theavailable array would be initialized to these values. You may initialize themaximum array (which holds the maximum demand of each customer) using any method you nd convenient. Bibliographical Notes Most research involving deadlock was conducted many years ago. [Dijkstra (1965)] was one of the rst and most inuential contributors in the

of the rst and most inuential contributors in the deadlock area. [Holt (1972)] was the rst person to formalize the notion of deadlocks in terms of an allocationgraph model similar to the one presented in this chapter. Starvation was also covered by [Holt (1972)]. [Hyman (1985)] provided the deadlock example from the Kansas legislature. A study of deadlock handling is provided in [Levine (2003)]. The various prevention algorithms were suggested by [Havender (1968)], who devised the

suggested by [Havender (1968)], who devised the resourceordering scheme for the IBM OS360 system. The bankers algorithm for avoiding deadlocks was developed for a single resource type by [Dijkstra (1965)] and was extended to multiple resource types by [Habermann (1969)]. The deadlockdetection algorithm for multiple instances of a resource type, which is described in Section 7.6.2, was presented by [Coffman et al. (1971)]. [Bach (1987)] describes how many of the algorithms in the traditional UNIX

how many of the algorithms in the traditional UNIX kernel handle deadlock. Solutions to deadlock problems in networks are discussed in works such as [Culler et al. (1998)] and [Rodeheffer and Schroeder (1991)]. The witness lockorder verier is presented in [Baldwin (2002)]. Bibliography [Bach (1987)] M. J. Bach, The Design of the UNIX Operating System ,P r e n t i c eH a l l (1987). [Baldwin (2002)] J. Baldwin, Locking in the Multithreaded FreeBSD Kernel , USENIX BSD (2002).Bibliography 347

Kernel , USENIX BSD (2002).Bibliography 347 [Coffman et al. (1971)] E. G. Coffman, M. J. Elphick, and A. Shoshani, System Deadlocks ,Computing Surveys ,V o l u m e3 ,N u m b e r2( 1 9 7 1 ) ,p a g e s6 7  7 8 . [Culler et al. (1998)] D. E. Culler, J. P . Singh, and A. Gupta, Parallel Computer Architecture: A HardwareSoftware Approach ,M o r g a nK a u f m a n nP u b l i s h e r sI n c . (1998). [Dijkstra (1965)] E. W. Dijkstra, Cooperating Sequential Processes ,T e c h n i c a l report,

Sequential Processes ,T e c h n i c a l report, Technological University, Eindhoven, the Netherlands (1965). [Habermann (1969)] A. N. Habermann, Prevention of System Deadlocks , Communications of the ACM ,V o l u m e1 2 ,N u m b e r7( 1 9 6 9 ) ,p a g e s3 7 3  3 7 7 ,3 8 5 . [Havender (1968)] J. W. Havender, Avoiding Deadlock in Multitasking Sys tems ,IBM Systems Journal ,V o l u m e7 ,N u m b e r2( 1 9 6 8 ) ,p a g e s7 4  8 4 . [Holt (1972)] R. C. Holt, Some Deadlock Properties of Computer

R. C. Holt, Some Deadlock Properties of Computer Systems , Computing Surveys ,V o l u m e4 ,N u m b e r3( 1 9 7 2 ) ,p a g e s1 7 9  1 9 6 . [Hyman (1985)] D. Hyman, The Columbus Chicken Statute and More Bonehead Legislation ,S .G r e e n eP r e s s( 1 9 8 5 ) . [Levine (2003)] G. Levine, Dening Deadlock ,Operating Systems Review ,V o l  ume 37, Number 1 (2003). [Rodeheffer and Schroeder (1991)] T. L. Rodeheffer and M. D. Schroeder, Automatic Reconguration in Autonet ,Proceedings of the ACM

Reconguration in Autonet ,Proceedings of the ACM Symposium on Operating Systems Principles (1991), pages 18397.Part Three Memory Management The main purpose of a computer system is to execute programs. These programs, together with the data they access, must be at least partially in main memory during execution. To improve both the utilization of the CPU and the speed of its response to users, a generalpurpose computer must keep several pro cesses in memory. Many memorymanagement schemes exist,

in memory. Many memorymanagement schemes exist, reect ing various approaches, and the effectiveness of each algorithm depends on the situation. Selection of a memorymanagement scheme for a sys tem depends on many factors, especially on the hardware design of the system. Most algorithms require hardware support.8CHAPTER Main Memory In Chapter 6, we showed how the CPU can be shared by a set of processes. As ar e s u l to f CPU scheduling, we can improve both the utilization of the CPU and the

improve both the utilization of the CPU and the speed of the computers response to its users. To realize this increase in performance, however, we must keep several processes in memorythat is, we must share memory. In this chapter, we discuss various ways to manage memory. The memory management algorithms vary from a primitive baremachine approach to paging and segmentation strategies. Each approach has its own advantages and disadvantages. Selection of a memoryman agement method for a specic

of a memoryman agement method for a specic system depends on many factors, especially on the hardware design of the system. As we shall see, many algorithms require hardware support, leading many systems to have closely integrated hardware and operatingsystem memory management. CHAPTER OBJECTIVES To provide a detailed description of various ways of organizing memory hardware. To explore various techniques of allocating memory to processes. To discuss in detail how paging works in contemporary

discuss in detail how paging works in contemporary computer systems. 8.1 Background As we saw in Chapter 1, memory is central to the operation of a modern computer system. Memory consists of a large array of bytes, each with its own address. The CPU fetches instructions from memory according to the value of the program counter. These instructions may cause additional loading from and storing to specic memory addresses. At y p i c a li n s t r u c t i o n  e x e c u t i o nc y c l e ,f o re x a m

t i o n  e x e c u t i o nc y c l e ,f o re x a m p l e , r s tf e t c h e sa ni n s t r u c  tion from memory. The instruction is then decoded and may cause operands to be fetched from memory. After the instruction has been executed on the operands, results may be stored back in memory. The memory unit sees only 351352 Chapter 8 Main Memory as t r e a mo fm e m o r ya d d r e s s e s ;i td o e sn o tk n o wh o wt h e ya r eg e n e r a t e d( b y the instruction counter, indexing, indirection,

y the instruction counter, indexing, indirection, literal addresses, and so on) or what they are for (instructions or data). Accordingly, we can ignore how a program generates a memory address. We are interested only in the sequence of memory addresses generated by the running program. We begin our discussion by covering several issues that are pertinent to managing memory: basic hardware, the binding of symbolic memory addresses to actual physical addresses, and the distinction between logical

addresses, and the distinction between logical and physical addresses. We conclude the section with a discussion of dynamic linking and shared libraries. 8.1.1 Basic Hardware Main memory and the registers built into the processor itself are the only generalpurpose storage that the CPU can access directly. There are machine instructions that take memory addresses as arguments, but none that take disk addresses. Therefore, any instructions i ne x e c u t i o n ,a n da n yd a t ab e i n gu s e d by

c u t i o n ,a n da n yd a t ab e i n gu s e d by the instructions, must be in one of these directaccess storage devices. If the data are not in memory, they must be moved there before the CPU can operate on them. Registers that are built into the CPU are generally accessible within one cycle of the CPU clock. Most CPUsc a nd e c o d ei n s t r u c t i o n sa n dp e r f o r ms i m p l e operations on register contents at the rate of one or more operations per clock tick. The same cannot be said

operations per clock tick. The same cannot be said of main memory, which is accessed via a transaction on the memory bus. Completing a memory access may take many cycles of the CPU clock. In such cases, the processor normally needs to stall ,s i n c ei td o e sn o th a v et h ed a t ar e q u i r e dt oc o m p l e t et h ei n s t r u c t i o nt h a ti t is executing. This situation is intolerable because of the frequency of memory accesses. The remedy is to add fast memory between the CPU and

remedy is to add fast memory between the CPU and main memory, typically on the CPU chip for fast access. Such a cache was described in Section 1.8.3. To manage a cache built into the CPU, the hardware automatically speeds up memory access without any operatingsystem control. Not only are we concerned with the relative speed of accessing physical memory, but we also must ensure correct operation. For proper system operation we must protect the operating system from access by user processes. On

operating system from access by user processes. On multiuser systems, we must additionally protect user processes from one another. This protection must be provided by the hardware because the operating system doesnt usually intervene between the CPU and its memory accesses (because of the resulting performance penalty). Hardware implements this production in several different ways, as we show throughout the chapter. Here, we outline one possible implementation. We rst need to make sure that

implementation. We rst need to make sure that each process has a separate memory space. Separate perprocess memory space protects the processes from each other and is fundamental to having multiple processes loaded in memory for concurrent execution. To separate memory spaces, we need the ability to determine the range of legal addresses that the process may access and to ensure that the process can access only these legal addresses. We can provide this protection by using two registers, usually

this protection by using two registers, usually a base and a limit, as illustrated in Figure 8.1. The base register holds the smallest legal physical memory address; the limit register species the size of the range. For example, if the base register holds8.1 Background 353 operating system0 256000 300040300040 base 120900 limit420940 880000 1024000process process process Figure 8.1 Ab a s ea n dal i m i tr e g i s t e rd e  n eal o g i c a la d d r e s ss p a c e . 300040 and the limit register

d r e s ss p a c e . 300040 and the limit register is 120900, then the program can legally access all addresses from 300040 through 420939 (inclusive). Protection of memory space is accomplished by having the CPU hardware compare every address generated in user mode with the registers. Any attempt by a program executing in user mode to access operatingsystem memory or other users memory results in a trap to the operating system, which treats the attempt as a fatal error (Figure 8.2). This scheme

attempt as a fatal error (Figure 8.2). This scheme prevents a user program from (accidentally or deliberately) modifying the code or data structures of either the operating system or other users. The base and limit registers can be loaded only by the operating system, which uses a special privileged instruction. Since privileged instructions can be executed only in kernel mode, and since only the operating system executes in kernel mode, only the operating system can load the base and limit

the operating system can load the base and limit registers.base memorytrap to operating system monitoraddressing erroraddress yes yes no noCPUbase H11001 limit   Figure 8.2 Hardware address protection with base and limit registers.354 Chapter 8 Main Memory This scheme allows the operating system to change the value of the registers but prevents user programs from ch anging the registers contents. The operating system, executing in kernel mode, is given unrestricted access to both operatingsystem

given unrestricted access to both operatingsystem memory and users memory. This provision allows the operating system to load users programs into users memory, to dump out those programs in case of errors, to access and modify parameters of system calls, to perform IO to and from user memory, and to provide many other services. Consider, for example, that an operating system for a multiprocessing system must execute context switches, storing the state of one process from the registers into main

state of one process from the registers into main memory before loading the next processs context from main memory into the registers. 8.1.2 Address Binding Usually, a program resides on a disk as a binary executable le. To be executed, the program must be brought into memory and placed within a process. Depending on the memory management in use, the process may be moved between disk and memory during its execution . The processes on the disk that are waiting to be brought into memory for

that are waiting to be brought into memory for execution form the input queue . The normal singletasking procedure is to select one of the processes in the input queue and to load that process into memory. As the process is executed, it accesses instructions and data from memory. Eventually, the process terminates, and its memory space is declared available. Most systems allow a user process to reside in any part of the physical memory. Thus, although the address space of the computer may start

the address space of the computer may start at 00000, the rst address of the user process need not be 00000. You will see later how au s e rp r o g r a ma c t u a l l yp l a c e sap r o c e s si np h y s i c a lm e m o r y . In most cases, a user program goes through several stepssome of which may be optionalbefore being executed (Figure 8.3). Addresses may be represented in different ways during these steps. Addresses in the source program are generally symbolic (such as the variable count ). A

symbolic (such as the variable count ). A compiler typically binds these symbolic addresses to relocatable addresses (such as 14 bytes from the beginning of this module ). The linkage editor or loader in turn binds the relocatable addresses to absolute addresses (such as 74014). Each binding is a mapping from one address space to another. Classically, the binding of instructions and data to memory addresses can be done at any step along the way: Compile time .I fy o uk n o wa tc o m p i l et i m

Compile time .I fy o uk n o wa tc o m p i l et i m ew h e r et h ep r o c e s sw i l lr e s i d e in memory, then absolute code can be generated. For example, if you know that a user process will reside starting at location R,then the generated compiler code will start at that location and extend up from there. If, at some later time, the starting location changes, then it will be necessary to recompile this code. The MSDOS .COM format programs are bound at compile time. Load time .I fi ti sn o

are bound at compile time. Load time .I fi ti sn o tk n o w na tc o m p i l et i m ew h e r et h ep r o c e s sw i l lr e s i d e in memory, then the compiler must generate relocatable code .I nt h i sc a s e , nal binding is delayed until load time. If the starting address changes, we need only reload the user code to incorporate this changed value.8.1 Background 355 dynamic linkingsource program object module linkage editor load module loader inmemory binary memory imageother object

loader inmemory binary memory imageother object modulescompile time load time execution time (run time)compiler or assembler system librarydynamically loaded system library Figure 8.3 Multistep processing of a user program. Execution time .I ft h ep r o c e s sc a nb em o v e dd u r i n gi t se x e c u t i o nf r o m one memory segment to another, then binding must be delayed until run time. Special hardware must be available for this scheme to work, as will be discussed in Section 8.1.3. Most

work, as will be discussed in Section 8.1.3. Most generalpurpose operating systems use this method. Am a j o rp o r t i o no ft h i sc h a p t e ri sd e v o t e dt os h o w i n gh o wt h e s ev a r i o u sb i n d  ings can be implemented effectively in a computer system and to discussing appropriate hardware support. 8.1.3 Logical Versus Physical Address Space An address generated by the CPU is commonly referred to as a logical address , whereas an address seen by the memory unitthat is, the one

an address seen by the memory unitthat is, the one loaded into thememoryaddress register of the memoryis commonly referred to as a physical address . The compiletime and loadtime addressbinding methods generate iden tical logical and physical addr esses. However, the executiontime address356 Chapter 8 Main Memory H11001 MMUCPU memory 1434614000relocation register 346logical addressphysical address Figure 8.4 Dynamic relocation using a relocation register. binding scheme results in differing

register. binding scheme results in differing logical and physical addresses. In this case, we usually refer to the logical address as a virtual address . We use logical address and virtual address interchangeably in this text. The set of all logical addresses generated by a program is a logical address space .T h es e t of all physical addresses corresponding to these logical addresses is a physical address space .T h u s ,i nt h ee x e c u t i o n  t i m ea d d r e s s  b i n d i n gs c h e m

o n  t i m ea d d r e s s  b i n d i n gs c h e m e ,t h el o g i c a l and physical address spaces differ. The runtime mapping from virtual to physical addresses is done by a hardware device called the memorymanagement unit (MMU ).W ec a nc h o o s e from many different methods to accomplish such mapping, as we discuss in Section 8.3 through Section 8.5. For the time being, we illustrate this mapping with a simple MMU scheme that is a generalization of the baseregister scheme described in

of the baseregister scheme described in Section 8.1.1. The base register is now called a relocation register . The value in the relocation register is added to every address generated by a user process at the time the address is sent to memory (see Figure 8.4). For example, if the base is at 14000, then an attempt by the user to address location 0i sd y n a m i c a l l yr e l o c a t e dt ol o c a t i o n1 4 0 0 0 ;a na c c e s st ol o c a t i o n3 4 6i sm a p p e d to location 14346. The user

o n3 4 6i sm a p p e d to location 14346. The user program never sees the real physical addresses. The program can create a pointer to location 346, store it in memory, manipulate it, and compare it with other addressesall as the number 346. Only when it is used as a memory address (in an indirect load or store, perhaps) is it relocated relative to the base register. The user program deals with logical addresses. The memorymapping hardware converts logical addresses into physical addresses. This

logical addresses into physical addresses. This form of executiontime binding was discussed in Section 8.1.2. The nal location of ar e f e r e n c e dm e m o r ya d d r e s si sn o td e t e r m i n e du n t i lt h er e f e r e n c ei sm a d e . We now have two different types of addresses: logical addresses (in the range 0 to max)a n dp h y s i c a la d d r e s s e s( i nt h er a n g e R0t o Rmaxfor a base value R). The user program generates only logical addresses and thinks that the process

only logical addresses and thinks that the process runs in locations 0 to max. However, these logical addresses must be mapped to physical addresses before they are used. The concept of a logical8.1 Background 357 address space that is bound to a separate physical address space is central to proper memory management. 8.1.4 Dynamic Loading In our discussion so far, it has been necessary for the entire program and all data of a process to be in physical memory for the process to execute. The size

memory for the process to execute. The size of a process has thus been limited to the size of physical memory. To obtain better memoryspace utilization, we can use dynamic loading .W i t hd y n a m i c loading, a routine is not loaded until it is called. All routines are kept on disk in a relocatable load format. The main program is loaded into memory and is executed. When a routine needs to call another routine, the calling routine rst checks to see whether the other routine has been loaded. If

see whether the other routine has been loaded. If it has not, the relocatable linking loader is called to load the desired routine into memory and to update the programs address tables to reect this change. Then control is passed to the newly loaded routine. The advantage of dynamic loading is that a routine is loaded only when it is needed. This method is particularly useful when large amounts of code are needed to handle infrequently occurring cases, such as error routines. In this case,

cases, such as error routines. In this case, although the total program size may be large, the portion that is used (and hence loaded) may be much smaller. Dynamic loading does not require special support from the operating system. It is the responsibility of the users to design their programs to take advantage of such a method. Operating systems may help the programmer, however, by providing library routines to implement dynamic loading. 8.1.5 Dynamic Linking and Shared Libraries Dynamically

Dynamic Linking and Shared Libraries Dynamically linked libraries are system libraries that are linked to user programs when the programs are run (refer back to Figure 8.3). Some operating systems support only static linking ,i nw h i c hs y s t e ml i b r a r i e sa r et r e a t e d like any other object module and are combined by the loader into the binary program image. Dynamic linking, in contrast, is similar to dynamic loading. Here, though, linking, rather than loading, is postponed until

linking, rather than loading, is postponed until execution time. This feature is usually used with system libraries, such as language subroutine libraries. Without this facility, each program on a system must include a copy of its language library (or at least the routines referenced by the program) in the executable image. This requirement wastes both disk space and main memory. With dynamic linking, a stub is included in the image for each library routine reference. The stub is a small piece o

routine reference. The stub is a small piece o fc o d et h a ti n d i c a t e sh o wt ol o c a t e the appropriate memoryresident library routine or how to load the library if the routine is not already present. When the stub is executed, it checks to see whether the needed routine is already in memory. If it is not, the program loads the routine into memory. Either way, the stub replaces itself with the address of the routine and executes the routine. Thus, the next time that particular code

routine. Thus, the next time that particular code segment is reached, the library routine is executed directly, incurring no cost for dynamic linking. Under this scheme, all processes that use a language library execute only one copy of the library code. This feature can be extended to library updates (such as bug xes). A library may be replaced by a new version, and all programs that reference the library will automatically use the new version. Without dynamic linking, all such358 Chapter 8

Without dynamic linking, all such358 Chapter 8 Main Memory programs would need to be relinked to gain access to the new library. So that programs will not accidentally execute new, incompatible versions of libraries, version information is included in both the program and the library. More than one version of a library may be loaded into memory, and each program uses its version information to decide which copy of the library to use. Versions with minor changes retain the same version number,

with minor changes retain the same version number, whereas versions with major changes increment the number. Thus, only programs that are compiled with the new library version are affected by any incompatible changes incorporated in it. Other programs linked before the new library was installed will continue using the older library. This system is also known as shared libraries . Unlike dynamic loading, dynamic linking and shared libraries generally require help from the operating system. If the

require help from the operating system. If the processes in memory are protected from one another, then the operating system is the only entity that can check to see whether the needed routine i si na n o t h e rp r o c e s s  sm e m o r ys p a c e or that can allow multiple processes to access the same memory addresses. We elaborate on this concept when we discuss paging in Section 8.5.4. 8.2 Swapping A process must be in memory to be executed. A process, however, can be swapped temporarily out

A process, however, can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution (Figure 8.5). Swapping makes it possible for the total physical address space of all processes to exceed the real physical memory of the system, thus increasing the degree of multiprogramming in a system. 8.2.1 Standard Swapping Standard swapping involves moving p rocesses between main memory and ab a c k i n gs t o r e .T h eb a c k i n gs t o r ei sc o m m o

t o r e .T h eb a c k i n gs t o r ei sc o m m o n l yaf a s td i s k .I tm u s tb el a r g eoperating system swap out swap in user space main memorybacking storeprocess P2process P11 2 Figure 8.5 Swapping of two processes using a disk as a backing store.8.2 Swapping 359 enough to accommodate copies of all memory images for all users, and it must provide direct access to these memory images. The system maintains a ready queue consisting of all processes whose memory images are on the backing

processes whose memory images are on the backing store or in memory and are ready to run. Whenever the CPU scheduler decides to execute a process, it calls the dispatcher. The dispatcher checks to see whether the next process in the queue is in memory. If it is not, and if there is no free memory region, the dispatcher swaps out a process currently in memory and swaps in the desired process. It then reloads registers and transfers control to the selected process. The contextswitch time in such a

selected process. The contextswitch time in such a swapping system is fairly high. To get an idea of the contextswitch time, lets assume that the user process is 100 MBin size and the backing store is a standard hard disk with a transfer rate of 50 MB per second. The actual transfer of the 100 MBprocess to or from main memory takes 100 MB50 MBper second  2 seconds The swap time is 200 milliseconds. Since we must swap both out and in, the total swap time is about 4,000 milliseconds. (Here, we are

time is about 4,000 milliseconds. (Here, we are ignoring other disk performance aspects, which we cover in Chapter 10.) Notice that the major part of the swap time is transfer time. The total transfer time is directly proportional to the amount of memory swapped. If we have a computer system with 4 GBof main memory and a resident operating system taking 1 GB, the maximum size of the user process is 3 GB. However, many user processes may be much smaller than thissay, 100 MB.A1 0 0  M Bp r o c e s

than thissay, 100 MB.A1 0 0  M Bp r o c e s sc o u l db es w a p p e do u ti n2s e c o n d s ,c o m p a r e dw i t h the 60 seconds required for swapping 3 GB.C l e a r l y ,i tw o u l db eu s e f u lt o know exactly how much memory a user process isusing, not simply how much it might be using. Then we would need to swap only what is actually used, reducing swap time. For this method to be effective, the user must keep the system informed of any changes in memory requirements. Thus, ap r o c e s

changes in memory requirements. Thus, ap r o c e s sw i t hd y n a m i cm e m o r yr e q u i r e m e n t sw i l ln e e dt oi s s u es y s t e mc a l l s (request memory() and release memory() )t oi n f o r mt h eo p e r a t i n gs y s t e m of its changing memory needs. Swapping is constrained by other factors as well. If we want to swap ap r o c e s s ,w em u s tb es u r et h a ti ti sc o m p l e t e l yi d l e .O fp a r t i c u l a rc o n c e r n is any pending IO.Ap r o c e s sm a yb ew a i t

n is any pending IO.Ap r o c e s sm a yb ew a i t i n gf o ra n IO operation when we want to swap that process to free up memory. However, if the IO is asynchronously accessing the user memory for IObuffers, then the process cannot be swapped. Assume that the IO operation is queued because the device is busy. If we were to swap out process P1and swap in process P2,t h e IOoperation might then attempt to use memory that now belongs to process P2.T h e r ea r et w om a i ns o l u t i o n st ot h i

h e r ea r et w om a i ns o l u t i o n st ot h i sp r o b l e m :n e v e rs w a pap r o c e s sw i t h pending IO,o re x e c u t e IO operations only into operatingsystem buffers. Transfers between operatingsystem buffers and process memory then occur only when the process is swapped in. Note that this double buffering itself adds overhead. We now need to copy the data again, from kernel memory to user memory, before the user process can access it. Standard swapping is not used in modern

access it. Standard swapping is not used in modern operating systems. It requires too much swapping time and provides too little execution time to be a reasonable360 Chapter 8 Main Memory memorymanagement solution. Modied versions o fs w a p p i n g ,h o w e v e r ,a r e found on many systems, including UNIX ,L i n u x ,a n dW i n d o w s .I no n ec o m m o n variation, swapping is normally disabled but will start if the amount of free memory (unused memory available for the operating system or

memory available for the operating system or processes to use) falls below a threshold amount. Swapping is halted when the amount of free memory increases. Another variation involves swapping portions of processesrather than entire processesto decrease swap time. Typically, these modied forms of swapping work in conjunction with virtual memory, which we cover in Chapter 9. 8.2.2 Swapping on Mobile Systems Although most operating systems for PCsa n ds e r v e r ss u p p o r ts o m em o d i  e d

n ds e r v e r ss u p p o r ts o m em o d i  e d version of swapping, mobile systems typically do not support swapping in any form. Mobile devices generally use ash memory rather than more spacious hard disks as their persistent storage. The resulting space constraint is one reason why mobile operatingsystem designers avoid swapping. Other reasons include the limited number of writes that ash memory can tolerate before it becomes unreliable and the poor throughput between main memory and ash

the poor throughput between main memory and ash memory in these devices. Instead of using swapping, when free memory falls below a certain threshold, Apples i OSasks applications to voluntarily relinquish allocated memory. Readonly data (such as code) are removed from the system and later reloaded from ash memory if necessary. Data that have been modied (such as the stack) are never removed. However, any applications that fail to free up sufcient memory may be terminated by the operating system.

memory may be terminated by the operating system. Android does not support swapping and adopts a strategy similar to that used by i OS.I tm a yt e r m i n a t eap r o c e s si fi n s u f  c i e n tf r e em e m o r yi sa v a i l a b l e . However, before terminating a pr ocess, Android writes its application state to ash memory so that it can be quickly restarted. Because of these restrictions, developers for mobile systems must carefully allocate and release memory to ensure that their

allocate and release memory to ensure that their applications do not use too much memory or suffer from memory leaks. Note that both i OSand Android support paging, so they do have memorymanagement abilities. We discuss paging later in this chapter. 8.3 Contiguous Memory Allocation The main memory must accommodate both the operating system and the various user processes. We therefore need to allocate main memory in the most efcient way possible. This section explains one early method, contiguous

This section explains one early method, contiguous memory allocation. The memory is usually divided into two partitions: one for the resident operating system and one for the user processes. We can place the operating system in either low memory or high memory. The major factor affecting this decision is the location of the interrupt vector. Since the interrupt vector is often in low memory, programmers usually place the operating system in low memory as well. Thus, in this text, we discuss only

as well. Thus, in this text, we discuss only the situation in which8.3 Contiguous Memory Allocation 361 the operating system resides in low memory. The development of the other situation is similar. We usually want several user processes to reside in memory at the same time. We therefore need to consider how to allocate available memory to the processes that are in the input queue waiting to be brought into memory. In contiguous memory allocation ,e a c hp r o c e s si sc o n t a i n e di nas i

,e a c hp r o c e s si sc o n t a i n e di nas i n g l es e c t i o no f memory that is contiguous to the section containing the next process. 8.3.1 Memory Protection Before discussing memory allocation further, we must discuss the issue of memory protection. We can prevent a process from accessing memory it does not own by combining two ideas previously discussed. If we have a system with a relocation register (Section 8.1.3), together with a limit register (Section 8.1.1), we accomplish our

limit register (Section 8.1.1), we accomplish our goal. The relocation register contains the value of the smallest physical address; the limit register contains the range of logical addresses (for example, relocation  100040 and limit  74600). Each logical address must fall within the range specied by the limit register. The MMU maps the logical address dynamically by adding the value in the relocation register. This mapped address is sent to memory (Figure 8.6). When the CPU scheduler selects a

(Figure 8.6). When the CPU scheduler selects a process for execution, the dispatcher loads the relocation and limit registers with the correct values as part of the context switch. Because every address generated by a CPU is checked against these registers, we can protect both the operating system and the other users programs and data from being modied by this running process. The relocationregister scheme provides an effective way to allow the operating systems size to change dynamically. This

operating systems size to change dynamically. This exibility is desirable in many situations. For example, the operating system contains code and buffer space for device drivers. If a device driver (or other operatingsystem service) is not commonly used, we do not want to keep the code and data in memory, as we might be able to use that space for other purposes. Such code is sometimes called transient operatingsystem code; it comes and goes as needed. Thus, using this code changes the size of

needed. Thus, using this code changes the size of the operating system during program execution. CPU memorylogical address trap: addressing errornoyesphysical addressrelocation register H11001 H11021limit register Figure 8.6 Hardware support for relocation and limit registers.362 Chapter 8 Main Memory 8.3.2 Memory Allocation Now we are ready to turn to memory allocation. One of the simplest methods for allocating memory is to divide memory into several xedsized partitions . Each partition may

several xedsized partitions . Each partition may contain exactly one process. Thus, the degree of multiprogramming is bound by the number of partitions. In this multiple partition method ,w h e nap a r t i t i o ni sf r e e ,ap r o c e s si ss e l e c t e df r o mt h ei n p u t queue and is loaded into the free partition. When the process terminates, the partition becomes available for another process. This method was originally used by the IBM OS360 operating system (called MFT)but is no longer

operating system (called MFT)but is no longer in use. The method described next is a generalization of the xedpartition scheme (called MVT ); it is used primarily in batch environments. Many of the ideas presented here are also applicable to a timesharing environment in which pure segmentation is used for memory management (Section 8.4). In the variablepartition scheme, the operating system keeps a table indicating which parts of memory are available and which are occupied. Initially, all memory

and which are occupied. Initially, all memory is available for user processes and is considered one large block of available memory, a hole .E v e n t u a l l y ,a sy o uw i l ls e e ,m e m o r y contains a set of holes of various sizes. As processes enter the system, they are put into an input queue. The operating system takes into account the memory requirements of each process and the amount of available memory space in determining which processes are allocated memory. When a process is

processes are allocated memory. When a process is allocated space, it is loaded into memory, and it can then compete for CPU time. When a process terminates, it releases its memory, which the operating system may then ll with another process from the input queue. At any given time, then, we have a list of available block sizes and an input queue. The operating system can order the input queue according to as c h e d u l i n ga l g o r i t h m .M e m o r yi sa l l o c a t e dt op r o c e s s e su

e m o r yi sa l l o c a t e dt op r o c e s s e su n t i l , n a l l y ,t h e memory requirements of the next process cannot be satisedthat is, no available block of memory (or hole) is large enough to hold that process. The operating system can then wait until a large enough block is available, or it can skip down the input queue to see whether the smaller memory requirements of some other process can be met. In general, as mentioned, the memory blocks available comprise a setof holes of

memory blocks available comprise a setof holes of various sizes scattered throughout memory. When a process arrives and needs memory, the system searches the set for a hole that is large enough for this process. If the hole is too large, it is split into two parts. One part is allocated to the arriving process; the other is returned to the set of holes. When ap r o c e s st e r m i n a t e s ,i tr e l e a s e si t sb l o c ko fm e m o r y ,w h i c hi st h e np l a c e db a c k in the set of

h i c hi st h e np l a c e db a c k in the set of holes. If the new hole is adjace nt to other holes, these adjacent holes are merged to form one larger hole. At this point, the system may need to check whether there are processes waiting for memory and whether this newly freed and recombined memory could satisfy the demands of any of these waiting processes. This procedure is a particular instance of the general dynamic storage allocation problem ,w h i c hc o n c e r n sh o wt os a t i s f yar

,w h i c hc o n c e r n sh o wt os a t i s f yar e q u e s to fs i z e nfrom a list of free holes. There are many solutions to this problem. The rstt ,bestt , and worstt strategies are the ones most commonly used to select a free hole from the set of available holes.8.3 Contiguous Memory Allocation 363 First t .A l l o c a t et h e r s th o l et h a ti sb i ge n o u g h .S e a r c h i n gc a ns t a r te i t h e r at the beginning of the set of holes or at the location where the previous rstt

holes or at the location where the previous rstt search ended. We can stop searching as soon as we nd a free hole that is large enough. Best t .A l l o c a t et h es m a l l e s th o l et h a ti sb i ge n o u g h .W em u s ts e a r c ht h e entire list, unless the list is ordered by size. This strategy produces the smallest leftover hole. Worst t . Allocate the largest hole. Again, we must search the entire list, unless it is sorted by size. This strategy produces the largest leftover hole,

This strategy produces the largest leftover hole, which may be more useful than the smaller leftover hole from a bestt approach. Simulations have shown that both rst t and best t are better than worst t in terms of decreasing time and storage utilization. Neither rst t nor best t is clearly better than the other in terms of storage utilization, but rst t is generally faster. 8.3.3 Fragmentation Both the rstt and bestt strategies for memory allocation suffer from external fragmentation .A sp r o

suffer from external fragmentation .A sp r o c e s s e sa r el o a d e da n dr e m o v e df r o mm e m o r y , the free memory space is broken into little pieces. External fragmentation exists when there is enough total memory space to satisfy a request but the available spaces are not contiguous: storage is fragmented into a large number of small holes. This fragmentation problem can be severe. In the worst case, we could have a block of free (or wasted) memory between every two processes. If

(or wasted) memory between every two processes. If all these small pieces of memory were in one big free block instead, we might be able to run several more processes. Whether we are using the rstt or bestt strategy can affect the amount of fragmentation. (First t is better for some systems, whereas best t is better for others.) Another factor is which end of a free block is allocated. (Which is the leftover piecethe one on the top or the one on the bottom?) No matter which algorithm is used,

on the bottom?) No matter which algorithm is used, however, extern al fragmentation will be a problem. Depending on the total amount of memory storage and the average process size, external fragmentation may be a minor or a major problem. Statistical analysis of rst t, for instance ,r e v e a l st h a t ,e v e nw i t hs o m eo p t i m i z a t i o n , given Nallocated blocks, another 0.5 Nblocks will be lost to fragmentation. That is, onethird of memory may be unusable! This property is known as

memory may be unusable! This property is known as the 50percent rule . Memory fragmentation can be internal as well as external. Consider a multiplepartition allocation scheme with a hole of 18,464 bytes. Suppose that the next process requests 18,462 bytes. If we allocate exactly the requested block, we are left with a hole of 2 bytes. The overhead to keep track of this hole will be substantially larger than the hole itself. The general approach to avoiding this problem is to break the physical

to avoiding this problem is to break the physical memory into xedsized blocks and allocate memory in units based on block size. With this approach, the memory allocated to a process may be slightly larger than the requested memory. The difference between these two numbers is internal fragmentation unused memory that is internal to a partition.364 Chapter 8 Main Memory One solution to the problem of external fragmentation is compaction .T h e goal is to shufe the memory contents so as to place

is to shufe the memory contents so as to place all free memory together in one large block. Compaction is not always possible, however. If relocation is static and is done at assembly or load time, compaction cannot be done. It is possible only if relocation is dynamic and is done at execution time. If addresses are relocated dynamically, relocation r equires only moving the program and data and then changing the base register to reect the new base address. When compaction is possible, we must

base address. When compaction is possible, we must determine its cost. The simplest compaction algorithm is to move all processes toward one end of memory; all holes move in the other direction, producing one large hole of available memory. This scheme can be expensive. Another possible solution to the externalfragmentation problem is to permit the logical address space of the processes to be noncontiguous, thus allowing a process to be allocated physical memory wherever such memory is

allocated physical memory wherever such memory is available. Two complementary techniques achieve this solution: segmentation (Section 8.4) and paging (Section 8.5). These techniques can also be combined. Fragmentation is a general problem in computing that can occur wherever we must manage blocks of data. We discuss the topic further in the storage management chapters (Chapters 10 through and 12). 8.4 Segmentation As weve already seen, the users view of memory is not the same as the actual

users view of memory is not the same as the actual physical memory. This is equally true of the programmers view of memory. Indeed, dealing with memory in terms of its ph ysical properties is inconvenient to both the operating system and the programmer. What if the hardware could provide a memory mechanism that mapped the programmers view to the actual physical memory? The system would have more freedom to manage memory, while the programmer would have a more natural programming environment.

would have a more natural programming environment. Segmentation provides such a mechanism. 8.4.1 Basic Method Do programmers think of memory as a linear array of bytes, some containing instructions and others containing data? Most programmers would say no. Rather, they prefer to view memory as a collection of variablesized segments, with no necessary ordering among the segments (Figure 8.7). When writing a program, a programmer thinks of it as a main program with a set of methods, procedures, or

main program with a set of methods, procedures, or functions. It may also include various data structures: objects, arrays, stacks, variables, and so on. Each of these modules or data elements is referred to by name. The programmer talks about the stack,  the math library, and the main program without caring what addresses in memory these elements occupy. She is not concerned with whether the stack is stored before or after the Sqrt() function. Segments vary in length, and the length of each is

Segments vary in length, and the length of each is intrinsically dened by its purpose in the program. Elements within a segment are identied by their offset from the beginning of the segment: the rst statement of the program, the seventh stack frame entry in the stack, the fth instruction of the Sqrt() ,a n ds oo n . Segmentation is a memorymanagement scheme that supports this pro grammer view of memory. A logical address space is a collection of segments.8.4 Segmentation 365 logical address

of segments.8.4 Segmentation 365 logical address subroutine stack symbol table main programSqrt Figure 8.7 Programmers view of a program. Each segment has a name and a length. The addresses specify both the segment name and the offset within the segment. The programmer therefore species each address by two quantities: a segment name and an offset. For simplicity of implementation, segments are numbered and are referred to by a segment number, rather than by a segment name. Thus, a logical

rather than by a segment name. Thus, a logical address consists of a two tuple: segmentnumber, offset . Normally, when a program is compiled, the compiler automatically constructs segments reecting the input program. A C compiler might create separate segments for the following: 1.The code 2.Global variables 3.The heap, from which memory is allocated 4.The stacks used by each thread 5.The standard C library Libraries that are linked in during compile time might be assigned separate segments. The

time might be assigned separate segments. The loader would take all these segments and assign them segment numbers. 8.4.2 Segmentation Hardware Although the programmer can now refer to objects in the program by a twodimensional address, the actual physical memory is still, of course, a one dimensional sequence of bytes. Thus, we must dene an implementation to map twodimensional userdened addresses into onedimensional physical366 Chapter 8 Main Memory CPU physical memorysd  trap: addressing

Memory CPU physical memorysd  trap: addressing errornoyessegment tablelimit bases Figure 8.8 Segmentation hardware. addresses. This mapping is effected by a segment table .E a c he n t r yi nt h e segment table has a segment base and a segment limit .T h es e g m e n tb a s e contains the starting physical address where the segment resides in memory, and the segment limit species the length of the segment. The use of a segment table is illustrated in Figure 8.8. A logical address consists of two

in Figure 8.8. A logical address consists of two parts: a segment number, s,and an offset into that segment, d. The segment number is used as an in dex to the segment table. The offset dof the logical address must be between 0 and the segment limit. If it is not, we trap to the operating system (logical addressing attempt beyond end of segment). When an offset is legal, it is added to the segment base to produce the address in physical memory of the desired byte. The segment table is thus

of the desired byte. The segment table is thus essentially an array of baselimit register pairs. As an example, consider the situation shown in Figure 8.9. We have ve segments numbered from 0 through 4. The segments are stored in physical memory as shown. The segment table has a separate entry for each segment, giving the beginning address of the se gment in physical memory (or base) and the length of that segment (or limit). For example, segment 2 is 400 bytes long and begins at location 4300.

2 is 400 bytes long and begins at location 4300. Thus, a reference to byte 53 of segment 2 is mapped onto location 4300  53  4353. A reference to segment 3, byte 852, is mapped to 3200 (the base of segment 3)  852  4052. A reference to byte 1222 of segment 0w o u l dr e s u l ti nat r a pt ot h eo p e r a t i n gs y s t e m ,a st h i ss e g m e n ti so n l y1 , 0 0 0 bytes long. 8.5 Paging Segmentation permits the physical address space of a process to be non contiguous. Paging is another

a process to be non contiguous. Paging is another memorymanagement scheme that offers this advantage. However, paging avoids extern al fragmentation and the need for8.5 Paging 367 logical address spacesubroutine stack symbol table main programSqrt1400 physical memory2400 3200 segment 24300 4700 5700 6300 6700segment tablelimit 0 1 2 3 41000 400 400 1100 1000base 1400 6300 4300 3200 4700segment 0segment 3 segment 4 segment 2 segment 1segment 0 segment 3 segment 4 segment 1 Figure 8.9 Example of

3 segment 4 segment 1 Figure 8.9 Example of segmentation. compaction, whereas segmentation does not. It also solves the considerable problem of tting memory chunks of varying sizes onto the backing store. Most memorymanagement schemes used before the introduction of paging suffered from this problem. The problem arises because, when code fragments or data residing in main memory need to be swapped out, space must be found on the backing store. The backing store has the same fragmentation

The backing store has the same fragmentation problems discussed in connection with main memory, but access is much slower, so compaction is impossible. Because of its advantages over earlier methods, paging in its various forms is used in most operating systems, from those for mainframes through those for smartphones. Paging is implemented through cooperation between the operating system and the computer hardware. 8.5.1 Basic Method The basic method for implementing paging involves breaking

method for implementing paging involves breaking physical mem ory into xedsized blocks called frames and breaking logical memory into blocks of the same size called pages .W h e nap r o c e s si st ob ee x e c u t e d ,i t s pages are loaded into any available memory frames from their source (a le system or the backing store). The backing store is divided into xedsized blocks that are the same size as the memory frames or clusters of multiple frames. This rather simple idea has great

multiple frames. This rather simple idea has great functionality and wide ramications. For example, the logical address space is now totally separate from the physical address space, so a process can have a logical 64bit address space even though the system has less than 264bytes of physical memory. The hardware support for paging is illustrated in Figure 8.10. Every address generated by the CPU is divided into two parts: a page number (p) and a page368 Chapter 8 Main Memory physical memoryf

a page368 Chapter 8 Main Memory physical memoryf logical address page tablephysical addressCPU p p fd d ff0000  0000 f1111  1111 Figure 8.10 Paging hardware. offset (d) .T h ep a g en u m b e ri su s e da sa ni n d e xi n t oa page table .T h ep a g et a b l e contains the base address of each page in physical memory. This base address is combined with the page offset to dene the physical memory address that is sent to the memory unit. The paging model of memory is shown in Figure 8.11.page 0

model of memory is shown in Figure 8.11.page 0 page 1 page 2 page 3 logical memorypage 1 page 3page 0 page 2 physical memorypage tableframe number 1 4 3 70 1 2 30 1 2 3 4 5 6 7Figure 8.11 Paging model of logical and physical memory.8.5 Paging 369 The page size (like the frame size) is dened by the hardware. The size of a page is a power of 2, varying between 512 bytes and 1 GBper page, depending on the computer architecture. The selection of a power of 2 as a page size makes the translation of a

of 2 as a page size makes the translation of a logical address into a page number and page offset particularly easy. If the size of the logical address space is 2m,a n dap a g es i z ei s 2nbytes, then the highorder mnbits of a logical address designate the page number, and the nloworder bits designate the page offset. Thus, the logical address is as follows:pdpage number page offset m  n n where pis an index into the page table and dis the displacement within the page. As a concrete (although

within the page. As a concrete (although minuscule) example, consider the memory in Figure 8.12. Here, in the logical address, n2a n d m4 .U s i n gap a g es i z e of 4 bytes and a physical memory of 32 bytes (8 pages), we show how the programmers view of memory can be mapped into physical memory. Logical address 0 is page 0, offset 0. Indexing into the page table, we nd that page 0 logical memory physical memorypage tablei j k l m n o p a b c d e f g ha b c d e f g h i j k l m n o p0 1 2 3 4 5

e f g ha b c d e f g h i j k l m n o p0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1500 4 8 12 16 20 24 281 2 35 6 1 2 Figure 8.12 Paging example for a 32byte memory with 4byte pages.370 Chapter 8 Main Memory OBTAINING THE PAGE SIZE ON LINUX SYSTEMS On a Linux system, the page size varies according to architecture, and there are several ways of obtaining the page size. One approach is to use thegetpagesize() system call. Another strategy is to enter the following command on the command line: getconf

the following command on the command line: getconf PAGESIZE Each of these techniques returns the page size as a number of bytes. is in frame 5. Thus, logical address 0 maps to physical address 20 [ (5 4)  0]. Logical address 3 (page 0, offset 3) maps to physical address 23 [ (5 4)  3]. Logical address 4 is page 1, offset 0; according to the page table, page 1 is mapped to frame 6. Thus, logical address 4 maps to physical address 24 [ (6 4)  0]. Logical address 13 maps to physical address 9. You

Logical address 13 maps to physical address 9. You may have noticed that paging itself is a form of dynamic relocation. Every logical address is bound by the paging hardware to some physical address. Using paging is similar to using a table of base (or relocation) registers, one for each frame of memory. When we use a paging scheme, we hav en oe x t e r n a lf r a g m e n t a t i o n :a n yf r e e frame can be allocated to a process that needs it. However, we may have some internal

that needs it. However, we may have some internal fragmentation. Notice that frames are allocated as units. If the memory requirements of a process do not happen to coincide with page boundaries, the last frame allocated may not be completely full. For example, if page size is 2,048 bytes, a process of 72,766 bytes will need 35 pages plus 1,086 bytes. It will be allocated 36 frames, resulting in internal fragmentation of 2,048 1,086 9 6 2b y t e s .I nt h ew o r s tc a s e ,ap r o c e s sw o u l

s .I nt h ew o r s tc a s e ,ap r o c e s sw o u l dn e e d npages plus 1 byte. It would be allocated n1f r a m e s ,r e s u l t i n gi ni n t e r n a lf r a g m e n t a t i o no fa l m o s t an entire frame. If process size is independent of page size, we expect internal fragmentation to average onehalf page per process. This consideration suggests that small page sizes are desirable. However, overhead is involved in each pagetable entry, and this overhead is reduc ed as the size of the pages

this overhead is reduc ed as the size of the pages increases. Also, disk IOis more efcient when the amount data being transferred is larger (Chapter 10). Generally, page sizes have grown over time as processes, data sets, and main memory have become larger. Today, pages typically are between 4KBand 8 KBin size, and some systems support even larger page sizes. Some CPUsa n dk e r n e l se v e ns u p p o r tm u l t i p l ep a g es i z e s .F o ri n s t a n c e ,S o l a r i su s e s page sizes of 8

s t a n c e ,S o l a r i su s e s page sizes of 8 KBand 4 MB,d e p e n d i n go nt h ed a t as t o r e db yt h ep a g e s . Researchers are now developing support for variable onthey page size. Frequently, on a 32bit CPU, each pagetable entry is 4 bytes long, but that size can vary as well. A 32bit entry can point to one of 232physical page frames. If frame size is 4 KB(212), then a system with 4byte entries can address 244bytes (or 16 TB)o fp h y s i c a lm e m o r y .W es h o u l dn o t eh e r

y s i c a lm e m o r y .W es h o u l dn o t eh e r et h a tt h es i z eo fp h y s i c a l memory in a paged memory system is different from the maximum logical size of a process. As we further explore paging, we introduce other information that must be kept in the pagetable entries. That information reduces the number8.5 Paging 371 (a)freeframe list 14 13 18 20 1513 14 15 16 17 18 19 20 21page 0 page 1 page 2 page 3 new process (b)freeframe list 1513 page 1 page 0 page 2 page 314 15 16 17 18 19

1513 page 1 page 0 page 2 page 314 15 16 17 18 19 20 21page 0 page 1 page 2 page 3 new process newprocess page table140 1 2 313 18 20 Figure 8.13 Free frames (a) before allocation and (b) after allocation. of bits available to address page frames. Thus, a system with 32bit pagetable entries may address less physical m emory than the possible maximum. A 32bit CPU uses 32bit addresses, meaning that a given process space can only be 232 bytes (4 TB). Therefore, paging lets us use physical memory

TB). Therefore, paging lets us use physical memory that is larger than what can be addressed by the CPUs address pointer length. When a process arrives in the system to be executed, its size, expressed in pages, is examined. Each page of the pro cess needs one frame. Thus, if the process requires npages, at least nframes must be available in memory. If n frames are available, they are allocated to this arriving process. The rst page of the process is loaded into one of the allocated frames, and

is loaded into one of the allocated frames, and the frame number is put in the page table for this process. The next page is loaded into another frame, its frame number is put into the page table, and so on (Figure 8.13). An important aspect of paging is the clear separation between the program mers view of memory and the actual physical memory. The programmer views memory as one single space, containing only this one program. In fact, the user program is scattered throughout physical memory,

program is scattered throughout physical memory, which also holds other programs. The difference between the programmers view of memory and the actual physical memory is reconciled by the addresstranslation hardware. The logical addresses are translated into physical addresses. This mapping is hidden from the programmer and is controlled by the operating system. Notice that the user process by denition is unable to access memory it does not own. It has no way of addressing memory outside of its

It has no way of addressing memory outside of its page table, and the table includes only those pages that the process owns. Since the operating system is managing physical memory, it must be aware of the allocation details of physical memorywhich frames are allocated, which frames are available, how many total frames there are, and so on. This information is generally kept in a data structure called a frame table .T h ef r a m e table has one entry for each physical page frame, indicating

one entry for each physical page frame, indicating whether the latter372 Chapter 8 Main Memory is free or allocated and, if it is allocated, to which page of which process or processes. In addition, the operating system must be aware that user processes operate in user space, and all logical addresses must be mapped to produce physical addresses. If a user makes a system call (to do IO, for example) and provides an address as a parameter (a buffer, for instance), that address must be mapped to

for instance), that address must be mapped to produce the correct physical address. The operating system maintains a copy of the page table for each process, just as it maintains a copy of the instruction counter and register contents. This copy is used to translate logical addresses to physical addresses whenever the operating system must map a logical address to a physical address manually. It is also used by the CPU dispatcher to dene the hardware page table when a process is to be allocated

page table when a process is to be allocated the CPU.P a g i n g therefore increases the contextswitch time. 8.5.2 Hardware Support Each operating system has its own methods for storing page tables. Some allocate a page table for each process. A pointer to the page table is stored with the other register values (like the instruction counter) in the process control block. When the dispatcher is told to start a process, it must reload the user registers and dene the correct hardware pagetable

registers and dene the correct hardware pagetable values from the stored user page table. Other operating systems provide one or at most a few page tables, which decreases the overhead involved when processes are contextswitched. The hardware implementation of the page table can be done in several ways. In the simplest case, the page tab le is implemented as a set of dedicated registers .T h e s er e g i s t e r ss h o u l db eb u i l tw i t hv e r yh i g h  s p e e dl o g i ct om a k et h e

hv e r yh i g h  s p e e dl o g i ct om a k et h e pagingaddress translation efcient. Every access to memory must go through the paging map, so efciency is a major consideration. The CPU dispatcher reloads these registers, just as it reloads the other registers. Instructions to load or modify the pagetable registers are, of course, privileged, so that only the operating system can change the memory map. The DEC PDP11 is an example of such an architecture. The address consists of 16 bits, and the

The address consists of 16 bits, and the page size is 8 KB.T h ep a g et a b l et h u sc o n s i s t so fe i g h te n t r i e st h a ta r ek e p ti nf a s tr e g i s t e r s . The use of registers for the page table is satisfactory if the page table is reasonably small (for example, 256 entries). Most contemporary computers, however, allow the page table to be very large (for example, 1 million entries). For these machines, the use of fast registers to implement the page table is not feasible.

to implement the page table is not feasible. Rather, the page table is kept in main memory, and a pagetable base register (PTBR )points to the page table. Changing page tables requires changing only this one register, substantially reducing contextswitch time. The problem with this approach is the time required to access a user memory location. If we want to access location i,we must rst index into the page table, using the value in the PTBR offset by the page number for i.T h i s task requires

by the page number for i.T h i s task requires a memory access. It provides us with the frame number, which is combined with the page offset to produce the actual address. We can then access the desired place in memory. With this scheme, two memory accesses are needed to access a byte (one for the pagetable entry, one for the byte). Thus, memory access is slowed by a factor of 2. This delay would be intolerable under most circumstances. We might as well resort to swapping!8.5 Paging 373 The

as well resort to swapping!8.5 Paging 373 The standard solution to this problem is to use a special, small, fast lookup hardware cache called a translation lookaside buffer (TLB).T h e TLB is associative, highspeed memory. Each entry in the TLBconsists of two parts: ak e y( o rt a g )a n dav a l u e .W h e nt h ea s s o c i a t i v em e m o r yi sp r e s e n t e dw i t ha n item, the item is compared with all keys simultaneously. If the item is found, the corresponding value eld is returned. The

the corresponding value eld is returned. The search is fast; a TLB lookup in modern hardware is part of the instruction pipeline, essentially adding no performance penalty. To be able to execute the search within a pipeline step, however, the TLBmust be kept small. It is typically between 32 and 1,024 entries in size. Some CPUsi m p l e m e n ts e p a r a t ei n s t r u c t i o na n dd a t aa d d r e s s TLBs. That can double the number of TLBentries available, because those lookups occur in

available, because those lookups occur in different pipeline steps. We can see in this development an example of the evolution of CPU technology: systems have evolved from having no TLBst o having multiple levels of TLBs, just as they have multiple levels of caches. The TLB is used with page tables in the following way. The TLB contains only a few of the pagetable entries. When al o g i c a la d d r e s si sg e n e r a t e db yt h e CPU,i t sp a g en u m b e ri sp r e s e n t e dt ot h e TLB.I

a g en u m b e ri sp r e s e n t e dt ot h e TLB.I ft h ep a g en u m b e ri sf o u n d ,i t s frame number is immediately available and is used to access memory. As just mentioned, these steps are executed as part of the instruction pipeline within the CPU,a d d i n gn op e r f o r m a n c ep e n a l t yc o m p a r e dw i t has y s t e mt h a td o e s not implement paging. If the page number is not in the TLB (known as a TLB miss ), a memory reference to the page table must be made. Depending

to the page table must be made. Depending on the CPU,t h i sm a yb e done automatically in hardware or via an interrupt to the operating system. When the frame number is obtained, we can use it to access memory (Figure 8.14). In addition, we add the page number and frame number to the TLB,s o page tablefCPUlogical address p d fdphysical address physical memoryp TLB misspage numberframe number TLB hit TLB Figure 8.14 Paging hardware with TLB.374 Chapter 8 Main Memory that they will be found

Chapter 8 Main Memory that they will be found quickly on the next reference. If the TLBis already full of entries, an existing entry must be s elected for replacement. Replacement policies range from least recently used ( LRU)t h r o u g hr o u n d  r o b i nt or a n d o m . Some CPUsa l l o wt h eo p e r a t i n gs y s t e mt op a r t i c i p a t ei n LRU entry replacement, while others handle the matter themselves. Furthermore, some TLBsa l l o w certain entries to be wired down ,m e a n i n

o w certain entries to be wired down ,m e a n i n gt h a tt h e yc a n n o tb er e m o v e df r o m the TLB.T y p i c a l l y , TLBentries for key kernel code are wired down. Some TLBss t o r e addressspace identiers (ASID s)in each TLB entry. An ASID uniquely identies each process and is used to provide addressspace protection for that process. When the TLB attempts to resolve virtual page numbers, it ensures that the ASID for the currently running process matches the ASID associated with the

process matches the ASID associated with the virtual page. If the ASID sd on o tm a t c h ,t h ea t t e m p ti s treated as a TLBmiss. In addition to providing addressspace protection, an ASID allows the TLBto contain entries for several different processes simultaneously. If the TLBdoes not support separate ASID s, then every time a new page table is selected (for instance, with each context switch), the TLB must be ushed (or erased) to ensure that the next executing process does not use the

that the next executing process does not use the wrong translation information. Otherwise, the TLB could include old entries that contain valid virtual addresses but have incorrect or invalid physical addresses left over from the previous process. The percentage of times that the page number of interest is found in the TLB is called the hit ratio . An 80percent hit ratio, for example, means that we nd the desired page number in the TLB80 percent of the time. If it takes 100 nanoseconds to access

of the time. If it takes 100 nanoseconds to access memory, then a mappedmemory access takes 100 nanoseconds when the page number is in the TLB.I fw ef a i lt o n dt h ep a g e number in the TLB then we must rst access memory for the page table and frame number (100 nanoseconds) and then access the desired byte in memory (100 nanoseconds), for a total of 200 nanoseconds. (We are assuming that a pagetable lookup takes only one memory access, but it can take more, as we shall see.) To nd the

but it can take more, as we shall see.) To nd the effective memoryaccess time ,w ew e i g h tt h ec a s eb yi t s probability: effective access time  0.80 100  0.20 200 1 2 0n a n o s e c o n d s In this example, we suffer a 20percent slowdown in average memoryaccess time (from 100 to 120 nanoseconds). For a 99percent hit ratio, which is much more realistic, we have effective access time  0.99 100  0.01 200 1 0 1n a n o s e c o n d s This increased hit rate produces only a 1 percent slowdown in

hit rate produces only a 1 percent slowdown in access time. As we noted earlier, CPUst o d a ym a yp r o v i d em u l t i p l el e v e l so f TLBs. Calculating memory access times in modern CPUsi st h e r e f o r em u c hm o r e complicated than shown in the example abo ve. For instance, the Intel Core i7CPU has a 128entry L1 instruction TLB and a 64entry L1 data TLB.I nt h e case of a miss at L1, it takes the CPU six cycles to check for the entry in the L2 512entry TLB.Am i s si nL 2m e a n st

in the L2 512entry TLB.Am i s si nL 2m e a n st h a tt h e CPU must either walk through the8.5 Paging 375 pagetable entries in memory to nd the associated frame address, which can take hundreds of cycles, or interrupt to the operating system to have it do the work. Ac o m p l e t ep e r f o r m a n c ea n a l y s i so fp a g i n go v e r h e a di ns u c has y s t e m would require missrate information about each TLBtier. We can see from the general information above, however, that hardware

general information above, however, that hardware features can have a signif icant effect on memory performance and that operatingsystem improvements (such as paging) can result in and, in turn, be affected by hardware changes (such as TLBs). We will further explore the impact of the hit ratio on the TLBin Chapter 9. TLBsa r eah a r d w a r ef e a t u r ea n dt h e r e f o r ew o u l ds e e mt ob eo fl i t t l ec o n c e r n to operating systems and their designers. But the designer needs to

and their designers. But the designer needs to understand the function and features of TLBs, which vary by hardware platform. For optimal operation, an operatingsystem design for a given platform must implement paging according to the platforms TLBdesign. Likewise, a change in theTLBdesign (for example, between generations of Intel CPUs) may necessitate a change in the paging implementation of the operating systems that use it. 8.5.3 Protection Memory protection in a paged environment is

Memory protection in a paged environment is accomplished by protection bits associated with each frame. Normally, these bits are kept in the page table. One bit can dene a page to be readwrite or readonly. Every reference to memory goes through the page table to nd the correct frame number. At the same time that the physical address is being computed, the protection bits can be checked to verify that no writes are being made to a readonly page. An attempt to write to a readonly page causes a

An attempt to write to a readonly page causes a hardware trap to the operating system (or memoryprotection violation). We can easily expand this approach to provide a ner level of protection. We can create hardware to provide readonly, read  write, or executeonly protection; or, by providing separate protection bits for each kind of access, we can allow any combination of these accesses. Illegal attempts will be trapped to the operating system. One additional bit is generally attached t o each

One additional bit is generally attached t o each entry in the page table: a validinvalid bit. When this bit is set to valid, the associated page is in the processs logical address space and is thus a legal (or valid) page. When the bit is set to invalid, the page is not in the processs logical address space. Illegal addresses are trapped by use of the validinvalid bit. The operating system sets this bit for each page to allow or disallow access to the page. Suppose, for example, that in a

to the page. Suppose, for example, that in a system with a 14bit address space (0 to 16383), we have a program that should use only addresses 0 to 10468. Given ap a g es i z eo f2 KB,w eh a v et h es i t u a t i o ns h o w ni nF i g u r e8 . 1 5 .A d d r e s s e si n pages 0, 1, 2, 3, 4, and 5 are mapped normally through the page table. Any attempt to generate an address in pages 6 or 7, however, will nd that the validinvalid bit is set to invalid, and the computer will trap to the operating

and the computer will trap to the operating system (invalid page reference). Notice that this scheme has created a problem. Because the program extends only to address 10468, any reference beyond that address is illegal. However, references to page 5 are classied as valid, so accesses to addresses up to 12287 are valid. Only the addresses from 12288 to 16383 are invalid. This376 Chapter 8 Main Memory page 0page 0 page 1 page 2 page 3 page 4 page 5 page n000000 1 2 3 4 5 6 7 8 9frame number 0 1 2

5 page n000000 1 2 3 4 5 6 7 8 9frame number 0 1 2 3 4 5 6 72 3 4 7 8 9 0 0v v v v v v i i page tablevalidinvalid bit 10,468 12,287page 1 page 2 page 3 page 4 page 5 Figure 8.15 Valid (v) or invalid (i) bit in a page table. problem is a result of the 2 KBpage size and reects the internal fragmentation of paging. Rarely does a process use all its address range. In fact, many processes use only a small fraction of the address space available to them. It would be wasteful in these cases to create a

It would be wasteful in these cases to create a page table with entries for every page in the address range. Most of this table would be unused but would take up valuable memory space. Some systems provide hardware, in the form of a pagetable length register (PTLR ),t oi n d i c a t et h es i z eo ft h ep a g et a b l e .T h i sv a l u ei s checked against every logical address to verify that the address is in the valid range for the process. Failure of this test causes an error trap to the

Failure of this test causes an error trap to the operating system. 8.5.4 Shared Pages An advantage of paging is the possibility of sharing common code. This con sideration is particularly important in a timesharing environment. Consider a system that supports 40 users, each of whom executes a text editor. If the text editor consists of 150 KBof code and 50 KBof data space, we need 8,000 KBto support the 40 users. If the code is reentrant code (orpure code ), however, it can be shared, as shown

code ), however, it can be shared, as shown in Figure 8.16. Here, we see three processes sharing a threepage editoreach page 50 KBin size (the large page size is used to simplify the gure). Each process has its own data page. Reentrant code is nonselfmodifying code: it never changes during execu tion. Thus, two or more processes can execute the same code at the same time.8.5 Paging 377 765ed 2 4ed 1 32data 110 3 4 6 1 page table for P1 process P1data 1ed 2 ed 3ed 1 3 4 6 2 page table for P3

P1data 1ed 2 ed 3ed 1 3 4 6 2 page table for P3 process P3data 3ed 2 ed 3ed 13 4 6 7 page table for P2 process P2data 2ed 2 ed 3ed 1 8 9 10 11data 3 2 data ed 3 Figure 8.16 Sharing of code in a paging environment. Each process has its own copy of registers and data storage to hold the data for the processs execution. The data for two different processes will, of course, be different. Only one copy of the editor need be kept in physical memory. Each users page table maps onto the same physical

Each users page table maps onto the same physical copy of the editor, but data pages are mapped onto different frames. Thus, to support 40 users, we need only one copy of the editor (150 KB), plus 40 copies of the 50 KBof data space per user. The total space required is now 2,150 KBinstead of 8,000 KBa signicant savings. Other heavily used programs can also be sharedcompilers, window systems, runtime libraries, database systems, and so on. To be sharable, the code must be reentrant. The readonly

sharable, the code must be reentrant. The readonly nature of shared code should not be left to the correctness of the code; the operating system should enforce this property. The sharing of memory among processes on a system is similar to the sharing of the address space of a task by threads, described in Chapter 4. Furthermore, recall that in Chapter 3 we described shared memory as a method of interprocess communication. Some operating systems implement shared memory using shared pages.

implement shared memory using shared pages. Organizing memory according to pages provides numerous benets in addition to allowing several processes to share the same physical pages. We cover several other benets in Chapter 9.378 Chapter 8 Main Memory 8.6 Structure of the Page Table In this section, we explore some of the most common techniques for structuring the page table, including hierarchical paging, hashed page tables, and inverted page tables. 8.6.1 Hierarchical Paging Most modern

page tables. 8.6.1 Hierarchical Paging Most modern computer systems support a large logical address space (232to 264). In such an environment, the page table itself becomes excessively large. For example, consider a system with a 32bit logical address space. If the page size in such a system is 4 KB(212), then a page table may consist of up to 1 million entries (232212). Assuming that each entry consists of 4 bytes, each process may need up to 4 MBof physical address space for the page table

4 MBof physical address space for the page table alone. Clearly, we would not want to allocate the page table contiguously in main memory. One simple solution to this problem is to divide the page table into smaller pieces. We can accomplish this division in several ways. One way is to use a twolevel paging algorithm, in which the page table itself is also paged (Figure 8.17). For example, consider again the system with a3 2  b i tl o g i c a la d d r e s ss p a c ea n dap a g es i z eo f4 KB.Al

d d r e s ss p a c ea n dap a g es i z eo f4 KB.Al o g i c a la d d r e s si s divided into a page number consisting of 20 bits and a page offset consisting of 12 bits. Because we page the page table, the page number is further divided  outer page table page of page table page table memory929 900 92990070850010010 100 708     1 500 Figure 8.17 At w o  l e v e lp a g e  t a b l es c h e m e .8.6 Structure of the Page Table 379logical address outer page tablep1p2 p1 page of page tablep2dd Figure

page tablep1p2 p1 page of page tablep2dd Figure 8.18 Address translation for a twolevel 32bit paging architecture. into a 10bit page number and a 10bit page offset. Thus, a logical address is as follows:p1 p2 dpage number page offset 10 10 12 where p1is an index into the outer page table and p2is the displacement within the page of the inner page table. The addresstranslation method for this architecture is shown in Figure 8.18. Because address translation works from the outer page table inward,

works from the outer page table inward, this scheme is also known as a forwardmapped page table . Consider the memory management of one of the classic systems, the VA X minicomputer from Digital Equipment Corporation (DEC).T h e VAX was the most popular minicomputer of its time and was sold from 1977 through 2000. The VAX architecture supported a variation of twolevel paging. The VAX is a 32 bit machine with a page size of 512 bytes. The logical address space of a process is divided into four

address space of a process is divided into four equal sections, each of which consists of 230bytes. Each section represents a different part of the logical address space of a process. The rst 2 highorder bits of the logical address designate the appropriate section. The next 21 bits represent the logical page number of that section, and the nal 9b i t sr e p r e s e n ta no f f s e ti nt h ed e s i r e dp a g e .B yp a r t i t i o n i n gt h ep a g et a b l ei n this manner, the operating system

g et a b l ei n this manner, the operating system can leave partitions unused until a process needs them. Entire sections of virtual addr ess space are frequently unused, and multilevel page tables have no entries for these spaces, greatly decreasing the amount of memory needed to store virtual memory data structures. An address on the VAX architecture is as follows:sp dsection page offset 22 19 where sdesignates the section number, pis an index into the page table, and d is the displacement

into the page table, and d is the displacement within the page. Even when this scheme is used, the size of a onelevel page table for a VAX process using one section is 221bits 4380 Chapter 8 Main Memory bytes per entry  8 MB.T of u r t h e rr e d u c em a i n  m e m o r yu s e ,t h e VAX pages the userprocess page tables. For a system with a 64bit logical address space, a twolevel paging scheme is no longer appropriate. To illustrate this point, lets suppose that the page size in such a system

lets suppose that the page size in such a system is 4 KB(212). In this case, the page table consists of up to 252entries. If we use a twolevel paging sch eme, then the inner page tables can conveniently be one page long, or contain 2104byte entries. The addresses look like this:p1 p2 douter page inner page offset 42 10 12 The outer page table consists of 242entries, or 244bytes. The obvious way to avoid such a large table is to divide the outer page table into smaller pieces. (This approach is

page table into smaller pieces. (This approach is also used on some 32bit processors for added exibility and efciency.) We can divide the outer page table in various ways. For example, we can page the outer page table, giving us a th reelevel paging scheme. Suppose that the outer page table is made up of standardsize pages (210entries, or 212bytes). In this case, a 64bit address space is still daunting:p1 p2 p32nd outer page outer page inner page 32 10 10doffset 12 The outer page table is still

32 10 10doffset 12 The outer page table is still 234bytes (16 GB) in size. The next step would be a fourlevel paging scheme, where the secondlevel outer page table itself is also paged, and so forth. The 64bit Ultra SPARC would require seven levels of paginga prohibitive number of memory accesses to translate each logical address. You can see from this example why, for 64bit architectures, hierarchical page tab les are generally considered inappropriate. 8.6.2 Hashed Page Tables Ac o m m o na p

8.6.2 Hashed Page Tables Ac o m m o na p p r o a c hf o rh a n d l i n ga d d r e s ss p a c e sl a r g e rt h a n3 2b i t si st ou s e ahashed page table ,w i t ht h eh a s hv a l u eb e i n gt h ev i r t u a lp a g en u m b e r .E a c h entry in the hash table contains a linked list of elements that hash to the same location (to handle collisions). Each element consists of three elds: (1) the virtual page number, (2) the value of the mapped page frame, and (3) a pointer to the next element in

frame, and (3) a pointer to the next element in the linked list. The algorithm works as follows: The virtual page number in the virtual address is hashed into the hash table. The virtual page number is compared with eld 1 in the rst element in the linked list. If there is a match, the corresponding page frame (eld 2) is used to form the desired physical address. If there is no match, subsequent entries in the linked list are searched for a matching virtual page number. This scheme is shown in

virtual page number. This scheme is shown in Figure 8.19. A variation of this scheme that is useful for 64bit address spaces has been proposed. This variation uses clustered page tables ,w h i c ha r es i m i l a rt o8.6 Structure of the Page Table 381 hash tableqslogical addressphysical address physical memorypd rd prhash function   Figure 8.19 Hashed page table. hashed page tables except that each en try in the hash table refers to several pages (such as 16) rather than a single page.

pages (such as 16) rather than a single page. Therefore, a single pagetable entry can store the mappings for multiple physicalpage frames. Clustered page tables are particularly useful for sparse address spaces, where memory references are noncontiguous and scattered throughout the address space. 8.6.3 Inverted Page Tables Usually, each process has an associated page table. The page table has one entry for each page that the process is using (or one slot for each virtual address, regardless of

one slot for each virtual address, regardless of the latters validity). This table representation is a natural one, since processes reference pages through the pages virtual addresses. The operating system must then translate this reference into a physical memory address. Since the table is sorted by virtual address, the operating system is able to calculate where in the table the associated physical address entry is located and to use that value directly. One of the drawbacks of this method is

directly. One of the drawbacks of this method is that each page table may consist of millions of entries. These tables may consume large amounts of physical memory just to keep track of how other physical memory is being used. To solve this problem, we can use an inverted page table .A ni n v e r t e d page table has one entry for each real page (or frame) of memory. Each entry consists of the virtual address of the page stored in that real memory location, with information about the process

location, with information about the process that owns the page. Thus, only one page table is in the system, and it has only one entry for each page of physical memory. Figure 8.20 shows the operation of an inverted page table. Compare it with Figure 8.10, which depicts a standard page table in operation. Inverted page tables often require that an addressspace identier (Section 8.5.2) be stored in each entry of the page table, since the table usually contains several different address spaces

usually contains several different address spaces mapping physical memory. Storing the addressspace identier ensures that a logical page for a particular process is mapped to the corresponding physical page frame. Examples of systems using inverted page tables include the 64bit Ultra SPARC and Power PC.382 Chapter 8 Main Memory page tableCPUlogical addressphysical addressphysical memory ipid p pidsearch pd id Figure 8.20 Inverted page table. To illustrate this method, we describe a simplied

To illustrate this method, we describe a simplied version of the inverted page table used in the IBM RT .IBMwas the rst major company to use inverted page tables, starting with the IBM System 38 and continuing through the RS6000 and the current IBMPower CPUs. For the IBM RT ,e a c hv i r t u a la d d r e s s in the system consists of a triple: processid, pagenumber, offset . Each inverted pagetable entry is a pair processid, pagenumber where the processid assumes the role of the addressspace

the processid assumes the role of the addressspace identier. When a memory reference occurs, part of the virtual address, consisting of processid, page number ,i sp r e s e n t e dt ot h em e m o r ys u b s y s t e m .T h ei n v e r t e dp a g et a b l e is then searched for a match. If a match is foundsay, at entry ithen the physical address i,offset is generated. If no match is found, then an illegal address access has been attempted. Although this scheme decreases the amount of memory needed

this scheme decreases the amount of memory needed to store each page table, it increases the amount o ft i m en e e d e dt os e a r c ht h et a b l ew h e n a page reference occurs. Because th e inverted page table is sorted by physical address, but lookups occur on virtual addresses, the whole table might need to be searched before a match is found. This search would take far too long. To alleviate this problem, we use a hash table, as described in Section 8.6.2, to limit the search to oneor at

in Section 8.6.2, to limit the search to oneor at most a fewpagetable entries. Of course, each access to the hash table adds a memory reference to the procedure, so one virtual memory reference requires at least two real memory readsone for the hashtable entry and one for the page table. (Recall that the TLBis searched rst, before the hash table is consulted, offering some performance improvement.) Systems that use inverted page tables h ave difculty implementing shared memory. Shared memory is

implementing shared memory. Shared memory is usually implemented as multiple virtual addresses (one for each process sharing the memory) that are mapped to one physical address. This standard method cannot be used with inverted page tables; because there is only one virtual page entry for every physical page, one8.7 Example: Intel 32 and 64bit Architectures 383 physical page cannot have two (or more) shared virtual addresses. A simple technique for addressing this issue is to allow the page

for addressing this issue is to allow the page table to contain only one mapping of a virtual address to the shared physical address. This means that references to virtual addresses that are not mapped result in page faults. 8.6.4 Oracle SPARC Solaris Consider as a nal example a modern 64bit CPU and operating system that are tightly integrated to provide lowoverhead virtual memory. Solaris running on the SPARC CPU is a fully 64bit operating system and as such has to solve the problem of virtual

and as such has to solve the problem of virtual memory without using up all of its physical memory by keeping multiple levels of page tables. Its approach is a bit complex but solves the problem efciently using hashed page tables. There are two hash tablesone for the kernel and o ne for all user processes. Each maps memory addresses from virtual to physical memory. Each hashtable entry represents a contiguous area of mapped virtual memory, which is more efcient than having a separate hashtable

is more efcient than having a separate hashtable entry for each page. Each entry has a base address and a span indicating the number of pages the entry represents. Virtualtophysical translation would take too long if each address required searching through a hash table, so the CPU implements a TLB that holds translation table entries ( TTEs) for fast hardware lookups. A cache of these TTEs reside in a translation storage buffer ( TSB), which includes an entry per recently accessed page. When a

an entry per recently accessed page. When a virtual address reference occurs, the hardware searches the TLB for a translation. If none is found, the hardware walks through the inmemory TSBlooking for the TTEthat corresponds to the virtual address that caused the lookup. This TLBwalk functionality is found on many modern CPUs. If a match is found in the TSB,t h e CPU copies the TSBentry into the TLB,a n d the memory translation completes. If no match is found in the TSB,t h ek e r n e l is

no match is found in the TSB,t h ek e r n e l is interrupted to search the hash table. The kernel then creates a TTEfrom the appropriate hash table and stores it in the TSBfor automatic loading into the TLB by the CPU memorymanagement unit. Finally, the interrupt handler returns control to the MMU ,w h i c hc o m p l e t e st h ea d d r e s st r a n s l a t i o na n dr e t r i e v e st h e requested byte or word from main memory. 8.7 Example: Intel 32 and 64bit Architectures The architecture of

32 and 64bit Architectures The architecture of Intel chips has dominated the personal computer landscape for several years. The 16bit Intel 8086 appeared in the late 1970s and was soon followed by another 16bit chipthe Intel 8088which was notable for being the chip used in the original IBM PC .B o t ht h e8 0 8 6c h i pa n dt h e8 0 8 8c h i pw e r e based on a segmented architecture. In tel later produced a series of 32bit chips the IA32which included the family of 32bit Pentium processors. The

the family of 32bit Pentium processors. The IA32 architecture supported both paging and segmentation. More recently, Intel has produced a series of 64bit chips based on the x8664 architecture. Currently, all the most popular PCoperating systems run on Intel chips, including Windows, Mac OS X ,a n dL i n u x( a l t h o u g hL i n u x ,o fc o u r s e ,r u n s on several other architectures as well). Notably, however, Intels dominance has not spread to mobile systems, where the ARM architecture

to mobile systems, where the ARM architecture currently enjoys considerable success (see Section 8.8).384 Chapter 8 Main MemoryCPUlogical address segmentation unitlinear address paging unitphysical address physical memory Figure 8.21 Logical to physical address translation in IA32. In this section, we examine address translation for both IA32 and x8664 architectures. Before we proceed, however, it is important to note that because Intel has released several versionsas well as variationsof its

several versionsas well as variationsof its architectures over the years, we cannot provide a complete description of the memory management structure of all its chips. Nor can we provide all of the CPU details, as that information is best left to books on computer architecture. Rather, we present the major memorymanagemen t concepts of these Intel CPUs. 8.7.1 IA32 Architecture Memory management in IA32 systems is divided into two components segmentation and pagingand works as follows: The CPU

and pagingand works as follows: The CPU generates logical addresses, which are given to the segmentation unit. The segmentation unit produces a linear address for each logic al address. The linear address is then given to the paging unit, which in turn ge nerates the physical address in main memory. Thus, the segmentation and paging units form the equivalent of the memorymanagement unit ( MMU ). This scheme is shown in Figure 8.21. 8.7.1.1 IA32 Segmentation The IA32 architecture allows a segment

The IA32 architecture allows a segment to be as large as 4 GB,a n dt h em a x i m u m number of segments per process is 16 K. The logical address space of a process is divided into two partitions. The rst partition consists of up to 8 Ksegments that are private to that process. The second partition consists of up to 8 Ksegments that are shared among all the processes. Information about the rst partition is kept in the local descriptor table (LDT);i n f o r m a t i o na b o u tt h es e c o n dp a

n f o r m a t i o na b o u tt h es e c o n dp a r t i t i o n is kept in the global descriptor table (GDT ).E a c he n t r yi nt h e LDT and GDT consists of an 8byte segment descriptor with detailed information about a particular segment, including the base location and limit of that segment. The logical address is a pair (selector, offset), where the selector is a 16bit number:p 2g 1s 13 in which sdesignates the segment number, gindicates whether the segment is in the GDT orLDT,a n d pdeals

the segment is in the GDT orLDT,a n d pdeals with protection. The offset is a 32bit number specifying the location of the byte within the segment in question. The machine has six segment registers, allowing six segments to be addressed at any one time by a process. It also has six 8byte microprogram registers to hold the corresponding descriptors from either the LDT orGDT. This cache lets the Pentium avoid ha ving to read the descriptor from memory for every memory reference.8.7 Example: Intel

for every memory reference.8.7 Example: Intel 32 and 64bit Architectures 385 logical address selector descriptor table segment descriptor  32bit linear addressoffset Figure 8.22 IA32 segmentation. The linear address on the IA32 is 32 bits long and is formed as follows. The segment register points to the appropriate entry in the LDT orGDT.T h e base and limit information about the segment in question is used to generate alinear address .F i r s t ,t h el i m i ti su s e dt oc h e c kf o ra d d r

,t h el i m i ti su s e dt oc h e c kf o ra d d r e s sv a l i d i t y .I ft h e address is not valid, a memory fault is generated, resulting in a trap to the operating system. If it is valid, then the value of the offset is added to the value of the base, resulting in a 32bit linear address. This is shown in Figure 8.22. In the following section, we discuss how the paging unit turns this linear address into a physical address. 8.7.1.2 IA32 Paging The IA32 architecture allows a page size of

Paging The IA32 architecture allows a page size of either 4 KBor 4 MB.F o r4  KBpages, IA32 uses a twolevel paging scheme in w hich the division of the 32bit linear address is as follows:p1 p2 dpage number page offset 10 10 12 The addresstranslation scheme for this architecture is similar to the scheme shown in Figure 8.18. The IA32 address translation is shown in more detail in Figure 8.23. The 10 highorder bits reference an entry in the outermost page table, which IA32 terms the page directory

page table, which IA32 terms the page directory .( T h e CR3 register points to the page directory for the current process.) The page directory entry points to an inner page table that is indexed by the contents of the innermost 10 bits in the linear address. Finally, the loworder bits 011 refer to the offset in the 4 KB page pointed to in the page table. One entry in the page directory is the Page Size ag, whichif set indicates that the size of the page frame is 4 MBand not the standard 4 KB.

the page frame is 4 MBand not the standard 4 KB. If this ag is set, the page directory points directly to the 4 MBpage frame, bypassing the inner page table; and the 22 loworder bits in the linear address refer to the offset in the 4 MBpage frame.386 Chapter 8 Main Memory page directory page directoryCR3 registerpage directorypage table4KB page 4MB pagepage table offsetoffset(linear address) 31 22 21 12 11 0 21 31 22 0 Figure 8.23 Paging in the IA32 architecture. To improve the efciency of

the IA32 architecture. To improve the efciency of physical memory use, IA32 page tables can be swapped to disk. In this case, an invalid bit is used in the page directory entry to indicate whether the table to which the entry is pointing is in memory or on disk. If the table is on disk, the operating system can use the other 31 bits to specify the disk location of the table. The table can then be brought into memory on demand. As software developers began to discover the 4 GBmemory limitations

began to discover the 4 GBmemory limitations of 32bit architectures, Intel adopted a page address extension (PAE),w h i c h allows 32bit processors to access a physical address space larger than 4 GB.T h e fundamental difference introduced by PAEsupport was that paging went from at w o  l e v e ls c h e m e( a ss h o w ni nF i g u r e8 . 2 3 )t oat h r e e  l e v e ls c h e m e ,w h e r e the top two bits refer to a page directory pointer table .F i g u r e8 . 2 4i l l u s t r a t e s aPAE

.F i g u r e8 . 2 4i l l u s t r a t e s aPAE system with 4 KBpages. ( PAE also supports 2 MBpages.) 31 30 29 21 20 12 11 0page table offset page directory 4KB page page tablepage directory pointer tableCR3 register page directory Figure 8.24 Page address extensions.8.7 Example: Intel 32 and 64bit Architectures 387unusedpage map level 4page directory pointer tablepage directorypage table offset 6363 47 48 39 38 30 29 21 20 12 11 0 Figure 8.25 x8664 linear address. PAE also increased the

8.25 x8664 linear address. PAE also increased the pagedirectory and pagetable entries from 32 to 64 bits in size, which allowed the base address of page tables and page frames to extend from 20 to 24 bits. Combined with the 12bit offset, adding PAEsupport toIA32 increased the address space to 36 bits, which supports up to 64 GB of physical memory. It is important to note that operating system support is required to use PAE.B o t hL i n u xa n dI n t e lM a c OS X support PAE.H o w e v e r ,

n dI n t e lM a c OS X support PAE.H o w e v e r , 32bit versions of Windows desktop operating systems still provide support for only 4 GBof physical memory, even if PAE is enabled. 8.7.2 x8664 Intel has had an interesting history of d eveloping 64bit architectures. Its initial entry was the IA64 (later named Itanium )a r c h i t e c t u r e ,b u tt h a ta r c h i t e c t u r e was not widely adopted. Meanwhile, another chip manufacturer AMD  began developing a 64bit architecture known as x8664

developing a 64bit architecture known as x8664 that was based on extending the existing IA32 instruction set. The x8664 supported much larger logical and physical address spaces, as well as several other architectural advances. Historically, AMD had often developed chips based on Intels architecture, but now the roles were reversed as Intel adopted AMD s x8664 architecture. In discussing this architecture, rather than using the commercial names AMD 64and Intel 64 , we will use the more general

AMD 64and Intel 64 , we will use the more general term x8664 . Support for a 64bit address space yields an astonishing 264bytes of addressable memorya number greater than 16 quintillion (or 16 exabytes). However, even though 64bit systems can potentially address this much memory, in practice far fewer than 64 bits are used for address representation in current designs. The x8664 architecture currently provides a 48bit virtual address with support for page sizes of 4 KB,2MB,o r1 GBusing four

for page sizes of 4 KB,2MB,o r1 GBusing four levels of paging hierarchy. The representation of the linear address appears in Figure 8.25. Because this addressing scheme can use PAE,v i r t u a la d d r e s s e sa r e4 8b i t s in size but support 52bit physical addresses (4096 terabytes). 64BIT COMPUTING History has taught us that even though memory capacities, CPU speeds, and similar computer capabilities seem large enough to satisfy demand for the foreseeable future, the growth of technology

the foreseeable future, the growth of technology ultimately absorbs available capacities, and we nd ourselves in need of additional memory or processing power, often sooner than we think. What might the future of technology bring that would make a 64bit address space seem too small?388 Chapter 8 Main Memory 8.8 Example: ARM Architecture Although Intel chips have dominated the personal computer market for over 30 years, chips for mobile devices such as smartphones and tablet computers often

such as smartphones and tablet computers often instead run on 32bit ARM processors. Interestingly, whereas Intel both designs and manufactures chips, ARM only designs them. It then licenses its designs to chip manufacturers. Apple has licensed the ARM design for its iPhone and iPad mobile devices, and several Androidbased smartphones use ARM processors as well. The 32bit ARM architecture supports the following page sizes: 1.4KBand 16 KBpages 2.1MBand 16 MBpages (termed sections ) The paging

2.1MBand 16 MBpages (termed sections ) The paging system in use depends on whether a page or a section is being referenced. Onelevel paging is used for 1 MBand 16 MBsections; twolevel paging is used for 4 KBand 16 KBpages. Address translation with the ARM MMU is shown in Figure 8.26. The ARM architecture also supports two levels of TLBs. At the outer level are two micro TLBsa separate TLB for data and another for instructions. The micro TLBsupports ASID sa sw e l l .A tt h ei n n e rl e v e li

ASID sa sw e l l .A tt h ei n n e rl e v e li sas i n g l e main TLB. Address translation begins at the micro TLB level. In the case of a miss, the main TLBis then checked. If both TLBsy i e l dm i s s e s ,ap a g et a b l ew a l km u s tb e performed in hardware.outer page inner page offset 4KB or 16KB page 1MB or 16MB section32 bits Figure 8.26 Logical address translation in ARM.8.9 Summary 389 8.9 Summary Memorymanagement algorithms for multiprogrammed operating systems range from the simple

operating systems range from the simple singleuser system approach to segmentation and paging. The most important determinant of the method used in a particular system is the hardware provided. Every memory address generated by the CPU must be checked for legality and possibly mapped to a physical address. The checking cannot be implemented (efciently) in software. Hence, we are constrained by the hardware available. The various memorymanagement algorithms (contiguous allocation, pag ing,

algorithms (contiguous allocation, pag ing, segmentation, and combinations of paging and segmentation) differ in many aspects. In comparing different memorymanagement strategies, we use the following considerations: Hardware support .As i m p l eb a s er e g i s t e ro rab a s e  l i m i tr e g i s t e rp a i ri s sufcient for the single and m ultiplepartition schemes, whereas paging and segmentation need mapping tables to dene the address map. Performance .A st h em e m o r y  m a n a g e m e n

.A st h em e m o r y  m a n a g e m e n ta l g o r i t h mb e c o m e sm o r e complex, the time required to map a logical address to a physical address increases. For the simple systems, we need only compare or add to the logical addressoperations that are fast. Paging and segmentation can be as fast if the mapping table is implemented in fast registers. If the table is in memory, however, user memory accesses can be degraded substantially. ATLBcan reduce the performance degradation to an

ATLBcan reduce the performance degradation to an acceptable level. Fragmentation .Am u l t i p r o g r a m m e ds y s t e mw i l lg e n e r a l l yp e r f o r mm o r e efciently if it has a higher level of multiprogramming. For a given set of processes, we can increase the multiprogramming level only by packing more processes into memory. To accomplish this task, we must reduce memory waste, or fragmentation. Systems with xedsized allo cation units, such as the singlepartition scheme and paging,

such as the singlepartition scheme and paging, suffer from internal fragmentation. Systems with variablesized allocation units, such as the multiplepartition scheme and segmentation, suffer from external fragmentation. Relocation .O n es o l u t i o nt ot h ee x t e r n a l  f r a g m e n t a t i o np r o b l e mi sc o m  paction. Compaction involves shifting a program in memory in such a way that the program does not notice the change. This consideration requires that logical addresses be reloc

requires that logical addresses be reloc ated dynamically, at execution time. If addresses are relocated only at load time, we cannot compact storage. Swapping .S w a p p i n gc a nb ea d d e dt oa n ya l g o r i t h m .A ti n t e r v a l sd e t e r  mined by the operating system, usually dictated by CPUscheduling poli cies, processes are copied from main memory to a backing store and later are copied back to main memory. This scheme allows more processes to be run than can be t into memory at

processes to be run than can be t into memory at one time. In general, PCoperating systems support paging, and operating systems for mobile devices do not. Sharing .A n o t h e rm e a n so fi n c r e a s i n gt h em u l t i p r o g r a m m i n gl e v e li st o share code and data among different processes. Sharing generally requires that either paging or segmentation be used to provide small packets of390 Chapter 8 Main Memory information (pages or segments) that can be shared. Sharing is a

or segments) that can be shared. Sharing is a means of running many processes with a limited amount of memory, but shared programs and data must be designed carefully. Protection .I fp a g i n go rs e g m e n t a t i o ni sp r o v i d e d ,d i f f e r e n ts e c t i o n so fa user program can be declared executeonly, readonly, or readwrite. This restriction is necessary with shared code or data and is generally useful in any case to provide simple runtime checks for common programming errors.

runtime checks for common programming errors. Practice Exercises 8.1 Name two differences between logical and physical addresses. 8.2 Consider a system in which a program can be separated into two parts: code and data. The CPU knows whether it wants an instruction (instruction fetch) or data (data fetch or store). Therefore, two base limit register pairs are provided: one for instructions and one for data. The instruction baselimit register pair is automatically readonly, so programs can be

pair is automatically readonly, so programs can be shared among different users. Discuss the advantages and disadvantages of this scheme. 8.3 Why are page sizes always powers of 2? 8.4 Consider a logical address space of 64 pages of 1,024 words each, mapped onto a physical memory of 32 frames. a. How many bits are there in the logical address? b. How many bits are there in the physical address? 8.5 What is the effect of allowing two entries in a page table to point to the same page frame in

in a page table to point to the same page frame in memory? Explain how this effect could be used to decrease the amount of time needed to copy a large amount of memory from one place to another. What effect would updating some byte on the one page have on the other page? 8.6 Describe a mechanism by which one segment could belong to the address space of two different processes. 8.7 Sharing segments among processes without requiring that they have the same segment number is possible in a

they have the same segment number is possible in a dynamically linked segmentation system. a. Dene a system that allows static linking and sharing of segments without requiring that the segment numbers be the same. b. Describe a paging scheme that allows pages to be shared without requiring that the page numbers be the same. 8.8 In the IBM370 ,m e m o r yp r o t e c t i o ni sp r o v i d e dt h r o u g ht h eu s eo f keys . Ak e yi sa4  b i tq u a n t i t y .E a c h2  Kblock of memory has a key

a n t i t y .E a c h2  Kblock of memory has a key (the storage key) associated with it. The CPU also has a key (the protection key) associated with it. A store operation is allowed only if both keysExercises 391 are equal or if either is 0. Which of the following memorymanagement schemes could be used successfully with this hardware? a. Bare machine b. Singleuser system c. Multiprogramming with a xed number of processes d. Multiprogramming with a variable number of processes e. Paging f.

with a variable number of processes e. Paging f. Segmentation Exercises 8.9 Explain the difference between internal and external fragmentation. 8.10 Consider the following process for generating binaries. A compiler is used to generate the object code for individual modules, and a linkage editor is used to combine multiple object modules into a single program binary. How does the linkage editor change the binding of instructions and data to memory addresses? What information needs to be passed

addresses? What information needs to be passed from the compiler to the linkage editor to facilitate the memorybinding tasks of the linkage editor? 8.11 Given six memory partitions of 300 KB,6 0 0 KB,3 5 0 KB,2 0 0 KB,7 5 0 KB, and 125 KB(in order), how would the rstt, bestt, and worstt algorithms place processes of size 115 KB,5 0 0 KB,3 5 8 KB,2 0 0 KB,a n d 375 KB(in order)? Rank the algorithms in terms of how efciently they use memory. 8.12 Most systems allow a program to allocate more

8.12 Most systems allow a program to allocate more memory to its address space during execution. Allocation of data in the heap segments of programs is an example of such allocated memory. What is required to support dynamic memory allocation in the following schemes? a. Contiguous memory allocation b. Pure segmentation c. Pure paging 8.13 Compare the memory organization schemes of contiguous memory allocation, pure segmentation, and pure paging with respect to the following issues: a. External

with respect to the following issues: a. External fragmentation b. Internal fragmentation c. Ability to share code across processes 8.14 On a system with paging, a process cannot access memory that it does not own. Why? How could the operating system allow access to other memory? Why should it or should it not?392 Chapter 8 Main Memory 8.15 Explain why mobile operating systems such as i OSand Android do not support swapping. 8.16 Although Android does not support swapping on its boot disk, it is

does not support swapping on its boot disk, it is possible to set up a swap space using a separate SDnonvolatile memory card. Why would Android disallow swapping on its boot disk yet allow it on a secondary disk? 8.17 Compare paging with segmentation with respect to how much memory the address translation structures require to convert virtual addresses to physical addresses. 8.18 Explain why address space identiers ( ASID s) are used. 8.19 Program binaries in many systems are typically

Program binaries in many systems are typically structured as follows. Code is stored starting with a small, xed virtual address, such as 0. The code segment is followed by the data segment that is used for storing the program variables. When the program starts executing, the stack is allocated at the other end of the virtual address space and is allowed to grow toward lower virtual addresses. What is the signicance of this structure for the following schemes? a. Contiguous memory allocation b.

schemes? a. Contiguous memory allocation b. Pure segmentation c. Pure paging 8.20 Assuming a 1 KBpage size, what are the page numbers and offsets for the following address reference s( p r o v i d e da sd e c i m a ln u m b e r s ) : a. 3085 b. 42095 c. 215201 d. 650000 e. 2000001 8.21 The BTV operating system has a 21bit virtual address, yet on certain embedded devices, it has only a 16bit physical address. It also has a 2KBpage size. How many entries are there in each of the following? a. A

entries are there in each of the following? a. A conventional, singlelevel page table b. An inverted page table 8.22 What is the maximum amount of physical memory? 8.23 Consider a logical address space of 256 pages with a 4 KBpage size, mapped onto a physical memory of 64 frames. a. How many bits are required in the logical address? b. How many bits are required in the physical address?Exercises 393 8.24 Consider a computer system with a 32bit logical address and 4 KBpage size. The system

logical address and 4 KBpage size. The system supports up to 512 MBof physical memory. How many entries are there in each of the following? 8.25 Consider a paging system with the page table stored in memory. a. If a memory reference takes 50 nanoseconds, how long does a paged memory reference take? b. If we add TLBs, and 75 percent of all pagetable references are found in the TLBs, what is the effective memory reference time? (Assume that nding a pagetable entry in the TLBs takes 2 nanoseconds,

a pagetable entry in the TLBs takes 2 nanoseconds, if the entry is present.) 8.26 Why are segmentation and paging sometimes combined into one scheme? 8.27 Explain why sharing a reentrant module is easier when segmentation is used than when pure paging is used. 8.28 Consider the following segment table: Segment Base Length 02 1 9 6 0 0 12 3 0 0 1 4 29 0 1 0 0 31 3 2 7 5 8 0 41 9 5 2 9 6 What are the physical addresses for the following logical addresses? a. 0,430 b. 1,10 c. 2,500 d. 3,400 e.

addresses? a. 0,430 b. 1,10 c. 2,500 d. 3,400 e. 4,112 8.29 What is the purpose of paging the page tables? 8.30 Consider the hierarchical paging scheme used by the VAX architecture. How many memory operations are performed when a user program executes a memoryload operation? 8.31 Compare the segmented paging scheme with the hashed page table scheme for handling large address spaces. Under what circumstances is one scheme preferable to the other? 8.32 Consider the Intel addresstranslation scheme

8.32 Consider the Intel addresstranslation scheme shown in Figure 8.22. a. Describe all the steps taken by the Intel Pentium in translating a logical address into a physical address. b. What are the advantages to the operating system of hardware that provides such complicated memory translation?394 Chapter 8 Main Memory c. Are there any disadvantages to this addresstranslation system? If so, what are they? If not, why is this scheme not used by every manufacturer? Programming Problems 8.33

by every manufacturer? Programming Problems 8.33 Assume that a system has a 32bit virtual address with a 4 KBpage size. Write a C program that is passed a virtual address (in decimal) on the command line and have it output the page number and offset for the given address. As an example, your program would run as follows: .a.out 19986 Your program would output: The address 19986 contains: page number  4 offset  3602 Writing this program will require using the appropriate data type to store 32

using the appropriate data type to store 32 bits. We encourage you to use unsigned data types as well. Bibliographical Notes Dynamic storage allocation was discussed by [Knuth (1973)] (Section 2.5), who found through simulation that rst t is generally superior to best t. [Knuth (1973)] also discussed the 50percent rule. The concept of paging can be credited to the designers of the Atlas system, which has been described by [Kilburn et al. (1961)] and by [Howarth et al. (1961)]. The concept of

and by [Howarth et al. (1961)]. The concept of segmentation was rst discussed by [Dennis (1965)]. Paged segmentation was rst supported in the GE 645 ,o nw h i c h MULTICS was originally implemented ([Organick (1972)] and [Daley and Dennis (1967)]). Inverted page tables are discussed in an article about the IBM RT storage manager by [Chang and Mergen (1988)]. [Hennessy and Patterson (2012)] explains the hardware aspects of TLBs, caches, and MMU s. [Talluri et al. (1995)] discusses page tables for

[Talluri et al. (1995)] discusses page tables for 64bit address spaces. [Jacob and Mudge (2001)] describes techniques for managing the TLB. [Fang et al. (2001)] evaluates support for large pages. http:msdn.microsoft.comenuslibrarywindowshardwaregg487512. aspx discusses PAE support for Windows systems. http:www.intel.comcontentwwwusenprocessorsarchitecturessof twaredevelopermanuals.html provides various manuals for Intel 64 and IA32 architectures.

manuals for Intel 64 and IA32 architectures. http:www.arm.comproductsprocessorscortexacortexa9.php pro vides an overview of the ARM architecture. Bibliography [Chang and Mergen (1988)] A. Chang and M. F. Mergen, 801 Storage: Archi tecture and Programming ,ACM Transactions on Computer Systems ,V o l u m e6 , Number 1 (1988), pages 2850.Bibliography 395 [Daley and Dennis (1967)] R. C. Daley and J. B. Dennis, Virtual Memory, Processes, and Sharing in Multics ,Proceedings of the ACM Symposium on

in Multics ,Proceedings of the ACM Symposium on Operating Systems Principles (1967), pages 121128. [Dennis (1965)] J. B. Dennis, Segmentation and the Design of Multipro grammed Computer Systems ,Communications of the ACM ,V o l u m e8 ,N u m b e r 4 (1965), pages 589602. [Fang et al. (2001)] Z. Fang, L. Zhang, J. B. Carter, W. C. Hsieh, and S. A. McKee, Reevaluating Online Superpage Promotion with Hardware Support ,Proceed ings of the International Symposium on HighPerformance Computer

Symposium on HighPerformance Computer Architecture , Volume 50, Number 5 (2001). [Hennessy and Patterson (2012)] J. Hennessy and D. Patterson, Computer Archi tecture: A Quantitative Approach, Fifth Edition, Morgan Kaufmann (2012). [Howarth et al. (1961)] D. J. Howarth, R. B. Payne, and F. H. Sumner, The Manchester University Atlas Operating System, Part II: Users Description , Computer Journal ,V o l u m e4 ,N u m b e r3( 1 9 6 1 ) ,p a g e s2 2 6  2 2 9 . [Jacob and Mudge (2001)] B. Jacob and

2 6  2 2 9 . [Jacob and Mudge (2001)] B. Jacob and T. Mudge, Uniprocessor Virtual Mem ory Without TLBs ,IEEE Transactions on Computers ,V o l u m e5 0 ,N u m b e r5 (2001). [Kilburn et al. (1961)] T. Kilburn, D. J. Howarth, R. B. Payne, and F. H. Sumner, The Manchester University Atlas Operating System, Part I: Internal Organiza tion,Computer Journal ,V o l u m e4 ,N u m b e r3( 1 9 6 1 ) ,p a g e s2 2 2  2 2 5 . [Knuth (1973)] D. E. Knuth, The Art of Computer Programming, Volume 1: Funda mental

of Computer Programming, Volume 1: Funda mental Algorithms, Second Edition, AddisonWesley (1973). [Organick (1972)] E. I. Organick, The Multics System: An Examination of Its Structure ,M I TP r e s s( 1 9 7 2 ) . [Talluri et al. (1995)] M. Talluri, M. D. Hill, and Y. A. Khalidi, AN e wP a g e Table for 64bit Address Spaces ,Proceedings of the ACM Symposium on Operating Systems Principles (1995), pages 184200.9CHAPTER Virtual Memory In Chapter 8, we discussed various memorymanagement strategies

we discussed various memorymanagement strategies used in computer systems. All these strategies have the same goal: to keep many processes in memory simultaneously to allow multiprogramming. However, they tend to require that an entire process be in memory before it can execute. Virtual memory is a technique that allows the execution of processes that are not completely in memory. One major advantage of this scheme is that programs can be larger than physical memory. Further, virtual memory

than physical memory. Further, virtual memory abstracts main memory into an extremely large, uniform array of storage, separating logical memory as viewed by the user from physical memory. This technique frees programmers from the concerns of memorystorage limitations. Virtual memory also allows processes to share les easily and to implement shared memory. In addition, it provides an efcient mechanism for process creation. Virtual memory is not easy to implement, however, and may substantially

easy to implement, however, and may substantially decrease performance if it is used carelessly. In this chapter, we discuss virtual memory in the form of demand paging and examine its complexity and cost. CHAPTER OBJECTIVES To describe the benets of a virtual memory system. To explain the concepts of demand paging, pagereplacement algorithms, and allocation of page frames. To discuss the principles of the workingset model. To examine the relationship between shared memory and memorymapped les.

between shared memory and memorymapped les. To explore how kernel memory is managed. 9.1 Background The memorymanagement algorithms outlined in Chapter 8 are necessary because of one basic requirement: The instructions being executed must be 397398 Chapter 9 Virtual Memory in physical memory. The rst approach t o meeting this requirement is to place the entire logical address space in physical memory. Dynamic loading can help to ease this restriction, but it generally requires special

restriction, but it generally requires special precautions and extra work by the programmer. The requirement that instructions must be in physical memory to be executed seems both necessary and reasonable; but it is also unfortunate, since it limits the size of a program to the size of physical memory. In fact, an examination of real programs shows us that, in many cases, the entire program is not needed. For instance, consider the following: Programs often have code to handle unusual error

Programs often have code to handle unusual error conditions. Since these errors seldom, if ever, occur in practice, this code is almost never executed. Arrays, lists, and tables are often allocated more memory than they actually need. An array may be declared 100 by 100 elements, even though it is seldom larger than 10 by 10 elements. An assembler symbol table may have room for 3,000 symbols, although the average program has less than 200 symbols. Certain options and features of a program may be

Certain options and features of a program may be used rarely. For instance, the routines on U.S. government c omputers that balance the budget have not been used in many years. Even in those cases where the entire program is needed, it may not all be needed at the same time. The ability to execute a program that is only partially in memory would confer many benets: Ap r o g r a mw o u l dn ol o n g e rb ec o n s t r a i n e db yt h ea m o u n to fp h y s i c a l memory that is available. Users

fp h y s i c a l memory that is available. Users would be able to write programs for an extremely large virtual address space, simplifying the programming task. Because each user program could take less physical memory, more programs could be run at the same time, with a corresponding increase in CPU utilization and throughput but with no increase in response time or turnaround time. Less IOwould be needed to load or swap user programs into memory, so each user program would run faster. Thus,

so each user program would run faster. Thus, running a program that is not entirely in memory would benet both the system and the user. Virtual memory involves the separation of logical memory as perceived by users from physical memory. This separation allows an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available (Figure 9.1). Virtual memory makes the task of program ming much easier, because the programmer no longer needs to worry about

the programmer no longer needs to worry about the amount of physical memory available; she can concentrate instead on the problem to be programmed. The virtual address space of a process refers to the logical (or virtual) view of how a process is stored in memory. Typically, this view is that a process begins at a certain logical addresssay, address 0and exists in contiguous memory, as shown in Figure 9.2. Recall from Chapter 8, though, that in fact9.1 Background 399 virtual memorymemory map

in fact9.1 Background 399 virtual memorymemory map physical memory  page 0 page 1 page 2 page v Figure 9.1 Diagram showing virtual memory that is larger than physical memory. physical memory may be organized in page frames and that the physical page frames assigned to a process may not be contiguous. It is up to the memory management unit ( MMU )t om a pl o g i c a lp a g e st op h y s i c a lp a g ef r a m e si n memory. Note in Figure 9.2 that we allow the heap to grow upward in memory as it

we allow the heap to grow upward in memory as it is used for dynamic memory allocation. Similarly, we allow for the stack to code 0Max dataheapstack Figure 9.2 Virtual address space.400 Chapter 9 Virtual Memory shared librarystack shared pages codedataheap codedataheapshared librarystack Figure 9.3 Shared library using virtual memory. grow downward in memory through successive function calls. The large blank space (or hole) between the heap and the stack is part of the virtual address space but

the stack is part of the virtual address space but will require actual physical pages only if the heap or stack grows. Virtual address spaces that include holes are known as sparse address spaces. Using a sparse address space is benecial because the holes can be lled as the stack or heap segments grow or if we wish to dynamically link libraries (or possibly other shared objects) during program execution. In addition to separating logical memory from physical memory, virtual memory allows les and

physical memory, virtual memory allows les and memory to be shared by two or more processes through page sharing (Section 8.5.4). This leads to the following benets: System libraries can be shared by several processes through mapping of the shared object into a virtual address space. Although each process considers the libraries to be part of its virtual address space, the actual pages where the libraries reside in physical memory are shared by all the processes (Figure 9.3). Typically, a

by all the processes (Figure 9.3). Typically, a library is mapped readonly into the space of each process that is linked with it. Similarly, processes can share memory. Recall from Chapter 3 that two or more processes can communicate through the use of shared memory. Virtual memory allows one process to create a region of memory that it can share with another process. Processes sharing this region consider it part of their virtual address space, yet the actual physical pages of memory are

space, yet the actual physical pages of memory are shared, much as is illustrated in Figure 9.3. Pages can be shared during process creation with the fork() system call, thus speeding up process creation. We further explore these  and other  benets of virtual memory later in this chapter. First, though, we discuss implementing virtual memory through demand paging.9.2 Demand Paging 401 9.2 Demand Paging Consider how an executable program might be loaded from disk into memory. One option is to

be loaded from disk into memory. One option is to load the entire program in physical memory at program execution time. However, a problem with this approach is that we may not initially need the entire program in memory. Suppose a program starts with a list of available options from which the user is to select. Loading the entire program into memory results in loading the executable code for alloptions, regardless of whether or not an option is ultimately selected by the user. An alternative

is ultimately selected by the user. An alternative strategy is to load pages only as they are needed. This technique is known as demand paging and is commonly used in virtual memory systems. With demandpaged virtual memory, pages are loaded only when they are demanded during program execution. Pages that are never accessed are thus never loaded into physical memory. Ad e m a n d  p a g i n gs y s t e mi ss i m i l a rt oap a g i n gs y s t e mw i t hs w a p p i n g (Figure 9.4) where processes

i t hs w a p p i n g (Figure 9.4) where processes reside in secondary memory (usually a disk). When we want to execute a process, we swap it into memory. Rather than swapping the entire process into memory, though, we use a lazy swapper . Al a z ys w a p p e rn e v e rs w a p sap a g ei n t om e m o r yu n l e s st h a tp a g ew i l lb e needed. In the context of a demandpaging system, use of the term swapper  is technically incorrect. A swapper manipulates entire processes, whereas a pager is

manipulates entire processes, whereas a pager is concerned with the individual pages of a process. We thus use pager,  rather than swapper, in connection with demand paging. program Aswap out 0 1 2 3 4 5 6 7 8 910 11 12 13 14 15 16 17 18 19 20 21 22 23swap inprogram B main memory Figure 9.4 Transfer of a paged memory to contiguous disk space.402 Chapter 9 Virtual Memory 9.2.1 Basic Concepts When a process is to be swapped in, the pager guesses which pages will be used before the process is

which pages will be used before the process is swapped out again. Instead of swapping in a whole process, the pager brings only those pages into memory. Thus, it avoids reading into memory pages that will not be used anyway, decreasing the swap time and the amount of physical memory needed. With this scheme, we need some form of hardware support to distinguish between the pages that are in memory and the pages that are on the disk. The validinvalid bit scheme described in Section 8.5.3 can be

bit scheme described in Section 8.5.3 can be used for this purpose. This time, however, when this bit is set to valid, the associated page is both legal and in memory. If the bit is set to invalid, the page either is not valid (that is, not in the logical address space of the process) or is valid but is currently on the disk. The pagetable entry for a page that is brought into memory is set as usual, but the pagetable entry for a page that is not currently in memory is either simply marked

is not currently in memory is either simply marked invalid or contains the address of the page on disk. This situation is depicted in Figure 9.5. Notice that marking a page invalid will have no effect if the process never attempts to access that page. Hence, if we guess right and page in all pages that are actually needed and only those pages, the process will run exactly as though we had brought in all pages. While the process executes and accesses pages that are memory resident ,e x e c u t i

pages that are memory resident ,e x e c u t i o np r o c e e d sn o r m a l l y . B D DEF H logical memoryvalidinvalid bitframe page table104 62 3 4 59 6 710 2 3 4 5 6 7iv v i i v i i physical memoryA AB C C FGHF10 2 3 4 5 6 7 98 10 11 12 13 14 15A C E G Figure 9.5 Page table when some pages are not in main memory.9.2 Demand Paging 403 load Mreferencetrap ipage is on backing store operating system restart instruction reset page tablepage table physical memorybring in missing pagefree frame123 6

memorybring in missing pagefree frame123 6 5 4 Figure 9.6 Steps in handling a page fault. But what happens if the process tries to access a page that was not brought into memory? Access to a page marked invalid causes a page fault .T h ep a g i n g hardware, in translating the address through the page table, will notice that the invalid bit is set, causing a trap to the operating system. This trap is the result of the operating systems failure to bring the desired page into memory. The procedure

bring the desired page into memory. The procedure for handling this page fault is straightforward (Figure 9.6): 1.We check an internal table (usually kept with the process control block) for this process to determine wheth er the reference was a valid or an invalid memory access. 2.If the reference was invalid, we terminate the process. If it was valid but we have not yet brought in that page, we now page it in. 3.We nd a free frame (by taking one from the freeframe list, for example). 4.We

one from the freeframe list, for example). 4.We schedule a disk operation to read the desired page into the newly allocated frame. 5.When the disk read is complete, we modify the internal table kept with the process and the page table to indicate that the page is now in memory. 6.We restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. In the extreme case, we can start executing a process with nopages in memory. When

executing a process with nopages in memory. When the operating system sets the instruction pointer to the rst404 Chapter 9 Virtual Memory instruction of the process, which is on a nonmemoryresident page, the process immediately faults for the page. After this page is brought into memory, the process continues to execute, faultin ga sn e c e s s a r yu n t i le v e r yp a g et h a ti t needs is in memory. At that point, it can execute with no more faults. This scheme is pure demand paging : never

faults. This scheme is pure demand paging : never bring a page into memory until it is required. Theoretically, some programs could access several new pages of memory with each instruction execution (one page for the instruction and many for data), possibly causing multiple page faults per instruction. This situation would result in unacceptable system performance. Fortunately, analysis of running processes shows that this behavior is exceedingly unlikely. Programs tend to have locality of

unlikely. Programs tend to have locality of reference ,d e s c r i b e di nS e c t i o n9 . 6 . 1 ,w h i c hr e s u l t si n reasonable performance from demand paging. The hardware to support demand paging is the same as the hardware for paging and swapping: Page table .T h i st a b l eh a st h ea b i l i t yt om a r ka ne n t r yi n v a l i dt h r o u g ha validinvalid bit or a special value of protection bits. Secondary memory .T h i sm e m o r yh o l d st h o s ep a g e st h a ta r en o tp r

r yh o l d st h o s ep a g e st h a ta r en o tp r e s e n t in main memory. The secondary memory is usually a highspeed disk. It is known as the swap device, and the section of disk used for this purpose is known as swap space .S w a p  s p a c ea l l o c a t i o ni sd i s c u s s e di nC h a p t e r1 0 . A crucial requirement for demand paging is the ability to restart any instruction after a page fault. Because we save the state (registers, condition code, instruction counter) of the

condition code, instruction counter) of the interrupted process when the page fault occurs, we must be able to restart the process in exactly the same place and state, except that the desired page is now in memory and is accessible. In most cases, this requirement is easy to meet. A page fault may occur at any memory reference. If the page fault occurs on the instruction fetch, we can restart by fetching the instruction again. If a page fault occurs while we are fetching an operand, we must

occurs while we are fetching an operand, we must fetch and decode the instruction again and then fetch the operand. As a worstcase example, consider a threeaddress instruction such as ADD the content of A to B, placing the result in C. These are the steps to execute this instruction: 1.Fetch and decode the instruction ( ADD ). 2.Fetch A. 3.Fetch B. 4.Add A and B. 5.Store the sum in C. If we fault when we try to store in C (because C is in a page not currently in memory), we will have to get the

not currently in memory), we will have to get the desired page, bring it in, correct the page table, and restart the instruction. The restart will require fetching the instruction again, decoding it again, fetching the two operands again, and then adding again. However, there is no tm u c hr e p e a t e dw o r k( l e s st h a no n e9.2 Demand Paging 405 complete instruction), and the repetition is necessary only when a page fault occurs. The major difculty arises when one instruction may modify

difculty arises when one instruction may modify several different locations. For example, consider the IBMSystem 360370 MVC (move character) instruction, which can move up to 256 bytes from one location to another (possibly overlapping) location. If either block (source or destination) straddles a page boundary, a page fault might occur after the move is partially done. In addition, if the source and destination blocks overlap, the source block may have been modied, in which case we cannot

may have been modied, in which case we cannot simply restart the instruction. This problem can be solved in two different ways. In one solution, the microcode computes and attempts to access both ends of both blocks. If a page fault is going to occur, it will happen at this step, before anything is modied. The move can then take place; we know that no page fault can occur, since all the relevant pages are in memory. The other solution uses temporary registers to hold the values of overwritten

registers to hold the values of overwritten locations. If there is a page fault, all the old values are written back into memory before the trap occurs. This action restores memory to its state before the instruction was started, so that the instruction can be repeated. This is by no means the only architectural problem resulting from adding paging to an existing architecture to allow demand paging, but it illustrates some of the difculties involved. P aging is added between the CPU and the

involved. P aging is added between the CPU and the memory in a computer system. It should be entirely transparent to the user process. Thus, people often assume that paging can be added to any system. Although this assumption is true for a nondemandpaging environment, where a page fault represents a fatal error, it is not true where a page fault means only that an additional page must be brought into memory and the process restarted. 9.2.2 Performance of Demand Paging Demand paging can

Performance of Demand Paging Demand paging can signicantly affect the performance of a computer system. To see why, lets compute the effective access time for a demandpaged memory. For most computer systems, the memoryaccess time, denoted ma, ranges from 10 to 200 nanoseconds. As long as we have no page faults, the effective access time is equal to the memory access time. If, however, a page fault occurs, we must rst read the relevant page from disk and then access the desired word. Letpbe the

disk and then access the desired word. Letpbe the probability of a page fault (0 p1). We would expect pto be close to zerothat is, we would expect to have only a few page faults. The effective access time is then effective access time  (1 p)mappage fault time. To compute the effective access time, we must know how much time is needed to service a page fault. A page fault causes the following sequence to occur: 1.Trap to the operating system. 2.Save the user registers and process state.406

2.Save the user registers and process state.406 Chapter 9 Virtual Memory 3.Determine that the interrupt was a page fault. 4.Check that the page reference was legal and determine the location of the page on the disk. 5.Issue a read from the disk to a free frame: a. Wait in a queue for this device until the read request is serviced. b. Wait for the device seek andor latency time. c. Begin the transfer of the page to a free frame. 6.While waiting, allocate the CPU to some other user ( CPU

waiting, allocate the CPU to some other user ( CPU scheduling, optional). 7.Receive an interrupt from the disk IOsubsystem ( IOcompleted). 8.Save the registers and process state for the other user (if step 6 is executed). 9.Determine that the interrupt was from the disk. 10. Correct the page table and other tables to show that the desired page is now in memory. 11. Wait for the CPU to be allocated to this process again. 12. Restore the user registers, process state, and new page table, and then

process state, and new page table, and then resume the interrupted instruction. Not all of these steps are necessary in every case. For example, we are assuming that, in step 6, the CPU is allocated to another process while the IOoccurs. This arrangement allows multiprogramming to maintain CPU utilization but requires additional time to resume the pagefault service routine when the IO transfer is complete. In any case, we are faced with three major components of the pagefault service time:

major components of the pagefault service time: 1.Service the pagefault interrupt. 2.Read in the page. 3.Restart the process. The rst and third tasks can be reduced, with careful coding, to several hundred instructions. These tasks may take from 1 to 100 microseconds each. The pageswitch time, however, will probably be close to 8 milliseconds. (A typical hard disk has an average latency of 3 milliseconds, a seek of 5m i l l i s e c o n d s ,a n dat r a n s f e rt i m eo f0 . 0 5m i l l i s e c o

r a n s f e rt i m eo f0 . 0 5m i l l i s e c o n d s .T h u s ,t h et o t a l paging time is about 8 milliseconds, including hardware and software time.) Remember also that we are looking at only the deviceservice time. If a queue of processes is waiting for the device, we have to add devicequeueing time as we wait for the paging device to be free to service our request, increasing even more the time to swap.9.2 Demand Paging 407 With an average pagefault service time of 8 milliseconds and a

pagefault service time of 8 milliseconds and a memory access time of 200 nanoseconds, the effe ctive access time in nanoseconds is effective access time  (1 p)(200)  p(8 milliseconds) ( 1 p)200  p8,000,000 2 0 07 , 9 9 9 , 8 0 0 p. We see, then, that the effective access time is directly proportional to the pagefault rate .I fo n ea c c e s so u to f1 , 0 0 0c a u s e sap a g ef a u l t ,t h ee f f e c t i v ea c c e s s time is 8.2 microseconds. The computer will be slowed down by a factor of

The computer will be slowed down by a factor of 40 because of demand paging! If we want performance degradation to be less than 10 percent, we need to keep the probability of page faults at the following level: 220200  7,999,800 p, 207,999,800 p, p0.0000025. That is, to keep the slowdown due to paging at a reasonable level, we can allow fewer than one memory access out of 399,990 to pagefault. In sum, it is important to keep the pagefault rate low in a demandpaging system. Otherwise, the

rate low in a demandpaging system. Otherwise, the effective access time increases, slowing process execution dramatically. An additional aspect of demand paging is the handling and overall use of swap space. Disk IOto swap space is generally faster than that to the le system. It is a faster le system because swap space is allocated in much larger blocks, and le lookups and indirect allocation methods are not used (Chapter 10). The system can therefore gain better paging throughput by copying an

gain better paging throughput by copying an entire le image into the swap space at process startup and then performing demand paging from the swap space. Another option is to demand pages from the le system initially but to write the pages to swap space as they are replaced. This approach will ensure that only needed pages are read from the le system but that all subsequent paging is done from swap space. Some systems attempt to limit the amount of swap space used through demand paging of binary

of swap space used through demand paging of binary les. Demand pages for such les are brought directly from the le system. However, when page replacement is called for, these frames can simply be overwritten (beca use they are never modied), and the pages can be read in from the le system again if needed. Using this approach, the le system itself serves as the backing store. However, swap space must still be used for pages not associated with a le (known as anonymous memory ); these pages

a le (known as anonymous memory ); these pages include the stack and heap for a process. This method appears to be a good compromise and is used in several systems, including Solaris and BSD UNIX . Mobile operating systems typically do not support swapping. Instead, these systems demandpage from the le system and reclaim readonly pages (such as code) from applications if memory becomes constrained. Such data can be demandpaged from the le system if it is later needed. Under i OS, anonymous

if it is later needed. Under i OS, anonymous memory pages are never reclaimed from an application unless the application is terminated or explicitly releases the memory.408 Chapter 9 Virtual Memory 9.3 CopyonWrite In Section 9.2, we illustrated how a process can start quickly by demandpaging in the page containing the rst instr uction. However, process creation using the fork() system call may initially bypass the need for demand paging by using at e c h n i q u es i m i l a rt op a g es h a r i

at e c h n i q u es i m i l a rt op a g es h a r i n g( c o v e r e di nS e c t i o n8 . 5 . 4 ) .T h i st e c h n i q u e provides rapid process creation and minimizes the number of new pages that must be allocated to the newly created process. Recall that the fork() system call creates a child process that is a duplicate of its parent. Traditionally, fork() worked by creating a copy of the parents address space for the child, duplicating the pages belonging to the parent. However, considering

belonging to the parent. However, considering that many c hild processes invoke the exec() system call immediately after creation, the copying of the parents address space may be unnecessary. Instead, we can use a technique known as copyonwrite , which works by allowing the parent and child processes initially to share the same pages. These shared pages are marked as copyonwrite pages, meaning that if either process writes to a shared page, a copy of the shared page is created. Copyonwrite is

copy of the shared page is created. Copyonwrite is illustrated in Figures 9.7 and 9.8, which show the contents of the physical memory before and after process 1 modies page C. For example, assume that the child process attempts to modify a page containing portions of the stack, with the pages set to be copyonwrite. The operating system will create a copy of this page, mapping it to the address space of the child process. The child process will then modify its copied page and not the page

will then modify its copied page and not the page belonging to the parent process. Obviously, when the copyonwrite technique is used, only the pages that ar em o d i  e db ye i t h e rp r o c e s sa r ec o p i e d ; all unmodied pages can be shared by the parent and child processes. Note, too, that only pages that can be modied need be marked as copyonwrite. Pages that cannot be modied (pages containing executable code) can be shared by the parent and child. Copyonwrite is a common technique

and child. Copyonwrite is a common technique used by several operating systems, including Windows XP,L i n u x ,a n dS o l a r i s . When it is determined that a page is going to be duplicated using copy onwrite, it is important to note the location from which the free page will be allocated. Many operating systems provide a pool of free pages for such requests. These free pages are typically allocated when the stack or heap for a process must expand or when there are copyonwrite pages to be

expand or when there are copyonwrite pages to be managed.process1physical memory page A page B page Cprocess2 Figure 9.7 Before process 1 modies page C.9.4 Page Replacement 409process1physical memory page A page B page C Copy of page Cprocess2 Figure 9.8 After process 1 modies page C. Operating systems typically allocate these pages using a technique known as zerollondemand .Z e r o   l l  o n  d e m a n dp a g e sh a v eb e e nz e r o e d  o u tb e f o r e being allocated, thus erasing the

o u tb e f o r e being allocated, thus erasing the previous contents. Several versions of UNIX (including Solaris and Linux) provide a variation of the fork() system call vfork() (forvirtual memory fork )thatoperates differently from fork() with copyonwrite. With vfork() ,t h ep a r e n tp r o c e s s is suspended, and the child process uses the address space of the parent. Because vfork() does not use copyonwrite, if the child process changes any pages of the parents address space, the altered

pages of the parents address space, the altered pages will be visible to the parent once it resumes. Therefore, vfork() must be used with caution to ensure that the child process does not modify the address space of the parent. vfork() is intended to be used when the child process calls exec() immediately after creation. Because no copying of pages takes place, vfork() is an extremely efcient method of process creation and is sometimes used to implement UNIX commandline shell interfaces. 9.4

implement UNIX commandline shell interfaces. 9.4 Page Replacement In our earlier discussion of the pagefault rate, we assumed that each page faults at most once, when it is rst referenced. This representation is not strictly accurate, however. If a process of ten pages actually uses only half of them, then demand paging saves the IOnecessary to load the ve pages that are never used. We could also increase our degree of multiprogramming by running twice as many processes. Thus, if we had forty

twice as many processes. Thus, if we had forty frames, we could run eight processes, rather than the four that c ould run if each required ten frames (ve of which were never used). If we increase our degree of multiprogramming, we are overallocating memory. If we run six processes, each of which is ten pages in size but actually uses only ve pages, we have higher CPU utilization and throughput, with ten frames to spare. It is possible, however, that each of these processes, for a particular data

each of these processes, for a particular data set, may suddenly try to use all ten of its pages, resulting in a need for sixty frames when only forty are available. Further, consider that system memory is not used only for holding program pages. Buffers for IOalso consume a considerable amount of memory. This use410 Chapter 9 Virtual Memory monitor load M physical memory10 2 3 4 5 6 7H load M J M logical memory for user 10 PC1 2 3 B Mvalidinvalid bitframe page table for user 1i A B D E logical

bitframe page table for user 1i A B D E logical memory for user 20 1 2 3validinvalid bitframe page table for user 2i43 5v v v 72v v6vD H J A E Figure 9.9 Need for page replacement. can increase the strain on memoryplacement algorithms. Deciding how much memory to allocate to IOand how much to program pages is a signicant challenge. Some systems allocate a xed percentage of memory for IObuffers, whereas others allow both user processes and the IOsubsystem to compete for all system memory.

the IOsubsystem to compete for all system memory. Overallocation of memory manifests itself as follows. While a user process is executing, a page fault occurs. The operating system determines where the desired page is residing on the disk but then nds that there are nofree frames on the freeframe list; all memory is in use (Figure 9.9). The operating system has several options at this point. It could terminate the user process. However, demand paging is the operating systems attempt to improve

paging is the operating systems attempt to improve the computer systems utilization and throughput. Users should not be aware that their processes are running on a paged systempaging should be logically transparent to the user. So this option is not the best choice. The operating system could instead swap out a process, freeing all its frames and reducing the level of multiprogramming. This option is a good one in certain circumstances, and we consider it further in Section 9.6. Here, we discuss

it further in Section 9.6. Here, we discuss the most common solution: page replacement . 9.4.1 Basic Page Replacement Page replacement takes the following approach. If no frame is free, we nd one that is not currently being used and free it. We can free a frame by writing its contents to swap space and changing the page table (and all other tables) to indicate that the page is no longer in memory (Figure 9.10). We can now use the freed frame to hold the page for which the process faulted. We

to hold the page for which the process faulted. We modify the pagefault service routine to include page replacement:9.4 Page Replacement 411validinvalid bit frame f page tablevictimchange to invalidpage out victim page page in desired pagereset page table for new page physical memory2 41 3f0i v Figure 9.10 Page replacement. 1.Find the location of the desired page on the disk. 2.Find a free frame: a. If there is a free frame, use it. b. If there is no free frame, use a pagereplacement algorithm

is no free frame, use a pagereplacement algorithm to select avictim frame . c. Write the victim frame to the disk; change the page and frame tables accordingly. 3.Read the desired page into the new ly freed frame; change the page and frame tables. 4.Continue the user process from where the page fault occurred. Notice that, if no frames are free, two page transfers (one out and one in) are required. This situation effectively doubles the pagefault service time and increases the effective access

service time and increases the effective access time accordingly. We can reduce this overhead by using a modify bit (ordirty bit ). When this scheme is used, each page or frame has a modify bit associated with it in the hardware. The modify bit for a page is set by the hardware whenever any byte in the page is written into, indicating that the page has been modied. When we select a page for replacement, we examine its modify bit. If the bit is set, we know that the page has been modied since it

we know that the page has been modied since it was read in from the disk. In this case, we must write the page to the disk. If the modify bit is not set, however, the page has notbeen modied since it was read into memory. In this case, we need not write the memory page to the disk: it is already there. This technique also applies to readonly pages (for example, pages of binary code).412 Chapter 9 Virtual Memory Such pages cannot be modied; thus, they may be discarded when desired. This scheme

they may be discarded when desired. This scheme can signicantly reduce the time required to service a page fault, since it reduces IOtime by onehalf ifthe page has not been modied. Page replacement is basic to demand paging. It completes the separation between logical memory and physical memory. With this mechanism, an enormous virtual memory can be provided for programmers on a smaller physical memory. With no demand paging, user addresses are mapped into physical addresses, and the two sets of

into physical addresses, and the two sets of addresses can be different. All the pages of a process still must be in physical memory, however. With demand paging, the size of the logical address space is no longer constrained by physical memory. If we have a user process of twenty pages, we can execute it in ten frames simply by using demand paging and using a replacement algorithm to nd a free frame whenever necessary. If a page that has been modied is to be replaced, its contents are copied to

is to be replaced, its contents are copied to the disk. A later reference to that page will cause a page fault. At that time, the page will be brought back into memory, perhaps replacing some other page in the process. We must solve two major problems to implement demand paging: we must develop a frameallocation algorithm and a pagereplacement algorithm . That is, if we have multiple processes in memory, we must decide how many frames to allocate to each process; and when page replacement is

to each process; and when page replacement is required, we must select the frames that are to be replaced. Designing appropriate algorithms to solve these problems is an important task, because disk IO is so expensive. Even slight improvemen ts in demandpaging methods yield large gains in system performance. There are many different pager eplacement algorithms. Every operating system probably has its own replacement scheme. How do we select a particular replacement algorithm? In general, we want

replacement algorithm? In general, we want the one with the lowest pagefault rate. We evaluate an algorithm by running it on a particular string of memory references and computing the number of page faults. The string of memory references is called a reference string .W ec a ng e n e r a t er e f e r e n c es t r i n g s articially (by using a randomnumber generator, for example), or we can trace ag i v e ns y s t e ma n dr e c o r dt h ea d d r e s so fe a c hm e m o r yr e f e r e n c e .T h

r e s so fe a c hm e m o r yr e f e r e n c e .T h el a t t e r choice produces a large number of data (on the order of 1 million addresses per second). To reduce the number of data, we use two facts. First, for a given page size (and the page size is generally xed by the hardware or system), we need to consider only the page number, rather than the entire address. Second, if we have a reference to a page p,then any references to page pthat immediately follow will never cause a page fault. Page

follow will never cause a page fault. Page pwill be in memory after the rst reference, so the immediately following references will not fault. For example, if we trace a particular process, we might record the following address sequence: 0100, 0432, 0101, 0612, 0102, 0103, 0104, 0101, 0611, 0102, 0103, 0104, 0101, 0610, 0102, 0103, 0104, 0101, 0609, 0102, 0105 At 100 bytes per page, this sequence is r educed to the following reference string: 1, 4, 1, 6, 1, 6, 1, 6, 1, 6, 19.4 Page Replacement

4, 1, 6, 1, 6, 1, 6, 1, 6, 19.4 Page Replacement 413number of page faults16 14 12 10 8 6 4 2 123 number of frames456 Figure 9.11 Graph of page faults versus number of frames. To determine the number of page faults for a particular reference string and pagereplacement algorithm, we also need to know the number of page frames available. Obviously, as the number of frames available increases, the number of page faults decreases. For the reference string considered previously, for example, if we had

considered previously, for example, if we had three or more frames, we would have only three faults one fault for the rst reference to each page. In contrast, with only one frame available, we would have a replacement with every reference, resulting in eleven faults. In general, we expect a curve such as that in Figure 9.11. As the number of frames increases, the number of page faults drops to some minimal level. Of course, adding physical memory increases the number of frames. We next

memory increases the number of frames. We next illustrate several pagereplacement algorithms. In doing so, we use the reference string 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1 for a memory with three frames. 9.4.2 FIFO Page Replacement The simplest pagereplacement algorithm is a rstin, rstout ( FIFO )a l g o r i t h m . AFIFO replacement algorithm associates with each page the time when that page was brought into memory. When a page must be replaced, the oldest page is chosen.

page must be replaced, the oldest page is chosen. Notice that it is not strictly necessary to record the time when ap a g ei sb r o u g h ti n .W ec a nc r e a t ea FIFO queue to hold all pages in memory. We replace the page at the head of the queue. When a page is brought into memory, we insert it at the tail of the queue. For our example reference string, our three frames are initially empty. The rst three references (7, 0, 1) cause page faults and are brought into these empty frames. The next

and are brought into these empty frames. The next reference (2) replaces page 7, because page 7 was brought in rst. Since 0 is the next reference and 0 is already in memory, we have no fault for this reference. The rst reference to 3 results in replacement of page 0, since it is now rst in line. Because of this replacement, the next reference, to 0, will414 Chapter 9 Virtual Memory 77 07 0 1 page framesreference string 2 0 12 3 12 3 04 3 04 2 04 2 30 2 37 1 27 0 27 0 10 1 307012030423 0 7 1 10

2 30 2 37 1 27 0 27 0 10 1 307012030423 0 7 1 10 212 03 1 2 Figure 9.12 FIFO pagereplacement algorithm. fault. Page 1 is then replaced by page 0. This process continues as shown in Figure 9.12. Every time a fault occurs, we show which pages are in our three frames. There are fteen faults altogether. The FIFO pagereplacement algorithm is easy to understand and program. However, its performance is not always good. On the one hand, the page replaced may be an initialization module that was used a

may be an initialization module that was used a long time ago and is no longer needed. On the other hand, it could contain a heavily used variable that was initialized early and is in constant use. Notice that, even if we select for replac ement a page that is in active use, everything still works correctly. After we replace an active page with a new one, a fault occurs almost immediately to retrieve the active page. Some other page must be replaced to bring the active page back into memory.

to bring the active page back into memory. Thus, a bad replacement choice increases the pagefault rate and slows process execution. It does not, however, cause incorrect execution. To illustrate the problems that are possible with a FIFO pagereplacement algorithm, consider the following reference string: 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5 Figure 9.13 shows the curve of page faults for this reference string versus the number of available frames. Notice that the number of faults for four frames

Notice that the number of faults for four frames (ten) is greater than the number of faults for three frames (nine)! This most unexpected result is known as Beladys anomaly :f o rs o m ep a g e  r e p l a c e m e n t algorithms, the pagefault rate may increase as the number of allocated frames increases. We would expect that giving more memory to a process would improve its performance. In some early research, investigators noticed that this assumption was not always true. Beladys anomaly was

was not always true. Beladys anomaly was discovered as a result. 9.4.3 Optimal Page Replacement One result of the discovery of Beladys anomaly was the search for an optimal pagereplacement algorithm the algorithm that has the lowest pagefault rate of all algorithms and will never suffer from Beladys anomaly. Such an algorithm does exist and has been called OPT orMIN.I ti ss i m p l yt h i s : Replace the page that will not be used for the longest period of time. Use of this pagereplacement

period of time. Use of this pagereplacement algorith mg u a r a n t e e st h el o w e s tp o s s i b l ep a g e  fault rate for a xed number of frames.9.4 Page Replacement 415number of page faults16 14 12 10 8 6 4 2 123 number of frames4567 Figure 9.13 Pagefault curve for FIFO replacement on a reference string. For example, on our sample reference string, the optimal pagereplacement algorithm would yield nine page faults, as shown in Figure 9.14. The rst three references cause faults that ll the

The rst three references cause faults that ll the three empty frames. The reference to page 2r e p l a c e sp a g e7 ,b e c a u s ep a g e7w i l ln o tb eu s e du n t i lr e f e r e n c e1 8 ,w h e r e a s page 0 will be used at 5, and page 1 at 14. The reference to page 3 replaces page 1, as page 1 will be the last of the three pages in memory to be referenced again. With only nine page faults, optimal replacement is much better than aFIFO algorithm, which results in fteen faults. (If we ignore

which results in fteen faults. (If we ignore the rst three, which all algorithms must suffer, then optimal replacement is twice as good as FIFO replacement.) In fact, no replacement algorithm can process this reference string in three frames with fewer than nine faults. Unfortunately, the optimal pagereplacement algorithm is difcult to implement, because it requires future knowledge of the reference string. (We encountered a similar situation with the SJF CPU scheduling algorithm in Section

with the SJF CPU scheduling algorithm in Section 6.3.2.) As a result, the optimal algorithm is used mainly for comparison studies. For instance, it may be useful to know that, although a new algorithm is not optimal, it is within 12.3 percent of optimal at worst and within 4.7 percent on average. page framesreference string 77 07 0 12 0 12 0 32 4 32 0 37 0 12 0 17012030423 0 7 1 10 212 03 Figure 9.14 Optimal pagereplacement algorithm.416 Chapter 9 Virtual Memory 9.4.4 LRU Page Replacement If the

9 Virtual Memory 9.4.4 LRU Page Replacement If the optimal algorithm is not feasible, perhaps an approximation of the optimal algorithm is possible. The key distinction between the FIFO and OPT algorithms (other than looking backward versus forward in time) is that the FIFO algorithm uses the time when a page was brought into memory, whereas the OPT algorithm uses the time when a page is to be used. If we use the recent past as an approximation of the near future, then we can replace the page

of the near future, then we can replace the page that has not been used for the longest period of time. This approach is the least recently used ( LRU)a l g o r i t h m . LRU replacement associates with each page the time of that pages last use. When a page must be replaced, LRU chooses the page that has not been used for the longest period of time. We can think of this strategy as the optimal pagereplacement algorithm looking backward in time, rather than forward. (Strangely, if we let SRbe the

than forward. (Strangely, if we let SRbe the reverse of a reference string S,then the pagefault rate for the OPT algorithm on Sis the same as the pagefault rate for the OPT algorithm on SR.S i m i l a r l y ,t h ep a g e  f a u l tr a t ef o rt h e LRU algorithm on Sis the same as the pagefault rate for the LRU algorithm on SR.) The result of applying LRU replacement to our example reference string is shown in Figure 9.15. The LRU algorithm produces twelve faults. Notice that the rst ve faults

twelve faults. Notice that the rst ve faults are the same as those for optimal replacement. When the reference to page 4 occurs, however, LRU replacement sees that, of the three frames in memory, page 2 was used least recently. Thus, the LRU algorithm replaces page 2, not knowing that page 2 is about to be used. When it then faults for page 2, the LRU algorithm replaces page 3, since it is now the least recently used of the three pages in memory. Despite these problems, LRU replacement with

Despite these problems, LRU replacement with twelve faults is much better than FIFO replacement with fteen. The LRU policy is often used as a pagereplacement algorithm and is considered to be good. The major problem is how to implement LRU replacement. An LRU pagereplacement algorithm may require substantial hardware assistance. The problem is to determine an order for the frames dened by the time of last use. Two implementations are feasible: Counters . In the simplest case, we associate with

Counters . In the simplest case, we associate with each pagetable entry a timeofuse eld and add to the CPU al o g i c a lc l o c ko rc o u n t e r .T h ec l o c ki s incremented for every memory reference. Whenever a reference to a page is made, the contents of the clock register are copied to the timeofuse eld in the pagetable entry for that page. In this way, we always have page framesreference string 77 07 0 12 0 12 0 34 0 34 0 24 3 20 3 21 3 21 0 21 0 77012030423 0 7 1 10 212 03 Figure 9.15

21 0 21 0 77012030423 0 7 1 10 212 03 Figure 9.15 LRU pagereplacement algorithm.9.4 Page Replacement 417 thetime of the last reference to each page. We replace the page with the smallest time value. This scheme requires a search of the page table to nd the LRU page and a write to memory (to the timeofuse eld in the page table) for each memory access. The times must also be maintained when page tables are changed (due to CPU scheduling). Overow of the clock must be considered. Stack .A n o t h e

the clock must be considered. Stack .A n o t h e ra p p r o a c ht oi m p l e m e n t i n g LRU replacement is to keep as t a c ko fp a g en u m b e r s .W h e n e v e rap a g ei sr e f e r e n c e d ,i ti sr e m o v e d from the stack and put on the top. In this way, the most recently used page is always at the top of the stack and the least recently used page is always at the bottom (Figure 9.16). Because entries must be removed from the middle of the stack, it is best to implement this

middle of the stack, it is best to implement this approach by using a doubly linked list with a head pointer and a tail pointer. Removing a page and putting it on the top of the stack then requires changing six pointers at worst. Each update is a little more expensive, but there is no search for ar e p l a c e m e n t ;t h et a i lp o i n t e rp o i n t st ot h eb o t t o mo ft h es t a c k ,w h i c hi s the LRU page. This approach is particularly appropriate for software or microcode

particularly appropriate for software or microcode implementations of LRU replacement. Like optimal replacement, LRU replacement does not suffer from Beladys anomaly. Both belong to a class of pagereplacement algorithms, called stack algorithms ,t h a tc a nn e v e re x h i b i tB e l a d y  s anomaly. A stack algorithm is an algorithm for which it can be shown that the set of pages in memory for n frames is always a subset of the set of pages that would be in memory with n 1f r a m e s .F o r

that would be in memory with n 1f r a m e s .F o r LRU replacement, the set of pages in memory would be the n most recently referenced pages. If the number of frames is increased, these n pages will still be the most recently referenced and so will still be in memory. Note that neither implementation of LRU would be conceivable without hardware assistance beyond the standard TLB registers. The updating of the clock elds or stack must be done for every memory reference. If we were to use an

for every memory reference. If we were to use an interrupt for every reference to allow software to update such data structures, it would slow every memory reference by a factor of at least ten, 2 1 0 47 stack before a7 2 1 40 stack after breference string 4707101212 2 7 ab1 Figure 9.16 Use of a stack to record the most recent page references.418 Chapter 9 Virtual Memory hence slowing every user process by a factor of ten. Few systems could tolerate that level of overhead for memory management.

that level of overhead for memory management. 9.4.5 LRUApproximation Page Replacement Few computer systems provide sufcient hardware support for true LRU page replacement. In fact, some systems provide no hardware support, and other pagereplacement algorithms (such as a FIFO algorithm) must be used. Many systems provide some help, however, in the form of a reference bit .T h e reference bit for a page is set by the hardware whenever that page is referenced (either a read or a write to any byte

referenced (either a read or a write to any byte in the page). Reference bits are associated with each entry in the page table. Initially, all bits are cleared (to 0) by the operating system. As a user process executes, the bit associated with each page referenced is set (to 1) by the hardware. After some time, we can determine which pages have been used and which have not been used by examining the reference bits, although we do not know the order of use. This information is the basis for many

of use. This information is the basis for many pagereplacement algorithms that approximate LRU replacement. 9.4.5.1 AdditionalReferenceBits Algorithm We can gain additional ordering information by recording the reference bits at regular intervals. We can keep an 8bit byte for each page in a table in memory. At regular intervals (say, every 100 milliseconds), a timer interrupt transfers control to the operating system. The operating system shifts the reference bit for each page into the highorder

the reference bit for each page into the highorder bit of its 8bit byte, shifting the other bits right by 1 bit and discarding the loworder bit. These 8bit shift registers contain the history of page use for the last eight time periods. If the shift register contains 00000000, for example, then the page has not been used for eight time periods. A page that is used at least once in each period has a shift register value of 11111111. A page with a history register value of 11000100 has been used

a history register value of 11000100 has been used more recently than one with a value of 01110111. If we interpret these 8bit bytes as unsigned integers, the page with the lowest number is the LRU page, and it can be replaced. Notice that the numbers are not guaranteed to be unique, however. We can either replace (swap out) all pages with the smallest value or use the FIFO method to choose among them. The number of bits of history included in the shift register can be varied, of course, and is

shift register can be varied, of course, and is selected (depending on the hardware available) to make the updating as fast as possible. In the extreme case, the number can be reduced to zero, leaving only the reference bit itself. This algorithm is called thesecondchance pagereplacement algorithm . 9.4.5.2 SecondChance Algorithm The basic algorithm of secondchance replacement is a FIFO replacement algorithm. When a page has been selected, however, we inspect its reference bit. If the value is

we inspect its reference bit. If the value is 0, we proceed to replace this page; but if the reference bit is set to 1, we give the page a second chance and move on to select the next FIFO page. When a page gets a second chance, its reference bit is cleared, and its arrival time is reset to the current time. Thus, a page that is given a second chance will not be replaced until all other pages have been replaced (or given9.4 Page Replacement 419 circular queue of pages (a)next victim0reference

circular queue of pages (a)next victim0reference bitspages 0 1 1 0 1 1  circular queue of pages (b)0reference bitspages 0 0 0 0 1 1  Figure 9.17 Secondchance (clock) pagereplacement algorithm. second chances). In addition, if a page is used often enough to keep its reference bit set, it will never be replaced. One way to implement the secondchance algorithm (sometimes referred to as the clock algorithm) is as a circular queue. A pointer (that is, a hand on the clock) indicates which page is to

a hand on the clock) indicates which page is to be replaced next. When a frame is needed, the pointer advances until it nds a page with a 0 reference bit. As it advances, it clears the reference bits (Figure 9.17). Once a victim page is found, the page is replaced, and the new page is inserted in the circular queue in that position. Notice that, in the worst case, when all bits are set, the pointer cycles through the whole queue, giving each page a second c hance. It clears all the reference

page a second c hance. It clears all the reference bits before selecting the next page for replacement. Secondchance replacement degenerates to FIFO replacement if all bits are set. 9.4.5.3 Enhanced SecondChance Algorithm We can enhance the secondchance algorithm by considering the reference bit and the modify bit (described in Section 9.4.1) as an ordered pair. With these two bits, we have the following four possible classes: 1.(0, 0) neither recently used nor modiedbest page to replace 2.(0,

recently used nor modiedbest page to replace 2.(0, 1) not recently used but modiednot quite as good, because the page will need to be written out before replacement420 Chapter 9 Virtual Memory 3.(1, 0) recently used but cleanprobably will be used again soon 4.(1, 1) recently used and modiedprobably will be used again soon, and the page will be need to be written out to disk before it can be replaced Each page is in one of these four classes. When page replacement is called for, we use the same

page replacement is called for, we use the same scheme as in the clock algorithm; but instead of examining whether the page to which we are pointing has the reference bit set to 1, we examine the class to which that page belongs. We replace the rst page encountered in the lowest nonempty class. Notice that we may have to scan the circular queue several times before we nd a page to be replaced. The major difference between th is algorithm and the simpler clock algo rithm is that here we give

the simpler clock algo rithm is that here we give preference to those pages that have been modied in order to reduce the number of IOs required. 9.4.6 CountingBased Page Replacement There are many other algorithms that can be used for page replacement. For example, we can keep a counter of the number of references that have been made to each page and develop th ef o l l o w i n gt w os c h e m e s . The least frequently used (LFU)pagereplacement algorithm requires that the page with the smallest

algorithm requires that the page with the smallest count be replaced. The reason for this selection is that an actively used page should have a large reference count. A problem arises, however, when a page is used heavily during the initial phase of ap r o c e s sb u tt h e ni sn e v e ru s e da g a i n .S i n c ei tw a su s e dh e a v i l y ,i th a sa large count and remains in memory even though it is no longer needed. One solution is to shift the counts right by 1 bit at regular intervals,

the counts right by 1 bit at regular intervals, forming an exponentially decaying average usage count. The most frequently used (MFU )pagereplacement algorithm is based on the argument that the page with the smallest count was probably just brought in and has yet to be used. As you might expect, neither MFU nor LFU replacement is common. The implementation of these algorithms is expensive, and they do not approximate OPT replacement well. 9.4.7 PageBuffering Algorithms Other procedures are often

Algorithms Other procedures are often used in addi tion to a specic pagereplacement algorithm. For example, systems commonly keep a pool of free frames. When a page fault occurs, a victim frame is chosen as before. However, the desired page is read into a free frame from the pool before the victim is written out. This procedure allows the process to restart as soon as possible, without waiting for the victim page to be written out. When the victim is later written out, its frame is added to the

is later written out, its frame is added to the freeframe pool. An expansion of this idea is to main tain a list of modied pages. Whenever the paging device is idle, a modied page is selected and is written to the disk. Its modify bit is then reset. This scheme increases the probability that a page will be clean when it is selected for replacement and will not need to be written out.9.5 Allocation of Frames 421 Another modication is to keep a pool of free frames but to remember which page was in

of free frames but to remember which page was in each frame. Since the frame contents are not modied when af r a m ei sw r i t t e nt ot h ed i s k ,t h eo l dp a g ec a nb er e u s e dd i r e c t l yf r o mt h e freeframe pool if it is needed before that frame is reused. No IOis needed in this case. When a page fault occurs, we rst check whether the desired page is in the freeframe pool. If it is not, we must select a free frame and read into it. This technique is used in the VAXVMS system

it. This technique is used in the VAXVMS system along with a FIFO replace ment algorithm. When the FIFO replacement algorithm mistakenly replaces a page that is still in active use, that page is quickly retrieved from the freeframe pool, and no IOis necessary. The freeframe buffer provides protection against the relatively poor, but simple, FIFO replacement algorithm. This method is necessary because the early versions of VAX did not implement the reference bit correctly. Some versions of the

the reference bit correctly. Some versions of the UNIX system use this method in conjunction with the secondchance algorithm. It can be a useful augmentation to any page replacement algorithm, to reduce the penalty incurred if the wrong victim page is selected. 9.4.8 Applications and Page Replacement In certain cases, applications accessing data through the operating systems virtual memory perform worse than if the operating system provided no buffering at all. A typical example is a database,

buffering at all. A typical example is a database, which provides its own memory management and IO buffering. Applications like this understand their memory use and disk use better than does an operating system that is implementing algorithms for generalpurpose use. If the operating system is buffering IOand the application is doing so as well, however, then twice the memory is being used for a set of IO. In another example, data warehouses frequently perform massive sequen tial disk reads,

frequently perform massive sequen tial disk reads, followed by computations and writes. The LRU algorithm would be removing old pages and preserving new ones, while the application would more likely be reading older pages than newer ones (as it starts its sequential reads again). Here, MFU would actually be more efcient than LRU. Because of such problems, some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks, without any

sequential array of logical blocks, without any lesystem data structures. This array is sometimes called the raw disk ,a n d IOto this array is termed raw IO.R a w IObypasses all the le system services, such as le IOdemand paging, le locking, prefetching, space allocation, le names, and directories. Note that although certain applications are more efcient when implementing their own specialpurpose storage services on a raw partition, most applications perform better when they use the regular

perform better when they use the regular lesystem services. 9.5 Allocation of Frames We turn next to the issue of allocation. How do we allocate the xed amount of free memory among the various processes? If we have 93 free frames and two processes, how many frames does each process get? The simplest case is the singleuser system. Consider a singleuser system with 128 KBof memory composed of pages 1 KBin size. This system has 128422 Chapter 9 Virtual Memory frames. The operating system may take

Memory frames. The operating system may take 35 KB,l e a v i n g9 3f r a m e sf o rt h eu s e r process. Under pure demand paging, all 93 frames would initially be put on the freeframe list. When a user process started execution, it would generate a sequence of page faults. The rst 93 page faults would all get free frames from the freeframe list. When the freeframe list was exhausted, a pagereplacement algorithm would be used to select one of the 93 inmemory pages to be replaced with the 94th,

93 inmemory pages to be replaced with the 94th, and so on. When the process terminated, the 93 frames would once again be placed on the freeframe list. There are many variations on this simple strategy. We can require that the operating system allocate all its buffer and table space from the freeframe list. When this space is not in use by the operating system, it can be used to support user paging. We can try to keep three free frames reserved on the freeframe list at all times. Thus, when a

on the freeframe list at all times. Thus, when a page fault occurs, there is a free frame available to page into. While the page swap is taking place, a replacement can be selected, which is then written to the disk as the user process continues to execute. Other variants are also possible, but the basic strategy is clear: the user process is allocated any free frame. 9.5.1 Minimum Number of Frames Our strategies for the allocation of frames are constrained in various ways. We cannot, for

are constrained in various ways. We cannot, for example, allocate more than the total number of available frames (unless there is page sharing). We must also allocate at least a minimum number of frames. Here, we look more closely at the latter requirement. One reason for allocating at least a minimum number of frames involves performance. Obviously, as the number of frames allocated to each process decreases, the pagefault rate increases, slowin gp r o c e s se x e c u t i o n .I na d d i t i o

gp r o c e s se x e c u t i o n .I na d d i t i o n , remember that, when a page fault occurs before an executing instruction is complete, the instruction must be restarted. Consequently, we must have enough frames to hold all the different pages that any single instruction can reference. For example, consider a machine in which all memoryreference instruc tions may reference only one memory address. In this case, we need at least one frame for the instruction and one frame for the memory

for the instruction and one frame for the memory reference. In addition, if onelevel indirect addressing is allowed (for example, a load instruction on page 16 can refer to an address on page 0, which is an indirect reference to page 23), then paging requires at least three frames per process. Think about what might happen if a process had only two frames. The minimum number of frames is dened by the computer architecture. For example, the move instruction for the PDP11 includes more than one

instruction for the PDP11 includes more than one word for some addressing modes, and thus the instruction itself may straddle two pages. In addition, each of its two operands may be indirect references, for a total of six frames. Another example is the IBM 370 MVC instruction. Since the instruction is from storage location to storage location, it takes 6 bytes and can straddle two pages. The block of characters to move and the area to which it is to be moved can each also straddle two pages.

is to be moved can each also straddle two pages. This situation would require six frames. The worst case occurs when the MVC instruction is the operand of anEXECUTE instruction that straddles a page boundary; in this case, we need eight frames.9.5 Allocation of Frames 423 The worstcase scenario occurs in computer architectures that allow multiple levels of indirection (for example, each 16bit word could contain a1 5  b i ta d d r e s sp l u sa1  b i ti n d i r e c ti n d i c a t o r ) .T h e o r

b i ti n d i r e c ti n d i c a t o r ) .T h e o r e t i c a l l y ,as i m p l el o a d instruction could reference an indirect address that could reference an indirect address (on another page) that could also reference an indirect address (on yet another page), and so on, until every page in virtual memory had been touched. Thus, in the worst case, the entire virtual memory must be in physical memory. To overcome this difculty, we must plac eal i m i to nt h el e v e l so fi n d i r e c t i o

i m i to nt h el e v e l so fi n d i r e c t i o n( f o r example, limit an instruction to at most 16 l evels of indirection). When the rst indirection occurs, a counter is set to 16; the counter is then decremented for each successive indirection for this instruction. If the counter is decremented to 0, a trap occurs (excessive indirection). This limitation reduces the maximum number of memory references per instruction to 17, requiring the same number of frames. Whereas the minimum number of

number of frames. Whereas the minimum number of frames per process is dened by the architecture, the maximum number is dened by the amount of available physical memory. In between, we are still left with signicant choice in frame allocation. 9.5.2 Allocation Algorithms The easiest way to split mframes among nprocesses is to give everyone an equal share, mnframes (ignoring frames needed by the operating system for the moment). For instance, if there are 93 frames and ve processes, each process

there are 93 frames and ve processes, each process will get 18 frames. The three leftover frames can be used as a freeframe buffer pool. This scheme is called equal allocation . An alternative is to recognize that various processes will need differing amounts of memory. Consider a system with a 1 KBframe size. If a small student process of 10 KBand an interactive database of 127 KBare the only two processes running in a system with 62 free frames, it does not make much sense to give each process

it does not make much sense to give each process 31 frames. The student process does not need more than 10 frames, so the other 21 are, strictly speaking, wasted. To solve this problem, we can use proportional allocation ,i nw h i c hw e allocate available memory to each process according to its size. Let the size of the virtual memory for process pibesi,a n dd e  n e Ssummationtextsi. Then, if the total number of available frames is m,we allocate aiframes to process pi,w h e r e aiis

allocate aiframes to process pi,w h e r e aiis approximately aisiSm. Of course, we must adjust each aito be an integer that is greater than the minimum number of frames required by the instruction set, with a sum not exceeding m. With proportional allocation, we would split 62 frames between two processes, one of 10 pages and one of 127 pages, by allocating 4 frames and 57 frames, respectively, since 10137 624, and 127137 6257.424 Chapter 9 Virtual Memory In this way, both processes share the

Memory In this way, both processes share the available frames according to their needs, rather than equally. In both equal and proportional allocation, of course, the allocation may vary according to the multiprogramming level. If the multiprogramming level is increased, each process will lose some frames to provide the memory needed for the new process. Conversely, if t he multiprogramming level decreases, the frames that were allocated to the departed process can be spread over the remaining

departed process can be spread over the remaining processes. Notice that, with either equal or proportional allocation, a highpriority process is treated the same as a lowpriority process. By its denition, however, we may want to give the highpriority process more memory to speed its execution, to the detriment of lowpriority processes. One solution is to use a proportional allocation scheme wher ein the ratio of frames depends not on the relative sizes of processes but rather on the priorities

sizes of processes but rather on the priorities of processes or on a combination of size and priority. 9.5.3 Global versus Local Allocation Another important factor in the way frames are allocated to the various processes is page replacement. With multiple processes competing for frames, we can classify pagereplacement algorithms into two broad categories: global replacement and local replacement . Global replacement allows a process to select a replacement frame from the set of all frames, even

replacement frame from the set of all frames, even if that frame is currently allocated to some other process; that is, one process can take a frame from another. Local replacement requir es that each process select from only its own set of allocated frames. For example, consider an allocation scheme wherein we allow highpriority processes to select frames from lowpriority processes for replacement. A process can select a replacement from among its own frames or the frames of any lowerpriority

its own frames or the frames of any lowerpriority process. This approach allows a highpriority process to increase its frame allocation at the expense of a lowpriority process. With a local replacement strategy, the number of frames allocated to a process does not change. With global replacement, a p rocess may happen to select only frames allocated to other processes, thus increasing the number of frames allocated to it (assuming that other processes do not choose itsframes for replacement).

do not choose itsframes for replacement). One problem with a global replacement algorithm is that a process cannot control its own pagefault rate. The set of pages in memory for a process depends not only on the paging behavior of that process but also on the paging behavior of other processes. Therefore, the same process may perform quite differently (for example, taking 0.5 seconds for one execution and 10.3 seconds for the next execution) because of totally external circumstances. Such is not

of totally external circumstances. Such is not the case with a local replacement algorithm. Under local replacement, the set of pages in memory for a process is affected by the paging behavior of only that process. Local replacement might hinder a process, however, by not making available to it other, less used pages of memory. Thus, global replacement generally results in greater system throughput and is therefore the more commonly used method. 9.5.4 NonUniform Memory Access Thus far in our

9.5.4 NonUniform Memory Access Thus far in our coverage of virtual memory, we have assumed that all main memory is created equalor at least that it is accessed equally. On many9.6 Thrashing 425 computer systems, that is not the case. Often, in systems with multiple CPUs (Section 1.3.2), a given CPU can access some sections of main memory faster than it can access others. These performance differences are caused by how CPUsa n dm e m o r ya r ei n t e r c o n n e c t e di nt h es y s t e m .F r e

t e r c o n n e c t e di nt h es y s t e m .F r e q u e n t l y ,s u c has y s t e m is made up of several system boards, each containing multiple CPUsa n ds o m e memory. The system boards are interconnected in various ways, ranging from system buses to highspeed network connections like InniBand. As you might expect, the CPUso nap a r t i c u l a rb o a r dc a na c c e s st h em e m o r yo nt h a tb o a r dw i t h less delay than they can access memory on other boards in the system. Systems in

memory on other boards in the system. Systems in which memory access times vary signicantly are known collectively as nonuniform memory access (NUMA )systems, and without exception, they are slower than systems in which memory and CPUsa r el o c a t e do nt h es a m e motherboard. Managing which page frames are stored at which locations can signicantly affect performance in NUMA systems. If we treat memory as uniform in such as y s t e m , CPUsm a yw a i ts i g n i  c a n t l yl o n g e rf o rm

a yw a i ts i g n i  c a n t l yl o n g e rf o rm e m o r ya c c e s st h a ni fw e modify memory allocation algorithms to take NUMA into account. Similar changes must be made to the scheduling system. The goal of these changes is to have memory frames allocated as close as possible to the CPU on which the process is running. The denition of close iswith minimum latency,  which typically means on the same system board as the CPU. The algorithmic changes consist of having the scheduler track the

changes consist of having the scheduler track the last CPU on which each process ran. If the schedule rt r i e st os c h e d u l ee a c hp r o c e s so n t o its previous CPU,a n dt h em e m o r y  m a n a g e m e n ts y s t e mt r i e st oa l l o c a t ef r a m e s for the process close to the CPU on which it is being scheduled, then improved cache hits and decreased memory access times will result. The picture is more complicated once threads are added. For example, a process with many running

added. For example, a process with many running threads may end up with those threads scheduled on many different system boards. How is the memory to be allocated in this case? Solaris solves the problem by creating lgroups (for latency groups )i n the kernel. Each lgroup gathers together close CPUsa n dm e m o r y .I nf a c t ,t h e r e is a hierarchy of lgroups based on the amount of latency between the groups. Solaris tries to schedule all threads of a process and allocate all memory of a

threads of a process and allocate all memory of a process within an lgroup. If that is not possible, it picks nearby lgroups for the rest of the resources needed. This practice minimizes overall memory latency and maximizes CPU cache hit rates. 9.6 Thrashing If the number of frames allocated to a lowpriority process falls below the minimum number required by the computer architecture, we must suspend that processs execution. We should then page out its remaining pages, freeing all its allocated

out its remaining pages, freeing all its allocated frames. This provision in troduces a swapin, swapout level of intermediate CPU scheduling. In fact, look at any process that does not have enough frames. If the process does not have the number of frames it needs to support pages in active use, it will quickly pagefault. At this point, it must replace some page. However, since all its pages are in active use, it must replace a page that will be needed again right away. Consequently, it quickly

needed again right away. Consequently, it quickly faults again, and again, and again, replacing pages that it must bring back in immediately.426 Chapter 9 Virtual Memory This high paging activity is called thrashing .Ap r o c e s si st h r a s h i n gi fi ti s spending more time paging than executing. 9.6.1 Cause of Thrashing Thrashing results in severe performance pr oblems. Consider the following scenario, which is based on the actual behavior of early paging systems. The operating system

of early paging systems. The operating system monitors CPU utilization. If CPU utilization is too low, we increase the degree of multiprogramming by introducing a new process to the system. A global pagereplacement algorithm is used; it replaces pages without regard to the process to which they belong. Now suppose that a process enters a new phase in its execution and needs more frames. It starts faulting and taking frames away from other processes. These processes need those pages, however, and

These processes need those pages, however, and so they also fault, taking frames from other processes. These faulting processes must use the paging device to swap pages in and out. As they queue up for the paging device, the ready queue empties. As processes wait for the paging device, CPU utilization decreases. The CPU scheduler sees the decreasing CPU utilization and increases the degree of multiprogramming as a result. The new process tries to get started by taking frames from running

tries to get started by taking frames from running processes, causing more page faults and a longer queue for the paging device. As a result, CPU utilization drops even further, and the CPU scheduler tries to increase the degree of multiprogramming even more. Thrashing has occurred, and system throughput plunges. The page fault rate increases tremendously. As a result, the effective memoryaccess time increases. No work is getting done, because the processes are spending all their time paging.

the processes are spending all their time paging. This phenomenon is illustrated in Figure 9.18, in which CPU utilization is plotted against the degree of multiprogramming. As the degree of multi programming increases, CPU utilization also increases, although more slowly, until a maximum is reached. If the degree of multiprogramming is increased even further, thrashing sets in, and CPU utilization drops sharply. At this point, to increase CPU utilization and stop thrashing, we must decrease the

and stop thrashing, we must decrease the degree of multiprogramming. thrashing degree of multiprogrammingCPU utilization Figure 9.18 Thrashing.9.6 Thrashing 427 We can limit the effects of thrashing by using a local replacement algorithm (orpriority replacement algorithm ). With local replacement, if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash as well. However, the problem is not entirely solved. If processes are thrashing, they will

solved. If processes are thrashing, they will be in the queue for the paging device most of the time. The average service time for a page fault will increase because of the longer average queue for the paging device. Thus, the effective access time will increase even for a process that is not thrashing. To prevent thrashing, we must provide a process with as many frames as it needs. But how do we know how many frames it needs ?T h e r ea r es e v e r a l techniques. The workingset strategy

es e v e r a l techniques. The workingset strategy (Section 9.6.2) starts by looking at how many frames a process is actually using. This approach denes the locality model of process execution. The locality model states that, as a process executes, it moves from locality to locality. A locality is a set of pages that are actively used together (Figure 9.19). A program is generally composed of several different localities, which may overlap. For example, when a function is called, it denes a new

example, when a function is called, it denes a new locality. In this locality, memory references are made to the instructions of the function call, its local variables, and a subset of the global variables. When we exit the function, the process leaves this locality, since the local variables and instructions of the function are no longer in active use. We may return to this locality later. Thus, we see that localities are dened by the program structure and its data structures. The locality

structure and its data structures. The locality model states that all programs will exhibit this basic memory reference structure. Note that the locality model is the unstated principle behind the caching discussions so far in this book. If accesses to any types of data were random rather than patterned, caching would be useless. Suppose we allocate enough frames to a process to accommodate its current locality. It will fault for the pages in its locality until all these pages are in memory;

its locality until all these pages are in memory; then, it will not fault again until it changes localities. If we do not allocate enough frames to accommodate the size of the current locality, the process will thrash, since it cannot keep in memory all the pages that it is actively using. 9.6.2 WorkingSet Model As mentioned, the workingset model is based on the assumption of locality. This model uses a parameter, Delta1,t od e  n et h e workingset window .T h ei d e a is to examine the most

window .T h ei d e a is to examine the most recent Delta1page references. The set of pages in the most recent Delta1page references is the working set (Figure 9.20). If a page is in active use, it will be in the working set. If it is no longer being used, it will drop from the working set Delta1time units after its last reference. Thus, the working set is an approximation of the programs locality. For example, given the sequence of memory references shown in Figure 9.20, if Delta11 0m e m o r yr

shown in Figure 9.20, if Delta11 0m e m o r yr e f e r e n c e s ,t h e nt h ew o r k i n gs e ta tt i m e t1is1, 2, 5, 6, 7. By time t2,t h ew o r k i n gs e th a sc h a n g e dt o 3, 4. The accuracy of the working set depends on the selection of Delta1.I fDelta1is too small, it will not encompass the entire locality; if Delta1is too large, it may overlap several localities. In the extreme, if Delta1is innite, the working set is the set of pages touched during the process execution.428 Chapter

touched during the process execution.428 Chapter 9 Virtual Memory 182022242628303234page numbers memory address execution time Figure 9.19 Locality in a memoryreference pattern. The most important property of the working set, then, is its size. If we compute the workingset size, WSS i,f o re a c hp r o c e s si nt h es y s t e m ,w ec a n then consider that DsummationtextWSS i, where Dis the total demand for frames. Each process is actively using the pages in its working set. Thus, process

using the pages in its working set. Thus, process ineeds WSS iframes. If the total demand is greater than the total number of available frames ( Dm), thrashing will occur, because some processes will not have enough frames. Once Delta1has been selected, use of the workingset model is simple. The operating system monitors the working set of each process and allocates to9.6 Thrashing 429 page reference table . . . 2 6 1 5 7 7 7 7 5 1 6 2 3 4 1 2 3 4 4 4 3 4 3 4 4 4 1 3 2 3 4 4 4 3 4 4 4 . . .  t1

4 4 4 3 4 3 4 4 4 1 3 2 3 4 4 4 3 4 4 4 . . .  t1 WS(t1)  1,2,5,6,7 t2 WS(t2)  3,4 Figure 9.20 Workingset model. that working set enough frames to provide it with its workingset size. If there are enough extra frames, another process can be initiated. If the sum of the workingset sizes increases, exceeding t he total number of available frames, the operating system selects a process to suspend. The processs pages are written out (swapped), and its frames are reallocated to other processes. The

its frames are reallocated to other processes. The suspended process can be restarted later. This workingset strategy prevents th rashing while keeping the degree of multiprogramming as high as possible. Thus, it optimizes CPU utilization. The difculty with the workingset model is keeping track of the working set. The workingset window is a moving window. At each memory reference, a new reference appears at one end, and the oldest reference drops off the other end. Ap a g ei si nt h ew o r k i n

off the other end. Ap a g ei si nt h ew o r k i n gs e ti fi ti sr e f e r e n c e da n y w h e r ei nt h ew o r k i n g  s e t window. We can approximate the workingset model with a xedinterval timer interrupt and a reference bit. For example, assume that Delta1equals 10,000 references and that we can cause a timer interrupt every 5,000 references. When we get a timer interrupt, we copy and clear the referencebit values for each page. Thus, if a page fault occurs, we can examine the current

if a page fault occurs, we can examine the current reference bit and two inmemory bits to determine whether a page was used within the last 10,000 to 15,000 references. If it was used, at least one of these bits will be on. If it has not been used, these bits will be off. Pages with at least one bit on will be considered to be in the working set. Note that this arrangement is not entirely accurate, because we cannot tell where, within an interval of 5,000, a reference occurred. We can reduce the

of 5,000, a reference occurred. We can reduce the uncertainty by increasing the number of history bits and the frequency of inter rupts (for example, 10 bits and interrupts every 1,000 references). However, the cost to service these more frequent interrupts will be correspondingly higher. 9.6.3 PageFault Frequency The workingset model is successful, and knowledge of the working set can be useful for prepaging (Section 9.9.1), but it seems a clumsy way to control thrashing. A strategy that uses

way to control thrashing. A strategy that uses the pagefault frequency (PFF)takes a more direct approach. The specic problem is how to preven tt h r a s h i n g .T h r a s h i n gh a sah i g h pagefault rate. Thus, we want to control the pagefault rate. When it is too high, we know that the process needs more frames. Conversely, if the pagefault rate is too low, then the process may have too many frames. We can establish upper and lower bounds on the desired pagefault rate (Figure 9.21). If the

the desired pagefault rate (Figure 9.21). If the actual pagefault rate exceeds the upper limit, we allocate the process another430 Chapter 9 Virtual Memory number of framesincrease number of frames upper bound lower bound decrease number of framespagefault rate Figure 9.21 Pagefault frequency. frame. If the pagefault rate falls below the lower limit, we remove a frame from the process. Thus, we can directly measure and control the pagefault rate to prevent thrashing. As with the workingset

rate to prevent thrashing. As with the workingset strategy, we may have to swap out a process. If the pagefault rate increases and no free frames are available, we must select some process and swap it out to backing store. The freed frames are then distributed to processes with high pagefault rates. 9.6.4 Concluding Remarks Practically speaking, thrashing and the resulting swapping have a disagreeably large impact on performance. The current best practice in implementing a computer facility is

practice in implementing a computer facility is to include enough physical memory, whenever possible, to avoid thrashing and swapping. From smartphones through mainframes, providing enough memory to keep all working sets in memory concurrently, except under extreme conditions, gives the best user experience. 9.7 MemoryMapped Files Consider a sequential read of a le on disk using the standard system calls open() ,read() ,a n d write() .E a c h l ea c c e s sr e q u i r e sas y s t e mc a l la n

ea c c e s sr e q u i r e sas y s t e mc a l la n dd i s k access. Alternatively, we can use the virtual memory techniques discussed so far to treat le IOas routine memory accesses. This approach, known as memory mapping a l e ,a l l o w sap a r to ft h ev i r t u a la d d r e s ss p a c et ob el o g i c a l l y associated with the le. As we shall see, this can lead to signicant performance increases. 9.7.1 Basic Mechanism Memory mapping a le is accomplished by mapping a disk block to a page (or

accomplished by mapping a disk block to a page (or pages) in memory. Initial access to the le proceeds through ordinary demand paging, resulting in a page fault. However, a pagesized portion of the le is read from the le system into a physical page (some systems may opt to read9.7 MemoryMapped Files 431 WORKING SETS AND PAGEFAULT RATES There is a direct relationship between the working set of a process and its pagefault rate. Typically, as shown in Figure 9.20, the working set of a process

shown in Figure 9.20, the working set of a process changes over time as references to data and code sections move from one locality to another. Assuming there is sufcient memory to store the working set of a process (that is, the process is not thrashing), the pagefault rate of the process will transition between peaks and valleys over time. This general behavior is shown below:1 0 timeworking set page fault rate Ap e a ki nt h ep a g e  f a u l tr a t eo c c u r sw h e nw eb e g i nd e m a n d

tr a t eo c c u r sw h e nw eb e g i nd e m a n d  p a g i n gan e w locality. However, once the working set of this new locality is in memory, the pagefault rate falls. When the process moves to a new working set, the pagefault rate rises toward a peak once again, returning to a lower rate once the new working set is loaded into memory. The span of time between the start of one peak and the start of the next peak represents the transition from one working set to another. in more than a

from one working set to another. in more than a pagesized chunk of memory at a time). Subsequent reads and writes to the le are handled as routine memory accesses. Manipulating les through memory rather than incurring the overhead of using the read() and write() system calls simplies and speeds up le access and usage. Note that writes to the le mapped in memory are not necessarily immediate (synchronous) writes to the le on disk. Some systems may choose to update the physical le when the

may choose to update the physical le when the operating system periodically checks whether the page in memory has been modied. When the le is closed, all the memorymapped data are written back to disk and removed from the virtual memory of the process. Some operating systems provide memory mapping only through a specic system call and use the standard system calls to perform all other le IO. However, some systems choose to memorymap a le regardless of whether the le was specied as memorymapped.

of whether the le was specied as memorymapped. Lets take Solaris as an example. If a l ei ss p e c i  e da sm e m o r y  m a p p e d( u s i n gt h e mmap() system call), Solaris maps the le into the address space of the process. If a le is opened and accessed using ordinary system calls, such as open() ,read() ,a n d write() ,432 Chapter 9 Virtual Memory process A virtual memory1 1 1234562 33 4 5 5 4 2661 2 3 4 5 6 process B virtual memory physical memory disk file Figure 9.22 Memorymapped les.

memory disk file Figure 9.22 Memorymapped les. Solaris still memorymaps the le; however, the le is mapped to the kernel address space. Regardless of how the le is opened, then, Solaris treats all le IOas memorymapped, allowing le access to take place via the efcient memory subsystem. Multiple processes may be allowed to map the same le concurrently, to allow sharing of data. Writes by any of the processes modify the data in virtual memory and can be seen by all others that map the same section

be seen by all others that map the same section of the le. Given our earlier discussions of virtual memory, it should be clear how the sharing of memorymapped sections of memory is implemented: the virtual memory map of each sharing process points to the same page of physical memorythe page that holds a copy of the disk block. This memory sharing is illustrated in Figure 9.22. The memorymapping system calls can also support copyonwrite functionality, allowing processes to share a le in readonly

allowing processes to share a le in readonly mode but to have their own copies of any data they modify. So that access to the shared data is coordinated, the processes involved might use one of the mechanisms for achieving mutual exclusion described in Chapter 5. Quite often, shared memory is in fact implemented by memory mapping les. Under this scenario, processes can communicate using shared memory by having the communicating processes memorymap the same le into their virtual address spaces.

the same le into their virtual address spaces. The memorymapped le serves as the region of shared memory between the communicating processes (Figure 9.23). We have already seen this in Section 3.4.1, where a POSIX shared memory object is created and each communicating process memorymaps the object into its address space. In the following section, we illustrate support in the Windows APIfor shared memory using memorymapped les.9.7 MemoryMapped Files 433process1 memorymapped file shared

Files 433process1 memorymapped file shared memoryshared memoryshared memoryprocess2 Figure 9.23 Shared memory using memorymapped IO. 9.7.2 Shared Memory in the Windows API The general outline for creating a region of shared memory using memory mapped les in the Windows APIinvolves rst creating a le mapping for the le to be mapped and then establishing a view of the mapped le in a processs virtual address space. A second process can then open and create a view of the mapped le in its virtual

and create a view of the mapped le in its virtual address space. The mapped le represents the sharedmemory object that will enable communication to take place between the processes. We next illustrate these steps in more detail. In this example, a producer process rst creates a sharedmemory object using the memorymapping features available in the Windows API.T h ep r o d u c e rt h e nw r i t e sam e s s a g e to shared memory. After that, a consumer process opens a mapping to the sharedmemory

process opens a mapping to the sharedmemory object and reads the message written by the consumer. To establish a memorymapped le, a process rst opens the le to be mapped with the CreateFile() function, which returns a HANDLE to the opened le. The process then creates a mapping of this le HANDLE using theCreateFileMapping() function. Once the le mapping is established, the process then establishes a view of the mapped le in its virtual address space with the MapViewOfFile() function. The view of

with the MapViewOfFile() function. The view of the mapped le represents the portion of the le being mapped in the virtual address space of the process the entire le or only a portion of it may be mapped. W e illustrate this sequence in the program shown in Figure 9.24. (We eliminate much of the error checking for code brevity.) The call to CreateFileMapping() creates a named sharedmemory object called SharedObject .T h ec o n s u m e rp r o c e s sw i l lc o m m u n i c a t eu s i n gt h i s

e s sw i l lc o m m u n i c a t eu s i n gt h i s sharedmemory segment by creating a m apping to the same named object. The producer then creates a view of the memorymapped le in its virtual address space. By passing the last three parameters the value 0, it indicates that the mapped view is the entire le. It could instead have passed values specifying an offset and size, thus creating a view containing only a subsection of the le. (It is important to note that the entire mapping may not be

to note that the entire mapping may not be loaded into memory when the mapping is established. Rather, the mapped le may be demandpaged, thus bringing pages into memory only as they are accessed.) TheMapViewOfFile() function returns a pointer to the sharedmemory object; any accesses to this memory location are thus accesses to the memorymapped434 Chapter 9 Virtual Memory include windows.h include stdio.h int main(int argc, char argv[])  HANDLE hFile, hMapFile; LPVOID lpMapAddress; hFile

hFile, hMapFile; LPVOID lpMapAddress; hFile  CreateFile(temp.txt,  file name  GENERIC READ  GENERIC WRITE,  readwrite access  0,  no sharing of the file  NULL,  default security  OPEN ALWAYS,  open new or existing file  FILE ATTRIBUTE NORMAL,  routine file attributes  NULL);  no file template  hMapFile  CreateFileMapping(hFile,  file handle  NULL,  default security  PAGE READWRITE,  readwrite access to mapped pages  0,  map entire file  0, TEXT(SharedObject));  named shared memory object

TEXT(SharedObject));  named shared memory object  lpMapAddress  MapViewOfFile(hMapFile,  mapped object handle  FILE MAP ALL ACCESS,  readwrite access  0,  mapped view of entire file  0, 0);  write to shared memory  sprintf(lpMapAddress,Shared memory message); UnmapViewOfFile(lpMapAddress); CloseHandle(hFile); CloseHandle(hMapFile);  Figure 9.24 Producer writing to shared memory using the Windows API. le. In this instance, the pr oducer process writes the message Shared memory message to shared

writes the message Shared memory message to shared memory. Ap r o g r a mi l l u s t r a t i n gh o wt h ec o n s u m e rp r o c e s se s t a b l i s h e sav i e wo f the named sharedmemory object is shown in Figure 9.25. This program is somewhat simpler than the one shown in Figure 9.24, as all that is necessary is for the process to create a mapping to the existing named sharedmemory object. The consumer process must a lso create a view of the mapped le, just as the producer process did in the

mapped le, just as the producer process did in the program in Figure 9.24. The consumer then reads from shared memory the message Shared memory message that was written by the producer process.9.7 MemoryMapped Files 435 include windows.h include stdio.h int main(int argc, char argv[])  HANDLE hMapFile; LPVOID lpMapAddress; hMapFile  OpenFileMapping(FILE MAP ALL ACCESS,  RW access  FALSE,  no inheritance  TEXT(SharedObject));  name of mapped file object  lpMapAddress  MapViewOfFile(hMapFile,

object  lpMapAddress  MapViewOfFile(hMapFile,  mapped object handle  FILE MAP ALL ACCESS,  readwrite access  0,  mapped view of entire file  0, 0);  read from shared memory  printf(Read message s, lpMapAddress); UnmapViewOfFile(lpMapAddress); CloseHandle(hMapFile);  Figure 9.25 Consumer reading from shared memory using the Windows API. Finally, both processes remove the view of the mapped le with a call to UnmapViewOfFile() .W ep r o v i d eap r o g r a m m i n ge x e r c i s ea tt h ee n do ft

o g r a m m i n ge x e r c i s ea tt h ee n do ft h i s chapter using shared memory with memory mapping in the Windows API. 9.7.3 MemoryMapped IO In the case of IO,a sm e n t i o n e di nS e c t i o n1 . 2 . 1 ,e a c h IOcontroller includes registers to hold commands and the data being transferred. Usually, special IO instructions allow data transfers between these registers and system memory. To allow more convenient access to IOdevices, many computer architectures provide memorymapped IO.I nt

architectures provide memorymapped IO.I nt h i sc a s e ,r a n g e so fm e m o r ya d d r e s s e sa r e set aside and are mapped to the device registers. Reads and writes to these memory addresses cause the data to be transferred to and from the device registers. This method is appropriate for devic es that have fast response times, such as video controllers. In the IBM PC ,e a c hl o c a t i o no nt h es c r e e ni sm a p p e d to a memory location. Displaying text on the screen is almost as

Displaying text on the screen is almost as easy as writing the text into the appropriate memorymapped locations. Memorymapped IOis also convenient for other devices, such as the serial and parallel ports used to connect modems and printers to a computer. The CPU transfers data through these kinds of devices by reading and writing a few device registers, called an IOport . To send out a long string of bytes through a memorymapped serial port, the CPU writes one data byte to the data register and

CPU writes one data byte to the data register and sets a bit in the control register to signal that the byte is available. The device436 Chapter 9 Virtual Memory takes the data byte and then clears the bit in the control register to signal that it is ready for the next byte. Then the CPU can transfer the next byte. If the CPU uses polling to watch the control bit, constantly looping to see whether the device is ready, this method of operation is called programmed IO(PIO). If the CPU does not

is called programmed IO(PIO). If the CPU does not poll the control bit, but instead receives an interrupt when the device is ready for the next byte, the data transfer is said to be interrupt driven . 9.8 Allocating Kernel Memory When a process running in user mode requests additional memory, pages are allocated from the list of free page frames maintained by the kernel. This list is typically populated using a pagereplacement algorithm such as those discussed in Section 9.4 and most likely

as those discussed in Section 9.4 and most likely contains free pages scattered throughout physical memory, as explained earlier. Remember, too, that if a user process requests a single byte of memory, internal fragmentation will result, as the process will be granted an entire page frame. Kernel memory is often allocated from a freememory pool different from the list used to satisfy ordinary usermode processes. There are two primary reasons for this: 1.The kernel requests memory for data

for this: 1.The kernel requests memory for data structures of varying sizes, some of which are less than a page in size. As a result, the kernel must use memory conservatively and attempt to minimize waste due to fragmentation. This is especially important because many operating systems do not subject kernel code or data to the paging system. 2.Pages allocated to usermode processes do not necessarily have to be in contiguous physical memory. However , certain hardware devices interact directly

, certain hardware devices interact directly with physical memorywithout the benet of a virtual memory interfaceand consequently may require memory residing in physically contiguous pages. In the following sections, we examine two strategies for managing free memory that is assigned to kernel processes: the buddy system and slab allocation. 9.8.1 Buddy System The buddy system allocates memory from a xedsize segment consisting of physically contiguous pages. Memory is allocated from this segment

pages. Memory is allocated from this segment using a powerof2 allocator ,w h i c hs a t i s  e sr e q u e s t si nu n i t ss i z e da sap o w e ro f2 (4KB,8KB,1 6 KB, and so forth). A request in units not appropriately sized is rounded up to the next highest power of 2. For example, a request for 11 KBis satised with a 16 KBsegment. Lets consider a simple example. Assume the size of a memory segment is initially 256 KBand the kernel requests 21 KBof memory. The segment is initially divided into

KBof memory. The segment is initially divided into two buddies which we will call ALand AReach 128 KBin size. One of these buddies is further divided into two 64 KBbuddies BLand BR.H o w e v e r ,t h en e x t  h i g h e s tp o w e ro f2f r o m2 1 KBis 32 KBso either BLorBRis again divided into two 32 KBbuddies, CLand CR. One of these9.8 Allocating Kernel Memory 437 physically contiguous pages 256 KB128 KB AL 64 KB BR64 KB BL 32 KB CL32 KB CR128 KB AR Figure 9.26 Buddy system allocation. buddies

KB AR Figure 9.26 Buddy system allocation. buddies is used to satisfy the 21 KBrequest. This scheme is illustrated in Figure 9.26, where CLis the segment allocated to the 21 KBrequest. An advantage of the buddy system is how quickly adjacent buddies can be combined to form larger segmen ts using a technique known as coalescing .I n Figure 9.26, for example, when the kernel releases the CLunit it was allocated, the system can coalesce CLand CRinto a 64 KBsegment. This segment, BL,c a n in turn be

a 64 KBsegment. This segment, BL,c a n in turn be coalesced with its buddy BRto form a 128 KBsegment. Ultimately, we can end up with the original 256 KBsegment. The obvious drawback to the buddy system is that rounding up to the next highest power of 2 is very likely to cause fragmentation within allocated segments. For example, a 33 KBrequest can only be satised with a 64 KBsegment. In fact, we cannot guarantee that less than 50 percent of the allocated unit will be wasted due to internal

the allocated unit will be wasted due to internal fragmentation. In the following section, we explore a memory allocation scheme where no space is lost due to fragmentation. 9.8.2 Slab Allocation As e c o n ds t r a t e g yf o ra l l o c a t i n gk e r n e lm e m o r yi sk n o w na s slab allocation .A slab is made up of one or more physically contiguous pages. A cache consists of one or more slabs. There is a single cache for each unique kernel data structure for example, a separate cache for

data structure for example, a separate cache for the data structure representing process descriptors, a separate cache for le objects, a separate cache for semaphores, and so forth. Each cache is populated with objects that are instantiations of the kernel data structure the cache represents. For example, the cache representing semaphores stores instances of semaphore o bjects, the cache representing process descriptors stores instances of process descriptor objects, and so forth. The

of process descriptor objects, and so forth. The relationship among slabs, caches, and objects is shown in Figure 9.27. The gure shows two kernel objects 3 KBin size and three objects 7 KBin size, each stored in a separate cache.438 Chapter 9 Virtual Memory 3KB objects 7KB objectskernel objects caches slabs physically contiguous pages Figure 9.27 Slab allocation. The slaballocation algorithm uses caches to store kernel objects. When a cache is created, a number of objectswhich are initially

is created, a number of objectswhich are initially marked as free are allocated to the cache. The number of objects in the cache depends on the size of the associated slab. For example, a 12 KBslab (made up of three continguous 4KBpages) could store six 2 KBobjects. Initially, all objects in the cache are marked as free. When a new object for a kernel data structure is needed, the allocator can assign any free object from the cache to satisfy the request. The object assigned from the cache is

the request. The object assigned from the cache is marked as used . Lets consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor. In Linux systems, ap r o c e s sd e s c r i p t o ri so ft h et y p e struct task struct ,w h i c hr e q u i r e s approximately 1.7 KBof memory. When the Linux kernel creates a new task, it requests the necessary memory for the struct task struct object from its cache. The cache will fulll the

object from its cache. The cache will fulll the request using a struct task struct object that has already been allocated in a slab and is marked as free. In Linux, a slab may be in one of three possible states: 1.Full.A l lo b j e c t si nt h es l a ba r em a r k e da su s e d . 2.Empty .A l lo b j e c t si nt h es l a ba r em a r k e da sf r e e . 3.Partial .T h es l a bc o n s i s t so fb o t hu s e da n df r e eo b j e c t s . The slab allocator rst attempts to satisfy the request with a

rst attempts to satisfy the request with a free object in a partial slab. If none exists, a free object is assigned from an empty slab. If no empty slabs are available, a new slab is allocated from contiguous physical pages and assigned to a cache; memory for the object is allocated from this slab. The slab allocator provides two main benets: 1.No memory is wasted due to fragmentation. Fragmentation is not an issue because each unique kernel data structure has an associated cache, and each cache

structure has an associated cache, and each cache is made up of one or more slabs that are divided into9.9 Other Considerations 439 chunks the size of the objects being represented. Thus, when the kernel requests memory for an object, the slab allocator returns the exact amount of memory required to represent the object. 2.Memory requests can be satised quickly. The slab allocation scheme is thus particularly effective for managing memory when objects are frequently allocated and deallocated, as

are frequently allocated and deallocated, as is often the case with requests from the kernel. The act of allocatingand releasingmemory can be at i m e  c o n s u m i n gp r o c e s s .H o w e v e r ,o bjects are created in advance and thus can be quickly allocated from the cache. Furthermore, when the kernel has nished with an object and releases it, it is marked as free and returned to its cache, thus makin gi ti m m e d i a t e l ya v a i l a b l ef o rs u b s e q u e n t requests from the

a b l ef o rs u b s e q u e n t requests from the kernel. The slab allocator rst appeared in the Solaris 2.4 kernel. Because of its generalpurpose nature, this allocator is now also used for certain usermode memory requests in Solaris. Linux originally used the buddy system; however, beginning with Version 2.2, the Linux kernel adopted the slab allocator. Recent distributions of Linux now includ et w oo t h e rk e r n e lm e m o r ya l l o  catorsthe SLOB and SLUB allocators. (Linux refers to

SLOB and SLUB allocators. (Linux refers to its slab implementation asSLAB .) The SLOB allocator is designed for systems with a limited amount of memory, such as embedded systems. SLOB (which stands for Simple List of Blocks) works by maintaining three lists of objects: small (for objects less than 256 bytes), medium (for objects less than 1,024 bytes), and large (for objects less than 1,024 bytes). Memory requests are allocated from an object on an appropriately sized list using a rstt policy.

an appropriately sized list using a rstt policy. Beginning with Version 2.6.24, the SLUB allocator replaced SLAB as the default allocator for the Linux kernel. SLUB addresses performance issues with slab allocation by reducing much of the overhead required by the SLAB allocator. One change is to move the metadata that is stored with each slab under SLAB allocation to the page structure the Linux kernel uses for each page. Additionally, SLUB removes the per CPU queues that the SLAB allocator

removes the per CPU queues that the SLAB allocator maintains for objects in each cache. For systems with a large number of processors, the amount of memory allocated to these queues was not insignicant. Thus, SLUB provides better performance as the number of processors on a system increases. 9.9 Other Considerations The major decisions that we make for a paging system are the selections of ar e p l a c e m e n ta l g o r i t h ma n da na l l o c a t i o np o l i c y ,w h i c hw ed i s c u s s e

a t i o np o l i c y ,w h i c hw ed i s c u s s e de a r l i e r in this chapter. There are many other considerations as well, and we discuss several of them here. 9.9.1 Prepaging An obvious property of pure demand paging is the large number of page faults that occur when a process is started. This situation results from trying to get the initial locality into memory. The same situation may arise at other times. For440 Chapter 9 Virtual Memory instance, when a swappedout process is restarted,

instance, when a swappedout process is restarted, all its pages are on the disk, and each must be brought in by its own page fault. Prepaging is an attempt to prevent this high level of initial paging. The strategy is to bring into memory at one time all the pages that will be needed. Some operating systemsnotably Solarisprepage the page frames for small les. In a system using the workingset model, for example, we could keep with each process a list of the pages in its working set. If we must

a list of the pages in its working set. If we must suspend a process (due to an IOwait or a lack of free frames), we remember the working set for that process. When the process is to be resumed (because IOhas nished or enough free frames have become available), we automatically bring back into memory its entire working set before restarting the process. Prepaging may offer an advantage in some cases. The question is simply whether the cost of using prepaging is less than the cost of servicing

using prepaging is less than the cost of servicing the corresponding page faults. It may well be the case that many of the pages brought back into memory by prepaging will not be used. Assume that spages are prepaged and a fraction H9251of these spages is actually used (0 H92511). The question is whether the cost of the sH9251saved page faults is greater or less than the cost of prepaging s( 1H9251)u n n e c e s s a r y pages. If H9251is close to 0, prepaging loses; if H9251is close to 1,

to 0, prepaging loses; if H9251is close to 1, prepaging wins. 9.9.2 Page Size The designers of an operating system for an existing machine seldom have a choice concerning the page size. However, when new machines are being designed, a decision regarding the best page size must be made. As you might expect, there is no single best page size. Rather, there is a set of factors that support various sizes. Page sizes are invariably powers of 2, generally ranging from 4,096 (212)t o4 , 1 9 4 , 3 0 4(

ranging from 4,096 (212)t o4 , 1 9 4 , 3 0 4( 222)b y t e s . How do we select a page size? One concern is the size of the page table. For a given virtual memory space, decreasing the page size increases the number of pages and hence the size of the page table. For a virtual memory of 4 MB (222), for example, there would be 4,096 pages of 1,024 bytes but only 512 pages of 8,192 bytes. Because each active process must have its own copy of the page table, a large page size is desirable. Memory is

table, a large page size is desirable. Memory is better utilized with smaller pages, however. If a process is allocated memory starting at location 00000 and continuing until it has as much as it needs, it probably will not end exactly on a page boundary. Thus, a part of the nal page must be allocated (because pages are the units of allocation) but will be unused (creating internal fragmentation). Assuming independence of process size and page size, we can expect that, on the average, half of

size, we can expect that, on the average, half of the nal page of each process will be wasted. This loss is only 256 bytes for a page of 512 bytes but is 4,096 bytes for a page of 8,192 bytes. To minimize internal fragmentation, then, we need a small page size. Another problem is the time required to read or write a page. IOtime is composed of seek, latency, and transfer times. Transfer time is proportional to the amount transferred (that is, the page size)a fact that would seem to argue for a

page size)a fact that would seem to argue for a small page size. However, as we shall see in Section 10.1.1, latency and seek time normally dwarf transfer time. At a transfer rate of 2 MBper second, it takes only 0.2 milliseconds to transfer 512 bytes. Latency time, though, is perhaps 8 milliseconds, and seek time 20 milliseconds. Of the total IOtime9.9 Other Considerations 441 (28.2 milliseconds), therefore, only 1 percent is attributable to the actual transfer. Doubling the page size increases

actual transfer. Doubling the page size increases IOtime to only 28.4 milliseconds. It takes 28.4 milliseconds to read a single page of 1,024 bytes but 56.4 milliseconds to read the same amount as two pages of 512 bytes each. Thus, a desire to minimize IOtime argues for a larger page size. With a smaller page size, though, total IOshould be reduced, since locality will be improved. A smaller page size allows each page to match program locality more accurately. For example, consider a process 200

accurately. For example, consider a process 200 KBin size, of which only half (100 KB) is actually used in an execution. If we have only one large page, we must bring in the entire page, a total of 200 KBtransferred and allocated. If instead we had pages of only 1 byte, then we could bring in only the 100 KBthat are actually used, resulting in only 100 KBtransferred and allocated. With a smaller page size, then, we have better resolution ,a l l o w i n g us to isolate only the memory that is

l o w i n g us to isolate only the memory that is actually needed. With a larger page size, we must allocate and transfer not only what is needed but also anything else that happens to be in the page, wheth er it is needed or not. Thus, a smaller page size should result in less IOand less total allocated memory. But did you notice that with a page size of 1 byte, we would have a page fault for each byte? A process of 200 KBthat used only half of that memory would generate only one page fault

of that memory would generate only one page fault with a page size of 200 KBbut 102,400 page faults with a page size of 1 byte. Each page fault generates the large amount of overhead needed for processing the interrupt, saving registers, replacing a page, queueing for the paging device, and updating tables. To minimize the number of page faults, we need to have a large page size. Other factors must be considered as well (such as the relationship between page size and sector size on the paging

between page size and sector size on the paging device). The problem has no best answer. As we have seen, some fac tors (internal fragmentation, locality) argue for a small page size, whereas others (table size, IOtime) argue for a large page size. Nevertheless, the historical trend is toward larger page sizes, even for mobile systems. Indeed, the rst edition of Operating System Concepts (1983) used 4,096 bytes as the upper bound on page sizes, and this value was the most common page size in

and this value was the most common page size in 1990. Modern systems may now use much larger page sizes, as we will see in the following section. 9.9.3 TLB Reach In Chapter 8, we introduced the hit ratio of the TLB.R e c a l lt h a tt h eh i tr a t i o for the TLB refers to the percentage of virtual address translations that are resolved in the TLB rather than the page table. Clearly, the hit ratio is related to the number of entries in the TLB,a n dt h ew a yt oi n c r e a s et h eh i tr a t i

n dt h ew a yt oi n c r e a s et h eh i tr a t i oi s by increasing the number of entries in the TLB. This, however, does not come cheaply, as the associative memory used to construct the TLBis both expensive and power hungry. Related to the hit ratio is a similar metric: the TLBreach .T h e TLBreach refers to the amount of memory accessible from the TLB and is simply the number of entries multiplied by the page size. Ideally, the working set for a process is stored in the TLB.I fi ti sn o t ,t

a process is stored in the TLB.I fi ti sn o t ,t h ep r o c e s sw i l ls p e n dac o n s i d e r a b l ea m o u n to f time resolving memory references in the page table rather than the TLB.I fw e double the number of entries in the TLB,w ed o u b l et h e TLB reach. However,442 Chapter 9 Virtual Memory for some memoryintensive applications, this may still prove insufcient for storing the working set. Another approach for increasing the TLB reach is to either increase the size of the page or

is to either increase the size of the page or provide multiple page sizes. If we increase the page size say , from 8 KBto 32 KBwe quadruple the TLB reach. However, this may lead to an increase in fragmentation for some applications that do not require such a large page size. Alternatively, an operating system may provide several different page sizes. For example, the Ultra SPARC supports page sizes of 8 KB, 64KB,5 1 2 KB,a n d4 MB.O ft h e s ea v a i l a b l ep a g e ss i z e s ,S o l a r i su s

a i l a b l ep a g e ss i z e s ,S o l a r i su s e sb o t h8  KB and 4 MBpage sizes. And with a 64entry TLB,t h e TLBreach for Solaris ranges from 512 KBwith 8 KBpages to 256 MBwith 4 MBpages. For the majority of applications, the 8 KBpage size is sufcient, although Solaris maps the rst 4 MB of kernel code and data with two 4 MBpages. Solaris also allows applications such as databasesto take advantage of the large 4 MBpage size. Providing support for multiple page sizes requires the operating

for multiple page sizes requires the operating system not hardwareto manage the TLB.F o re x a m p l e ,o n eo ft h e e l d si na TLB entry must indicate the size of the page frame corresponding to the TLBentry. Managing the TLBin software and not hardware comes at a cost in performance. However, the increased hit ratio and TLB reach offset the performance costs. Indeed, recent trends indic ate a move toward softwaremanaged TLBsa n d operatingsystem support for multiple page sizes. 9.9.4

support for multiple page sizes. 9.9.4 Inverted Page Tables Section 8.6.3 introduced the concept of the inverted page table. The purpose of this form of page management is to reduce the amount of physical memory needed to track virtualtophysical address translations. We accomplish this savings by creating a table that has one entry per page of physical memory, indexed by the pair processid, pagenumber . Because they keep information about which virtual memory page is stored in each physical

virtual memory page is stored in each physical frame, inverted page tables reduce the amount of physical memory needed to store this information. However, the inverted page table no longer contains complete information about the logical address space of a process, and that information is required if a referenced page is not currently in memory. Demand paging requires this information to process page faults. For the information to be available, an external page table (one per process) must be

an external page table (one per process) must be kept. Each such table looks like the traditional perprocess page table and contains information on where each virtual page is located. But do external page tables negate the utility of inverted page tables? Since these tables are referenced only when a page fault occurs, they do not need to be available quickly. Instead, they are themselves paged in and out of memory as necessary. Unfortunately, a page fault may now cause the virtual memory

a page fault may now cause the virtual memory manager to generate another page fault as it pages in the external page table it needs to locate the virtual page on the backing store. This special case requires careful handling in the kernel and a delay in the pagelookup processing. 9.9.5 Program Structure Demand paging is designed to be transparent to the user program. In many cases, the user is completely unaware of the paged nature of memory. In other9.9 Other Considerations 443 cases, however,

other9.9 Other Considerations 443 cases, however, system performance can be improved if the user (or compiler) has an awareness of the underlying demand paging. Lets look at a contrived but informative example. Assume that pages are 128 words in size. Consider a C program whose function is to initialize to 0 each element of a 128by128 array. The following code is typical: int i, j; int[128][128] data; for (j  0; j  128; j) for (i  0; i  128; i) data[i][j]  0; Notice that the array is stored row

data[i][j]  0; Notice that the array is stored row major; that is, the array is stored data[0][0] ,data[0][1] ,,data[0][127] ,data[1][0] ,data[1][1] ,, data[127][127] . For pages of 128 words, each row takes one page. Thus, the preceding code zeros one word in each page, then another word in each page, and so on. If the operating system allocates fewer than 128 frames to the entire program, then its execution will result in 128 128  16,384 page faults. In contrast, suppose we change the code to

faults. In contrast, suppose we change the code to int i, j; int[128][128] data; for (i  0; i  128; i) for (j  0; j  128; j) data[i][j]  0; This code zeros all the words on one page before starting the next page, reducing the number of page faults to 128. Careful selection of data structures and programming structures can increase locality and hence lower the pagefault rate and the number of pages in the working set. For example, a stack has good locality, since access is always made to the top.

locality, since access is always made to the top. A hash table, in contrast, is designed to scatter references, producing bad locality. Of course, locality of reference is just one measure of the efciency of the use of a data structure. Other heavily weighted factors include search speed, total number of memory references, and total number of pages touched. At a later stage, the compiler and loader can have a signicant effect on paging. Separating code and data and generating reentrant code

code and data and generating reentrant code means that code pages can be readonly and hence will never be modied. Clean pages do not have to be paged out to be replaced. The loader can avoid placing routines across page boundaries, keeping each routine completely in one page. Routines that call each other many times can be packed into the same page. This packaging is a variant of the binpacking problem of operations research: try to pack the variablesized load segments into the xedsized pages so

load segments into the xedsized pages so that interpage references are minimized. Such an approach is particularly useful for large page sizes.444 Chapter 9 Virtual Memory 9.9.6 IO Interlock and Page Locking When demand paging is used, we sometimes need to allow some of the pages to be locked in memory. One such situation occurs when IOis done to or from user (virtual) memory. IOis often implemented by a separate IOprocessor. For example, a controller for a USBstorage device is generally given

for a USBstorage device is generally given the number of bytes to transfer and a memory address for the buffer (Figure 9.28). When the transfer is complete, the CPU is interrupted. We must be sure the following sequence of events does not occur: A process issues an IOrequest and is put in a queue for that IOdevice. Meanwhile, the CPU is given to other processes. These processes cause page faults, and one of them, using a global replacement algorithm, replaces the page containing the memory

algorithm, replaces the page containing the memory buffer for the waiting process. The pages are paged out. Some time later, when the IOrequest advances to the head of the device queue, the IO occurs to the specied address. How ever, this frame is now being used for a different page belonging to another process. There are two common solutions to this problem. One solution is never to execute IOto user memory. Instead, data are always copied between system memory and user memory. IO takes place

system memory and user memory. IO takes place only between system memory and the IOdevice. To write a block on tape, we rst copy the block to system memory and then write it to tape. This extra copying may result in unacceptably high overhead. Another solution is to allow pages to be locked into memory. Here, a lock bit is associated with every frame. If th ef r a m ei sl o c k e d ,i tc a n n o tb es e l e c t e d for replacement. Under this approach, to write a block on tape, we lock into

approach, to write a block on tape, we lock into memory the pages containing the block. The system can then continue as usual. Locked pages cannot be replaced. When the IOis complete, the pages are unlocked. bufferdisk drive Figure 9.28 The reason why frames used for IO must be in memory.9.10 OperatingSystem Examples 445 Lock bits are used in various situations. Frequently, some or all of the operatingsystem kernel is locked into memory. Many operating systems cannot tolerate a page fault caused

systems cannot tolerate a page fault caused by the kernel or by a specic kernel module, including the one performing memory ma nagement. User processes may also need to lock pages into memory. A database process may want to manage ac h u n ko fm e m o r y ,f o re x a m p l e ,m o v i n gb l o c k sb e t w e e nd i s ka n dm e m o r y itself because it has the best knowledge of how it is going to use its data. Such pinning of pages in memory is fairly common, and most operating systems have a

fairly common, and most operating systems have a system call allowing an application to request that a region of its logical address space be pinned. Note that this feature could be abused and could cause stress on the memorymanagement algorithms. Therefore, an application frequently requires special privileges to make such a request. Another use for a lock bit inv olves normal page replacement. Consider the following sequence of events: A lowpriority process faults. Selecting a replacement

process faults. Selecting a replacement frame, the paging system reads the necessary page into memory. Ready to continue, the lowpriority process enters the ready queue and waits for the CPU.S i n c ei ti sal o w  p r i o r i t yp r o c e s s ,i tm a yn o tb es e l e c t e db yt h e CPU scheduler for a time. While the lowpriority process waits, a highpriority process faults. Looking for a replacement, the paging system sees a page that is in memory but has not been referenced or modied: it is

but has not been referenced or modied: it is the page that the lowpriority process just brought in. This page looks like a perfect replacement: it is clean and will not need to be written out, and it apparently has not been used for a long time. Whether the highpriority process should be able to replace the lowpriority process is a policy decision. After all, we are simply delaying the lowpriority process for the benet of the highpriority process. However, we are wasting the effort spent to

However, we are wasting the effort spent to bring in the page for the lowpriority process. If we decide to prevent replacement of a newly broughtin page until it can be used at least once, then we can use the lock bit to implement this mechanism. When a page is selected for replacement, its lock bit is turned on. It remains on until the faulting process is again dispatched. Using a lock bit can be dangerous: the lock bit may get turned on but never turned off. Should this situation occu r

but never turned off. Should this situation occu r (because of a bug in the operating system, for example), the locked frame becomes unusable. On a singleuser system, the overuse of locking would hurt only the user doing the locking. Multiuser systems must be less trusting of users. For instance, Solaris allows locking hints, but it is free to disregard these hints if the freeframe pool becomes too small or if an individual process requests that too many pages be locked in memory. 9.10

that too many pages be locked in memory. 9.10 OperatingSystem Examples In this section, we describe how Windows and Solaris implement virtual memory. 9.10.1 Windows Windows implements virtual memory using demand paging with clustering . Clustering handles page faults by bringing in not only the faulting page but also446 Chapter 9 Virtual Memory several pages following the faulting page. When a process is rst created, it is assigned a workingset minimum and maximum. The workingset minimum is the

minimum and maximum. The workingset minimum is the minimum number of pages the pro cess is guaranteed to have in memory. If sufcient memory is available, a process may be assigned as many pages as itsworkingset maximum .( I ns o m ec i r c u m s t a n c e s ,ap r o c e s sm a yb ea l l o w e d to exceed its workingset maximum.) The virtual memory manager maintains a list of free page frames. Associated with this list is a threshold value that is used to indicate whether sufcient free memory is

used to indicate whether sufcient free memory is available. If a page fault occurs for ap r o c e s st h a ti sb e l o wi t sw o r k i n g  s e tm a x i m u m ,t h ev i r t u a lm e m o r ym a n a g e r allocates a page from this list of free pages. If a process that is at its workingset maximum incurs a page fault, it must select a page for replacement using a local LRU pagereplacement policy. When the amount of free memory falls below the threshold, the virtual memory manager uses a tactic

the virtual memory manager uses a tactic known as automatic workingset trimming to restore the value above the threshold. Automatic workingset trimming works by evaluating the number of pages allocated to processes. If a process has been allocated more pages than its workingset minimum, the virtual memory manager removes pages until the process reaches its workingset minimum. Ap r o c e s st h a ti sa ti t sw o r k i n g  s e tm i n i m u mm a yb ea l l o c a t e dp a g e sf r o m the

m u mm a yb ea l l o c a t e dp a g e sf r o m the freepageframe list once sufcient free memory is available. Windows performs workingset trimming on both user mode and system processes. Virtual memory is discussed in great detail in the Windows case study in Chapter 19. 9.10.2 Solaris In Solaris, when a thread incurs a page fault, the kernel assigns a page to the faulting thread from the list of free pages it maintains. Therefore, it is imperative that the kernel keep a sufcient amount of free

that the kernel keep a sufcient amount of free memory available. Associated with this list of free pages is a parameter lotsfree that represents a threshold to begin paging. The lotsfree parameter is typically set to 1 64 the size of the physical memory. Four times per second, the kernel checks whether the amount of free memory is less than lotsfree .I ft h en u m b e ro f free pages falls below lotsfree ,ap r o c e s sk n o w na sa pageout starts up. The pageout process is similar to the

starts up. The pageout process is similar to the secondchance algorithm described in Section 9.4.5.2, except that it uses two hands while scanning pages, rather than one. The pageout process works as follows: The front hand of the clock scans all pages in memory, setting the reference bit to 0. Later, the back hand of the clock examines the reference bit for the pages in memory, appending each page whose reference bit is still set to 0 to the free list and writing to disk its contents if modied.

list and writing to disk its contents if modied. Solaris maintains a cache list of pages that have been freed but have not yet been overwritten. The free list contains frames that have invalid contents. Pages can be reclaimed from th ec a c h el i s ti ft h e ya r ea c c e s s e db e f o r e being moved to the free list. The pageout algorithm uses several parameters to control the rate at which pages are scanned (known as the scanrate ). The scanrate is expressed in pages per second and ranges

is expressed in pages per second and ranges from slowscan tofastscan .W h e nf r e em e m o r y falls below lotsfree ,s c a n n i n go c c u r sa t slowscan pages per second and progresses to fastscan, depending on the amount of free memory available. The default value of slowscan is 100 pages per second. Fastscan is typically9.10 OperatingSystem Examples 447 set to the value (total physical pages)2 pages per second, with a maximum of 8,192 pages per second. This is shown in Figure 9.29 (with

per second. This is shown in Figure 9.29 (with fastscan set to the maximum). The distance (in pages) between the hands of the clock is determined by a system parameter, handspread. The amount of time between the front hands clearing a bit and the back hands investigating its value depends on thescanrate and the handspread. Ifscanrate is 100 pages per second and handspread is 1,024 pages, 10 seconds can pass between the time a bit is set by the front hand and the time it is checked by the back

front hand and the time it is checked by the back hand. However, because of the demands placed on the memory system, a scanrate of several thousand is not uncommon. This means that the amount of time between clearing and investigating a bit is often a few seconds. As mentioned above, the pageout pr ocess checks memory four times per second. However, if free memory falls below the value of desfree (Figure 9.29), pageout will run a hundred times per second with the intention of keeping at least

per second with the intention of keeping at least desfree free memory available. If the pageout process is unable to keep the amount of free memory at desfree for a 30second average, the kernel begins swapping processes, thereby f reeing all pages allocated to swapped processes. In general, the kernel looks for processes that have been idle for long periods of time. If the system is unable to maintain the amount of free memory at minfree ,t h ep a g e o u tp r o c e s si sc a l l e df o re v e r

a g e o u tp r o c e s si sc a l l e df o re v e r yr e q u e s tf o ran e w page. Recent releases of the Solaris kernel have provided enhancements of the paging algorithm. One such enhancement involves recognizing pages from shared libraries. Pages belonging t ol i b r a r i e st h a ta r eb e i n gs h a r e db y several processeseven if they are eligible to be claimed by the scanner are skipped during the pagescanning pr ocess. Another enhancement concerns minfreescan rate 100 slowscan8192

concerns minfreescan rate 100 slowscan8192 fastscan desfree amount of free memorylotsfree Figure 9.29 Solaris page scanner.448 Chapter 9 Virtual Memory distinguishing pages that have been allocated to processes from pages allocated to regular les. This is known as priority paging and is covered in Section 12.6.2. 9.11 Summary It is desirable to be able to execute a process whose logical address space is larger than the available physical address space. Virtual memory is a technique that enables

space. Virtual memory is a technique that enables us to map a large logical address space onto a smaller physical memory. Virtual memory allows us to run extremely large processes and to raise the degree of multiprogramming, increasing CPU utilization. Further, it frees application programmers from worrying about memory availability. In addition, with virtual memory, several processes can share system libraries and memory. With virtual memory, we can also use an efcient type of process creation

can also use an efcient type of process creation known as copyonwrite, wherein parent and child processes share actual pages of memory. Virtual memory is commonly implemented by demand paging. Pure demand paging never brings in a page until that page is referenced. The rst reference causes a page fault to the operating system. The operatingsystem kernel consults an internal table to determin e where the page is located on the backing store. It then nds a free frame and reads the page in from the

nds a free frame and reads the page in from the backing store. The page table is updated to reect this change, and the instruction that caused the page fault is restarted. This approach allows a process to run even though its entire memory image is not in main memory at once. As long as the pagefault rate is reasonably low, performance is acceptable. We can use demand paging to reduce the number of frames allocated to ap r o c e s s .T h i sa r r a n g e m e n tc a ni n c r e a s et h ed e g r e

a n g e m e n tc a ni n c r e a s et h ed e g r e eo fm u l t i p r o g r a m m i n g (allowing more processes to be available for execution at one time) andin theory, at leastthe CPU utilization of the system. It also allows processes to be run even though their memory r equirements exceed the total available physical memory. Such processes run in virtual memory. If total memory requirements exceed the capacity of physical memory, then it may be necessary to replace pages from memory to free

be necessary to replace pages from memory to free frames for new pages. Various pagereplacement algorithms are used. FIFO page replace ment is easy to program but suffers from Beladys anomaly. Optimal page replacement requires future knowledge. LRU replacement is an approxima tion of optimal page replacement, but even it may be difcult to implement. Most pagereplacement algorithms, such as the secondchance algorithm, are approximations of LRU replacement. In addition to a pagereplacement

LRU replacement. In addition to a pagereplacement algorithm, a frameallocation policy is needed. Allocation can be xed, suggesting local page replacement, or dynamic, suggesting global replacement. The workingset model assumes that processes execute in localities. The working set is the set of pages in the current locality. Accordingly, each process should be allocated enough frames for its current working set. If a process does not have enough memory for its working set, it will thrash.

enough memory for its working set, it will thrash. Providing enough frames to each process to avoid thrashing may require process swapping and scheduling. Most operating systems provide features for memory mapping les, thus allowing le IO to be treated as routine memory access. The Win 32 API implements shared memory through memory mapping of les.Practice Exercises 449 Kernel processes typically require memory to be allocated using pages that are physically contiguous. The buddy system allocates

physically contiguous. The buddy system allocates memory to kernel processes in units sized according to a power of 2, which often results in fragmentation. Slab allocators assign kernel data structures to caches associated with slabs, which are made up of one or more physically contiguous pages. With slab allocation, no memory is wasted due to fragmentation, and memory requests can be satised quickly. In addition to requiring us to solve the major problems of page replacement and frame

the major problems of page replacement and frame allocation, the proper design of a paging system requires that we consider prepaging, page size, TLB reach, inverted page tables, program structure, IOinterlock and page locking, and other issues. Practice Exercises 9.1 Under what circumstances do page faults occur? Describe the actions taken by the operating system when a page fault occurs. 9.2 Assume that you have a pagereference string for a process with m frames (initially all empty). The

a process with m frames (initially all empty). The pagereference string has length p,a n d ndistinct page numbers occur in it. Answer these questions for any pagereplacement algorithms: a. What is a lower bound on the number of page faults? b. What is an upper bound on the number of page faults? 9.3 Consider the page table shown in Figure 9.30 for a system with 12bit virtual and physical addresses and with 256byte pages. The list of free page frames is D,E,F(that is, Dis at the head of the list,

is D,E,F(that is, Dis at the head of the list, Eis second, and Fis last).Page Page Frame 0 1 2 3 4 5 2 C A  463 7 8B 90 Figure 9.30 Page table for Exercise 9.3.450 Chapter 9 Virtual Memory Convert the following virtual addr esses to their equivalent physical addresses in hexadecimal. All numbers are given in hexadecimal. (A dash for a page frame indicates that the page is not in memory.) 9EF 111 700 0FF 9.4 Consider the following pagereplacement algorithms. Rank these algo rithms on a vepoint

algorithms. Rank these algo rithms on a vepoint scale from badtoperfect according to their pagefault rate. Separate those algorithms that suffer from Beladys anomaly from those that do not. a. LRU replacement b. FIFO replacement c. Optimal replacement d. Secondchance replacement 9.5 Discuss the hardware support required to support demand paging. 9.6 An operating system supports a paged virtual memory. The central processor has a cycle time of 1 microsecond. It costs an additional 1 microsecond

microsecond. It costs an additional 1 microsecond to access a page other than the current one. Pages have 1,000 words, and the paging device is a drum that rotates at 3,000 revolutions per minute and transfers 1 million words per second. The following statistical measurements were obtained from the system: One percent of all instructions ex ecuted accessed a page other than the current page. Of the instructions that accessed another page, 80 percent accessed ap a g ea l r e a d yi nm e m o r y .

accessed ap a g ea l r e a d yi nm e m o r y . When a new page was required, the replaced page was modied 50 percent of the time. Calculate the effective instruction time on this system, assuming that the system is running one process only and that the processor is idle during drum transfers. 9.7 Consider the twodimensional array A: int A[][]  new int[100][100]; where A[0][0] is at location 200 in a paged memory system with pages of size 200. A small process that manipulates the matrix resides

small process that manipulates the matrix resides in page 0( l o c a t i o n s0t o1 9 9 ) .T h u s ,e v e r yi n s t r u c t i o nf e t c hw i l lb ef r o mp a g e0 . For three page frames, how many page faults are generated by the following arrayinitialization loops? Use LRU replacement, and assumePractice Exercises 451 that page frame 1 contains the process and the other two are initially empty. a.for (int j  0; j  100; j) for (int i  0; i  100; i) A[i][j]  0; b. for (int i  0; i  100; i) for

i) A[i][j]  0; b. for (int i  0; i  100; i) for (int j  0; j  100; j) A[i][j]  0; 9.8 Consider the following page reference string: 1, 2, 3, 4, 2, 1, 5, 6, 2, 1, 2, 3, 7, 6, 3, 2, 1, 2, 3, 6. How many page faults would occur for the following replacement algorithms, assuming one, two, three, four, ve, six, and seven frames? Remember that all frames are initially empty, so your rst unique pages will cost one fault each. LRU replacement FIFO replacement Optimal replacement 9.9 Suppose that you

Optimal replacement 9.9 Suppose that you want to use a paging algorithm that requires a reference bit (such as secondchance replace ment or workingset model), but the hardware does not provide one. Sketch how you could simulate a reference bit even if one were not provided by the hardware, or explain why it is not possible to do so. If it is possible, calculate what the cost would be. 9.10 You have devised a new pagereplacement algorithm that you think may be optimal. In some contorted test

you think may be optimal. In some contorted test cases, Beladys anomaly occurs. Is the new algorithm optimal? Explain your answer. 9.11 Segmentation is similar to paging but uses variablesized pages. Dene two segmentreplacement algorithms, one based on the FIFO page replacement scheme and the other on the LRU pagereplacement scheme. Remember that since segments are not the same size, the segment that is chosen for replacement may be too small to leave enough consecutive locations for the needed

leave enough consecutive locations for the needed segment. Consider strategies for systems where segments cannot be relocated and strategies for systems where they can. 9.12 Consider a demandpaged computer system where the degree of mul tiprogramming is currently xed at four. The system was recently measured to determine utilization of the CPU and the paging disk. Three alternative results are shown below. For each case, what is happening? Can the degree of multiprogramming be increased to

Can the degree of multiprogramming be increased to increase the CPU utilization? Is the paging helping? a. CPU utilization 13 percent; disk utilization 97 percent b. CPU utilization 87 percent; disk utilization 3 percent c. CPU utilization 13 percent; disk utilization 3 percent452 Chapter 9 Virtual Memory 9.13 We have an operating system for a machine that uses base and limit registers, but we have modied the machine to provide a page table. Can the page tables be set up to simulate base and

Can the page tables be set up to simulate base and limit registers? How can they be, or why can they not be? Exercises 9.14 Assume that a program has just referenced an address in virtual memory. Describe a scenario in which each of the following can occur. (If no such scenario can occur, explain why.) TLBmiss with no page fault TLBmiss and page fault TLBhit and no page fault TLBhit and page fault 9.15 As i m p l i  e dv i e wo ft h r e a ds t a t e si s Ready ,Running ,a n d Blocked ,w h e r e

t e si s Ready ,Running ,a n d Blocked ,w h e r e a thread is either ready and waiting to be scheduled, is running on the processor, or is blocked (for example, waiting for IO). This is illustrated in Figure 9.31. Assuming a thread is in the Running state, answer the following questions, and explain your answer: a. Will the thread change state if it incurs a page fault? If so, to what state will it change? b. Will the thread change state if it generates a TLBmiss that is resolved in the page

generates a TLBmiss that is resolved in the page table? If so, to what state will it change? c. Will the thread change state if an address reference is resolved in the page table? If so, to what state will it change? 9.16 Consider a system that uses pure demand paging. a. When a process rst starts execution, how would you characterize the pagefault rate? b. Once the working set for a process is loaded into memory, how would you characterize the pagefault rate? ReadyBlockedRunning Figure 9.31

pagefault rate? ReadyBlockedRunning Figure 9.31 Thread state diagram for Exercise 9.15.Exercises 453 c. Assume that a process changes its locality and the size of the new working set is too large to be stored in available free memory. Identify some options system designers could choose from to handle this situation. 9.17 What is the copyonwrite feature, and under what circumstances is its use benecial? What hardware support is required to implement this feature? 9.18 Ac e r t a i nc o m p u t e

this feature? 9.18 Ac e r t a i nc o m p u t e rp r o v i d e si t su s e r sw i t hav i r t u a lm e m o r ys p a c eo f 232bytes. The computer has 222bytes of physical memory. The virtual memory is implemented by paging, and the page size is 4,096 bytes. Au s e rp r o c e s sg e n e r a t e st h ev i r t u a la d d r e s s1 1 1 2 3 4 5 6 .E x p l a i nh o w the system establishes the corresponding physical location. Distinguish between software and hardware operations. 9.19 Assume that we have

and hardware operations. 9.19 Assume that we have a demandpaged memory. The page table is held in registers. It takes 8 milliseconds to service a page fault if an empty frame is available or if the replaced page is not modied and 20 milliseconds if the replaced page is modied. Memoryaccess time is 100 nanoseconds. Assume that the page to be replaced is modied 70 percent of the time. What is the maximum acceptable pagefault rate for an effective access time of no more than 200 nanoseconds? 9.20

access time of no more than 200 nanoseconds? 9.20 When a page fault occurs, the process requesting the page must block while waiting for the page to be brought from disk into physical memory. Assume that there exists a process with ve userlevel threads and that the mapping of user threads to kernel threads is one to one. If one user thread incurs a page fault while accessing its stack, would the other user threads belonging to the same process also be affected by the page faultthat is, would

also be affected by the page faultthat is, would they also have to wait for the faulting page to be brought into memory? Explain. 9.21 Consider the following page reference string: 7, 2, 3, 1, 2, 5, 3, 4, 6, 7, 7, 1, 0, 5, 4, 6, 2, 3, 0 , 1. Assuming demand paging with three frames, how many page faults would occur for the following replacement algorithms? LRU replacement FIFO replacement Optimal replacement 9.22 The page table shown in Figure 9.32 is for a system with 16bit virtual and physical

is for a system with 16bit virtual and physical addresses and with 4,096byte pages. The reference bit is set to 1 when the page has been referenced. Periodically, a thread zeroes out all values of the reference bit. A dash for a page frame indicates the page is not in memory. The pagereplacement algorithm is localized LRU,a n da l ln u m b e r sa r ep r o v i d e di nd e c i m a l . a. Convert the following virtual addresses (in hexadecimal) to the equivalent physical addresses. You m ay provide

equivalent physical addresses. You m ay provide answers in either454 Chapter 9 Virtual MemoryPage Page Frame Reference B it 09 0 11 0 21 4 0 31 0 0 4 0 51 3 0 68 0 71 5 0 8 0 90 0 10 5 0 11 4 0 12  0 13  0 14 3 0 15 2 0 Figure 9.32 Page table for Exercise 9.22. hexadecimal or decimal. Also set the reference bit for the appro priate entry in the page table. 0xE12C 0x3A9D 0xA9D9 0x7001 0xACA1 b. Using the above addresses as a guide, provide an example of a logical address (in hexadecimal) that

example of a logical address (in hexadecimal) that results in a page fault. c. From what set of page frames will the LRU pagereplacement algorithm choose in resolving a page fault? 9.23 Assume that you are monitoring the rate at which the pointer in the clock algorithm moves. (The pointer indicates the candidate page for replacement.) What can you say about the system if you notice the following behavior: a. Pointer is moving fast. b. Pointer is moving slow. 9.24 Discuss situations in which the

moving slow. 9.24 Discuss situations in which the least frequently used ( LFU)p a g e  replacement algorithm generates fewer page faults than the least recently used ( LRU)p a g e  r e p l a c e m e n ta l g o r i t h m .A l s od i s c u s su n d e rw h a tc i r  cumstances the opposite holds. 9.25 Discuss situations in which the most frequently used ( MFU )p a g e  replacement algorithm generates fewer page faults than the least recently used ( LRU)p a g e  r e p l a c e m e n ta l g o r i t h

LRU)p a g e  r e p l a c e m e n ta l g o r i t h m .A l s od i s c u s su n d e rw h a tc i r  cumstances the opposite holds.Exercises 455 9.26 The VAXVMS system uses a FIFO replacement algorithm for resident pages and a freeframe pool of recently used pages. Assume that the freeframe pool is managed using the LRU replacement policy. Answer the following questions: a. If a page fault occurs and the page does not exist in the freeframe pool, how is free space generated for the newly requested

is free space generated for the newly requested page? b. If a page fault occurs and the page exists in the freeframe pool, how is the resident page set and the freeframe pool managed to make space for the requested page? c. What does the system degenerate to if the number of resident pages is set to one? d. What does the system degenerate to if the number of pages in the freeframe pool is zero? 9.27 Consider a demandpaging system with the following timemeasured utilizations: CPU utilization 20

timemeasured utilizations: CPU utilization 20 Paging disk 97.7 Other IOdevices 5 For each of the following, indicate whether it will (or is likely to) improve CPU utilization. Explain your answers. a. Install a faster CPU. b. Install a bigger paging disk. c. Increase the degree of multiprogramming. d. Decrease the degree of multiprogramming. e. Install more main memory. f. Install a faster hard disk or multiple controllers with multiple hard disks. g. Add prepaging to the pagefetch algorithms.

g. Add prepaging to the pagefetch algorithms. h. Increase the page size. 9.28 Suppose that a machine provides instructions that can access memory locations using the onelevel indir ect addressing scheme. What sequence of page faults is incurred when all of the pages of a program are currently nonresident and the rst instruction of the program is an indirect memoryload operation? What happens when the operating system is using a perprocess frame allocation technique and only two pages are

frame allocation technique and only two pages are allocated to this process? 9.29 Suppose that your replacement policy (in a paged system) is to examine each page regularly and to discard that page if it has not been used since the last examination. What would you gain and what would you lose by using this policy rather than LRU or secondchance replacement?456 Chapter 9 Virtual Memory 9.30 A pagereplacement algorithm should minimize the number of page faults. We can achieve this minimization by

page faults. We can achieve this minimization by distributing heavily used pages evenly over all of memory, rather than having them compete for a small number of page frames. We can associate with each page frame ac o u n t e ro ft h en u m b e ro fp a g e sa s s o c i a t e dw i t ht h a tf r a m e .T h e n , to replace a page, we can search for the page frame with the smallest counter. a. Dene a pagereplacement algorithm using this basic idea. Specif ically address these problems: i. What is

Specif ically address these problems: i. What is the initial value of the counters? ii. When are counters increased? iii. When are counters decreased? iv. How is the page to be replaced selected? b. How many page faults occur for your algorithm for the following reference string with four page frames? 1, 2, 3, 4, 5, 3, 4, 1, 6, 7, 8, 7, 8, 9, 7, 8, 9, 5, 4, 5, 4, 2. c. What is the minimum number of page faults for an optimal page replacement strategy for the reference string in part b with four

for the reference string in part b with four page frames? 9.31 Consider a demandpaging system with a paging disk that has an average access and transfer time of 20 milliseconds. Addresses are translated through a page table in main memory, with an access time of 1 microsecond per memory access. Thus, each memory reference through the page table takes two accesses. To improve this time, we have added an associative memory that reduces access time to one memory reference if the pagetable entry is

to one memory reference if the pagetable entry is in the associative memory. Assume that 80 percent of the accesses are in the associative memory and that, of those remaining, 10 percent (or 2 percent of the total) cause page faults. What is the effective memory access time? 9.32 What is the cause of thrashing? How does the system detect thrashing? Once it detects thrashing, what can the system do to eliminate this problem? 9.33 Is it possible for a process to have two working sets, one

for a process to have two working sets, one representing data and another representing code? Explain. 9.34 Consider the parameter Delta1used to dene the workingset window in the workingset model. When Delta1is set to a small value, what is the effect on the pagefault frequency and the number of active (nonsuspended) processes currently executing in the system? What is the effect when Delta1 is set to a very high value? 9.35 In a 1,024 KBsegment, memory is allocated using the buddy system. Using

memory is allocated using the buddy system. Using Figure 9.26 as a guide, draw a tree illustrating how the following memory requests are allocated: Request 6 KBProgramming Problems 457 Request 250 bytes Request 900 bytes Request 1,500 bytes Request 7 KB Next, modify the tree for the following releases of memory. Perform coalescing whenever possible: Release 250 bytes Release 900 bytes Release 1,500 bytes 9.36 As y s t e mp r o v i d e ss u p p o r tf o ru s e r  l e v e la n dk e r n e l  l e v

r tf o ru s e r  l e v e la n dk e r n e l  l e v e lt h r e a d s .T h e mapping in this system is one to one (there is a corresponding kernel thread for each user thread). Does a multithreaded process consist of (a) a working set for the entire process or (b) a working set for each thread? Explain 9.37 The slaballocation algorithm uses a separate cache for each different object type. Assuming there is one cache per object type, explain why this scheme doesnt scale well with multiple CPUs. What

scheme doesnt scale well with multiple CPUs. What could be done to address this scalability issue? 9.38 Consider a system that allocates pages of different sizes to its processes. What are the advantages of such a paging scheme? What modications to the virtual memory system provide this functionality? Programming Problems 9.39 Write a program that implements the FIFO ,LRU, and optimal page replacement algorithms presented in this chapter. First, generate a random pagereference string where page

generate a random pagereference string where page numbers range from 0 to 9. Apply the random pagereference strin gt oe a c ha l g o r i t h m ,a n dr e c o r d the number of page faults incurred by each algorithm. Implement the replacement algorithms so that the number of page frames can vary from 1 to 7. Assume that demand paging is used. 9.40 Repeat Exercise 3.22, this time using Windows shared memory. In partic ular, using the producerconsumer strategy, design two programs that communicate

strategy, design two programs that communicate with shared memory using the Windows APIas outlined in Section 9.7.2. The producer will generate the numbers specied in the Collatz conjecture and write them to a shared memory object. The consumer will then read and o utput the sequence of numbers from shared memory. In this instance, the producer will be passed an integer parameter on the command line specifying how many numbers to produce (for example, providing 5 on the command line means the

example, providing 5 on the command line means the producer process will generate the rst ve numbers).458 Chapter 9 Virtual Memory Programming Projects Designing a Virtual Memory Manager This project consists of writing a program that translates logical to physical addresses for a virtual address space of size 216 65,536 bytes. Your program will read from a le containing logical addresses and, using a TLB as well as ap a g et a b l e ,w i l lt r a n s l a t ee a c hl o g i c a la d d r e s st oi

a n s l a t ee a c hl o g i c a la d d r e s st oi t sc o r r e s p o n d i n gp h y s i c a l address and output the value of the byte stored at the translated physical address. The goal behind this project is to simulate the steps involved in translating logical to physical addresses. Specics Your program will read a le contain ing several 32bit integer numbers that represent logical addresses. However, you n eed only be concerned with 16bit addresses, so you must mask the rightmost 16 bits of

so you must mask the rightmost 16 bits of each logical address. These 16 bits are divided into (1) an 8bit page number and (2) 8bit page offset. Hence, the addresses are structured as shown in Figure 9.33. Other specics include the following: 28entries in the page table Page size of 28bytes 16 entries in the TLB Frame size of 28bytes 256 frames Physical memory of 65,536 bytes (256 frames 256byte frame size) Additionally, your program need only be concerned with reading logical addresses and

be concerned with reading logical addresses and translating them to their corresponding physical addresses. You do not need to support writing to the logical address space. Address Translation Your program will translate logical to physical addresses using a TLBand page table as outlined in Section 8.5. First, the page number is extracted from the logical address, and the TLB is consulted. In the case of a TLBhit, the frame number is obtained from the TLB.I nt h ec a s eo fa TLBmiss, the page

from the TLB.I nt h ec a s eo fa TLBmiss, the page table must be consulted. In the latter case, either the frame number is obtained offset 0 78 1516 31page number Figure 9.33 Address structure.Programming Projects 459page number 0 1 2 15 0 1 2 255TLB page tableTLB h it TLB m isspage 0 page 255page 1 page 2frame number .... ....0 1 2 255 phys ical memoryframe 0 frame 255frame 1frame 2 ....page numberoffset frame numberoffset Figure 9.34 Ar e p r e s e n t a t i o no ft h ea d d r e s s  t r a n s

s e n t a t i o no ft h ea d d r e s s  t r a n s l a t i o np r o c e s s . from the page table or a page fault occurs. A visual representation of the addresstranslation process appears in Figure 9.34. Handling Page Faults Your program will implement demand paging as described in Section 9.2. The backing store is represented by the le BACKING STORE.bin ,ab i n a r y l eo fs i z e 65,536 bytes. When a page fault occurs, you will read in a 256byte page from the leBACKING STORE and store it in an

page from the leBACKING STORE and store it in an available page frame in physical memory. For example, if a logical address with page number 15 resulted in a page fault, your program would read in page 15 from BACKING STORE (remember that pages begin at 0 and are 256 bytes in size) and store it in a page frame in physical memory. Once this frame is stored (and the page table and TLB are updated), subsequent accesses to page 15 will be resolved by either the TLBor the page table. You will need to

either the TLBor the page table. You will need to treat BACKING STORE.bin as a randomaccess le so that you can randomly seek to certain positions of the le for reading. We suggest using the standard C library functions for performing IO,i n c l u d i n g fopen() , fread() ,fseek() ,a n d fclose() . The size of physical memory is the same as the size of the virtual address space65,536 bytesso you do not need to be concerned about page replacements during a page fault. Later, we describe a

during a page fault. Later, we describe a modication to this project using a smaller amount of physical memory; at that point, a pagereplacement strategy will be required.460 Chapter 9 Virtual Memory Test File We provide the le addresses.txt , which contains integer values represent ing logical addresses ranging from 0 65535 (the size of the virtual address space). Your program will open this le, read each logical address and translate it to its corresponding physical address, and output the

its corresponding physical address, and output the value of the signed byte at the physical address. How to Begin First, write a simple program that extracts the page number and offset (based on Figure 9.33) from the following integer numbers: 1,256,32768 ,32769 ,128,65534 ,33153 Perhaps the easiest way to do this is by using the operators for bitmasking and bitshifting. Once you can correctly establish the page number and offset from an integer number, you are ready to begin. Initially, we

number, you are ready to begin. Initially, we suggest that you bypass the TLBand use only a page table. You can integrate the TLB once your page table is working properly. Remember, address translation can work without a TLB;t h e TLBjust makes it faster. When you are ready to implement the TLB, recall that it has only 16 entries, so you will need to use a replacement strategy when you update a full TLB.Y o um a y use either a FIFO or an LRU policy for updating your TLB. How to Run Your Program

for updating your TLB. How to Run Your Program Your program should run as follows: .a.out addresses.txt Your program will read in the le addresses.txt ,w h i c hc o n t a i n s1 , 0 0 0l o g i c a l addresses ranging from 0 to 65535. Your program is to translate each logical address to a physical address and determin et h ec o n t e n t so ft h es i g n e db y t e stored at the correct physical address. (Recall that in the C language, the char data type occupies a byte of storage, so we suggest

type occupies a byte of storage, so we suggest using char values.) Your program is to output the following values: 1.The logical address being translated (the integer value being read from addresses.txt ). 2.The corresponding physical address (what your program translates the logical address to). 3.The signed byte value stored at the translated physical address. We also provide the le correct.txt ,w h i c hc o n t a i n st h ec o r r e c to u t p u t values for the le addresses.txt .Y o us h o u

u t values for the le addresses.txt .Y o us h o u l du s et h i s l et od e t e r m i n ei fy o u r program is correctly translating logical to physical addresses. Statistics After completion, your program is to report the following statistics:Bibliographical Notes 461 1.Pagefault rateThe percentage of address references that resulted in page faults. 2.TLBhit rateThe percentage of address references that were resolved in the TLB. Since the logical addresses in addresses.txt were generated

logical addresses in addresses.txt were generated randomly and do not reect any memory access locality, do not expect to have a high TLB hit rate. Modications This project assumes that physical memory is the same size as the virtual address space. In practice, physical memory is typically much smaller than a virtual address space. A suggested modication is to use a smaller physical address space. We recommend using 128 page frames rather than 256. This change will require modifying your program

This change will require modifying your program so that it keeps track of free page frames as well as implementing a pagere placement policy using either FIFO orLRU (Section 9.4). Bibliographical Notes Demand paging was rst used in the Atlas system, implemented on the Manchester University MUSE computer around 1960 ([Kilburn et al. (1961)]). Another early demandpaging system was MULTICS ,i m p l e m e n t e do nt h e GE 645 system ([Organick (1972)]). Virtual memory was added to Unix in 1979

(1972)]). Virtual memory was added to Unix in 1979 [Babaoglu and Joy (1981)] [Belady et al. (1969)] were the rst researchers to observe that the FIFO replacement strategy may produce the anomaly that bears Beladys name. [Mattson et al. (1970)] demonstrated that stack algorithms are not subject to Beladys anomaly. The optimal replacement algorithm was presented by [Belady (1966)] and was proved to be optimal by [Mattson et al. (1970)]. Beladys optimal algorithm is for a xed allocation; [Prieve

optimal algorithm is for a xed allocation; [Prieve and Fabry (1976)] presented an optimal algorithm for situations in which the allocation can vary. The enhanced clock algorithm was discussed by [Carr and Hennessy (1981)]. The workingset model was developed by [Denning (1968)]. Discussions concerning the workingset model were presented by [Denning (1980)]. The scheme for monitoring the pagefault rate was developed by [Wulf (1969)], who successfully applied this technique to the Burroughs B5500

applied this technique to the Burroughs B5500 computer system. Buddy system memory allocators were described in [Knowlton (1965)], [Peterson and Norman (1977)], and [Purdom, Jr. and Stigler (1970)]. [Bonwick (1994)] discussed the slab allocator, and [Bonwick and Adams (2001)] extended the discussion to multiple processors. Other memorytting algorithms can be found in [Stephenson (1983)], [Bays (1977)], and [Brent (1989)]. A survey of memoryallocation strategies can be found in [Wilson et al.

strategies can be found in [Wilson et al. (1995)]. [Solomon and Russinovich (2000)] and [Russinovich and Solomon (2005)] described how Windows implements virtual memory. [McDougall and Mauro462 Chapter 9 Virtual Memory (2007)] discussed virtual memory in Solaris. Virtual memory techniques in Linux and FreeBSD were described by [Love (2010)] and [McKusick and NevilleNeil (2005)], respectively. [Ganapathy and Schimmel (1998)] and [Navarro et al. (2002)] discussed operating system support for

al. (2002)] discussed operating system support for multiple page sizes. Bibliography [Babaoglu and Joy (1981)] O. Babaoglu and W. Joy, Converting a SwapBased System to Do Paging in an Architecture Lacking PageReference Bits ,Pro ceedings of the ACM Symposium on Operating Systems Principles (1981), pages 7886. [Bays (1977)] C. Bays, AC o m p a r i s o no fN e x t  F i t ,F i r s t  F i ta n dB e s t  F i t ,Com munications of the ACM ,V o l u m e2 0 ,N u m b e r3( 1 9 7 7 ) ,p a g e s1 9 1  1 9 2

0 ,N u m b e r3( 1 9 7 7 ) ,p a g e s1 9 1  1 9 2 . [Belady (1966)] L. A. Belady, AS t u d yo fR e p l a c e m e n tA l g o r i t h m sf o raV i r t u  alStorage Computer ,IBM Systems Journal ,V o l u m e5 ,N u m b e r2( 1 9 6 6 ) ,p a g e s 78101. [Belady et al. (1969)] L. A. Belady, R. A. Nelson, and G. S. Shedler, An Anomaly in SpaceTime Characteristics of Certain Programs Running in a Paging Machine ,Communications of the ACM ,V o l u m e1 2 ,N u m b e r6( 1 9 6 9 ) ,p a g e s 349353.

m e1 2 ,N u m b e r6( 1 9 6 9 ) ,p a g e s 349353. [Bonwick (1994)] J. Bonwick, The Slab Allocator: An ObjectCaching Kernel Memory Allocator ,USENIX Summer (1994), pages 8798. [Bonwick and Adams (2001)] J. Bonwick and J. Adams, Magazines and Vmem: Extending the Slab Allocator to Many CPUs and Arbitrary Resources ,Proceed ings of the 2001 USENIX Annual Technical Conference (2001). [Brent (1989)] R. Brent, Efcient Implementation of the FirstFit Strategy for Dynamic Storage Allocation ,ACM

Strategy for Dynamic Storage Allocation ,ACM Transactions on Programming Languages and Systems ,V o l u m e1 1 ,N u m b e r3( 1 9 8 9 ) ,p a g e s3 8 8  4 0 3 . [Carr and Hennessy (1981)] W. R. Carr and J. L. Hennessy, WSClockA Simple and Effective Algorithm for Virtual Memory Management ,Proceedings of the ACM Symposium on Operating Systems Principles (1981), pages 8795. [Denning (1968)] P. J . D e n n i n g , The Working Set Model for Program Behavior , Communications of the ACM ,V o l u m e1

Behavior , Communications of the ACM ,V o l u m e1 1 ,N u m b e r5( 1 9 6 8 ) ,p a g e s3 2 3  3 3 3 . [Denning (1980)] P. J . D e n n i n g , Working Sets Past and Present ,IEEE Transac tions on Software Engineering ,V o l u m eS E  6 ,N u m b e r1( 1 9 8 0 ) ,p a g e s6 4  8 4 . [Ganapathy and Schimmel (1998)] N. Ganapathy and C. Schimmel, General Purpose Operating System Support for Multiple Page Sizes ,Proceedings of the USENIX Technical Conference (1998). [Kilburn et al. (1961)] T. Kilburn,

(1998). [Kilburn et al. (1961)] T. Kilburn, D. J. Howarth, R. B. Payne, and F. H. Sumner, The Manchester University Atlas Operating System, Part I: Internal Organiza tion,Computer Journal ,V o l u m e4 ,N u m b e r3( 1 9 6 1 ) ,p a g e s2 2 2  2 2 5 .Bibliography 463 [Knowlton (1965)] K. C. Knowlton, AF a s tS t o r a g eA l l o c a t o r ,Communications of the ACM ,V o l u m e8 ,N u m b e r1 0( 1 9 6 5 ) ,p a g e s6 2 3  6 2 4 . [Love (2010)] R. Love, Linux Kernel Development, Third Edition,

R. Love, Linux Kernel Development, Third Edition, Developers Library (2010). [Mattson et al. (1970)] R. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger, Evaluation Techniques for Storage Hierarchies ,IBM Systems Journal ,V o l u m e 9, Number 2 (1970), pages 78117. [McDougall and Mauro (2007)] R. McDougall and J. Mauro, Solaris Internals, Second Edition, Prentice Hall (2007). [McKusick and NevilleNeil (2005)] M. K. McKusick and G. V . NevilleNeil, The Design and Implementation of the

NevilleNeil, The Design and Implementation of the FreeBSD UNIX Operating System ,A d d i s o n Wesley (2005). [Navarro et al. (2002)] J. Navarro, S. Lyer, P . Druschel, and A. Cox, Practical, Transparent Operating System Support for Superpages ,Proceedings of the USENIX Symposium on Operating Systems Design and Implementation (2002). [Organick (1972)] E. I. Organick, The Multics System: An Examination of Its Structure ,M I TP r e s s( 1 9 7 2 ) . [Peterson and Norman (1977)] J. L. Peterson and

. [Peterson and Norman (1977)] J. L. Peterson and T. A. Norman, Buddy Sys tems ,Communications of the ACM ,V o l u m e2 0 ,N u m b e r6( 1 9 7 7 ) ,p a g e s4 2 1  4 3 1 . [Prieve and Fabry (1976)] B. G. Prieve and R. S. Fabry, VMINAn Optimal Variable Space PageReplacement Algorithm ,Communications of the ACM , Volume 19, Number 5 (1976), pages 295297. [Purdom, Jr. and Stigler (1970)] P . W. Purdom, Jr. and S. M. Stigler, Statistical Properties of the Buddy System ,J. ACM ,V o l u m e1 7 ,N u m

of the Buddy System ,J. ACM ,V o l u m e1 7 ,N u m b e r4( 1 9 7 0 ) ,p a g e s 683697. [Russinovich and Solomon (2005)] M. E. Russinovich and D. A. Solomon, Microsoft Windows Internals, Fourth Edition, Microsoft Press (2005). [Solomon and Russinovich (2000)] D. A. Solomon and M. E. Russinovich, Inside Microsoft Windows 2000, Third Edition, Microsoft Press (2000). [Stephenson (1983)] C. J. Stephenson, Fast Fits: A New Method for Dynamic Storage Allocation ,Proceedings of the Ninth Symposium on

Allocation ,Proceedings of the Ninth Symposium on Operating Systems Principles (1983), pages 3032. [Wilson et al. (1995)] P. R . Wi l s o n , M . S . J o h n s t o n e , M . N e e l y, a n d D . B o l e s , Dynamic Storage Allocation: A Survey and Critical Review ,Proceedings of the International Workshop on Memory Management (1995), pages 1116. [Wulf (1969)] W. A. Wulf, Performance Monitors for Multiprogramming Sys tems ,Proceedings of the ACM Symposium on Operating Systems Principles (1969),

Symposium on Operating Systems Principles (1969), pages 175181.Part Four Storage Management Since main memory is usually too small to accommodate all the data and programs permanently, the computer system must provide secondary storage to back up main memory. Modern computer systems use disks as the primary online storage me dium for information (both programs and data). The le system provides the mechanism for online storage of and access to both data and programs residing on the disks. A le is

data and programs residing on the disks. A le is a collection of related information dened by its creator. The les are mapped by the operating system onto physical devices. Files are normally organized into directories for ease of use. The devices that attach to a computer vary in many aspects. Some devices transfer a character or a bl ock of characters at a time. Some can be accessed only sequentially, others randomly. Some transfer data synchronously, others asynch ronously. Some are

synchronously, others asynch ronously. Some are dedicated, some shared. They can be readonly or readwrite. They vary greatly in speed. In many ways, they are also the slowest major component of the computer. Because of all this device variation, the operating system needs to provide a wide range of functionality to applications, to allow them to control all aspects of the devices. One key goal of an operating systems IOsubsystem is to provide the simplest interface possible to the rest of the

the simplest interface possible to the rest of the system. Because devices are a performance bottleneck, another key is to optimize IOfor maximum concurrency.10CHAPTER Mass Storage Structure The le system can be viewed logically as consisting of three parts. In Chapter 11, we examine the user and programmer interface to the le system. In Chapter 12, we describe the internal data structures and algorithms used by the operating system to implement this interface. In this chapter, we begin a

this interface. In this chapter, we begin a discussion of le systems at the lowest level: the structure of secondary storage. We rst describe the physical structure of magnetic disks and magnetic tapes. We then describe diskscheduling algorithms, which schedule the order of disk IOs to maximize performance. Next, we discuss disk formatting and management of boot blocks, damaged blocks, and swap space. We conclude with an examination of the structure of RAID systems. CHAPTER OBJECTIVES To

structure of RAID systems. CHAPTER OBJECTIVES To describe the physical structure of secondary storage devices and its effects on the uses of the devices. To explain the performance characteristics of massstorage devices. To evaluate disk scheduling algorithms. To discuss operatingsystem services provided for mass storage, including RAID . 10.1 Overview of MassStorage Structure In this section, we present a general o verview of the physical structure of secondary and tertiary storage devices.

of secondary and tertiary storage devices. 10.1.1 Magnetic Disks Magnetic disks provide the bulk of secondary storage for modern computer systems. Conceptually, disks are relatively simple (Figure 10.1). Each disk platter has a at circular shape, like a CD.C o m m o np l a t t e rd i a m e t e r sr a n g e from 1.8 to 3.5 inches. The two surfaces of a platter are covered with a magnetic material. We store information by recording it magnetically on the platters. 467468 Chapter 10 MassStorage

on the platters. 467468 Chapter 10 MassStorage Structuretrack t sector sspindle cylinder c platter armreadwrite headarm assembly rotationFigure 10.1 Movinghead disk mechanism. Ar e a d  w r i t eh e a d iesjust above each surface of every platter. The heads are attached to a disk arm that moves all the heads as a unit. The surface of a platter is logically divided into circular tracks , which are subdivided into sectors .T h es e to ft r a c k st h a ta r ea to n ea r mp o s i t i o nm a k e su

h a ta r ea to n ea r mp o s i t i o nm a k e su pa cylinder . There may be thousands of concentric cylinders in a disk drive, and each track may contain hundreds of sectors. The storage capacity of common disk drives is measured in gigabytes. When the disk is in use, a drive motor spins it at high speed. Most drives rotate 60 to 250 times per second, specied in terms of rotations per minute (RPM ). Common drives spin at 5,400, 7,200, 10,000, and 15,000 RPM .D i s ks p e e d has two parts. The

15,000 RPM .D i s ks p e e d has two parts. The transfer rate is the rate at which data ow between the drive and the computer. The positioning time ,o rrandomaccess time ,c o n s i s t so f two parts: the time necessary to move the disk arm to the desired cylinder, called the seek time , and the time necessary for the desired sector to rotate to the disk head, called the rotational latency .T y p i c a ld i s k sc a nt r a n s f e rs e v e r a l megabytes of data per second, and they have seek

l megabytes of data per second, and they have seek times and rotational latencies of several milliseconds. Because the disk head ies on an extremely thin cushion of air (measured in microns), there is a danger that the head will make contact with the disk surface. Although the disk platters are coated with a thin protective layer, the head will sometimes damage the magnetic surface. This accident is called a head crash .Ah e a dc r a s hn o r m a l l yc a n n o tb er e p a i r e d ;t h ee n t i

a l l yc a n n o tb er e p a i r e d ;t h ee n t i r ed i s km u s tb e replaced. Ad i s kc a nb e removable ,a l l o w i n gd i f f e r e n td i s k st ob em o u n t e da sn e e d e d . Removable magnetic disks generally consist of one platter, held in a plastic case to prevent damage while not in the disk drive. Other forms of removable disks include CDs,DVD s, and Bluray discs as well as removable ashmemory devices known as ash drives (which are a type of solidstate drive).10.1 Overview of

are a type of solidstate drive).10.1 Overview of MassStorage Structure 469 Ad i s kd r i v ei sa t t a c h e dt oac o m p u t e rb yas e to fw i r e sc a l l e da n IO bus.S e v e r a lk i n d so fb u s e sa r ea v a i l a b l e ,i n c l u d i n g advanced technology attachment (ATA),serial ATA (SATA ),eSATA ,universal serial bus (USB),a n d bre channel (FC).T h ed a t at r a n s f e r so nab u sa r ec a r r i e do u tb ys p e c i a l electronic processors called controllers .T h e host

processors called controllers .T h e host controller is the controller at the computer end of the bus. A disk controller is built into each disk drive. To perform a disk IOoperation, the computer places a command into the host controller, typically using memorymapped IOports, as described in Section 9.7.3. The host controller then sends the command via messages to the disk controller, and the disk controller operates the diskdrive hardware to carry out the command. Disk controllers usually have

out the command. Disk controllers usually have a builtin cache. Data transfer at the disk drive happens between the cache and the disk surface, and data transfer to the host, at fast electronic speeds, occurs between the cache and the host controller. 10.1.2 SolidState Disks Sometimes old technologies are used in new ways as economics change or the technologies evolve. An example is the growing importance of solidstate disks ,o rSSDs.S i m p l yd e s c r i b e d ,a n SSDis nonvolatile memory

l yd e s c r i b e d ,a n SSDis nonvolatile memory that is used like a hard drive. There are many variations of this technology, from DRAM with a battery to allow it to maintain its state in a power failure through ashmemory technologies like singlelevel cell ( SLC)a n dm u l t i l e v e lc e l l( MLC )c h i p s . SSDsh a v et h es a m ec h a r a c t e r i s t i c sa st r a d i t i o n a lh a r dd i s k sb u tc a nb em o r e reliable because they have no moving parts and faster because they have

have no moving parts and faster because they have no seek time or latency. In addition, they consume less power. However, they are more expensive per megabyte than traditional hard disks, have less capacity than the larger hard disks, and may have shorter life spans than hard disks, so their uses are somewhat limited. One use for SSDsi si ns t o r a g ea r r a y s , where they hold lesystem metadata that require high performance. SSDsa r e also used in some laptop computers to make them smaller,

in some laptop computers to make them smaller, faster, and more energyefcient. Because SSDsc a nb em u c hf a s t e rt h a nm a g n e t i cd i s kd r i v e s ,s t a n d a r db u s interfaces can cause a major limit on throughput. Some SSDsa r ed e s i g n e dt o connect directly to the system bus ( PCI,f o re x a m p l e ) . SSDsa r ec h a n g i n go t h e r traditional aspects of computer design as well. Some systems use them as ad i r e c tr e p l a c e m e n tf o rd i s kd r i v e s ,w h i l

p l a c e m e n tf o rd i s kd r i v e s ,w h i l eo t h e r su s et h e ma san e wc a c h e tier, moving data between magnetic disks, SSDs, and memory to optimize performance. In the remainder of this chapter, some sections pertain to SSDs, while others do not. For example, because SSDs have no disk head, diskscheduling algorithms largely do not apply. Throughput and formatting, however, do apply. 10.1.3 Magnetic Tapes Magnetic tape was used as an early secondarystorage medium. Although it is

an early secondarystorage medium. Although it is relatively permanent and can hold large quantities of data, its access time is slow compared with that of main memory and magnetic disk. In addition, random access to magnetic tape is about a thousand times slower than random access to magnetic disk, so tapes are not very useful for secondary storage.470 Chapter 10 MassStorage Structure DISK TRANSFER RATES As with many aspects of computing, published performance numbers for disks are not the same

performance numbers for disks are not the same as realworld performance numbers. Stated transfer rates are always lower than effective transfer rates ,f o re x a m p l e .T h et r a n s f e r rate may be the rate at which bits can be read from the magnetic media by the disk head, but that is different from the rate at which blocks are delivered to the operating system. Tapes are used mainly for backup, for storage of infrequently used information, and as a medium for transferring information

and as a medium for transferring information from one system to another. At a p ei sk e p ti nas p o o la n di sw o u n do rr e w o u n dp a s tar e a dw r i t eh e a d . Moving to the correct spot on a tape can take minutes, but once positioned, tape drives can write data at speeds comparable to disk drives. Tape capacities vary greatly, depending on the particular kind of tape drive, with current capacities exceeding several terabytes. Some tapes h ave builtin compression that can more than

tapes h ave builtin compression that can more than double the effective storage. Tapes and their drivers are usually categorized by width, including 4, 8, and 19 millimeters and 14 and 12 inch. Some are named according to technology, such as LTO5 and SDLT . 10.2 Disk Structure Modern magnetic disk drives are addressed as large onedimensional arrays of logical blocks ,w h e r et h el o g i c a lb l o c ki st h es m a l l e s tu n i to ft r a n s f e r .T h es i z e of a logical block is usually

f e r .T h es i z e of a logical block is usually 512 bytes, although some disks can be lowlevel formatted to have a different logical block size, such as 1,024 bytes. This option is described in Section 10.5.1. The onedimensional array of logical blocks is mapped onto the sectors of the disk sequentially. Sector 0 is the rst sector of the rst track on the outermost cylinder. The mapping proceeds in order through that track, then through the rest of the tracks in that cylinder, and then through

of the tracks in that cylinder, and then through the rest of the cylinders from outermost to innermost. By using this mapping, we canat least in theoryconvert a logical block number into an oldstyle disk address that consists of a cylinder number, a track number within that cylinder, and a sector number within that track. In practice, it is difcult to perform this translation, for two reasons. First, most disks have some defective sectors, but the mapping hides this by substituting spare sectors

mapping hides this by substituting spare sectors from elsewhere on the disk. Second , the number of sectors per track is not a constant on some drives. Lets look more closely at the second reason. On media that use constant linear velocity (CLV ),t h ed e n s i t yo fb i t sp e rt r a c ki su n i f o r m .T h ef a r t h e ra track is from the center of the disk, the greater its length, so the more sectors it can hold. As we move from outer zones to inner zones, the number of sectors per track

to inner zones, the number of sectors per track decreases. Tracks in the outermost zone typically hold 40 percent more sectors than do tracks in the innermost zone. The drive increases its rotation speed as the head moves from the outer to the inner tracks to keep the same rate of data moving under the head. This method is used in CDROM10.3 Disk Attachment 471 and DVDROM drives. Alternatively, the disk rotation speed can stay constant; in this case, the density of bits decreases from inner

case, the density of bits decreases from inner tracks to outer tracks to keep the data rate constant. This method is used in hard disks and is known as constant angular velocity (CA V). The number of sectors per track has been increasing as disk technology improves, and the outer zone of a disk usually has several hundred sectors per track. Similarly, the number of cylinders per disk has been increasing; large disks have tens of thousands of cylinders. 10.3 Disk Attachment Computers access disk

10.3 Disk Attachment Computers access disk storage in two ways. One way is via IO ports (or hostattached storage ); this is common on small systems. The other way is via ar e m o t eh o s ti nad i s t r i b u t e d l es y s t e m ;t h i si sr e f e r r e dt oa s networkattached storage . 10.3.1 HostAttached Storage Hostattached storage is storage accessed through local IOports. These ports use several technologies. The typical desktop PCuses an IObus architecture called IDEorATA.T h i sa r c h i

architecture called IDEorATA.T h i sa r c h i t e c t u r es u p p o r t sam a x i m u mo ft w od r i v e sp e r IO bus. A newer, similar protocol that has simplied cabling is SATA . Highend workstations and servers gen erally use more sophisticated IO architectures such as bre channel ( FC), a highspeed serial architecture that can operate over optical ber or over a fourconductor copper cable. It has two variants. One is a large switched fabric having a 24bit address space. This variant is

having a 24bit address space. This variant is expected to dominate in the future and is the basis of storagearea networks (SAN s),d i s c u s s e di nS e c t i o n1 0 . 3 . 3 .B e c a u s eo ft h el a r g ea d d r e s ss p a c e and the switched nature of the communication, multiple hosts and storage devices can attach to the fabric, allowing great exibility in IOcommunication. The other FCvariant is an arbitrated loop (FCAL )that can address 126 devices (drives and controllers). Aw i d ev a r i

devices (drives and controllers). Aw i d ev a r i e t yo fs t o r a g ed e v i c e sa r es u i t a b l ef o ru s ea sh o s t  a t t a c h e d storage. Among these are hard disk drives, RAID arrays, and CD,DVD ,a n d tape drives. The IOcommands that initiate data transfers to a hostattached storage device are reads and writes of logic al data blocks directed to specically identied storage units (such as bus IDor target logical unit). 10.3.2 NetworkAttached Storage An e t w o r k  a t t a c h e ds

Storage An e t w o r k  a t t a c h e ds t o r a g e( NAS)d e v i c ei sas p e c i a l  p u r p o s es t o r a g es y s t e m that is accessed remotely over a data network (Figure 10.2). Clients access networkattached storage via a remoteprocedurecall interface such as NFS forUNIX systems or CIFS for Windows machines. The remote procedure calls (RPCs) are carried via TCP orUDP over an IPnetworkusually the same local area network ( LAN )t h a tc a r r i e sa l ld a t at r a f  ct ot h ec l i e n

a r r i e sa l ld a t at r a f  ct ot h ec l i e n t s .T h u s ,i tm a yb e easiest to think of NAS as simply another storageaccess protocol. The network attached storage unit is usually implemented as a RAID array with software that implements the RPC interface.472 Chapter 10 MassStorage StructureNASclient NAS clientclientLANWAN Figure 10.2 Networkattached storage. Networkattached storage provides a conven ient way for all the computers on a LAN to share a pool of storage with the same ease of

to share a pool of storage with the same ease of naming and access enjoyed with local hostattached storage. How ever, it tends to be less efcient and have lower performance than some directattached storage options. iSCSI is the latest networkattached storage pr otocol. In essence, it uses the IPnetwork protocol to carry the SCSI protocol. Thus, networksrather than SCSI cablescan be used as the interconnects between hosts and their storage. As a result, hosts can treat their storage as if it were

hosts can treat their storage as if it were directly attached, even if the storage is distant from the host. 10.3.3 StorageArea Network One drawback of networkattached storage systems is that the storage IO operations consume bandwidth on the data network, thereby increasing the latency of network communication. This problem can be particularly acute in large clientserver installationsthe communication between servers and clients competes for bandwidth with the communication among servers and

bandwidth with the communication among servers and storage devices. As t o r a g e  a r e an e t w o r k( SAN) is a private network (using storage protocols rather than networking protocols) connecting servers and storage units, as shown in Figure 10.3. The power of a SAN lies in its exibility. Multiple hosts and multiple storage arrays can attach to the same SAN,a n ds t o r a g ec a n be dynamically allocated to hosts. A SAN switch allows or prohibits access between the hosts and the storage.

access between the hosts and the storage. As one example, if a host is running low on disk space, the SAN can be congured to allocate more storage to that host. SANs make it possible for clusters of servers to share the same storage and for storage arrays to include multiple direct host connections. SANst y p i c a l l yh a v e more portsas well as more expensive portsthan storage arrays. FCis the most common SAN interconnect, although the simplicity of i SCSI is increasing its use. Another SAN

of i SCSI is increasing its use. Another SAN interconnect is InniBand  a specialpurpose bus architecture that provides hardware and software support for highspeed interconnection networks for servers and storage units. 10.4 Disk Scheduling One of the responsibilities of the operating system is to use the hardware efciently. For the disk drives, meeting this responsibility entails having fast10.4 Disk Scheduling 473LANWAN storage arraystorage array dataprocessing center web content

array dataprocessing center web content providerserverclient client clientserver tape librarySAN Figure 10.3 Storagearea network. access time and large disk bandwidth. For magnetic disks, the access time has two major components, as mentioned in Section 10.1.1. The seek time is the time for the disk arm to move the heads to the cylinder containing the desired sector. The rotational latency is the additional time for the disk to rotate the desired sector to the disk head. The disk bandwidth is

sector to the disk head. The disk bandwidth is the total number of bytes transferred, divided by the total time between the rst request for service and the completion of the last transfer. We can improve both the access time and the bandwidth by managing the order in which disk IOrequests are serviced. Whenever a process needs IOto or from the disk, it issues a system call to the operating system. The request species several pieces of information: Whether this operation is input or output What

Whether this operation is input or output What the disk address for the transfer is What the memory address for the transfer is What the number of sectors to be transferred is If the desired disk drive and controller are available, the request can be serviced immediately. If the drive or controller is busy, any new requests for service will be placed in the queue of pending requests for that drive. For a multiprogramming system with many processes, the disk queue may often have several pending

the disk queue may often have several pending requests. Thus, when one request is completed, the operating system chooses which pending request to service next. How does the operating system make this choice? Any one of several diskscheduling algorithms can be used, and we discuss them next. 10.4.1 FCFS Scheduling The simplest form of disk schedulin g is, of course, the rstcome, rstserved (FCFS )a l g o r i t h m .T h i sa l g o r i t h mi si n t r i n s i c a l l yf a i r ,b u ti tg e n e r a l

r i n s i c a l l yf a i r ,b u ti tg e n e r a l l yd o e sn o t provide the fastest service. Consider, for example, a disk queue with requests forIOto blocks on cylinders 98, 183, 37, 122, 14, 124, 65, 67,474 Chapter 10 MassStorage Structure 01 4 37536567 98122124 183199queue H11005 98, 183, 37, 122, 14, 124, 65, 67 head starts at 53 Figure 10.4 FCFS disk scheduling. in that order. If the disk head is initially at cylinder 53, it will rst move from 53 to 98, then to 183, 37, 122, 14, 124, 65,

from 53 to 98, then to 183, 37, 122, 14, 124, 65, and nally to 67, for a total head movement of 640 cylinders. This schedule is diagrammed in Figure 10.4. The wild swing from 122 to 14 and then back to 124 illustrates the problem with this schedule. If the requests for cylinders 37 and 14 could be serviced together, before or after the requests for 122 and 124, the total head movement could be decreased substantially, and performance could be thereby improved. 10.4.2 SSTF Scheduling It seems

thereby improved. 10.4.2 SSTF Scheduling It seems reasonable to service all the r equests close to the current head position before moving the head far away to service other requests. This assumption is the basis for the shortestseektimerst ( SSTF )a l g o r i t h m .T h e SSTF algorithm selects the request with the least seek time from the current head position. In other words, SSTF chooses the pending request c losest to the current head position. For our example request queue, the closest

For our example request queue, the closest request to the initial head position (53) is at cylinder 65. Once we are at cylinder 65, the next closest request is at cylinder 67. From there, th er e q u e s ta tc y l i n d e r3 7i sc l o s e rt h a nt h e one at 98, so 37 is served next. Continuing, we service the request at cylinder 14, then 98, 122, 124, and nally 183 (Figure 10.5). This scheduling method results in a total head movement of only 236 cylinderslittle more than onethird of the

only 236 cylinderslittle more than onethird of the distance needed for FCFS scheduling of this request queue. Clearly, this algorithm gives a substantial improvement in performance. SSTF scheduling is essentially a form of shortestjobrst ( SJF)s c h e d u l i n g ; and like SJFscheduling, it may cause starvation of some requests. Remember that requests may arrive at any time. Suppose that we have two requests in the queue, for cylinders 14 and 186, and while the request from 14 is being

14 and 186, and while the request from 14 is being serviced, a new request near 14 arrives. This new request will be serviced next, making the request at 186 wait. While this request is being serviced, another request close to 14 could arrive. In th eory, a continual stream of requests near one another could cause the request for cylinder 186 to wait indenitely.10.4 Disk Scheduling 475 01 4 37536567 98122124 183199queue H11005 98, 183, 37, 122, 14, 124, 65, 67 head starts at 53 Figure 10.5 SSTF

14, 124, 65, 67 head starts at 53 Figure 10.5 SSTF disk scheduling. This scenario becomes increasingly likely as the pendingrequest queue grows longer. Although the SSTF algorithm is a substantial improvement over the FCFS algorithm, it is not optimal. In the example, we can do better by moving the head from 53 to 37, even though the latter is not closest, and then to 14, before turning around to service 65, 67, 98, 122, 124, and 183. This strategy reduces the total head movement to 208

strategy reduces the total head movement to 208 cylinders. 10.4.3 SCAN Scheduling In the SCAN algorithm ,t h ed i s ka r ms t a r t sa to n ee n do ft h ed i s ka n dm o v e s toward the other end, servicing requests as it reaches each cylinder, until it gets to the other end of the disk. At the oth er end, the direction of head movement is reversed, and servicing continues. The h ead continuously scans back and forth across the disk. The SCAN algorithm is sometimes called the elevator algorithm

is sometimes called the elevator algorithm ,s i n c et h ed i s ka r mb e h a v e sj u s tl i k ea ne l e v a t o ri nab u i l d i n g , r s t servicing all the requests going up and then reversing to service requests the other way. Lets return to our example to illustrate. Before applying SCAN to schedule the requests on cylinders 98, 183, 37, 122, 14, 124, 65, and 67, we need to know the direction of head movement in addition to the heads current position. Assuming that the disk arm is moving

position. Assuming that the disk arm is moving toward 0 and that the initial head position is again 53, the head will next service 37 and then 14. At cylinder 0, the arm will reverse and will move toward t he other end of the disk, servicing the requests at 65, 67, 98, 122, 124, and 183 (Figure 10.6). If a request arrives in the queue just in front of the head, it will be serviced almost immediately; a request arriving just behind the head will have to wait until the arm moves to the end of the

have to wait until the arm moves to the end of the disk, reverses direction, and comes back. Assuming a uniform distribution of requests for cylinders, consider the density of requests when the head reaches one end and reverses direction. At this point, relatively few requests are immediately in front of the head, since these cylinders have recently been servic ed. The heaviest density of requests476 Chapter 10 MassStorage Structure 01 4 37536567 98122124 183199queue H11005 98, 183, 37, 122, 14,

98122124 183199queue H11005 98, 183, 37, 122, 14, 124, 65, 67 head starts at 53 Figure 10.6 SCAN disk scheduling. is at the other end of the disk. These r equests have also waited the longest, so why not go there rst? That is the idea of the next algorithm. 10.4.4 CSCAN Scheduling Circular SCAN (CSCAN )s c h e d u l i n g is a variant of SCAN designed to provide am o r eu n i f o r mw a i tt i m e .L i k e SCAN ,CSCAN moves the head from one end of the disk to the other, servicing requests along

of the disk to the other, servicing requests along the way. When the head reaches the other end, however, it immediately returns to the beginning of the disk without servicing any requests on the return trip (Figure 10.7). The CSCAN scheduling algorithm essentially treats the cylinders as a circular list that wraps around from the nal cylinder to the rst one. 01 4 37536567 98122124 183199queue  98, 183, 37, 122, 14, 124, 65, 67 head starts at 53 Figure 10.7 CSCAN disk scheduling.10.4 Disk

at 53 Figure 10.7 CSCAN disk scheduling.10.4 Disk Scheduling 477 10.4.5 LOOK Scheduling As we described them, both SCAN and CSCAN move the disk arm across the full width of the disk. In practice, neither algorithm is often implemented this way. More commonly, the arm goes only as far as the nal request in each direction. Then, it reverses direction immediately, without going all the way to the end of the disk. Versions of SCAN and CSCAN that follow this pattern are called LOOK and CLOOK

that follow this pattern are called LOOK and CLOOK scheduling ,b e c a u s et h e y look for a request before continuing to move in a given direction (Figure 10.8). 10.4.6 Selection of a DiskScheduling Algorithm Given so many diskscheduling algorithms, how do we choose the best one? SSTF is common and has a natural appeal because it increases performance over FCFS .SCAN and CSCAN perform better for systems that place a heavy load on the disk, because they are less likely to cause a starvation

because they are less likely to cause a starvation problem. For any particular list of requests, we can dene an optimal order of retrieval, but the computation needed to nd an optimal schedule may not justify the savings over SSTF orSCAN .W i t ha n ys c h e d u l i n ga l g o r i t h m ,h o w e v e r ,p e r f o r m a n c e depends heavily on the number and types of requests. For instance, suppose that the queue usually has just one outstanding request. Then, all scheduling algorithms behave the

Then, all scheduling algorithms behave the same, because they have only one choice of where to move the disk head: they all behave like FCFS scheduling. Requests for disk service can be greatly inuenced by the leallocation method. A program reading a contiguously allocated le will generate several requests that are close together on the disk, resulting in limited head movement. Al i n k e do ri n d e x e d l e ,i nc o n t r a s t ,m a yi n c l u d eb l o c k st h a ta r ew i d e l y scattered on

d eb l o c k st h a ta r ew i d e l y scattered on the disk, resulting in greater head movement. The location of directories and ind ex blocks is also important. Since every le must be opened to be used, and opening a le requires searching the directory structure, the directories will be accessed frequently. Suppose that a directory entry is on the rst cylinder and a les data are on the nal cylinder. In this case, the disk head has to move the entire width of the disk. If the directory 01 4

entire width of the disk. If the directory 01 4 37536567 98122124 183199queue  98, 183, 37, 122, 14, 124, 65, 67 head starts at 53 Figure 10.8 CLOOK disk scheduling.478 Chapter 10 MassStorage Structure DISK SCHEDULING and SSDs The diskscheduling algorithms discussed in this section focus primarily on minimizing the amount of disk head movement in magnetic disk drives. SSDswhich do not contain moving disk headscommonly use a simple FCFS policy. For example, the Linux Noop scheduler uses an FCFS

For example, the Linux Noop scheduler uses an FCFS policy but modies it to merge adjacent requests. The observed behavior of SSDs indicates that the time required to service reads is uniform but that, because of the properties of ash memory, write service time is not uniform. Some SSD schedulers have exploited this property and merge only adjacent write requests, servicing all read requests in FCFS order. entry were on the middle cylinder, the h ead would have to move only onehalf the width.

h ead would have to move only onehalf the width. Caching the directories and index blocks in main memory can also help to reduce diskarm movement, particularly for read requests. Because of these complexities, the diskscheduling algorithm should be written as a separate module of the operating system, so that it can be replaced with a different algorithm if necessary. Either SSTF orLOOK is a reasonable choice for the default algorithm. The scheduling algorithms described here consider only the

algorithms described here consider only the seek distances. For modern disks, the rotational latency can be nearly as large as the average seek time. It is difcult for the operating system to schedule for improved rotational latency, though, because modern disks do not disclose the physical location of logical blocks. Disk manufacturers have been alleviating this problem by implementing diskscheduling algorithms in the controller hardware built into the disk drive. If the operating system sends

into the disk drive. If the operating system sends a batch of requests to the controller, the controller can queue them and then schedule them to improve both the seek time and the rotational latency. IfIO performance were the only consideration, the operating system would gladly turn over the responsibility of disk scheduling to the disk hard ware. In practice, however, the operating system may have other constraints on the service order for requests. For instance, demand paging may take

for requests. For instance, demand paging may take priority over application IO, and writes are more urgent than reads if the cache is running out of free pages. Also, it may be desirable to guarantee the order of a set of disk writes to make the le system robust in the face of system crashes. Consider what could happen if the operating system allocated a disk page to a le and the application wrote data into that page before the operating system had a chance to ush the le system metadata back to

had a chance to ush the le system metadata back to disk. To accommodate such requirements, an operating system may choose to do its own disk scheduling and to spoonfeed the requests to the disk controller, one by one, for some types of IO. 10.5 Disk Management The operating system is responsible for several other aspects of disk manage ment, too. Here we discuss disk initialization, booting from disk, and badblock recovery.10.5 Disk Management 479 10.5.1 Disk Formatting An e wm a g n e t i cd i

10.5.1 Disk Formatting An e wm a g n e t i cd i s ki sab l a n ks l a t e :i ti sj u s tap l a t t e ro fam a g n e t i cr e c o r d i n g material. Before a disk can store data, it must be divided into sectors that the disk controller can read and write. This process is called lowlevel formatting , orphysical formatting .L o w  l e v e lf o r m a t t i n g l l st h ed i s kw i t has p e c i a ld a t a structure for each sector. The data structure for a sector typically consists of a header, a

for a sector typically consists of a header, a data area (usually 512 bytes in size), and a trailer. The header and trailer contain information used by the disk controller, such as a sector number and an errorcorrecting code (ECC). When the controller writes a sector of data during normal IO,t h e ECC is updated with a value calculated from all the bytes in the data area. When the sector is read, the ECC is recalculated and compared with the stored value. If the stored and calculated numbers are

value. If the stored and calculated numbers are different, this mismatch indicates that the data area of the sector has become corrupted and that the disk sector may be bad (Section 10.5.3). The ECC is an errorcorrecting code because it contains enough information, if only a few bits of data have been corrupted, to enable the controller to identify which bits have changed and calculate what their correct values should be. It then reports a recoverable soft error . The controller automatically

soft error . The controller automatically does the ECC processing whenever a sector is read or written. Most hard disks are lowlevelformatted at the factory as a part of the manufacturing process. This formatting enables the manufacturer to test the disk and to initialize the mapping from logical block numbers to defectfree sectors on the disk. For many hard disks, when the disk controller is instructed to lowlevelformat the disk, it can also be told how many bytes of data space to leave between

told how many bytes of data space to leave between the header and trailer of all sectors. It is usually possible to choose among a few sizes, such as 256, 512, and 1,024 bytes. Formatting a disk with a larger sector size means that fewer sectors can t on each track; but it also means that fewer headers and trailers are written on each track and more space is available for user data. Some operating systems can handle only a sector size of 512 bytes. Before it can use a disk to hold les, the

bytes. Before it can use a disk to hold les, the operating system still needs to record its own data structures on the disk. It does so in two steps. The rst step is to partition the disk into one or more groups of cylinders. The operating system can treat each partition as though it were a separate disk. For instance, one partition can hold a copy of the operating systems executable code, while another holds user les. The second step is logical formatting ,o rc r e a t i o no fa le system. In

formatting ,o rc r e a t i o no fa le system. In this step, the operating system stores the initial lesystem data structures onto the disk. These data structures may include maps of free and allocated space and an initial empty directory. To increase efciency, most le systems group blocks together into larger chunks, frequently called clusters .D i s k IOis done via blocks, but le system IOis done via clusters, effectively assuring that IOhas more sequentialaccess and fewer randomaccess

IOhas more sequentialaccess and fewer randomaccess characteristics. Some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks, without any lesystem data structures. This array is sometimes called the raw disk, and IOto this array is termed raw IO.For example, some database systems prefer raw IO because it enables them to control the exact disk location where each database record is stored. Raw IObypasses all the lesystem

record is stored. Raw IObypasses all the lesystem services, such480 Chapter 10 MassStorage Structure as the buffer cache, le locking, prefetching, space allocation, le names, and directories. We can make certain applications more efcient by allowing them to implement their own specialpurpose storage services on a raw partition, but most applications perform better when they use the regular lesystem services. 10.5.2 Boot Block For a computer to start runningfor instance, when it is powered up or

runningfor instance, when it is powered up or rebootedit must have an initial program to run. This initial bootstrap program tends to be simple. It initializes all aspects of the system, from CPU registers to device controllers and the contents of main memory, and then starts the operating system. To do its job, the bootstrap program nds the operatingsystem kernel on disk, loads that kernel into memory, and jumps to an initial address to begin the operatingsystem execution. For most computers,

the operatingsystem execution. For most computers, the bootstrap is stored in readonly memory (ROM ). This location is convenient, because ROM needs no initialization and is at a xed location that the processor can start executing when powered up or reset. And, since ROM is read only, it cannot be infected by a computer virus. The problem is that changing this bootstrap code requires changing the ROM hardware chips. For this reason, most systems store a tiny bootstrap loader program in the boot

store a tiny bootstrap loader program in the boot ROM whose only job is to bring in a full bootstrap program from disk. The full bootstrap program can be changed easily: a new version is simply written onto the disk. The full bootstrap program is stored in the boot blocks at a xed location on the disk. A disk that has a boot partition is called a boot disk or system disk . The code in the boot ROM instructs the disk controller to read the boot blocks into memory (no device drivers are loaded at

into memory (no device drivers are loaded at this point) and then starts executing that code. The full bootstrap pro gram is more sophisticated than the bootstrap loader in the boot ROM .I ti sa b l et ol o a dt h ee n t i r eo p e r a t i n gs y s t e m from a nonxed location on disk and to start the operating system running. Even so, the full bootstrap code may be small. Lets consider as an example the boot process in Windows. First, note that Windows allows a hard disk to be divided into

that Windows allows a hard disk to be divided into partitions, and one partition identied as the boot partition contains the operating system and device drivers. The Windows system places its boot code in the rst sector on the hard disk, which it terms the master boot record ,o rMBR .B o o t i n gb e g i n sb yr u n n i n g code that is resident in the systems ROM memory. This code directs the system to read the boot code from the MBR . In addition to containing boot code, the MBR contains a

to containing boot code, the MBR contains a table listing the partitions for the hard disk and a ag indicating which partition the system is to be booted from, as illustrated in Figure 10.9. Once the system identies the boot partition, it reads the rst sector from that partition (which is called the boot sector ) and continues with the remainder of the boot process, which includes loading the various subsystems and system services. 10.5.3 Bad Blocks Because disks have moving parts and small

Blocks Because disks have moving parts and small tolerances (recall that the disk head ies just above the disk surface), they are prone to failure. Sometimes the failure is complete; in this case, the disk needs to be replaced and its contents10.5 Disk Management 481 MBR partition 1 partition 2 partition 3 partition 4boot code partition table boot partitionFigure 10.9 Booting from disk in Windows. restored from backup media to the new disk. More frequently, one or more sectors become defective.

frequently, one or more sectors become defective. Most disks even come from the factory with bad blocks . Depending on the disk and controller in use, these blocks are handled in a variety of ways. On simple disks, such as some disks with IDEcontrollers, bad blocks are handled manually. One strategy is to scan the disk to nd bad blocks while the disk is being formatted. Any bad blocks that are discovered are agged as unusable so that the le system does not allocate them. If blocks go bad during

does not allocate them. If blocks go bad during normal operation, a special program (such as the Linux badblocks command) must be run manually to search for the bad blocks and to lock them away. Data that resided on the bad blocks usually are lost. More sophisticated disks are smarter about badblock recovery. The con troller maintains a list of bad blocks on the disk. The list is initialized during the lowlevel formatting at the factory an di su p d a t e do v e rt h el i f eo ft h ed i s k .

su p d a t e do v e rt h el i f eo ft h ed i s k . Lowlevel formatting also sets aside spare sectors not visible to the operating system. The controller can be told to replace each bad sector logically with one of the spare sectors. This scheme is known as sector sparing orforwarding . At y p i c a lb a d  s e c t o rt r a n s a c t i o nm i g h tb ea sf o l l o w s : The operating system tries to read logical block 87. The controller calculates the ECC and nds that the sector is bad. It reports

the ECC and nds that the sector is bad. It reports this nding to the operating system. The next time the system is rebooted, a special command is run to tell the controller to replace the bad sector with a spare. After that, whenever the system requests logical block 87, the request is translated into the replacement sectors address by the controller. Note that such a redirection by the controller could invalidate any opti mization by the operating systems diskscheduling algorithm! For this

systems diskscheduling algorithm! For this reason, most disks are formatted to provide a few spare sectors in each cylinder and as p a r ec y l i n d e ra sw e l l .W h e nab a db l o c ki sr e m a p p e d ,t h ec o n t r o l l e ru s e sa spare sector from the same cylinder, if possible. As an alternative to sector sparing, some controllers can be instructed to replace a bad block by sector slipping .H e r ei sa ne x a m p l e :S u p p o s et h a t482 Chapter 10 MassStorage Structure logical

h a t482 Chapter 10 MassStorage Structure logical block 17 becomes defective and the rst available spare follows sector 202. Sector slipping then remaps all the sectors from 17 to 202, moving them all down one spot. That is, sector 202 is copied into the spare, then sector 201 into 202, then 200 into 201, and so on, until sector 18 is copied into sector 19. Slipping the sectors in this way frees up the space of sector 18 so that sector 17 can be mapped to it. The replacement of a bad block

be mapped to it. The replacement of a bad block generally is not totally automatic, because the data in the bad block are usually lost. Soft errors may trigger a process in which a copy of the block data is made and the block is spared or slipped. An unrecoverable hard error ,h o w e v e r ,r e s u l t si nl o s td a t a .W h a t e v e r l ew a s using that block must be repaired (for instance, by restoration from a backup tape), and that requires manual intervention. 10.6 SwapSpace Management

manual intervention. 10.6 SwapSpace Management Swapping was rst presented in Section 8.2, where we discussed moving entire processes between disk and m ain memory. Swapping in that setting occurs when the amount of physical memory reaches a critically low point and processes are moved from memory to swap space to free available memory. In practice, very few modern operating systems implement swapping in this fashion. Rather, systems now combine swapping with virtual memory techniques (Chapter 9)

with virtual memory techniques (Chapter 9) and swap pages, no tn e c e s s a r i l ye n t i r ep r o c e s s e s .I nf a c t , some systems now use the terms swapping and paging interchangeably, reecting the merging of these two concepts. Swapspace management is another lowlevel task of the operating system. Virtual memory uses disk space as an extension of main memory. Since disk access is much slower than memory access, using swap space signicantly decreases system performance. The main goal

decreases system performance. The main goal for the design and implementation of swap space is to provide the best throughput for the virtual memory system. In this section, we discuss how swap space is used, where swap space is located on disk, and how swap space is managed. 10.6.1 SwapSpace Use Swap space is used in various ways by different operating systems, depending on the memorymanagement algorithms in use. For instance, systems that implement swapping may use swap space to hold an entire

swapping may use swap space to hold an entire process image, including the code and data segments. Paging systems may simply store pages that have been pushed out of main memory. The amount of swap space needed on a system can therefore vary from a few megabytes of disk space to gigabytes, depending on the amount of physical memory, the amount of virtual memory it is backing, and the way in which the virtual memory is used. Note that it may be safer to overestimate than to underestimate the

be safer to overestimate than to underestimate the amount of swap space required, because if a system runs out of swap space it may be forced to abort processes or may crash entirely. Overestimation wastes disk space that could otherwise be used for les, but it does no other harm. Some systems recommend the amount to be set aside for swap space. Solaris, for example, suggests setting swap space equal to the amount by which virtual memory exceeds pageable physical memory. In the past, Linux has

pageable physical memory. In the past, Linux has suggested10.6 SwapSpace Management 483 setting swap space to double the amount of physical memory. Today, that limitation is gone, and most Linux systems use considerably less swap space. Some operating systemsincluding Linuxallow the use of multiple swap spaces, including both les and dedicated swap partitions. These swap spaces are usually placed on separate disks so that the load placed on the IO system by paging and swapping can be spread over

system by paging and swapping can be spread over the systems IO bandwidth. 10.6.2 SwapSpace Location As w a ps p a c ec a nr e s i d ei no n eo ft w op l a c e s :i tc a nb ec a r v e do u to ft h e normal le system, or it can be in a separate disk partition. If the swap space is simply a large le within the le system, normal lesystem routines can be used to create it, name it, and allocate its space. This approach, though easy to implement, is inefcient. Navigating the directory structure and

inefcient. Navigating the directory structure and the disk allocation data structures takes time and (possibly) extra disk accesses. External fragmentation can greatly increase swapping times by forcing multiple seeks during reading or writing of a process image. We can improve performance by caching the block location information in physical memory and by using special tools to allocate physically contiguous blocks for the swap le, but the cost of traversing the lesystem data structures

cost of traversing the lesystem data structures remains. Alternatively, swap space can be created in a separate raw partition .N o le system or directory structure is placed in this space. Rather, a separate swapspace storage manager is used to allocate and deallocate the blocks from the raw partition. This manager uses algorithms optimized for speed rather than for storage efciency, because swap space is accessed much more frequently than le systems (when it is used). Internal fragmentation may

(when it is used). Internal fragmentation may increase, but this tradeoff is acceptable because the life of data in the swap space generally is much shorter than that of les in the le system. Since swap space is reinitialized at boot time, any fragmentation is shortlived. The rawpartition approach creates a xed amount of swap space during disk partitioning. Adding more swap space requires either repartitioning the disk (which involves moving the other lesystem partitions or destroying them and

other lesystem partitions or destroying them and restoring them from backup) or addin ga n o t h e rs w a ps p a c ee l s e w h e r e . Some operating systems are exible and can swap both in raw partitions and in lesystem space. Linux is an example: the policy and implementation are separate, allowing the machines administrator to decide which type of swapping to use. The tradeoff is between the convenience of allocation and management in the le system and the performance of swapping in raw

le system and the performance of swapping in raw partitions. 10.6.3 SwapSpace Management: An Example We can illustrate how swap space is used by following the evolution of swapping and paging in various UNIX systems. The traditional UNIX kernel started with an implementation of swapping that copied entire processes between contiguous disk regions and memory. UNIX later evolved to a combination of swapping and paging as paging hardware became available. In Solaris 1 ( SunOS), the designers

available. In Solaris 1 ( SunOS), the designers changed standard UNIX methods to improve efciency and reect technological developments. When a process executes, textsegment pages containing code are brought in from the le484 Chapter 10 MassStorage Structure swap area page slot swap partition or swap file swap map 10301 Figure 10.10 The data structures for swapping on Linux systems. system, accessed in main memory, and thrown away if selected for pageout. It is more efcient to reread a page from

pageout. It is more efcient to reread a page from the le system than to write it to swap space and then reread it from there. Swap space is only used as a backing store for pages of anonymous memory, which includes memory allocated for the stack, heap, and uninitialized data of a process. More changes were made in later versions of Solaris. The biggest change is that Solaris now allocates swap space only when a page is forced out of physical memory, rather than when the virtual memory page is

rather than when the virtual memory page is rst created. This scheme gives better performance on modern computers, which have more physical memory than older systems and tend to page less. Linux is similar to Solaris in that swap space is used only for anonymous memorythat is, memory not backed by any le. Linux allows one or more swap areas to be established. A swap area may be in either a swap le on a regular le system or a dedicated swap partition. Each swap area consists of a series of 4

Each swap area consists of a series of 4 KBpage slots ,w h i c ha r eu s e dt oh o l ds w a p p e dp a g e s .A s s o c i a t e d with each swap area is a swap map an array of integer counters, each corresponding to a page slot in the swap area. If the value of a counter is 0, the corresponding page slot is available. Values greater than 0 indicate that the page slot is occupied by a swapped page. The value of the counter indicates the number of mappings to the swapped page. For example, a value

mappings to the swapped page. For example, a value of 3 indicates that the swapped page is mapped to three different processes (which can occur if the swapped page is storing a region of memory shared by three processes). The data structures for swapping on Linux systems are shown in Figure 10.10. 10.7 RAID Structure Disk drives have continued to get smaller and cheaper, so it is now econom ically feasible to attach many disks to a computer system. Having a large number of disks in a system

system. Having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written, if the disks are operated in parallel. Furthermore, this setup offers the potential for improving the reliability of data storage, because redundant information can be stored on multiple disks. Thus, failure of one disk does not lead to loss of data. A variety of diskorganization techniques, collectively called redundant arrays of independent disks (RAID ), are

redundant arrays of independent disks (RAID ), are commonly used to address the performance and reliability issues. In the past, RAID sc o m p o s e do fs m a l l ,c h e a pd i s k sw e r ev i e w e da sa costeffective alternative to large, expensive disks. Today, RAID sa r eu s e df o r10.7 RAID Structure 485 STRUCTURING RAID RAID storage can be structured in a variety of ways. For example, a system can have disks directly attached to its buses. In this case, the operating system or system

In this case, the operating system or system software can implement RAID functionality. Alternatively, an intelligent host controller can control multiple attached disks and can implement RAID on those disks in hardware. Finally, a storage array ,o rRAID array ,c a nb eu s e d .A RAID array is a standalone unit with its own controller, cache (usually), and disks. It is attached to the host via one or more standard controllers (for example, FC). This common setup allows an operating system or

This common setup allows an operating system or software without RAID functionality to have RAID protected disks. It is even used on systems that do have RAID software layers because of its simplicity and exibility. their higher reliability and higher datatransfer rate, rather than for economic reasons. Hence, the IinRAID ,which once stood for inexpensive, now stands forindependent.  10.7.1 Improvement of Reliability via Redundancy Lets rst consider the reliability of RAID s. The chance that

the reliability of RAID s. The chance that some disk out of a set of Ndisks will fail is much higher than the chance that a specic single disk will fail. Suppose that the mean time to failure of a single disk is 100,000 hours. Then the mean time to failure of some disk in an array of 100 disks will be 100,000100  1,000 hours, or 41 .66 days, which is not long at all! If we store only one copy of the data, then each disk failure will result in loss of a signicant amount of dataand such a high

loss of a signicant amount of dataand such a high rate of data loss is unacceptable. The solution to the problem of reliability is to introduce redundancy ;w e store extra information that is not normally needed but that can be used in the event of failure of a disk to rebuild the lost information. Thus, even if a disk fails, data are not lost. The simplest (but most expensive) approac h to introducing redundancy is to duplicate every disk. This technique is called mirroring .W i t hm i r r o r

technique is called mirroring .W i t hm i r r o r i n g ,a logical disk consists of two physical disks, and every write is carried out on both disks. The result is called a mirrored volume .I fo n eo ft h ed i s k si nt h e volume fails, the data can be read from the other. Data will be lost only if the second disk fails before the rst failed disk is replaced. The mean time to failure of a mirrored volumewhere failure is the loss of datadepends on two factors. One is the mean time to failure of

on two factors. One is the mean time to failure of the individual disks. The other is the mean time to repair ,w h i c hi st h et i m ei tt a k e s( o n average) to replace a failed disk and to restore the data on it. Suppose that the failures of the two disks are independent; that is, the failure of one disk is not connected to the failure of the other. Then, if the mean time to failure of a single disk is 100,000 hours and the mean time to repair is 10 hours, the mean time to data loss of a

is 10 hours, the mean time to data loss of a mirrored disk system is 100 ,0002(210)500106hours, or 57,000 years!486 Chapter 10 MassStorage Structure You should be aware that we cannot really assume that disk failures will be independent. Power failures and natural disasters, such as earthquakes, res, and oods, may result in damage to both disks at the same time. Also, manufacturing defects in a batch of disks can cause correlated failures. As disks age, the probability of failure grows,

As disks age, the probability of failure grows, increasing the chance that a second disk will fail while the rst is being repaired. In spite of all these considerations, however, mirroreddisk systems offer much higher reliability than do singledisk systems. Power failures are a particular source of concern, since they occur far more frequently than do natural disasters. Even with mirroring of disks, if writes are in progress to the same block in both disks, and power fails before both blocks are

both disks, and power fails before both blocks are fully written, the two blocks can be in an inconsistent state. One solution to this problem is to write one copy rst, then the next. Another is to add a solidstate nonvolatile RAM (NVRAM )cache to the RAID array. This writeback cache is protected from data loss during power failures, so the write can be considered complete at that point, assuming the NVRAM has some kind of error protection and correction, such as ECC or mirroring. 10.7.2

and correction, such as ECC or mirroring. 10.7.2 Improvement in Performance via Parallelism Now lets consider how parallel access to multiple disks improves perfor mance. With disk mirroring, the rate at which read requests can be handled is doubled, since read requests can be sent to either disk (as long as both disks in a pair are functional, as is almost always the case). The transfer rate of each read is the same as in a singledisk system, but the number of reads per unit time has doubled.

but the number of reads per unit time has doubled. With multiple disks, we can improve the transfer rate as well (or instead) by striping data across the disks. In its simplest form, data striping consists of splitting the bits of each byte across multiple disks; such striping is called bitlevel striping .F o re x a m p l e ,i fw eh a v ea na r r a yo fe i g h td i s k s ,w ew r i t e bitiof each byte to disk i.The array of eight disks can be treated as a single disk with sectors that are eight

as a single disk with sectors that are eight times the normal size and, more important, that have eight times the access rate. Every disk participates in every access (read or write); so the number of accesses that can be processed per second is about the same as on a single disk, but each access can read eight times as many data in the same time as on a single disk. Bitlevel striping can be generalized to include a number of disks that either is a multiple of 8 or divides 8. For example, if we

a multiple of 8 or divides 8. For example, if we use an array of four disks, bits iand 4 iof each byte go to disk i.Further, striping need not occur at the bit level. In blocklevel striping , for instance, blocks of a le are striped across multiple disks; with ndisks, block iof a le goes to disk ( imod n)1. Other levels of striping, such as bytes of a sector or sectors of a block, also are possible. Blocklevel striping is the most common. Parallelism in a disk system, as achieved through

Parallelism in a disk system, as achieved through striping, has two main goals: 1.Increase the throughput of multiple small accesses (that is, page accesses) by load balancing. 2.Reduce the response time of large accesses.10.7 RAID Structure 487 10.7.3 RAID Levels Mirroring provides high reliability, but it is expensive. Striping provides high datatransfer rates, but it does not improve reliability. Numerous schemes to provide redundancy at lower cost by using disk striping combined with parity

cost by using disk striping combined with parity bits (which we describe shortly) have been proposed. These schemes have different costperformance trad eoffs and are classied according to levels called RAID levels .W ed e s c r i b et h ev a r i o u sl e v e l sh e r e ;F i g u r e1 0 . 1 1 shows them pictorially (in the gure, Pindicates errorcorrecting bits and C indicates a second copy of the data). In all cases depicted in the gure, four disks worth of data are stored, and the extra disks are

worth of data are stored, and the extra disks are used to store redundant information for failure recovery.(a) RAID 0: nonredundant striping.(b) RAID 1: mirrored disks.CCCC(c) RAID 2: memorystyle errorcorrecting codes.(d) RAID 3: bitinterleaved parity.(e) RAID 4: blockinterleaved parity. (f) RAID 5: blockinterleaved distributed parity.PPPPPPP (g) RAID 6: P H11001 Q redundancy.P PP P PP PP PP PP P Figure 10.11 RAID levels.488 Chapter 10 MassStorage Structure RAID level 0 .RAID level 0 refers to

Structure RAID level 0 .RAID level 0 refers to disk arrays with striping at the level of blocks but without any redundancy (such as mirroring or parity bits), as shown in Figure 10.11(a). RAID level 1 .RAID level 1 refers to disk mirroring. Figure 10.11(b) shows am i r r o r e do r g a n i z a t i o n . RAID level 2 .RAID level 2 is also known as memorystyle errorcorrecting code ( ECC)o r g a n i z a t i o n .M e m o r ys y s t e m sh a v el o n gd e t e c t e dc e r t a i n errors by using

o n gd e t e c t e dc e r t a i n errors by using parity bits. Each byte in a memory system may have a parity bit associated with it that records whether the number of bits in the byte set to 1 is even (parity  0) or odd (parity  1). If one of the bits in the byte is damaged (either a 1 becomes a 0, or a 0 becomes a 1), the parity of the byte changes and thus does not match the stored parity. Similarly, if the stored parity bit is damaged, it does not match the computed parity. Thus, all

it does not match the computed parity. Thus, all singlebit errors are detected by the memory system. Errorcorrecting schemes store two or more extra bits and can reconstruct the data if a single bit is damaged. The idea of ECC can be used directly in disk arrays via striping of bytes across disks. For example, the rst bit of each byte can be stored in disk 1, the second bit in disk 2, and so on until the eighth bit is stored in disk 8; the errorcorrection bits are stored in further disks. This

bits are stored in further disks. This scheme is shown in Figure 10.11(c), where the disks labeled Pstore the error correction bits. If one of the disks fails, the remaining bits of the byte and the associated errorcorrection bits can be read from other disks and used to reconstruct the damaged data. Note that RAID level 2 requires only three disks overhead for four disks of data, unlike RAID level 1, which requires four disks overhead. RAID level 3 .RAID level 3, or bitinterleaved parity

level 3 .RAID level 3, or bitinterleaved parity organization, improves on level 2 by taking into account the fact that, unlike memory systems, disk controllers can detect whether a sector has been read correctly, so a single parity bit can be used for error correction as well as for detection. The idea is as follows: If one of the sectors is damaged, we know exactly which sector it is, and we can gure out whether any bit in the sector is a 1 or a0b yc o m p u t i n gt h ep a r i t yo ft h ec o r

yc o m p u t i n gt h ep a r i t yo ft h ec o r r e s p o n d i n gb i t sf r o ms e c t o r si nt h e other disks. If the parity of the remaining bits is equal to the stored parity, the missing bit is 0; otherwise, it is 1. RAID level 3 is as good as level 2 but is less expensive in the number of extra disks required (it has only a onedisk overhead), so level 2 is not used in practice. Level 3 is shown pictorially in Figure 10.11(d). RAID level 3 has two advantages over level 1. First, the

3 has two advantages over level 1. First, the storage over head is reduced because only one parity disk is needed for several regular disks, whereas one mirror disk is needed for every disk in level 1. Second, since reads and writes of a byte are spread out over multiple disks with Nway striping of data, the transfer rate for reading or writing a single block is Ntimes as fast as with RAID level 1. On the negative side, RAID level 3 supports fewer IOs per second, since every disk has to

fewer IOs per second, since every disk has to participate in every IOrequest. A further performance problem with RAID 3and with all parity based RAID levelsis the expense of computing and writing the parity.10.7 RAID Structure 489 This overhead results in signicantly slower writes than with nonparity RAID arrays. To moderate this performance penalty, many RAID storage arrays include a hardware controller with dedicated parity hardware. This controller ofoads the parity computation from the CPU

ofoads the parity computation from the CPU to the array. The array has an NVRAM cache as well, to store the blocks while the parity is computed and to buffer the writes from the controller to the spindles. This combination can make parity RAID almost as fast as nonparity. In fact, a caching array doing parity RAID can outperform a noncaching nonparity RAID . RAID level 4 .RAID level 4, or blockinterleaved parity organization, uses blocklevel striping, as in RAID 0, and in addition keeps a parity

as in RAID 0, and in addition keeps a parity block on a separate disk for corresponding blocks from Nother disks. This scheme is diagrammed in Figure 10.11(e). If one of the disks fails, the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk. Ab l o c kr e a da c c e s s e so n l yo n ed i s k ,a l l o w i n go t h e rr e q u e s t st ob e processed by the other disks. Thus, the datatransfer rate for each access is slower, but

datatransfer rate for each access is slower, but multiple read accesses can proceed in parallel, leading to a higher overall IOrate. The transfer rates for large reads are high, since all the disks can be read in parallel. Large writes also have high transfer rates, since the data and parity can be written in parallel. Small independent writes cannot be performed in parallel. An operating system write of data smaller than a block requires that the block be read, modied with the new data, and

the block be read, modied with the new data, and written back. The parity block has to be updated as well. This is known as the readmodifywrite cycle .T h u s ,a single write requires four disk accesses: two to read the two old blocks and two to write the two new blocks. WAFL (which we cover in Chapter 12) uses RAID level 4 because this RAID level allows disks to be added to a RAID set seamlessly. If the added disks are initialized with blocks containing only zeros, then the parity value does

containing only zeros, then the parity value does not change, and the RAID set is still correct. RAID level 5 .RAID level 5, or blockinterleaved distributed parity, differs from level 4 in that it spreads data and parity among all N1d i s k s ,r a t h e r than storing data in Ndisks and parity in one disk. For each block, one of the disks stores the parity and the others store data. For example, with an array of ve disks, the parity for the nth block is stored in disk ( nmod 5) 1. The nth blocks

is stored in disk ( nmod 5) 1. The nth blocks of the other four disks store actual data for that block. This setup is shown in Figure 10.11(f), where the Psa r ed i s t r i b u t e da c r o s sa l l the disks. A parity block cannot store parity for blocks in the same disk, because a disk failure would result in loss of data as well as of parity, and hence the loss would not be recoverable. By spreading the parity across all the disks in the set, RAID 5a v o i d sp o t e n t i a lo v e r u s eo

RAID 5a v o i d sp o t e n t i a lo v e r u s eo fas i n g l ep a r i t y disk, which can occur with RAID 4.RAID 5i st h em o s tc o m m o np a r i t y RAID system. RAID level 6 .RAID level 6, also called the PQr e d u n d a n c ys c h e m e ,i s much like RAID level 5 but stores extra redundant information to guard against multiple disk failures. Instead of parity, errorcorrecting codes such as the ReedSolomon codes are used. In the scheme shown in Figure490 Chapter 10 MassStorage Structure

in Figure490 Chapter 10 MassStorage Structure 10.11(g), 2 bits of redundant data are stored for every 4 bits of data compared with 1 parity bit in level 5and the system can tolerate two disk failures. RAID levels 0  1 and 1  0 .RAID level 0  1 refers to a combination of RAID levels 0 and 1. RAID 0p r o v i d e st h ep e r f o r m a n c e ,w h i l e RAID 1p r o v i d e s the reliability. Generally, this level provides better performance than RAID 5. It is common in environments where both

RAID 5. It is common in environments where both performance and reliability are important. Unfortunately, like RAID 1, it doubles the number of disks needed for storage, so it is also relatively expensive. In RAID 0  1, a set of disks are striped, and then the stripe is mirrored to another, equivalent stripe. Another RAID option that is becoming available commercially is RAID level 1  0, in which disks are mirrored in pairs and then the resulting mirrored pairs are striped. This scheme has some

mirrored pairs are striped. This scheme has some theoretical advantages over RAID 01 .F o re x a m p l e ,i fas i n g l ed i s kf a i l si n RAID 01 ,a ne n t i r e stripe is inaccessible, leaving only the other stripe. With a failure in RAID 1 0 ,as i n g l ed i s ki su n a v a i l a b l e ,b u tt h ed i s kt h a tm i r r o r si ti ss t i l la v a i l a b l e , as are all the rest of the disks (Figure 10.12). Numerous variations have been proposed to the basic RAID schemes described here. As a

to the basic RAID schemes described here. As a result, some confusion may exist about the exact denitions of the different RAID levels.x xmirror a) RAID 0 H11001 1 with a single disk failure.stripe stripe mirror b) RAID 1 H11001 0 with a single disk failure.stripe mirror mirror mirror Figure 10.12 RAID 0  1 and 1  0.10.7 RAID Structure 491 The implementation of RAID is another area of variation. Consider the following layers at which RAID can be implemented. Volumemanagement software can

can be implemented. Volumemanagement software can implement RAID within the kernel or at the system software layer. In this case, the storage hardware can provide minimal features and still be part of a full RAID solution. Parity RAID is fairly slow when implemented in software, so typically RAID 0, 1, or 0  1 is used. RAID can be implemented in the host busadapter ( HBA )h a r d w a r e .O n l y the disks directly connected to the HBA can be part of a given RAID set. This solution is low in

part of a given RAID set. This solution is low in cost but not very exible. RAID can be implemented in the hardware of the storage array. The storage array can create RAID sets of various levels and can even slice these sets into smaller volumes, which are then presented to the operating system. The operating system need only implement the le system on each of the volumes. Arrays can have multiple connections available or can be part of aSAN, allowing multiple hosts to take advantage of the

allowing multiple hosts to take advantage of the arrays features. RAID can be implemented in the SAN interconnect layer by disk virtualiza tion devices. In this case, a device sits between the hosts and the storage. It accepts commands from the servers and manages access to the storage. It could provide mirroring, for example, by writing each block to two separate storage devices. Other features, such as snapshots and replication, can be implemented at each of these levels as well. A snapshot is

at each of these levels as well. A snapshot is a view of the le system before the last update took place. (Snapshots are covered more fully in Chapter 12.) Replication involves the automatic duplication of writes between separate sites for redundancy and disaster recovery. Replication can be synchronous or asynchronous. In synchronous replication, each block must be written locally and remotely before the write is considered complete, whereas in asynchronous replication, the writes are grouped

asynchronous replication, the writes are grouped together and written periodically. Asynchronous replication can result in data loss if the primary site fails, but it is faster and has no distance limitations. The implementation of these features differs depending on the layer at which RAID is implemented. For example, if RAID is implemented in software, then each host may need to carry out and manage its own replication. If replication is implemented in the storage array or in the SAN

is implemented in the storage array or in the SAN interconnect, however, then whatever the host operating system or its features, the hosts data can be replicated. One other aspect of most RAID implementations is a hot spare disk or disks. Ahot spare is not used for data but is congured to be used as a replacement in case of disk failure. For instance, a hot spare can be used to rebuild a mirrored pair should one of the disks in the pair fail. In this way, the RAID level can be reestablished

In this way, the RAID level can be reestablished automatically, without waiting for the failed disk to be replaced. Allocating more than one hot spare allows more than one failure to be repaired without human intervention.492 Chapter 10 MassStorage Structure 10.7.4 Selecting a RAID Level Given the many choices they have, how do system designers choose a RAID level? One consideration is rebuild performance. If a disk fails, the time needed to rebuild its data can be signicant. This may be an

rebuild its data can be signicant. This may be an important factor if a continuous supply of data is required, as it is in highperformance or interactive database systems. Furthermore, rebuild performance inuences the mean time to failure. Rebuild performance varies with the RAID level used. Rebuilding is easiest for RAID level 1, since data can be copied from another disk. For the other levels, we need to access all the other disks in the array to rebuild data in a failed disk. Rebuild times

to rebuild data in a failed disk. Rebuild times can be hours for RAID 5r e b u i l d so fl a r g ed i s ks e t s . RAID level 0 is used in highperformance applications where data loss is not critical. RAID level 1 is popular for applications that require high reliability with fast recovery. RAID 01a n d10a r eu s e dw h e r eb o t hp e r f o r m a n c ea n d reliability are importantfor example, for small databases. Due to RAID 1s high space overhead, RAID 5i so f t e np r e f e r r e df o rs t

RAID 5i so f t e np r e f e r r e df o rs t o r i n gl a r g ev o l u m e so f data. Level 6 is not supported currently by many RAID implementations, but it should offer better reliability than level 5. RAID system designers and administrators of storage have to make several other decisions as well. For example, how many disks should be in a given RAID set? How many bits should be protected by each parity bit? If more disks are in an array, datatransfer rates are higher, but the system is more

rates are higher, but the system is more expensive. If more bits are protected by a parity bit, the space overhead due to parity bits is lower, but the chance that a second disk will fail before the rst failed disk is repaired is greater, and that will result in data loss. 10.7.5 Extensions The concepts of RAID have been generalized to oth er storage devices, including arrays of tapes, and even to the broadcast of data over wireless systems. When applied to arrays of tapes, RAID structures are

applied to arrays of tapes, RAID structures are able to recover data even if one of the tapes in an array is damaged. When applied to broadcast of data, a block of data is split into short units and is broadcast along with a parity unit. If one of the units is not received for any reason, it can be reconstructed from the other units. Commonly, tapedrive robots containing multiple tape drives will stripe data across all the drives to increase throughput and decrease backup time. 10.7.6 Problems

and decrease backup time. 10.7.6 Problems with RAID Unfortunately, RAID does not always assure that data are available for the operating system and its users. A pointer to a le could be wrong, for example, or pointers within the le structure could be wrong. Incomplete writes, if not properly recovered, could result in corrupt data. Some other process could accidentally write over a le systems structures, too. RAID protects against physical media errors, but not other hardware and software

media errors, but not other hardware and software errors. As large as is the landscape of software and hardware bugs, that is how numerous are the potential perils for data on a system. The Solaris ZFSle system takes an innovative approach to solving these problems through the use of checksums a technique used to verify the10.7 RAID Structure 493 THE InServ STORAGE ARRAY Innovation, in an effort to provide better, faster, and less expensive solutions, frequently blurs the lines that separated

frequently blurs the lines that separated previous technologies. Consider the InServ storage array from 3Par. Unlike most other storage arrays, InServ does not require that a set of disks be congured at a specic RAID level. Rather, each disk is broken into 256 MBchunklets. RAID is then applied at the chunklet level. A disk can thus participate in multiple and various RAID levels as its chunklets are used for multiple volumes. InServ also provides snapshots similar to those created by the WAFL le

snapshots similar to those created by the WAFL le system. The format of InServ snapshots can be readwrite as well as read only, allowing multiple hosts to mount copies of a given le system without needing their own copies of the entire le system. Any changes a host makes in its own copy are copyonwrite and so are not reected in the other copies. Af u r t h e ri n n o v a t i o ni s utility storage .S o m e l es y s t e m sd on o te x p a n d or shrink. On these systems, the original size is the

shrink. On these systems, the original size is the only size, and any change requires copying data. An administrator can congure InServ to provide a host with a large amount of logical storage that initially occupies only a small amount of physical storage. As the host starts using the storage, unused disks are allocated to the host, up to the original logical level. The host thus can believe that it has a large xed storage space, create its le systems there, and so on. Disks can be added or

le systems there, and so on. Disks can be added or removed from the le system by InServ without the le systems noticing the change. This feature can reduce the number of drives needed by hosts, or at least delay the purchase of disks until they are really needed. integrity of data. ZFS maintains internal checksums of all blocks, including data and metadata. These checksums are not kept with the block that is being checksummed. Rather, they are stored with the pointer to that block. (See Figure

stored with the pointer to that block. (See Figure 10.13.) Consider an inode ad a t as t r u c t u r ef o rs t o r i n g l es y s t e mm e t a d a t a w i t hp o i n t e r st oi t sd a t a .W i t h i nt h ei n o d ei st h ec h e c k s u mo fe a c hb l o c k of data. If there is a problem with the data, the checksum will be incorrect, and the le system will know about it. If the data are mirrored, and there is a block with a correct checksum and one with an incorrect checksum, ZFSwill

and one with an incorrect checksum, ZFSwill automatically update the bad block with the good one. Similarly, the directory entry that points to the inode has a checksum for the inode. Any problem in the inode is detected when the directory is accessed. This checksumming takes places throughout all ZFSstructures, providing a much higher level of consistency, error detection, and error correction than is found in RAID disk sets or standard le systems. The extra overhead that is created by the

systems. The extra overhead that is created by the checksum calculation and extra block readmodifywrite cycles is not noticeable because the overall performance of ZFSis very fast. Another issue with most RAID implementations is lack of exibility. Consider a storage array with twenty disks divided into four sets of ve disks. Each set of ve disks is a RAID level 5 set. As a result, there are four separate volumes, each holding a le system. But what if one le system is too large to t on a vedisk

if one le system is too large to t on a vedisk RAID level 5 set? And what if another le system needs very little space? If such factors are known ahead of time, then the disks and volumes494 Chapter 10 MassStorage Structure metadata block 1 address 1 checksum MB2 checksumaddress 2metadata block 2 address checksum D1 checksum D2 data 1 data 2address Figure 10.13 ZFS checksums all metadata and data. can be properly allocated. Very frequently, however, disk use and requirements change over time.

disk use and requirements change over time. Even if the storage array allowed the entire set of twenty disks to be created as one large RAID set, other issues could arise. Several volumes of various sizes could be built on the set. But some volume managers do not allow us to change a volumes size. In that case, we would be left with the same issue described abovemismatched lesystem sizes. Some volume managers allow size changes, but some le systems do not allow for lesystem growth or shrinkage.

do not allow for lesystem growth or shrinkage. The volumes could change sizes, but the le systems would need to be recreated to take advantage of those changes. ZFS combines lesystem management and volume management into a unit providing greater functionality than the traditional separation of those functions allows. Disks, or partitions of disks, are gathered together via RAID sets into pools of storage. A pool can hold one or more ZFSle systems. The entire pools free space is available to all

The entire pools free space is available to all le systems within that pool. ZFSuses the memory model of malloc() andfree() to allocate and release storage for each le system as blocks are used and freed within the le system. As a result, there are no articial limits on storage use and no need to relocate le systems between volumes or resize volumes. ZFSprovides quotas to limit the size of a le system and reservations to assure that a le system can grow by a specied amount, but those variables

can grow by a specied amount, but those variables can be changed by the lesystem owner at any time. Figure 10.14(a) depicts traditional volumes and le systems, and Figure 10.14(b) shows the ZFSmodel. 10.8 StableStorage Implementation In Chapter 5, we introduced the writeahead log, which requires the availability of stable storage. By denition, information residing in stable storage is never lost. To implement such storage, we need to replicate the required information10.8 StableStorage

the required information10.8 StableStorage Implementation 495FS volume ZFS ZFS storage poolZFSvolume volumeFS FS (a) Traditional volumes and file systems. (b) ZFS and pooled storage.Figure 10.14 (a) Traditional volumes and le systems. (b) A ZFS pool and le systems. on multiple storage devices (usually disks) with independent failure modes. We also need to coordinate the writing of updates in a way that guarantees that a failure during an update will not leave all the copies in a damaged state

will not leave all the copies in a damaged state and that, when we are recovering from a failure, we can force all copies to a consistent and correct value, even if anoth er failure occurs during the recovery. In this section, we discuss how to meet these needs. A disk write results in one of three outcomes: 1.Successful completion . The data were written correctly on disk. 2.Partial failure .Af a i l u r eo c c u r r e di nt h em i d s to ft r a n s f e r ,s oo n l ys o m eo f the sectors were

a n s f e r ,s oo n l ys o m eo f the sectors were written with the new data, and the sector being written during the failure may have been corrupted. 3.Total failure .T h ef a i l u r eo c c u r r e db e f o r et h ed i s kw r i t es t a r t e d ,s ot h e previous data values on the disk remain intact. Whenever a failure occurs during writing of a block, the system needs to detect it and invoke a recovery procedu re to restore the block to a consistent state. To do that, the system must

to a consistent state. To do that, the system must maintain two physical blocks for each logical block. An output operation is executed as follows: 1.Write the information onto the rst physical block. 2.When the rst write completes successfully, write the same information onto the second physical block. 3.Declare the operation complete only after the second write completes successfully.496 Chapter 10 MassStorage Structure During recovery from a failure, each pair of physical blocks is examined.

failure, each pair of physical blocks is examined. If both are the same and no detectable error exists, then no further action is necessary. If one block contains a detectable error then we replace its contents with the value of the other block. If neither block contains a detectable error, but the blocks differ in content, then we replace the content of the rst block with that of the second. This recovery pro cedure ensures that a write to stable storage either succeeds completely or results in

storage either succeeds completely or results in no change. We can extend this procedure easily to allow the use of an arbitrarily large number of copies of each block of stable storage. Although having a large number of copies further reduces the probability of a failure, it is usually reasonable to simulate stable storage with only two copies. The data in stable storage are guaranteed to be safe unless a failure destroys all the copies. Because waiting for disk writes to complete (synchronous

waiting for disk writes to complete (synchronous IO)i st i m e consuming, many storage arrays add NVRAM as a cache. Since the memory is nonvolatile (it usually has battery power to back up the units power), it can be trusted to store the data en route to the disks. It is thus considered part of the stable storage. Writes to it are much faster than to disk, so performance is greatly improved. 10.9 Summary Disk drives are the major secondary storage IOdevices on most computers. Most secondary

IOdevices on most computers. Most secondary storage devices are either magnetic disks or magnetic tapes, although solidstate disks are growing in importance. Modern disk drives are structured as large onedimensional arrays of logical disk blocks. Generally, these logical blocks are 512 bytes in size. Disks may be attached to a computer system in one of two ways: (1) through the local IOports on the host computer or (2) through a network connection. Requests for disk IOare generated by the le

Requests for disk IOare generated by the le system and by the virtual memory system. Each request species the address on the disk to be referenced, in the form of a logical block number. Diskscheduling algorithms can improve the effective bandwidth, the average response time, and the variance in response time. Algorithms such as SSTF ,SCAN ,CSCAN ,LOOK ,a n d CLOOK are designed to make such improvements through strategies for diskqueue ordering. Performance of diskscheduling algorithms can vary

Performance of diskscheduling algorithms can vary greatly on magnetic disks. In contrast, because solidstate disks have no moving parts, performance varies little among algorithms, and quite often a simple FCFS strategy is used. Performance can be harmed by external fragmentation. Some systems have utilities that scan the le system to identify fragmented les; they then move blocks around to decrease the fragmentation. Defragmenting a badly fragmented le system can signicantly improve

badly fragmented le system can signicantly improve performance, but the system may have reduced performance while the defragmentation is in progress. Sophisticated le systems, such as the UNIX Fast File System, incorporate many strategies to control fragmentation during space allocation so that disk reorganization is not needed. The operating system manages the disk blocks. First, a disk must be low levelformatted to create the sectors on the raw hardwarenew disks usually come preformatted.

raw hardwarenew disks usually come preformatted. Then, the disk is partitioned, le systems are created, andPractice Exercises 497 boot blocks are allocated to store the systems bootstrap program. Finally, when a block is corrupted, the system must have a way to lock out that block or to replace it logically with a spare. Because an efcient swap space is a key to good performance, systems usually bypass the le system and use rawdisk access for paging IO.S o m e systems dedicate a rawdisk

for paging IO.S o m e systems dedicate a rawdisk partition to swap space, and others use a le within the le system instead. Still other systems allow the user or system administrator to make the decision by providing both options. Because of the amount of storage required on large systems, disks are frequently made redundant via RAID algorithms. These algorithms allow more than one disk to be used for a given operation and allow continued operation and even automatic recovery in the face of a

and even automatic recovery in the face of a disk failure. RAID algorithms are organized into different levels; each level provides some combination of reliability and high transfer rates. Practice Exercises 10.1 Is disk scheduling, other than FCFS scheduling, useful in a singleuser environment? Explain your answer. 10.2 Explain why SSTF scheduling tends to favor middle cylinders over the innermost and outermost cylinders. 10.3 Why is rotational latency usually not considered in disk scheduling?

latency usually not considered in disk scheduling? How would you modify SSTF ,SCAN ,a n d CSCAN to include latency optimization? 10.4 Why is it important to balance lesystem IOamong the disks and controllers on a system in a multitasking environment? 10.5 What are the tradeoffs involved in rereading code pages from the le system versus using swap space to store them? 10.6 Is there any way to implement truly stable storage? Explain your answer. 10.7 It is sometimes said that tape is a

answer. 10.7 It is sometimes said that tape is a sequentialaccess medium, whereas am a g n e t i cd i s ki sar a n d o m  a c c e s sm e d i u m .I nf a c t ,t h es u i t a b i l i t y of a storage device for random access depends on the transfer size. The term streaming transfer rate denotes the rate for a data transfer that is underway, excluding the effect of access latency. In contrast, theeffective transfer rate is the ratio of total bytes per total seconds, including overhead time such as

per total seconds, including overhead time such as access latency. Suppose we have a computer with the following characteristics: the level2 cache has an access latency of 8 nanoseconds and a streaming transfer rate of 800 megabytes per second, the main memory has an access latency of 60 nanoseconds and a streaming transfer rate of 80 megabytes per second, the magnetic disk has an access latency of 15 milliseconds and a streaming transfer rate of 5 megabytes per second, and a tape drive has an

of 5 megabytes per second, and a tape drive has an access latency of 60 seconds and a streaming transfer rate of 2 megabytes per second.498 Chapter 10 MassStorage Structure a. Random access causes the effective transfer rate of a device to decrease, because no data are transferred during the access time. For the disk described, what is the effective transfer rate if an average access is followed by a streaming transfer of (1) 512 bytes, (2) 8 kilobytes, (3) 1 megabyte, and (4) 16 megabytes? b.

(3) 1 megabyte, and (4) 16 megabytes? b. The utilization of a device is the ratio of effective transfer rate to streaming transfer rate. Calculate the utilization of the disk drive for each of the four transfer sizes given in part a. c. Suppose that a utilization of 25 percent (or higher) is considered acceptable. Using the performance gures given, compute the smallest transfer size for disk that gives acceptable utilization. d. Complete the following sentence: A disk is a randomaccess device

sentence: A disk is a randomaccess device for transfers larger than bytes and is a sequential access device for smaller transfers. e. Compute the minimum transfer sizes that give acceptable utiliza tion for cache, memory, and tape. f. When is a tape a randomaccess device, and when is it a sequentialaccess device? 10.8 Could a RAID level 1 organization achieve better performance for read requests than a RAID level 0 organization (with nonredundant striping of data)? If so, how? Exercises 10.9

striping of data)? If so, how? Exercises 10.9 None of the diskscheduling disciplines, except FCFS ,i st r u l yf a i r (starvation may occur). a. Explain why this assertion is true. b. Describe a way to modify algorithms such as SCAN to ensure fairness. c. Explain why fairness is an important goal in a timesharing system. d. Give three or more examples of circumstances in which it is important that the operating system be unfair in serving IO requests. 10.10 Explain why SSDso f t e nu s ea n

requests. 10.10 Explain why SSDso f t e nu s ea n FCFS diskscheduling algorithm. 10.11 Suppose that a disk drive has 5,000 cylinders, numbered 0 to 4,999. The drive is currently serving a request at cylinder 2,150, and the previous request was at cylinder 1,805. The queue of pending requests, in FIFO order, is: 2,069, 1,212, 2,296, 2,800, 544, 1,618, 356, 1,523, 4,965, 3681Exercises 499 Starting from the current head position, what is the total distance (in cylinders) that the disk arm moves to

distance (in cylinders) that the disk arm moves to satisfy all the pending requests for each of the following diskscheduling algorithms? a. FCFS b. SSTF c. SCAN d. LOOK e. CSCAN f. CLOOK 10.12 Elementary physics states that when an ob ject is subjected to a constant acceleration a,the relationship between distance dand time tis given byd1 2at2.S u p p o s et h a t ,d u r i n gas e e k ,t h ed i s ki nE x e r c i s e1 0 . 1 1 accelerates the disk arm at a constant rate for the rst half of the

arm at a constant rate for the rst half of the seek, then decelerates the disk arm at the same rate for the second half of the seek. Assume that the disk can perform a seek to an adjacent cylinder in 1 millisecond and a fullstroke seek over all 5,000 cylinders in 18 milliseconds. a. The distance of a seek is the number of cylinders over which the head moves. Explain why the seek time is proportional to the square root of the seek distance. b. Write an equation for the seek time as a function of

an equation for the seek time as a function of the seek distance. This equation should be of the form txy L,w h e r e tis the time in milliseconds and Lis the seek distance in cylinders. c. Calculate the total seek time for each of the schedules in Exercise 10.11. Determine which schedule is the fastest (has the smallest total seek time). d. The percentage speedup is the time saved divided by the original time. What is the percentage speedup of the fastest schedule over FCFS ? 10.13 Suppose that

fastest schedule over FCFS ? 10.13 Suppose that the disk in Exercise 10.12 rotates at 7,200 RPM . a. What is the average rotational latency of this disk drive? b. What seek distance can be covered in the time that you found for part a? 10.14 Describe some advantages and disadvantages of using SSDsa sa caching tier and as a diskdrive replacement compared with using only magnetic disks. 10.15 Compare the performance of CSCAN and SCAN scheduling, assuming au n i f o r md i s t r i b u t i o no fr e

au n i f o r md i s t r i b u t i o no fr e q u e s t s .C o n s i d e rt h ea v e r a g er e s p o n s et i m e (the time between the arrival of a request and the completion of that requests service), the variation in response time, and the effective500 Chapter 10 MassStorage Structure bandwidth. How does performance depend on the relative sizes of seek time and rotational latency? 10.16 Requests are not usually uniformly distributed. For example, we can expect a cylinder containing the

example, we can expect a cylinder containing the lesystem metadata to be accessed more frequently than a cylinder containing only les. Suppose you know that 50 percent of the requests are for a small, xed number of cylinders. a. Would any of the scheduling algorithms discussed in this chapter be particularly good for this case? Explain your answer. b. Propose a diskscheduling algorithm that gives even better per formance by taking advantage of this hot spot on the disk. 10.17 Consider a RAID

this hot spot on the disk. 10.17 Consider a RAID level 5 organization comprising ve disks, with the parity for sets of four blocks on four disks stored on the fth disk. How many blocks are accessed in order to perform the following? a. A write of one block of data b. A write of seven continuous blocks of data 10.18 Compare the throughput achieved by a RAID level 5 organization with that achieved by a RAID level 1 organization for the following: a. Read operations on single blocks b. Read

a. Read operations on single blocks b. Read operations on multiple contiguous blocks 10.19 Compare the performance of write operations achieved by a RAID level 5o r g a n i z a t i o nw i t ht h a ta c h i e v e db ya RAID level 1 organization. 10.20 Assume that you have a mixed conguration comprising disks orga nized as RAID level 1 and RAID level 5 disks. Assume that the system has exibility in deciding which disk organization to use for storing a particular le. Which les should be stored in

a particular le. Which les should be stored in the RAID level 1 disks and which in the RAID level 5 disks in order to optimize performance? 10.21 The reliability of a harddisk drive is typically described in terms of aq u a n t i t yc a l l e d mean time between failures (MTBF ).A l t h o u g ht h i s quantity is called a time, theMTBF actually is measured in drivehours per failure. a. If a system contains 1,000 disk drives, each of which has a 750,000 hour MTBF ,w h i c ho ft h ef o l l o w i n

hour MTBF ,w h i c ho ft h ef o l l o w i n gb e s td e s c r i b e sh o wo f t e na drive failure will occur in that disk farm: once per thousand years, once per century, once per decade, once per year, once per month, once per week, once per day, once per hour, once per minute, or once per second? b. Mortality statistics indicate that, on the average, a U.S. resident has about 1 chance in 1,000 of dying between the ages of 20 and 21. Deduce the MTBF hours for 20yearolds. Convert this gure from

MTBF hours for 20yearolds. Convert this gure from hours to years. What does this MTBF tell you about the expected lifetime of a 20yearold?Bibliographical Notes 501 c. The manufacturer guarantees a 1millionhour MTBF for a certain model of disk drive. What can you conclude about the number of years for which one of these drives is under warranty? 10.22 Discuss the relative advantages and disadvantages of sector sparing and sector slipping. 10.23 Discuss the reasons why the operating system might

Discuss the reasons why the operating system might require accurate information on how blocks are stored on a disk. How could the oper ating system improve lesystem performance with this knowledge? Programming Problems 10.24 Write a program that implements the following diskscheduling algo rithms: a. FCFS b. SSTF c. SCAN d. CSCAN e. LOOK f. CLOOK Your program will service a disk with 5,000 cylinders numbered 0 to 4,999. The program will generate a random series of 1,000 cylinder requests and

a random series of 1,000 cylinder requests and service them according to each of the algorithms listed above. The program will be passed the initial position of the disk head (as a parameter on the command line) and report the total amount of head movement required by each algorithm. Bibliographical Notes [Services (2012)] provides an overview of data storage in a variety of modern computing environments. [Teorey and Pinkerton (1972)] present an early comparative analysis of diskscheduling

an early comparative analysis of diskscheduling algorithms using simulations that model a disk for which seek time is linear in the number of cylinders crossed. Scheduling optimizations that exploit disk idle times are discussed in [Lumb et al. (2000)]. [Kim et al. (2009)] discusses diskscheduling algorithms for SSDs. Discussions of redundant arrays of independent disks ( RAID s) are pre sented by [Patterson et al. (1988)]. [Russinovich and Solomon (2009)], [McDougall and Mauro (2007)], and

Solomon (2009)], [McDougall and Mauro (2007)], and [Love (2010)] discuss le system details in Windows, Solaris, and Linux, respectively. The IOsize and randomness of the workload inuence disk performance considerably. [Ousterhout et al. (1985)] and [Ruemmler and Wilkes (1993)] report numerous interesting workload characteristicsfor example, most les are small, most newly created les are deleted soon thereafter, most les that502 Chapter 10 MassStorage Structure are opened for reading are read

Structure are opened for reading are read sequentially in their entirety, and most seeks are short. The concept of a storage hierarchy has been studied for more than forty years. For instance, a 1970 paper by [Mattson et al. (1970)] describes a mathematical approach to predicting the performance of a storage hierarchy. Bibliography [Kim et al. (2009)] J. Kim, Y. Oh, E. Kim, J. C. D. Lee, and S. Noh, Disk schedulers for solid state drivers (2009), pages 295304. [Love (2010)] R. Love, Linux Kernel

pages 295304. [Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developers Library (2010). [Lumb et al. (2000)] C. Lumb, J. Schindler, G. R. Ganger, D. F. Nagle, and E. Riedel, Towards Higher Disk Head Utilization: Extracting Free Bandwidth From Busy Disk Drives ,Symposium on Operating Systems Design and Implemen tation (2000). [Mattson et al. (1970)] R. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger, Evaluation Techniques for Storage Hierarchies ,IBM Systems Journal ,V o l

Storage Hierarchies ,IBM Systems Journal ,V o l u m e 9, Number 2 (1970), pages 78117. [McDougall and Mauro (2007)] R. McDougall and J. Mauro, Solaris Internals, Second Edition, Prentice Hall (2007). [Ousterhout et al. (1985)] J. K. Ousterhout, H. D. Costa, D. Harrison, J. A. Kunze, M. Kupfer, and J. G. Thompson, AT r a c e  D r i v e nA n a l y s i so ft h eU N I X4 . 2B S D File System ,Proceedings of the ACM Symposium on Operating Systems Principles (1985), pages 1524. [Patterson et al.

Principles (1985), pages 1524. [Patterson et al. (1988)] D. A. Patterson, G. Gibson, and R. H. Katz, AC a s e for Redundant Arrays of Inexpensive Disks (RAID) ,Proceedings of the ACM SIGMOD International Conference on the Management of Data (1988), pages 109 116. [Ruemmler and Wilkes (1993)] C. Ruemmler and J. Wilkes, Unix Disk Access Patterns ,Proceedings of the Winter USENIX Conference (1993), pages 405420. [Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon, Win dows

M. E. Russinovich and D. A. Solomon, Win dows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition, Microsoft Press (2009). [Services (2012)] E. E. Services, Information Storage and Management: Storing, Managing, and Protecting Digital Information in Classic, Virtualized, and Cloud Environments ,W i l e y( 2 0 1 2 ) . [Teorey and Pinkerton (1972)] T. J. Teorey and T. B. Pinkerton, AC o m p a r a t i v e Analysis of Disk Scheduling Policies ,Communications of the ACM ,V o l u

Policies ,Communications of the ACM ,V o l u m e1 5 , Number 3 (1972), pages 177184.11CHAPTER File System Interface For most users, the le system is the most visible aspect of an operating system. It provides the mechanism for online storage of and access to both data and programs of the operating system and all the users of the computer system. The le system consists of two distinct parts: a collection of les, each storing related data, and a directory struc ture, which organizes and provides

directory struc ture, which organizes and provides information about all the les in the system. File systems live on devices, which we described in the preceding chapter and will continue to discuss in the following one. In this chapter, we consider the various aspects of les and the major directory structures. We also discuss the semantics of sharing les among multiple processes, users, and computers. Finally, we discuss ways to handle le protection, necessary when w eh a v em u l t i p l eu s

necessary when w eh a v em u l t i p l eu s e r sa n dw ew a n tt o control who may access les and how les may be accessed. CHAPTER OBJECTIVES To explain the function of le systems. To describe the interfaces to le systems. To discuss lesystem design tradeoffs, including access methods, le sharing, le locking, and directory structures. To explore lesystem protection. 11.1 File Concept Computers can store information on various storage media, such as magnetic disks, magnetic tapes, and optical

as magnetic disks, magnetic tapes, and optical disks. So that the computer system will be convenient to use, the operating system provides a uniform logical view of stored information. The operating system abstracts from the physical properties of its storage devices to dene a logical storage unit, the le.F i l e sa r e mapped by the operating system onto physical devices. These storage devices are usually nonvolatile, so the contents are persistent between system reboots. 503504 Chapter 11

between system reboots. 503504 Chapter 11 FileSystem Interface A l ei san a m e dc o l l e c t i o no fr e l a t e di n f o r m a t i o nt h a ti sr e c o r d e do n secondary storage. From a users perspective, a le is the smallest allotment of logical secondary storage; that is, data cannot be written to secondary storage unless they are within a le. Commonly, les represent programs (both source and object forms) and data. Data les may be numeric, alphabetic, alphanumeric, or binary. Files may

alphabetic, alphanumeric, or binary. Files may be free form, such as text les, or may be formatted rigidly. In general, a le is a sequence of bits, bytes, lines, or records, the meaning of which is dened by the les creator and user. The concept of a l ei st h u se x t r e m e l yg e n e r a l . The information in a le is dened by its creator. Many different types of information may be stored in a lesource or executable programs, numeric or text data, photos, music, video, and so on. A le has a

data, photos, music, video, and so on. A le has a certain dened structure, which depends on its type. A text le is a sequence of characters organized into lines (and possibly pages). A source le is a sequence of functions, each of which is further organized as declarations followed by executable statements. Anexecutable le is a series of code sections that the loader can bring into memory and execute. 11.1.1 File Attributes A l ei sn a m e d ,f o rt h ec o n v e n i e n c eo fi t sh u m a nu s e

rt h ec o n v e n i e n c eo fi t sh u m a nu s e r s ,a n di sr e f e r r e dt ob y its name. A name is usually a string of characters, such as example.c .S o m e systems differentiate between uppercase and lowercase characters in names, whereas other systems do not. When a le is named, it becomes independent of the process, the user, and even the system that created it. For instance, one user might create the le example.c ,a n da n o t h e ru s e rm i g h te d i tt h a t l eb y specifying its

s e rm i g h te d i tt h a t l eb y specifying its name. The les owner might write the le to a USB disk, send it as an email attachment, or copy it across a network, and it could still be called example.c on the destination system. A l e  sa t t r i b u t e sv a r yf r o mo n eo p e r a t i n gs y s t e mt oa n o t h e rb u tt y p i c a l l y consist of these: Name .T h es y m b o l i c l en a m ei st h eo n l yi n f o r m a t i o nk e p ti nh u m a n  readable form. Identier .T h i su n i q u

u m a n  readable form. Identier .T h i su n i q u et a g ,u s u a l l yan u m b e r ,i d e n t i  e st h e l ew i t h i nt h e le system; it is the nonhumanreadable name for the le. Type .T h i si n f o r m a t i o ni sn e e d e df o rs y s t e m st h a ts u p p o r td i f f e r e n tt y p e s of les. Location .T h i si n f o r m a t i o ni sap o i n t e rt oad e v i c ea n dt ot h el o c a t i o no f the le on that device. Size .T h ec u r r e n ts i z eo ft h e l e( i nb y t e s ,w o r d s ,o

n ts i z eo ft h e l e( i nb y t e s ,w o r d s ,o rb l o c k s )a n dp o s s i b l y the maximum allowed size are included in this attribute. Protection .A c c e s s  c o n t r o li n f o r m a t i o nd e t e r m i n e sw h oc a nd or e a d i n g , writing, executing, and so on. Time, date, and user identication .T h i si n f o r m a t i o nm a yb ek e p tf o r creation, last modication, and last use. These data can be useful for protection, security, and usage monitoring.11.1 File Concept 505

and usage monitoring.11.1 File Concept 505 Figure 11.1 A l ei n f ow i n d o wo nM a cO SX . Some newer le systems also support extended le attributes ,i n c l u d i n g character encoding of the le and security features such as a le checksum. Figure 11.1 illustrates a le info window on Mac OS X , which displays a les attributes. The information about all les is kept in the directory structure, which also resides on secondary storage. Typically, a directory entry consists of the les name and its

a directory entry consists of the les name and its unique identier. The identier in turn locates the other506 Chapter 11 FileSystem Interface le attributes. It may take more than a kilobyte to record this information for each le. In a system with many les, the size of the directory itself may be megabytes. Because directories, like les, must be nonvolatile, they must be stored on the device and brought into memory piecemeal, as needed. 11.1.2 File Operations A l ei sa na b s t r a c td a t at y

Operations A l ei sa na b s t r a c td a t at y p e .T od e  n ea l ep r o p e r l y ,w en e e dt oc o n s i d e rt h e operations that can be performed on les. The operating system can provide system calls to create, write, read, reposition, delete, and truncate les. Lets examine what the operating system must do to perform each of these six basic le operations. It should then be easy to see how other similar operations, such as renaming a le, can be implemented. Creating a le .T w os t e p sa

can be implemented. Creating a le .T w os t e p sa r en e c e s s a r yt oc r e a t ea l e .F i r s t ,s p a c ei nt h e le system must be found for the le. We discuss how to allocate space for the le in Chapter 12. Second, an entry for the new le must be made in the directory. Writing a le .T ow r i t ea l e ,w em a k eas y s t e mc a l ls p e c i f y i n gb o t ht h e name of the le and the information to be written to the le. Given the name of the le, the system searches the directory to nd

of the le, the system searches the directory to nd the les location. The system must keep a write pointer to the location in the le where the next write is to take place. The write pointer must be updated whenever a write occurs. Reading a le .T or e a df r o ma l e ,w eu s eas y s t e mc a l lt h a ts p e c i  e st h e name of the le and where (in memory) the next block of the le should be put. Again, the directory is searched for the associated entry, and the system needs to keep a read

entry, and the system needs to keep a read pointer to the location in the le where the next read is to take place. Once the read has taken place, the read pointer is updated. Because a process is usually either reading from or writing to a l e ,t h ec u r r e n to p e r a t i o nl o c a t i o nc a nb ek e p ta sap e r  p r o c e s s current leposition pointer .B o t ht h er e a da n dw r i t eo p e r a t i o n su s et h i ss a m e pointer, saving space and reducing system complexity.

saving space and reducing system complexity. Repositioning within a le .T h ed i r e c t o r yi ss e a r c h e df o rt h ea p p r o p r i a t e entry, and the currentleposition pointer is repositioned to a given value. Repositioning within a le need not involve any actual IO.T h i s l e operation is also known as a le seek . Deleting a le .T od e l e t ea l e ,w es e a r c ht h ed i r e c t o r yf o rt h en a m e d l e . Having found the associated directory entry, we release all le space, so

directory entry, we release all le space, so that it can be reused by other les, and erase the directory entry. Truncating a le .T h eu s e rm a yw a n tt oe r a s et h ec o n t e n t so fa l eb u t keep its attributes. Rather than forcing the user to delete the le and then recreate it, this function allows all attributes to remain unchangedexcept for le lengthbut lets the le be reset to length zero and its le space released. These six basic operations comprise the minimal set of required le

operations comprise the minimal set of required le operations. Other common operations include appending new information11.1 File Concept 507 to the end of an existing le and renaming an existing le. These primitive operations can then be combined to per form other le operations. For instance, we can create a copy of a leor copy the le to another IOdevice, such as a printer or a displayby creating a new le and then reading from the old and writing to the new. We also want to have operations that

to the new. We also want to have operations that allow a user to get and set the various attributes of a le. For example, we may want to have operations that allow a user to determine the status of a le, such as the les length, and to set le attributes, such as the les owner. Most of the le operations mentioned involve searching the directory for the entry associated with the named le. To avoid this constant searching, many systems require that an open() system call be made before a le is rst

an open() system call be made before a le is rst used. The operating system keeps a table, called the openle table ,c o n t a i n i n g information about all open les. When a le operation is requested, the le is specied via an index into this table, so no searching is required. When the le is no longer being actively used, it is closed by the process, and the operating system removes its entry from the openle table. create() anddelete() are system calls that work with closed rather than open

calls that work with closed rather than open les. Some systems implicitly open a le when the rst reference to it is made. The le is automatically closed when the job or program that opened the le terminates. Most systems, however, require that the programmer open a le explicitly with the open() system call before that le can be used. The open() operation takes a le name and searches the directory, copying the directory entry into the openle table. The open() call can also accept access mode

table. The open() call can also accept access mode informationcreate, readonly, readwrite, appendonly, and so on. This mode is checked against the les permissions. If the request mode is allowed, the le is opened for the process. The open() system call typically returns a pointer to the entry in the openle table. This pointer, not the actual le name, is used in all IOoperations, avoiding any further searching and simplifying the systemcall interface. The implementation of the open() and close()

The implementation of the open() and close() operations is more complicated in an environment where several processes may open the le simultaneously. This may occur in a system where several different applications open the same le at the same time. Typically, the operating system uses two levels of internal tables: a perprocess table and a systemwide table. The per process table tracks all les that a process has open. Stored in this table is information regarding the processs use of the le. For

regarding the processs use of the le. For instance, the current le pointer for each le is found here. Access rights to the le and accounting information can also be included. Each entry in the perprocess table in turn points to a systemwide openle table. The systemwide table contains processindependent information, such as the location of the le on disk, access dates, and le size. Once a le has been opened by one process, the systemwide table includes an entry for the le. When another process

includes an entry for the le. When another process executes an open() call, a new entry is simply added to the processs openle table pointing to the appropriate entry in the systemwide table. Typically, the openle table also has an open count associated with each le to indicate how many processes have the le open. Each close() decreases this open count, and when the open count reaches zero, the le is no longer in use, and the les entry is removed from the openle table.508 Chapter 11 FileSystem

from the openle table.508 Chapter 11 FileSystem Interface In summary, several pieces of information are associated with an open le. File pointer .O ns y s t e m st h a td on o ti n c l u d ea l eo f f s e ta sp a r to ft h e read() and write() system calls, the system must track the last read write location as a currentleposition pointer. This pointer is unique to each process operating on the le and therefore must be kept separate from the ondisk le attributes. Fileopen count .A s l e sa r ec l

le attributes. Fileopen count .A s l e sa r ec l o s e d ,t h eo p e r a t i n gs y s t e mm u s tr e u s ei t s openle table entries, or it could run out of space in the table. Multiple processes may have opened a le, and the system must wait for the last le to close before removing the openle table entry. The leopen count tracks the number of opens and closes and reaches zero on the last close. The system can then remove the entry. Disk location of the le .M o s t l eo p e r a t i o n sr e q u

of the le .M o s t l eo p e r a t i o n sr e q u i r et h es y s t e mt om o d i f y data within the le. The information needed to locate the le on disk is kept in memory so that the system does not have to read it from disk for each operation. Access rights . Each process opens a le in an access mode. This information is stored on the perprocess table so the operating system can allow or deny subsequent IOrequests. Some operating systems provide facilities for locking an open le (or sections of

facilities for locking an open le (or sections of a le). File locks allow one pro cess to lock a le and prevent other processes from gaining access to it. File locks are useful for les that are shared by several processesfor example, a system log le that can be accessed and modied by a number of processes in the system. File locks provide functionality similar to readerwriter locks, covered in Section 5.7.2. A shared lock is akin to a reader lock in that several processes can acquire the lock

in that several processes can acquire the lock concurrently. An exclusive lock behaves like a writer lock; only one process at a time can acquire such a lock. It is important to note that not all operating systems provide both types of locks: some systems only provide exclusive le locking. FILE LOCKING IN JAVA In the Java API,a c q u i r i n gal o c kr e q u i r e s r s to b t a i n i n gt h e FileChannel for the le to be locked. The lock() method of the FileChannel is used to acquire the lock.

of the FileChannel is used to acquire the lock. The APIof the lock() method is FileLock lock(long begin, long end, boolean shared) where begin andend are the beginning and ending positions of the region being locked. Setting shared totrue is for shared locks; setting shared tofalse acquires the lock exclusively. The lock is released by invoking the release() of the FileLock returned by the lock() operation. The program in Figure 11.2 illustrates le locking in Java. This program acquires two

le locking in Java. This program acquires two locks on the le file.txt .T h e r s th a l fo ft h e l ei sa c q u i r e d as an exclusive lock; the lock for the second half is a shared lock.11.1 File Concept 509 FILE LOCKING IN JAVA (Continued) import java.io. ; import java.nio.channels. ; public class LockingExample  public static final boolean EXCLUSIVE  false; public static final boolean SHARED  true; public static void main(String args[]) throws IOException  FileLock sharedLock  null;

throws IOException  FileLock sharedLock  null; FileLock exclusiveLock  null; try  RandomAccessFile raf  new RandomAccessFile(file.txt,rw);  get the channel for the file FileChannel ch  raf.getChannel();  this locks the first half of the file  exclusive exclusiveLock  ch.lock(0, raf.length()2, EXCLUSIVE); Now modify the dat a...   release the lock exclusiveLock.release();  this locks the second half of the file  shared sharedLock  ch.lock(raf.length()21,raf.length(),SHARED); Now read the data . .

Now read the data . . .   release the lock sharedLock.release(); catch (java.io.IOException ioe)  System.err.println(ioe);  finally  if (exclusiveLock ! null) exclusiveLock.release(); if (sharedLock ! null) sharedLock.release();    Figure 11.2 Filelocking example in Java. Furthermore, operating systems may provide either mandatory oradvi sory lelocking mechanisms. If a lock is mandatory, then once a process acquires an exclusive lock, the operating system will prevent any other process510

operating system will prevent any other process510 Chapter 11 FileSystem Interface from accessing the locked le. For example, assume a process acquires an exclusive lock on the le system.log .I fw ea t t e m p tt oo p e n system.log from another processfor example, a text editorthe operating system will prevent access until the exclusive lock i sr e l e a s e d .T h i so c c u r se v e ni ft h et e x t editor is not written explicitly to acquire the lock. Alternatively, if the lock is advisory,

the lock. Alternatively, if the lock is advisory, then the operating system will not prevent the text editor from acquiring access to system.log .R a t h e r ,t h et e x te d i t o rm u s tb ew r i t t e ns ot h a t it manually acquires the lock before accessing the le. In other words, if the locking scheme is mandatory, the operating system ensures locking integrity. For advisory locking, it is up to software developers to ensure that locks are appropriately acquired and released. As a general

appropriately acquired and released. As a general rule, Windows operating systems adopt mandatory locking, and UNIX systems employ advisory locks. The use of le locks requires the same precautions as ordinary process synchronization. For example, programmers developing on systems with mandatory locking must be careful to hold exclusive le locks only while they are accessing the le. Otherwise, they will prevent other processes from accessing the le as well. Furthermore, some measures must be

the le as well. Furthermore, some measures must be taken to ensure that two or more processes do not become involved in a deadlock while trying to acquire le locks. 11.1.3 File Types When we design a le systemindeed, an entire operating systemwe always consider whether the operating system should recognize and support le types. If an operating system recognizes the type of a le, it can then operate on the le in reasonable ways. For example, a common mistake occurs when a user tries to output the

mistake occurs when a user tries to output the binaryobject form of a program. This attempt normally produces garbage; however, the attempt can succeed if the operating system has been told that the le is a binaryobject program. Ac o m m o nt e c h n i q u ef o ri m p l e m e n t i n g l et y p e si st oi n c l u d et h et y p e as part of the le name. The name is split into two partsa name and an extension, usually separated by a period (Figure 11.3). In this way, the user and the operating

11.3). In this way, the user and the operating system can tell from the name alone what the type of a le is. Most operating systems allow users to specify a le name as a sequence of characters followed by a period and terminated by an extension made up of additional characters. Examples include resume.docx ,server.c ,a n d ReaderThread.cpp . The system uses the extension to indicate the type of the le and the type of operations that can be done on that le. Only a le with a .com ,.exe ,o r.sh

on that le. Only a le with a .com ,.exe ,o r.sh extension can be executed, for instance. The .com and.exe les are two forms of binary executable les, whereas the .shle is a shell script containing, in ASCII format, commands to the operating system. Application programs also use extensions to indicate le types in which they are interested. For example, Java compilers expect source les to have a .java extension, and the Microsoft Word word processor expects its les to end with a .doc or.docx

expects its les to end with a .doc or.docx extension. These extensions are not always required, so a user may specify a le without the extension (to save typing), and the application will look for a le with the given name and the extension it e xpects. Because these extensions are not supported by the operating system, they can be considered hints to the applications that operate on them.11.1 File Concept 511file type usual extension function readytorun machine language program executable exe,

machine language program executable exe, com, bin or none compiled, machine language, not linked object obj, o binary file containing audio or AV information multimedia mpeg, mov, mp3, mp4, avirelated files grouped into one file, sometimes com pressed, for archiving or storagearchive rar, zip, tarASCII or binary file in a format for printing or viewingprint or view gif, pdf, jpglibraries of routines for programmerslibrary lib, a, so, dllvarious wordprocessor formatsword processor docxcommands to

formatsword processor docxcommands to the command interpreterbatch bat, sh textual data, documents markup xml, html, texsource code in various languagessource code c, cc, java, perl, asm xml, rtf, Figure 11.3 Common le types. Consider, too, the Mac OS X operating system. In this system, each le has a type, such as .app (for application). Each le also has a creator attribute containing the name of the program that created it. This attribute is set by the operating system during the create() call,

by the operating system during the create() call, so its use is enforced and supported by the system. For instance, a le produced by a word processor has the word processors name as its creator. When the user opens that le, by doubleclicking the mouse on the icon representing the le, the word processor is invoked automatically and the le is loaded, ready to be edited. The UNIX system uses a crude magic number stored at the beginning of some les to indicate roughly the type of the leexecutable

to indicate roughly the type of the leexecutable program, shell script, PDF le, and so on. Not all les have magic numbers, so system features cannot be based solely on this information. UNIX does not record the name of the creating program, either. UNIX does allow lenameextension hints, but these extensions are neither enforce dn o rd e p e n d e do nb yt h eo p e r a t i n gs y s t e m ; they are meant mostly to aid users in determining what type of contents the le contains. Extensions can be

of contents the le contains. Extensions can be used or ignored by a given application, but that is up to the applications programmer. 11.1.4 File Structure File types also can be used to indicate the internal structure of the le. As mentioned in Section 11.1.3, source and object les have structures that match the expectations of the programs that read them. Further, certain les must512 Chapter 11 FileSystem Interface conform to a required structure that is understood by the operating system. For

that is understood by the operating system. For example, the operating system requires that an executable le have a specic structure so that it can determine where in memory to load the le and what the location of the rst instruction is. Some operating systems extend this idea into a set of systemsupported le structures, with sets of special operations for manipulating les with those structures. This point brings us to one of the disadvantages of having the operating system support multiple le

of having the operating system support multiple le structures: the resulting size of the operating system is cumbersome. If the operating system denes ve different le structures, it needs to contain the code to support these le structures. In addition, it may be necessary to dene every le as one of the le types supported by the operating system. When new applications require information structured in ways not supported by the operating system, severe problems may result. For example, assume that

problems may result. For example, assume that a system supports two types of les: text les (composed of ASCII characters separated by a carriage return and line feed) and executable binary les. Now, if we (as users) want to dene an encrypted le to protect the contents from being read by unauthorized people, we may nd neither le type to be appropriate. The encrypted le is not ASCII text lines but rather is (apparently) random bits. Although it may appear to be a binary le, it is not executable.

appear to be a binary le, it is not executable. As a result, we may have to circumvent or misuse the operating systems letype mechanism or abandon our encryption scheme. Some operating systems impose (and support) a minimal number of le structures. This approach has been adopted in UNIX ,W i n d o w s ,a n do t h e r s . UNIX considers each le to be a sequence of 8bit bytes; no interpretation of these bits is made by the operating system. This scheme provides maximum exibility but little

This scheme provides maximum exibility but little support. Each application program must include its own code to interpret an input le as to the appropriate structure. However, all operating systems must support at least one structurethat of an executable leso that the system is able to load and run programs. 11.1.5 Internal File Structure Internally, locating an offset within a le can be complicated for the operating system. Disk systems typically have a welldened block size determined by the

have a welldened block size determined by the size of a sector. All disk IOis performed in units of one block (physical record), and all blocks are the same size. It is unlikely that the physical record size will exactly match the length of the desired logical record. Logical records may even vary in length. Packing a number of logical records into physical blocks is a common solution to this problem. For example, the UNIX operating system denes all les to be simply streams of bytes. Each byte

all les to be simply streams of bytes. Each byte is individually addressable by its offset from the beginning (or end) of the le. In this case, the logical record size is 1 byte. The le system automatically packs and unpacks bytes into physical disk blocks say, 512 bytes per blockas necessary. The logical record size, physical block size, and packing technique deter mine how many logical records are in each physical block. The packing can be done either by the users application program or by the

either by the users application program or by the operating system. In either case, the le may be consider ed a sequence of blocks. All the basic IO11.2 Access Methods 513beginning endcurrent position rewind read or write Figure 11.4 Sequentialaccess le. functions operate in terms of blocks. The conversion from logical records to physical blocks is a relatively simple software problem. Because disk space is always allocated in blocks, some portion of the last block of each le is generally

portion of the last block of each le is generally wasted. If each block were 512 bytes, for example, then a le of 1,949 bytes would be allocated four blocks (2,048 bytes); the last 99 bytes would be wasted. The waste in curred to keep everything in units of blocks (instead of bytes) is internal fragmentation. All le systems suffer from internal fragmentation; the larger the block size, the greater the internal fragmentation. 11.2 Access Methods Files store information. When it is used, this

Files store information. When it is used, this information must be accessed and read into computer memory. The information in the le can be accessed in several ways. Some systems provide only one access method for les. while others support many access methods, and choosing the right one for ap a r t i c u l a ra p p l i c a t i o ni sam a j o rd e s i g np r o b l e m . 11.2.1 Sequential Access The simplest access method is sequential access . Information in the le is processed in order, one

. Information in the le is processed in order, one record after the other. This mode of access is by far the most common; for example, editors and compilers usually access les in this fashion. Reads and writes make up the bulk of the operations on a le. A read operation read next() readsthenextportionoftheleandautomatically advances a le pointer, which tracks the IO location. Similarly, the write operation write next() appendstotheendoftheleandadvancestothe end of the newly written material (the

end of the newly written material (the new end of le). Such a le can be reset to the beginning, and on some systems, a program may be able to skip forward or backward nrecords for some integer nperhaps only for n1 .S e q u e n t i a l access, which is depicted in Figure 11.4, is based on a tape model of a le and works as well on sequentialaccess devices as it does on randomaccess ones. 11.2.2 Direct Access Another method is direct access (orrelative access ). Here, a le is made up of xedlength

access ). Here, a le is made up of xedlength logical records that allow programs to read and write records rapidly in no particular order. The directaccess method is based on a disk model of a le, since disks allow random access to any le block. For direct514 Chapter 11 FileSystem Interface access, the le is viewed as a numbered s equence of blocks or records. Thus, we may read block 14, then read block 53, and then write block 7. There are no restrictions on the order of reading or writing for

on the order of reading or writing for a directaccess le. Directaccess les are of great use for immediate access to large amounts of information. Databases are often of this type. When a query concerning a particular subject arrives, we compute which block contains the answer and then read that block directly to provide the desired information. As a simple example, on an airlinereservation system, we might store all the information about a particular ight (for example, ight 713) in the block

ight (for example, ight 713) in the block identied by the ight number. Thus, the number of available seats for ight 713 is stored in block 713 of the reservation le. To store information about a larger set, such as people, we might compute a hash function on the peoples names or search a small inmemory index to determine a block to read and search. For the directaccess method, the le operations must be modied to include the block number as a parameter. Thus, we have read(n) ,w h e r e nis the

Thus, we have read(n) ,w h e r e nis the block number, rather than read next() ,a n d write(n) rather than write next() .A na l t e r n a t i v ea p p r o a c hi st or e t a i n read next() and write next() ,a sw i t hs e q u e n t i a la c c e s s ,a n dt oa d da no p e r a t i o n posi tion file(n) where nis the block number. Then, to effect a read(n) ,w e would position file(n) and then read next() . The block number provided by the user to the operating system is normally arelative block

the operating system is normally arelative block number .Ar e l a t i v eb l o c kn u m b e ri sa ni n d e xr e l a t i v et ot h e beginning of the le. Thus, the rst relative block of the le is 0, the next is 1, and so on, even though the absolute disk address may be 14703 for the rst block and 3192 for the second. The use of relative block numbers allows the operating system to decide where the le should be placed (called the allocation problem ,a sw ed i s c u s si nC h a p t e r1 2 )a n dh e

,a sw ed i s c u s si nC h a p t e r1 2 )a n dh e l p st op r e v e n tt h eu s e r from accessing portions of the le system that may not be part of her le. Some systems start their relative block numbers at 0; others start at 1. How, then, does the system satisfy a request for record Nin a le? Assuming we have a logical record length L,the request for record Nis turned into an IOrequest for Lbytes starting at location L(N)w i t h i nt h e l e( a s s u m i n gt h e rst record is N0 ) .S i n c el

s s u m i n gt h e rst record is N0 ) .S i n c el o g i c a lr e c o r d sa r eo fa x e ds i z e ,i ti sa l s oe a s yt o read, write, or delete a record. Not all operating systems support both sequential and direct access for les. Some systems allow only sequential le access; others allow only direct access. Some systems require that a le be dened as sequential or direct when it is created. Such a le can be accessed only in a manner consistent with its declaration. We can easily simulate

with its declaration. We can easily simulate sequential access on a directaccess le by simply keeping a variable cpthat denes our current position, as shown in Figure 11.5. Simulating a directaccess le on a sequentialaccess le, however, is extremely inefcient and clumsy. 11.2.3 Other Access Methods Other access methods can be built on top of a directaccess method. These methods generally involve the construction of an index for the le. The index , like an index in the back of a book, contains

, like an index in the back of a book, contains pointers to the various blocks. To11.3 Directory and Disk Structure 515sequential access reset readnext writenextcp 0; read cp ; cp cp 1; write cp ; cp cp 1;implementation for direct access Figure 11.5 Simulation of sequential access on a directaccess le. nd a record in the le, we rst search the index and then use the pointer to access the le directly and to nd the desired record. For example, a retailprice le might list the universal product codes

le might list the universal product codes ( UPCs) for items, with the associated prices. Each record consists of a 10digit UPC and a6  d i g i tp r i c e ,f o ra1 6  b y t er e c o r d .I fo u rd i s kh a s1 , 0 2 4b y t e sp e rb l o c k ,w e can store 64 records per block. A le of 120,000 records would occupy about 2,000 blocks (2 million bytes). By keeping the le sorted by UPC,w ec a nd e  n e an index consisting of the rst UPC in each block. This index would have 2,000 entries of 10 digits

This index would have 2,000 entries of 10 digits each, or 20,000 bytes, and thus could be kept in memory. To nd the price of a particular item, we can make a binary search of the index. From this search, we learn exactly which block contains the desired record and access that block. This structure allows us to search a large le doing little IO. With large les, the index le itself may become too large to be kept in memory. One solution is to create an index for the index le. The primary index le

an index for the index le. The primary index le contains pointers to secondary index les, which point to the actual data items. For example, IBMs indexed sequentialaccess method ( ISAM )u s e sas m a l l master index that points to disk blocks of a secondary index. The secondary index blocks point to the actual le blocks. The le is kept sorted on a dened key. To nd a particular item, we rst make a binary search of the master index, which provides the block number of the secondary index. This

the block number of the secondary index. This block is read in, and again a binary search is used to nd the block containing the desired record. Finally, this block is searched sequentially. In this way, any record can be located from its key by at most two directaccess reads. Figure 11.6 shows a similar situation as implemented by VMS index and relative les. 11.3 Directory and Disk Structure Next, we consider how to store les. Certainly, no generalpurpose computer stores just one le. There are

computer stores just one le. There are typically thousands, millions, even billions of les within a computer. Files are stored on randomaccess storage devices, including hard disks, optical disks, and solidstate (memorybased) disks. As t o r a g ed e v i c ec a nb eu s e di ni t se n t i r e t yf o ra l es y s t e m .I tc a na l s ob e subdivided for nergrained control. For example, a disk can be partitioned into quarters, and each quarter can hold a separate le system. Storage devices can also

a separate le system. Storage devices can also be collected together into RAID sets that provide protection from the failure of a single disk (as described in Section 10.7). Sometimes, disks are subdivided and also collected into RAID sets.516 Chapter 11 FileSystem Interface index file relative fileSmithlast name smith, john socialsecurity agelogical record number Adams Arthur Asher    Figure 11.6 Example of index and relative les. Partitioning is useful for limiting the sizes of individual le

is useful for limiting the sizes of individual le systems, putting multiple lesystem types on the same device, or leaving part of the device available for other uses, such as swap space or unformatted (raw) disk space. A le system can be created on each of these parts of the disk. Any entity containing a le system is generally known as a volume .T h ev o l u m em a yb e as u b s e to fad e v i c e ,aw h o l ed e v i c e , or multiple devices linked together into aRAID set. Each volume can be

linked together into aRAID set. Each volume can be thought of as a virtual disk. Volumes can also store multiple operating systems, allowing a system to boot and run more than one operating system. Each volume that contains a le system must also contain information about the les in the system. This information is kept in entries in a device directory orvolume table of contents .T h ed e v i c ed i r e c t o r y( m o r ec o m m o n l y known simply as the directory )r e c o r d si n f o r m a t i

as the directory )r e c o r d si n f o r m a t i o n  s u c ha sn a m e ,l o c a t i o n , size, and typefor all les on that volume. Figure 11.7 shows a typical lesystem organization. directory directory directoryfilespartition A partition Bpartition C filesdisk 1disk 2 disk 3files Figure 11.7 At y p i c a l l e  s y s t e mo r g a n i z a t i o n .11.3 Directory and Disk Structure 517  ufs devices devfs dev dev systemcontract ctfs proc proc etcmnttab mntfs etcsvcvolatile tmpfs systemobject

etcmnttab mntfs etcsvcvolatile tmpfs systemobject objfs liblibc.so.1 lofs devfd fd var ufs tmp tmpfs varrun tmpfs opt ufs zpbge zfs zpbgebackup zfs exporthome zfs varmail zfs varspoolmqueue zfs zpbg zfs zpbgzones zfs Figure 11.8 Solaris le systems. 11.3.1 Storage Structure As we have just seen, a generalpurpose computer system has multiple storage devices, and those devices can be slic ed up into volumes that hold le systems. Computer systems may have zero or more le systems, and the le systems

have zero or more le systems, and the le systems may be of varying types. For example, a typical Solaris system may have dozens of le systems of a dozen different types, as shown in the le system list in Figure 11.8. In this book, we consider only generalpurpose le systems. It is worth noting, though, that there are many specialpurpose le systems. Consider the types of le systems in the Solaris example mentioned above: tmpfs atemporary le system that is created in volatile main memory and has

that is created in volatile main memory and has its contents erased if the system reboots or crashes objfs a virtual le system (essentially an interface to the kernel that looks like a le system) that gives debuggers access to kernel symbols ctfsavirtuallesystemthatmaintains contract information to manage which processes start when the system boots and must continue to run during operation lofsaloop back le system that allows one le system to be accessed in place of another one procfs a virtual

accessed in place of another one procfs a virtual le system that presents information on all processes as a l es y s t e m ufs, zfs generalpurpose le systems518 Chapter 11 FileSystem Interface The le systems of computers, then, can be extensive. Even within a le system, it is useful to segregate les into groups and manage and act on those groups. This organization involves the use of directories. In the remainder of this section, we explore the topic of directory structure. 11.3.2 Directory

the topic of directory structure. 11.3.2 Directory Overview The directory can be viewed as a symbol table that translates le names into their directory entries. If we take such a view, we see that the directory itself can be organized in many ways. The organization must allow us to insert entries, to delete entries, to search for a named entry, and to list all the entries in the directory. In this section, we examine several schemes for dening the logical structure of the directory system. When

logical structure of the directory system. When considering a particular directory st ructure, we need to keep in mind the operations that are to be performed on a directory: Search for a le . We need to be able to search a directory structure to nd the entry for a particular le. Since les have symbolic names, and similar names may indicate a relationship among les, we may want to be able to nd all les whose names match a particular pattern. Create a le .N e w l e sn e e dt ob ec r e a t e da n

a le .N e w l e sn e e dt ob ec r e a t e da n da d d e dt ot h ed i r e c t o r y . Delete a le .W h e na l ei sn ol o n g e rn e e d e d ,w ew a n tt ob ea b l et or e m o v e it from the directory. List a directory .W en e e dt ob ea b l et ol i s tt h e l e si nad i r e c t o r ya n dt h e contents of the directory entry for each le in the list. Rename a le .B e c a u s et h en a m eo fa l er e p r e s e n t si t sc o n t e n t st oi t su s e r s , we must be able to change the name when the

r s , we must be able to change the name when the contents or use of the le changes. Renaming a le may also allow its position within the directory structure to be changed. Traverse the le system . We may wish to access every directory and every le within a directory structure. For reliability, it is a good idea to save the contents and structure of the entire le system at regular intervals. Often, we do this by copying all les to magnetic tape. This technique provides a backup copy in case of

This technique provides a backup copy in case of system failure. In addition, if a le is no longer in use, the le can be copied to tape and the disk space of that le released for reuse by another le. In the following sections, we describe the most common schemes for dening the logical structure of a directory. 11.3.3 SingleLevel Directory The simplest directory structure is the singlelevel directory. All les are contained in the same directory, which is easy to support and understand (Figure

which is easy to support and understand (Figure 11.9). As i n g l e  l e v e ld i r e c t o r yh a ss i g n i  c a n tl i m i t a t i o n s ,h o w e v e r ,w h e nt h e number of les increases or when the system has more than one user. Since all les are in the same directory, they must have unique names. If two users call11.3 Directory and Disk Structure 519cat filesdirectory bo a test data mail cont hex records Figure 11.9 Singlelevel directory. their data le test.txt ,t h e nt h eu n i q u e

their data le test.txt ,t h e nt h eu n i q u e  n a m er u l ei sv i o l a t e d .F o re x a m p l e , in one programming class, 23 students called the program for their second assignment prog2.c ; another 11 called it assign2.c .F o r t u n a t e l y ,m o s t l e systems support le names of up to 255 characters, so it is relatively easy to select unique le names. Even a single user on a singlelevel directory may nd it difcult to remember the names of all the les as the number of les increases.

of all the les as the number of les increases. It is not uncommon for a user to have hundreds of les on one computer system and an equal number of additional les on another system. Keeping track of so many les is a daunting task. 11.3.4 TwoLevel Directory As we have seen, a singlelevel directory often leads to confusion of le names among different users. The standard solution is to create a separate directory for each user. In the twolevel directory structure, each user has his own user le

directory structure, each user has his own user le directory (UFD ).T h e UFDsh a v es i m i l a rs t r u c t u r e s ,b u te a c hl i s t so n l yt h e les of a single user. When a user job starts or a user logs in, the systems master le directory (MFD )is searched. The MFD is indexed by user name or account number, and each entry points to the UFD for that user (Figure 11.10). When a user refers to a particular le, only his own UFD is searched. Thus, different users may have les with the same

Thus, different users may have les with the same name, as long as all the le names within each UFD are unique. To create a le for a user, the operating system searches only that users UFD to ascertain whether another le of that name exists. To delete a le, the operating system connes its search to the local UFD; thus, it cannot accidentally delete another users le that has the same name. cat bo a test x dataa auser 1 user 2 user 3 user 4 data a testuser file directorymaster file directory Figure

file directorymaster file directory Figure 11.10 Twolevel directory structure.520 Chapter 11 FileSystem Interface The user directories themselves must be created and deleted as necessary. A special system program is run with the appropriate user name and account information. The program creates a new UFD and adds an entry for it to the MFD . The execution of this program might be restricted to system administrators. The allocation of disk space for user directories can be handled with the

space for user directories can be handled with the techniques discussed in Chapter 12 for les themselves. Although the twolevel directory structure solves the namecollision prob lem, it still has disadvantages. This structure effectively isolates one user from another. Isolation is an advantage when the users are completely independent but is a disadvantage when the users want to cooperate on some task and to access one anothers les. Some systems simply do not allow local user les to be accessed

simply do not allow local user les to be accessed by other users. If access is to be permitted, one user must have the ability to name a le in another users directory. To name a particular le uniquely in a twolevel directory, we must give both the user name and the le name. A twolevel directory can be thought of as a tree, or an inverted tree, of height 2. The root of the tree is the MFD .I t sd i r e c td e s c e n d a n t sa r et h e UFDs. The descendants of the UFDsa r et h e l e st h e m s e

descendants of the UFDsa r et h e l e st h e m s e l v e s .T h e l e sa r et h el e a v e so ft h et r e e .S p e c i f y i n g au s e rn a m ea n da l en a m ed e  n e sap a t hi nt h et r e ef r o mt h er o o t( t h e MFD ) to a leaf (the specied le). Thus, a user name and a le name dene a path name .E v e r y l ei nt h es y s t e mh a sap a t hn a m e .T on a m ea l eu n i q u e l y ,au s e r must know the path name of the le desired. For example, if user A wishes to access her own test le

if user A wishes to access her own test le named test.txt , she can simply refer to test.txt .T oa c c e s st h e l en a m e d test.txt of user B (with directoryentry name userb ), however, she might have to refer touserbtest.txt .E v e r ys y s t e mh a si t so w ns y n t a xf o rn a m i n g l e si n directories other than the users own. Additional syntax is needed to specify th ev o l u m eo fa l e .F o ri n s t a n c e , in Windows a volume is specied by a letter followed by a colon. Thus, a

specied by a letter followed by a colon. Thus, a le specication might be C:userb test .S o m es y s t e m sg oe v e nf u r  ther and separate the volume, directory name, and le name parts of the specication. In VMS ,f o ri n s t a n c e ,t h e l e login.com might be specied as: u:[sst.jdeck]login.com;1 ,w h e r e uis the name of the volume, sstis the name of the directory, jdeck is the name of the subdirectory, and 1is the version number. Other systemssuch as UNIX and Linuxsimply treat the

systemssuch as UNIX and Linuxsimply treat the volume name as part of the directory name. The rst name given is that of the volume, and the rest is the directory and le. For instance, upbgtest might specify volume u,d i r e c t o r y pbg,a n d l e test . A special instance of this situation occurs with the system les. Programs provided as part of the systemloaders, assemblers, compilers, utility rou tines, libraries, and so onare generally dened as les. When the appropriate commands are given to

as les. When the appropriate commands are given to the operating system, these les are read by the loader and executed. Many command interpreters simply treat such a command as the name of a le to load and execute. In the directory system as we dened it above, this le name would be searched for in the current UFD.O n es o l u t i o n would be to copy the system les into each UFD. However, copying all the system les would waste an enormous amount of space. (If the system les require 5 MB,t h e ns

of space. (If the system les require 5 MB,t h e ns u p p o r t i n g1 2u s e r sw o u l dr e q u i r e5 12  60 MBjust for copies of the system les.)11.3 Directory and Disk Structure 521 The standard solution is to complicate the search procedure slightly. A special user directory is dened to contain the system les (for example, user 0). Whenever a le name is given to be loaded, the operating system rst searches the local UFD.I ft h e l ei sf o u n d ,i ti su s e d .I fi ti sn o tf o u n d ,t h

u n d ,i ti su s e d .I fi ti sn o tf o u n d ,t h es y s t e m automatically searches the special user directory that contains the system les. The sequence of directories searched when a le is named is called the search path .T h es e a r c hp a t hc a nb ee x t e n d e dt oc o n t a i na nu n l i m i t e dl i s to fd i r e c t o r i e s to search when a command name is given. This method is the one most used inUNIX and Windows. Systems can also be designed so that each user has his own search

be designed so that each user has his own search path. 11.3.5 TreeStructured Directories Once we have seen how to view a twolevel directory as a twolevel tree, the natural generalization is to extend the directory structure to a tree of arbitrary height (Figure 11.11). This generalization allows users to create their own subdirectories and to organize their les accordingly. A tree is the most common directory structure. The tree has a root directory, and every le in the system has a unique path

and every le in the system has a unique path name. Ad i r e c t o r y( o rs u b d i r e c t o r y )c o n t a i n sas e to f l e so rs u b d i r e c t o r i e s .A directory is simply another le, but it is treated in a special way. All directories have the same internal format. One bit in each directory entry denes the entry as a le (0) or as a subdirectory (1). Special system calls are used to create and delete directories. In normal use, each process has a current directory. The current

each process has a current directory. The current directory should contain most of the les that are of current interest to the process. When reference is made to a le, the current directory is searched. If a le is needed that is not in the current directory, then the user usually must list obj spellfind count hex reorderstat mail distroot spell bin programsp e mail reorder list find prog copy prt exp last firsthex count all Figure 11.11 Treestructured directory structure.522 Chapter 11

Treestructured directory structure.522 Chapter 11 FileSystem Interface either specify a path name or change the c urrent directory to be the directory holding that le. To change directories, a system call is provided that takes a directory name as a parameter and uses it to redene the current directory. Thus, the user can change her current d irectory whenever she wants. From one change directory() system call to the next, all open() system calls search the current directory for the specied le.

search the current directory for the specied le. Note that the search path may or may not contain a special entry that stands for the current directory.  The initial current directory of a u sers login shell is designated when the user job starts or the user logs in. The operating system searches the accounting le (or some other predened location) to nd an entry for this user (for accounting purposes). In the accounting le is a pointer to (or the name of) the users initial directory. This

(or the name of) the users initial directory. This pointer is copied to a local variable for this user that species the users initial current directory. From that shell, other processes can be spawned. The current directory of any subprocess is usually the current directory of the parent when it was spawned. Path names can be of two types: absolute and relative. An absolute path name begins at the root and follows a path down to the specied le, giving the directory names on the path. A relative

giving the directory names on the path. A relative path name denes a path from the current directory. For example, in the treestructured le system of Figure 11.11, if the current directory is rootspellmail , then the relative path name prtfirst refers to the same le as does the absolute path name rootspellmailprtfirst . Allowing a user to dene her own subdirectories permits her to impose as t r u c t u r eo nh e r l e s .T h i ss t r u c t u r em i g h tr e s u l ti ns e p a r a t ed i r e c t o

i g h tr e s u l ti ns e p a r a t ed i r e c t o r i e sf o r les associated with different topics (for example, a subdirectory was created to hold the text of this book) or different forms of information (for example, the directory programs may contain source programs; the directory binmay store all the binaries). An interesting policy decision in a treestructured directory concerns how to handle the deletion of a directory. If a directory is empty, its entry in the directory that contains it

empty, its entry in the directory that contains it can simply be deleted. However, suppose the directory to be deleted is not empty but contains several les or subdirectories. One of two approaches can be taken. Some systems will not delete a directory unless it is empty. Thus, to delete a directory, the user must rst delete all the les in that directory. If any subdirectories exist, this procedure must be applied recursively to them, so that they can be deleted also. This approach can result in

can be deleted also. This approach can result in a substantial amount of work. An alternative approach, such as that taken by the UNIX rmcommand, is to provide an option: when a request is made to delete a directory, all that directorys les and subdirectories are also to be deleted. Either approach is fairly easy to implement; the choice is one of policy. The latter policy is more convenient, but it is also more dangerous, because an entire directory structure can be removed with one command. If

structure can be removed with one command. If that command is issued in error, a large number of les and directories will need to be restored (assuming a backup exists). With a treestructured directory system, users can be allowed to access, in addition to their les, the les of other users. For example, user B can access a le of user A by specifying its path names. User B can specify either an absolute or a relative path name. Alternatively, user B can change her current directory to be user As

B can change her current directory to be user As directory and access the le by its le names.11.3 Directory and Disk Structure 523 11.3.6 AcyclicGraph Directories Consider two programmers who are working on a joint project. The les asso ciated with that project can be stored in a subdirectory, separating them from other projects and les of the two programmers. But since both programmers are equally responsible for the project, both want the subdirectory to be in their own directories. In this

to be in their own directories. In this situation, the common subdirectory should be shared. A shared directory or le exists in the le system in two (or more) places at once. A tree structure prohibits the sharing of les or directories. An acyclic graph that is, a graph with no cyclesallows directories to share subdirectories and les (Figure 11.12). The same le or subdirectory may be in two different directories. The acyclic graph is a natural generalization of the treestructured directory

generalization of the treestructured directory scheme. It is important to note that a shared le (or directory) is not the same as two copies of the le. With two copies, each programmer can view the copy rather than the original, but if one programmer changes the le, the changes will not appear in the others copy. With a shared le, only one actual le exists, so any changes made by one person are immediately visible to the other. Sharing is particularly important for subdirectories; a new le

important for subdirectories; a new le created by one person will automatically appear in all the shared subdirectories. When people are working as a team, all the les they want to share can be put into one directory. The UFD of each team member will contain this directory of shared les as a subdirectory. Even in the case of a single user, the users le organization may require that some le be placed in different subdirectories. For example, a program written for a particular project should be

program written for a particular project should be both in the directory of all programs and in the directory for that project. Shared les and subdirectories can be implemented in several ways. A common way, exemplied by many of the UNIX systems, is to create a new directory entry called a link. A link is effectively a pointer to another le list all w count wordslist list rade w7countroot dict spell Figure 11.12 Acyclicgraph directory structure.524 Chapter 11 FileSystem Interface or

structure.524 Chapter 11 FileSystem Interface or subdirectory. For example, a link may be implemented as an absolute or a relative path name. When a reference to a le is made, we search the directory. If the directory entry is marked as a link, then the name of the real le is included in the link information. We resolve the link by using that path name to locate the real le. Links are easily identied by their format in the directory entry (or by having a special type on systems that support

by having a special type on systems that support types) and are effectively indirect pointers. The operating system ignores these links when traversing directory trees to preserve the acyclic structure of the system. Another common approach to implementing shared les is simply to duplicate all information about them in both sharing directories. Thus, both entries are identical and equal. Consider th ed i f f e r e n c eb e t w e e nt h i sa p p r o a c h and the creation of a link. The link is

r o a c h and the creation of a link. The link is clearly different from the original directory entry; thus, the two are not equal. D uplicate directory entries, however, make the original and the copy indistinguishable. A major problem with duplicate directory entries is maintaining consistency when a le is modied. An acyclicgraph directory structure is more exible than a simple tree structure, but it is also more complex. Several problems must be considered carefully. A le may now have

must be considered carefully. A le may now have multiple absolute path names. Consequently, distinct le names may refer to the same le. This situation is similar to the aliasing problem for programming languages. If we are trying to traverse the entire le systemto nd a le, to accumulate statistics on all les, or to copy all les to backup storagethis problem becomes signicant, since we do not want to traverse shared structures more than once. Another problem involves deletion. When can the space

problem involves deletion. When can the space allocated to a shared le be deallocated and reused? One possibility is to remove the le whenever anyone deletes it, but this ac tion may leave dangling pointers to the nownonexistent le. Worse, if the remaining le pointers contain actual disk addresses, and the space is subsequently reused for other les, these dangling pointers may point into the middle of other les. In a system where sharing is implemented by symbolic links, this situation is

implemented by symbolic links, this situation is somewhat easier to handle. The deletion of a link need not affect the original le; only the link is removed. If the le entry itself is deleted, the space for the le is deallocated, leaving the links dangling. We can search for these links and remove them as well, but unless a list of the associated links is kept with each le, this search can be expen sive. Alternatively, we can leave the links until an attempt is made to use them. At that time, we

an attempt is made to use them. At that time, we can determine that the le of the name given by the link does not exist and can fail to resolve the link name; the access is treated just as with any other illegal le name. (In this case, the system designer should consider carefully what to do when a le is deleted and another le of the same name is created, before a symbolic link to the original le is used.) In the case of UNIX , symbolic links are left when a le is deleted, and it is up to the

are left when a le is deleted, and it is up to the user to realize that the original le is gone or has been replaced. Microsoft Windows uses the same approach. Another approach to deletion is to preserve the le until all references to it are deleted. To implement this approach, we must have some mechanism for determining that the last reference to the le has been deleted. We could keep a list of all references to a le (directory entries or symbolic links). When al i n ko rac o p yo ft h ed i r e

links). When al i n ko rac o p yo ft h ed i r e c t o r ye n t r yi se s t a b l i s h e d ,an e we n t r yi sa d d e dt o the lereference list. When a link or directory entry is deleted, we remove its entry on the list. The le is deleted when its lereference list is empty.11.3 Directory and Disk Structure 525 The trouble with this approach is the variable and potentially large size of the lereference list. However, we really do not need to keep the entire listwe need to keep only a count of the

the entire listwe need to keep only a count of the number of references. Adding a new link or directory entry increments the reference count. Deleting a link or entry decrements the count. When the count is 0, the le can be deleted; there are no remaining references to it. The UNIX operating system uses this approach for nonsymbolic links (or hard links ), keeping a reference count in the le information block (or inode; see Section A.7.2). By effectively prohibiting multiple references to

By effectively prohibiting multiple references to directories, we maintain an acyclicgraph structure. To avoid problems such as the ones just discussed, some systems simply do not allow shared directories or links. 11.3.7 General Graph Directory As e r i o u sp r o b l e mw i t hu s i n ga na c y c l i c  g r a p hs t r u c t u r ei se n s u r i n gt h a tt h e r e are no cycles. If we start with a twolevel directory and allow users to create subdirectories, a treestructured directory results.

a treestructured directory results. It should be fairly easy to see that simply adding new les and subdi rectories to an existing treestructured directory preserves the treestructured nature. However, when we add links, the tree structure is destroyed, resulting in a simple graph structure (Figure 11.13). The primary advantage of an acyclic graph is the relative simplicity of the algorithms to traverse the graph and to determine when there are no more references to a le. We want to avoid

are no more references to a le. We want to avoid traversing shared sections of an acyclic graph twice, mainly for performance reasons. If we have just searched a major shared subdirectory for a particular le without nding it, we want to avoid searching that subdirectory again; the se cond search would be a waste of time. If cycles are allowed to exist in the directory, we likewise want to avoid searching any component twice, for reasons of correctness as well as performance. A poorly designed

as well as performance. A poorly designed algorithm might result in an innite loop continually searching through the c ycle and never terminating. One solution text mailavi count unhex hexcount book book mail unhex hyproot avi tc jim Figure 11.13 General graph directory.526 Chapter 11 FileSystem Interface is to limit arbitrarily the number of directories that will be accessed during a search. As i m i l a rp r o b l e me x i s t sw h e nw ea r et r y i n gt od e t e r m i n ew h e na l e can be

r y i n gt od e t e r m i n ew h e na l e can be deleted. With acyclicgraph directory structures, a value of 0 in the reference count means that there are no more references to the le or directory, and the le can be deleted. However, when cycles exist, the reference count may not be 0 even when it is no longer possible to refer to a directory or le. This anomaly results from the possibility of selfreferencing (or a cycle) in the directory structure. In this case, we generally need to use a

In this case, we generally need to use a garbage collection scheme to determine when the last reference has been deleted and the disk space can be reallocated. Garbage collection involves traversing the entire le system, marking everything that can be accessed. Then, a second pass collects everything that is not marked onto a list of free space. (A similar marking procedure can be used to ensure that a traversal or search will cover everything in the le system once and only once.) Garbage

in the le system once and only once.) Garbage collection for a diskbased le system, however, is extremely time consuming and is thus seldom attempted. Garbage collection is necessary only because of possible cycles in the graph. Thus, an acyclicgraph structure is much easier to work with. The difculty is to avoid cycles as new links are added to the structure. How do we know when a new link will complete a cycle? There are algorithms to detect cycles in graphs; however, they are computationally

in graphs; however, they are computationally expensive, especially when the graph is on disk storage. A simpler algorithm in the special case of directories and links is to bypass links during directory traversal. Cycles are avoided, and no extra overhead is incurred. 11.4 FileSystem Mounting Just as a le must be opened before it is used, a le system must be mounted before it can be available to processes on the system. More specically, the directory structure may be built out of multiple

directory structure may be built out of multiple volumes, which must be mounted to make them available within the lesystem name space. The mount procedure is straightforward. The operating system is given the name of the device and the mount point thelocationwithinthelestructure where the le system is to be attached. Some operating systems require that a le system type be provided, while others inspect the structures of the device and determine the type of le system. Typically, a mount point is

the type of le system. Typically, a mount point is an empty directory. For instance, on a UNIX system, a le system containing a users home directories might be mounted as home ;t h e n ,t oa c c e s st h ed i r e c t o r ys t r u c t u r e within that le system, we could precede the directory names with home ,a s inhomejane .M o u n t i n gt h a t l es y s t e mu n d e r users would result in the path name usersjane ,w h i c hw ec o u l du s et or e a c ht h es a m ed i r e c t o r y . Next, the

e a c ht h es a m ed i r e c t o r y . Next, the operating system veries that the device contains a valid le system. It does so by asking the device driver to read the device directory and verifying that the directory has the expected format. Finally, the operating system notes in its directory structure that a le system is mounted at the specied mount point. This scheme enables the operating system to traverse its directory structure, switching among le systems, and even le systems of varying

among le systems, and even le systems of varying types, as appropriate.11.4 FileSystem Mounting 527users bill fred helpsue jane progdoc (a) (b) Figure 11.14 File system. (a) Existing system. (b) Unmounted volume. To illustrate le mounting, consider the le system depicted in Figure 11.14, where the triangles represent subtrees of directories that are of interest. Figure 11.14(a) shows an existing le system, while Figure 11.14(b) shows an unmounted volume residing on devicedsk .A tt h i sp o i n t

volume residing on devicedsk .A tt h i sp o i n t ,o n l yt h e l e s on the existing le system can be accessed. Figure 11.15 shows the effects of mounting the volume residing on devicedsk over users .I ft h ev o l u m ei s unmounted, the le system is restored to the situation depicted in Figure 11.14. Systems impose semantics to clarify functionality. For example, a system may disallow a mount over a directory that contains les; or it may make the mounted le system available at that directory

the mounted le system available at that directory and obscure the directorys existing les until the le system is unmounted, terminating the use of the le system and allowing access to the original les in that directory. As another example, a system may allow the same le system to be mounted repeatedly, at different mount points; or it may only allow one mount per le system. users sue jane progdocFigure 11.15 Mount point.528 Chapter 11 FileSystem Interface Consider the actions of the Mac OS X

Interface Consider the actions of the Mac OS X operating system. Whenever the system encounters a disk for the rst time (either at boot time or while the system is running), the Mac OS X operating system searches for a le system on the device. If it nds one, it automatically mounts the le system under theVolumes directory, adding a folder icon labeled with the name of the le system (as stored in the device directory). The user is then able to click on the icon and thus display the newly mounted

on the icon and thus display the newly mounted le system. The Microsoft Windows family of operating systems maintains an extended twolevel directory structure, with devices and volumes assigned drive letters. Volumes have a general graph directory structure associated with the drive let ter. The path to a specic le takes the form of driveletter: path tofile . The more recent versions of Windows allow a le system to be mounted anywhere in the directory tree, just as UNIX does. Windows operating

tree, just as UNIX does. Windows operating systems automatically discover all devices and mount all located le systems at boot time. In some systems, like UNIX , the mount commands are explicit. A system conguration le contains a list of devices and mount points for automatic mounting at boot time, but other mounts may be executed manually. Issues concerning le system mounting are further discussed in Section 12.2.2 and in Section A.7.5. 11.5 File Sharing In the previous sections, we explored

File Sharing In the previous sections, we explored the motivation for le sharing and some of the difculties involved in allowing users to share les. Such le sharing is very desirable for users who want to collaborate and to reduce the effort required to achieve a computing goal. Therefor e, useroriented operating systems must accommodate the need to share les in spite of the inherent difculties. In this section, we examine more aspects of le sharing. We begin by discussing general issues that

We begin by discussing general issues that arise when multiple users share les. Once multiple users are allowed to share les, the challenge is to extend sharing to multiple le systems, including remote le systems; we discuss that challenge as well. Finally, we consider what to do about conicting actions occurring on shared les. For instance, if multiple users are writing to a le, should all the writes be allowed to occur, or should the operating system protect the users actions from one another?

system protect the users actions from one another? 11.5.1 Multiple Users When an operating system accommodates multiple users, the issues of le sharing, le naming, and le protection become preeminent. Given a directory structure that allows les to be shared by users, the system must mediate the le sharing. The system can either allow a user to access the les of other users by default or require that a user specically grant access to the les. These are the issues of access control and protection,

are the issues of access control and protection, which are covered in Section 11.6. To implement sharing and protection, the system must maintain more le and directory attributes than are needed on a singleuser system. Although many approaches have been taken to meet this requirement, most systems have evolved to use the concepts of le (or directory) owner (oruser )a n d group . The owner is the user who can change attributes and grant access and who has11.5 File Sharing 529 the most control

and who has11.5 File Sharing 529 the most control over the le. The group attribute denes a subset of users who can share access to the le. For example, the owner of a le on a UNIX system can issue all operations on a le, while members of the les group can execute one subset of those operations, and all ot her users can execute another subset of operations. Exactly which operations can be executed by group members and other users is denable by the les owner. More details on permission attributes

les owner. More details on permission attributes are included in the next section. The owner and group IDso fag i v e n l e( o rd i r e c t o r y )a r es t o r e dw i t ht h e other le attributes. When a user requests an operation on a le, the user IDcan be compared with the owner attribute to determine if the requesting user is the owner of the le. Likewise, the group IDsc a nb ec o m p a r e d .T h er e s u l ti n d i c a t e s which permissions are applicable. The system then applies those

are applicable. The system then applies those permissions to the requested operation and allows or denies it. Many systems have multiple local le systems, including volumes of a single disk or multiple volumes on multiple attached disks. In these cases, the IDchecking and permission matching are straightforward, once the le systems are mounted. 11.5.2 Remote File Systems With the advent of networks (Chapter 17), communication among remote computers became possible. Networking allows the sharing

became possible. Networking allows the sharing of resources spread across a campus or even around the world. One obvious resource to share is data in the form of les. Through the evolution of network and le technology, remote lesharing methods have changed. The rst implemented method involves manually transferring les between machines via programs like ftp.T h es e c o n dm a j o r method uses a distributed le system (DFS)in which remote directories are visible from a local machine. In some

are visible from a local machine. In some ways, the third method, the World Wide Web ,i sar e v e r s i o nt ot h e r s t .Ab r o w s e ri sn e e d e dt og a i na c c e s st ot h e remote les, and separate operations (essentially a wrapper for ftp) are used to transfer les. Increasingly, cloud computing (Section 1.11.7) is being used for le sharing as well. ftp is used for both anonymous and authenticated access. Anonymous access allows a user to transfer les without having an account on the

to transfer les without having an account on the remote system. The World Wide Web uses anonymous le exchange almost exclusively. DFSinvolves a much tighter integration between the machine that is accessing the remote les and the machine providing the les. This integration adds complexity, as we describe in this section. 11.5.2.1 The ClientServer Model Remote le systems allow a computer to mount one or more le systems from one or more remote machines. In this case, the machine containing the les

In this case, the machine containing the les is the server , and the machine seeking access to the les is the client .T h e clientserver relationship is common with networked machines. Generally, the server declares that a resource is a vailable to clients an ds p e c i  e se x a c t l y which resource (in this case, which les) and exactly which clients. A server can serve multiple clients, and a client can use multiple servers, depending on the implementation details of a given clientserver

the implementation details of a given clientserver facility.530 Chapter 11 FileSystem Interface The server usually species the availab le les on a volume or directory level. Client identication is more dif cult. A client can be specied by an e t w o r kn a m eo ro t h e ri d e n t i  e r ,s u c ha sa n IPaddress, but these can bespoofed ,o ri m i t a t e d .A sar e s u l to fs p o o  n g ,a nu n a u t h o r i z e dc l i e n t could be allowed access to the server. More secure solutions include

to the server. More secure solutions include secure authentication of the client via encrypted keys. Unfortunately, with security come many challenges, including ensuring compatibility of the client and server (they must use the same encryption algorithms) and security of key exchanges (intercepted keys could again allow unauthorized access). Because of the difculty of solving these problems, unsecure authentication methods are most commonly used. In the case of UNIX and its network le system (

In the case of UNIX and its network le system ( NFS), authentication takes place via the client networking information, by default. In this scheme, the users IDso nt h ec l i e n ta n ds e r v e rm u s tm a t c h .I ft h e yd on o t ,t h es e r v e rw i l l be unable to determine access rights to les. Consider the example of a user who has an IDof 1000 on the client and 2000 on the server. A request from the client to the server for a specic  le will not be handled appropriately, as the server

will not be handled appropriately, as the server will determine if user 1000 has access to the le rather than basing the determination on the real user IDof 2000. Access is thus granted or denied based on incorrect authentication information. The server must trust the client to present the correct user ID.N o t et h a tt h e NFSprotocols allow manytomany relationships. That is, many servers can provide les to many clients. In fact, ag i v e nm a c h i n ec a nb eb o t has e r v e rt os o m e NFS

c h i n ec a nb eb o t has e r v e rt os o m e NFS clients and a client of other NFS servers. Once the remote le system is mounted, le operation requests are sent on behalf of the user across the network to the server via the DFS protocol. Typically, a leopen request is sent along with the IDof the requesting user. The server then applies the standard access checks to determine if the user has credentials to access the le in the mode requested. The request is either allowed or denied. If it is

The request is either allowed or denied. If it is allowed, a le handle is returned to the client application, and the application then can perform read, write, and other operations on the le. The client closes the le when access is completed. The operating system may apply semantics similar to those for a local lesystem mount or may use different semantics. 11.5.2.2 Distributed Information Systems To make client  server systems easier to manage, distributed information systems ,a l s ok n o w na

distributed information systems ,a l s ok n o w na s distributed naming services ,p r o v i d eu n i  e da c c e s s to the information needed for remote computing. The domain name system (DNS )provides hostnametonetworkaddress translation sf o rt h ee n t i r eI n t e r  net. Before DNS became widespread, les containing the same information were sent via email or ftp between all networked hosts. Obviously, this methodology was not scalable! DNS is further discussed in Section 17.4.1. Other

DNS is further discussed in Section 17.4.1. Other distributed information systems provide user namepassworduser IDgroup IDspace for a distributed facility. UNIX systems have employed a wide variety of distributed information methods. Sun Microsystems (now part of Oracle Corporation) introduced yellow pages (since renamed network information service ,o r NIS), and most of the industry adopted its use. It centralizes storage of user names, host names, printer information, and the like.11.5 File

names, printer information, and the like.11.5 File Sharing 531 Unfortunately, it uses unsecure authentication methods, including sending user passwords unencrypted (in clear text) and identifying hosts by IPaddress. Suns NISw a sam u c hm o r es e c u r er e p l a c e m e n tf o r NISbut was much more complicated and was not widely adopted. In the case of Microsofts common Internet le system (CIFS ),n e t w o r k information is used in conjunction with user authentication (user name and

with user authentication (user name and password) to create a network login th at the server uses to decide whether to allow or deny access to a requested le system. For this authentication to be valid, the user names must match from machine to machine (as with NFS). Microsoft uses active directory as a distributed naming structure to provide a single name space for users. Once established, the distributed naming facility is used by all clients and servers to authenticate users. The industry is

and servers to authenticate users. The industry is moving toward use of the lightweight directoryaccess protocol (LDAP )as a secure distributed naming mechanism. In fact, active directory is based on LDAP .O r a c l eS o l a r i sa n dm o s to t h e rm a j o ro p e r a t i n g systems include LDAP and allow it to be employed for user authentication as well as systemwide retrieval of information, such as availability of printers. Conceivably, one distributed LDAP directory could be used by an

one distributed LDAP directory could be used by an organization to store all user and resource information for all the organizations computers. The result would be secure single signon for users, who would enter their authentication information once for access to all computers within the organization. It would also ease systemadministration efforts by combining, in one location, information that is currently scattered in various les on each system or in different distributed information

system or in different distributed information services. 11.5.2.3 Failure Modes Local le systems can fail for a variety of reasons, including failure of the disk containing the le system, corruption of the directory structure or other diskmanagement information (collectively called metadata ), diskcontroller failure, cable failure, and hostadapter failure. User or systemadministrator failure can also cause les to be lost or entire directories or volumes to be deleted. Many of these failures will

volumes to be deleted. Many of these failures will cause a host to crash and an error condition to be displayed, and human intervention will be required to repair the damage. Remote le systems have even more failure modes. Because of the complexity of network systems and the required interactions between remote machines, many more problems can interfere with the proper operation of remote le systems. In the case of networks, the network can be interrupted between two hosts. Such interruptions

interrupted between two hosts. Such interruptions can result from hardware failure, poor hardware conguration, or networking implementation issues. Although some networks have builtin resiliency, including multiple paths between hosts, many do not. Any single failure can thus interrupt the ow of DFScommands. Consider a client in the midst of using a remote le system. It has les open from the remote host; among other activities, it may be performing directory lookups to open les, reading or

directory lookups to open les, reading or writing data to les, and closing les. Now consider a partitioning of the network, a crash of the server, or even a scheduled shutdown of the server. Suddenly, the remote le system is no longer reachable. This scenario is rather common, so it would not be appropriate for the client system to act as it would if a local le system were lost. Rather, the system can either terminate all operations to the lost server or delay operations until the532 Chapter 11

server or delay operations until the532 Chapter 11 FileSystem Interface server is again reachable. These failure semantics are dened and implemented as part of the remotelesystem protocol. Termination of all operations can result in users losing dataand patience. Thus, most DFS protocols either enforce or allow delaying of lesystem operations to remote hosts, with the hope that the remote host will become available again. To implement this kind of recovery from failure, some kind of state

kind of recovery from failure, some kind of state information may be maintained on both the client and the server. If both server and client maintain knowledge of their current activities and open les, then they can seamlessly recover from a failure. In the situation where the server crashes but must recognize that it has remotely mounted exported le systems and opened les, NFS takes a simple approach, implementing a stateless DFS. In essence, it assumes that a client request for a le read or

it assumes that a client request for a le read or write would not have occurred unless the le system had been remotely mounted and the le had been previously open. The NFSprotocol carries all the information needed to locate the appropriate le and perform the requested operation. Similarly, it does not track which clients have the exported volumes mounted, again assuming that if a request comes in, it must be legitimate. While this stateless approach makes NFS resilient and rather easy to

approach makes NFS resilient and rather easy to implement, it also makes it unsecure. For example, forged read or write requests could be allowed by an NFS server. These issues are addressed in the industry standard NFS Version 4, in which NFS is made stateful to improve its security, performance, and functionality. 11.5.3 Consistency Semantics Consistency semantics represent an important criterion for evaluating any le system that supports le sharing. These semantics specify how multiple users

These semantics specify how multiple users of a system are to access a shared le simultaneously. In particular, they specify when modications of data by one user will be observable by other users. These semantics are typically implemen ted as code with the le system. Consistency semantics are directly related to the process synchronization algorithms of Chapter 5. However, the co mplex algorithms of that chapter tend not to be implemented in the case of le IObecause of the great latencies and

case of le IObecause of the great latencies and slow transfer rates of disks and networks. For example, performing an atomic transaction to a remote disk could inv olve several network communications, several disk reads and writes, or both. Systems that attempt such a full set of functionalities tend to perform poorly. A successful implementation of complex sharing semantics can be found in the Andrew le system. For the following discussion, we assume that a series of le accesses (that is, reads

that a series of le accesses (that is, reads and writes) attempted by a user to the same le is always enclosed between the open() and close() operations. The series of accesses between theopen() and close() operations makes up a le session .T oi l l u s t r a t et h e concept, we sketch several prominen t examples of consistency semantics. 11.5.3.1 UNIX Semantics The UNIX le system (Chapter 17) uses the following consistency semantics: Writes to an open le by a user are visible immediately to

to an open le by a user are visible immediately to other users who have this le open.11.6 Protection 533 One mode of sharing allows users to share the pointer of current location into the le. Thus, the advancing of the pointer by one user affects all sharing users. Here, a le has a single image that interleaves all accesses, regardless of their origin. In the UNIX semantics, a le is associated with a single physical image that is accessed as an exclusive resource. Contention for this single

an exclusive resource. Contention for this single image causes delays in user processes. 11.5.3.2 Session Semantics The Andrew le system (Open AFS)u s e st h ef o l l o w i n gc o n s i s t e n c ys e m a n t i c s : Writes to an open le by a user are not visible immediately to other users that have the same le open. Once a le is closed, the changes made to it are visible only in sessions starting later. Already open instances of the le do not reect these changes. According to these semantics, a

these changes. According to these semantics, a le may be associated temporarily with several (possibly different) images at the same time. Consequently, multiple users are allowed to perform both read and write accesses concurrently on their images of the le, without delay. Almost no constraints are enforced on scheduling accesses. 11.5.3.3 ImmutableSharedFiles Semantics Au n i q u ea p p r o a c hi st h a to f immutable shared les . Once a le is declared as shared by its creator, it cannot be

is declared as shared by its creator, it cannot be modied. An immutable le has two key properties: its name may not be reused, and its contents may not be altered. Thus, the name of an immutable le signies that the contents of the le are xed. The implementation of these semantics in a distributed system (Chapter 17) is simple, because the sharing is disciplined (readonly). 11.6 Protection When information is stored in a computer system, we want to keep it safe from physical damage (the issue of

to keep it safe from physical damage (the issue of reliability) and improper access (the issue of protection). Reliability is generally provided by duplicate copies of les. Many comput ers have systems programs that automatically (or through computeroperator intervention) copy disk les to tape at regular intervals (once per day or week or month) to maintain a copy should a le system be accidentally destroyed. File systems can be damaged by hardware problems (such as errors in reading or

by hardware problems (such as errors in reading or writing), power surges or failures, head crashes, dirt, temperature extremes, and vandalism. Files may be deleted accidentally. Bugs in the lesystem soft ware can also cause le contents to be lost. Reliability is covered in more detail in Chapter 10.534 Chapter 11 FileSystem Interface Protection can be provided in many ways. For a singleuser laptop system, we might provide protection by locking the computer in a desk drawer or le cabinet. In a

the computer in a desk drawer or le cabinet. In a larger multiuser system, however, other mechanisms are needed. 11.6.1 Types of Access The need to protect les is a direct result of the ability to access les. Systems that do not permit access to the les of other users do not need protection. Thus, we could provide complete protection by prohibiting access. Alternatively, we could provide free access with no protection. Both approaches are too extreme for general use. What is needed is controlled

for general use. What is needed is controlled access. Protection mechanisms provide controlled access by limiting the types of le access that can be made. Access is permitted or denied depending on several factors, one of which is the type of access requested. Several different types of operations may be controlled: Read .R e a df r o mt h e l e . Write .W r i t eo rr e w r i t et h e l e . Execute .L o a dt h e l ei n t om e m o r ya n de x e c u t ei t . Append .W r i t en e wi n f o r m a t i

u t ei t . Append .W r i t en e wi n f o r m a t i o na tt h ee n do ft h e l e . Delete .D e l e t et h e l ea n df r e ei t ss p a c ef o rp o s s i b l er e u s e . List.L i s tt h en a m ea n da t t r i b u t e so ft h e l e . Other operations, such as renaming, copying, and editing the le, may also be controlled. For many systems, however, these higherlevel functions may be implemented by a system program that makes lowerlevel system calls. Protection is provided at only the lower level.

Protection is provided at only the lower level. For instance, copying a le may be implemented simply by a sequence of read requests. In this case, a user with read access can also cause the le to be copied, printed, and so on. Many protection mechanisms have been proposed. Each has advantages and disadvantages and must be appropriate for its intended application. A small computer system that is used by only a few members of a research group, for example, may not need the same types of protection

example, may not need the same types of protection as a large corporate computer that is used for research, nance, and personnel operations. We discuss some approaches to protection in the following sections and present a more complete treatment in Chapter 14. 11.6.2 Access Control The most common approach to the protection problem is to make access dependent on the identity of the user. Dif ferent users may need different types of access to a le or directory. The most general scheme to

to a le or directory. The most general scheme to implement identity dependent access is to associate with each le and directory an accesscontrol list(ACL)specifying user names and the types of access allowed for each user. When a user requests access to a particular le, the operating system checks the access list associated with that le. If that user is listed for the requested access, the access is allowed. Otherwise, a protection violation occurs, and the user job is denied access to the

occurs, and the user job is denied access to the le.11.6 Protection 535 This approach has the advantage of enabling complex access methodolo gies. The main problem with access lists is their length. If we want to allow everyone to read a le, we must list all users with read access. This technique has two undesirable consequences: Constructing such a list may be a tedious and unrewarding task, especially if we do not know in advance the list of users in the system. The directory entry, previously

in the system. The directory entry, previously of xed size, now must be of variable size, resulting in more complicated space management. These problems can be resolved by use of a condensed version of the access list. To condense the length of the accesscontrol list, many systems recognize three classications of users in connection with each le: Owner .T h eu s e rw h oc r e a t e dt h e l ei st h eo w n e r . Group . A set of users who are sharing the le and need similar access is a group, or

the le and need similar access is a group, or work group. Universe .A l lo t h e ru s e r si nt h es y s t e mc o n s t i t u t et h eu n i v e r s e . The most common recent approach is to combine accesscontrol lists with the more general (and easier to implement) owner, group, and universe access control scheme just described. For example, Solaris uses the three categories of access by default but allows accesscontrol lists to be added to specic les and directories when more negrained access

les and directories when more negrained access control is desired. To illustrate, consider a person, Sara, who is writing a new book. She has hired three graduate students (Jim, Dawn, and Jill) to help with the project. The text of the book is kept in a le named book.tex. The protection associated with this le is as follows: Sara should be able to invoke all operations on the le. Jim, Dawn, and Jill should be able only to read and write the le; they should not be allowed to delete the le. All

they should not be allowed to delete the le. All other users should be able to read, but not write, the le. (Sara is interested in letting as many people as possible read the text so that she can obtain feedback.) To achieve such protection, we must create a new group  say, text  with members Jim, Dawn, and Jill. The name of the group, text, must then be associated with the le book.tex, and the access rights must be set in accordance with the policy we have outlined. Now consider a visitor to

policy we have outlined. Now consider a visitor to whom Sara would like to grant temporary access to Chapter 1. The visitor cannot be added to the text group because that would give him access to all chapters. Because a le can be in only one group, Sara cannot add another group to Chapter 1. With the addition of accesscontrollist functionality, though, the visitor can be added to the access control list of Chapter 1.536 Chapter 11 FileSystem Interface PERMISSIONS IN A UNIX SYSTEM In the UNIX

Interface PERMISSIONS IN A UNIX SYSTEM In the UNIX system, directory protection and le protection are handled similarly. Associated with each subdirectory are three eldsowner, group, and universeeach consisting of the three bits rwx.T h u s ,au s e rc a nl i s t the content of a subdirectory only if the rbit is set in the appropriate eld. Similarly, a user can change his current directory to another current directory (say, foo)o n l yi ft h e xbit associated with the foo subdirectory is set in

associated with the foo subdirectory is set in the appropriate eld. As a m p l ed i r e c t o r yl i s t i n gf r o ma UNIX environment is shown in below: rwrwr drwx drwxrwxrx drwxrwx rwrr rwxrxrx drwxxx drwx drwxrwxrwx1 pbg 5 pbg 2 pbg 2 jwg 1 pbg 1 pbg 4 tag 3 pbg 3 pbgstaff staff staff student staff staff faculty staff staffintro.ps private doc studentproj program.c program lib mail testSep 3 08:30 Jul 8 09.33 Jul 8 09:35 Aug 3 14:13 Feb 24 2012 Feb 24 2012 Jul 31 10:31 Aug 29 06:52 Jul 8

2012 Feb 24 2012 Jul 31 10:31 Aug 29 06:52 Jul 8 09:3531200 512 512 512 9423 20471 512 1024 512 The rst eld describes the protection of the le or directory. A das the rst character indicates a subdirectory. Also shown are the number of links to the le, the owners name, the groups name, the size of the le in bytes, the date of last modication, and nally the les name (with optional extension). For this scheme to work properly, permissions and access lists must be controlled tightly. This control

lists must be controlled tightly. This control can be accomplished in several ways. For example, in the UNIX system, groups can be created and modied only by the manager of the facility (or by any superuser). Thus, control is achieved through human interaction. Access lists are discussed further in Section 14.5.2. With the more limited protection classi cation, only three elds are needed to dene protection. Often, each eld is a c ollection of bits, and each bit either allows or prevents the

bits, and each bit either allows or prevents the access associated with it. For example, the UNIX system denes three elds of 3 bits each rwx,w h e r e rcontrols read access, wcontrols write access, and xcontrols execution. A separate eld is kept for the le owner, for the les group, and for all other users. In this scheme, 9 bits per le are needed to record protection information. Thus, for our example, the protection elds for the le book.tex are as follows: for the owner Sara, all bits are set;

as follows: for the owner Sara, all bits are set; for the group text ,t h e randwbits are set; and for the universe, only the rbit is set. One difculty in combining approaches comes in the user interface. Users must be able to tell when the optional ACL permissions are set on a le. In the Solaris example, a is appended to the regular permissions, as in: 19 rwrr 1 jim staff 130 May 25 22:13 file1 As e p a r a t es e to fc o m m a n d s , setfacl and getfacl ,i su s e dt om a n a g et h e

setfacl and getfacl ,i su s e dt om a n a g et h e ACLs.11.6 Protection 537 Figure 11.16 Windows 7 accesscontrol list management. Windows users typically manage accesscontrol lists via the GUI.F i g u r e 11.16 shows a lepermission window on Windows 7 NTFS le system. In this example, user guest is specically denied access to the le ListPanel.java . Another difculty is assigning precedence when permission and ACLs conict. For example, if Joe is in a les group, which has read permission, but the

in a les group, which has read permission, but the le has an ACL granting Joe read and write permission, should a write by Joe be granted or denied? Solaris gives ACLsp r e c e d e n c e( a st h e ya r em o r e negrained and are not assigned by default). This follows the general rule that specicity should have priority. 11.6.3 Other Protection Approaches Another approach to the protection problem is to associate a password with each le. Just as access to the computer system is often controlled

access to the computer system is often controlled by a538 Chapter 11 FileSystem Interface password, access to each le can be controlled in the same way. If the passwords are chosen randomly and changed often, this scheme may be effective in limiting access to a le. The use of passwords has a few disadvantages, however. First, the number of passwords that a user needs to remember may become large, making the scheme impractical. Second, if only one password is used for all the les, then once it is

password is used for all the les, then once it is discovered, all les are accessible; protection is on an allornone basis. Some systems allow a user to associate a password with a subdirectory, rather than with an individual le, to address this problem. In a multilevel directory structure, we need to protect not only individual les but also collections of les in subdirectories; that is, we need to provide am e c h a n i s mf o rd i r e c t o r yp r o t e c t i o n .T h ed i r e c t o r yo p e r

r o t e c t i o n .T h ed i r e c t o r yo p e r a t i o n st h a tm u s tb e protected are somewhat different from the le operations. We want to control the creation and deletion of les in a directory. In addition, we probably want to control whether a user can determine the existence of a le in a directory. Sometimes, knowledge of the existence and name of a le is signicant in itself. Thus, listing the contents of a directory must be a protected operation. Similarly, if a path name refers to a

operation. Similarly, if a path name refers to a le in a directory, the user must be allowed access to both the directory and the le. In systems where les may have numerous path names (such as acyclic and general graphs), a given user may have different access rights to a particular le, depending on the path name used. 11.7 Summary A l ei sa na b s t r a c td a t at y p ed e  n e da n di m p l e m e n t e db yt h eo p e r a t i n g system. It is a sequence of logical records. A logical record

is a sequence of logical records. A logical record may be a byte, a line (of xed or variable length), or a more complex data item. The operating system may specically support various record types or may leave that support to the application program. The major task for the operating system is to map the logical le concept onto physical storage devices such as magnetic disk or tape. Since the physical record size of the device may not be the same as the logical record size, it may be necessary to

as the logical record size, it may be necessary to order logical records into physical records. Again, this task may be supported by the operating system or left for the application program. Each device in a le system keeps a volume table of contents or a device directory listing the location of the les on the device. In addition, it is useful to create directories to allow les to be organized. A singlelevel directory in a multiuser system causes naming problems, since each le must have a unique

naming problems, since each le must have a unique name. A twolevel directory solves this problem by creating a separate directory for each users les. The directory lists the les by name and includes the les location on the disk, length, type, owner, time of creation, time of last use, and so on. The natural generalization of a twolevel directory is a treestructured directory. A treestructured directory allows a user to create subdirectories to organize les. Acyclicgraph directory structures

to organize les. Acyclicgraph directory structures enable users to share subdirectories and les but complicate searching and deletion. A general graph structure allows complete exibility in the sharing of les and directories but sometimes requires garbage collection to recover unused disk space. Disks are segmented into one or more volumes, each containing a le system or left raw. File systems may be mounted into the systems namingPractice Exercises 539 structures to make them available. Th e

539 structures to make them available. Th e naming scheme varies by operating system. Once mounted, the les within the volume are available for use. File systems may be unmounted to disable access or for maintenance. File sharing depends on the semantics provided by the system. Files may have multiple readers, multiple writers, or limits on sharing. Distributed le systems allow client hosts to mount volumes or directories from servers, as long as they can access each other across a network.

as they can access each other across a network. Remote le systems present challenges in reliability, performance, and security. Distributed information systems maintain user, host, and access information so that clients and servers can share state information to manage use and access. Since les are the main informationstorage mechanism in most computer systems, le protection is needed. Access to les can be controlled separately for each type of accessread, write, execute, append, delete, list

accessread, write, execute, append, delete, list directory, and so on. File protection can be provided by access lists, passwords, or other techniques. Practice Exercises 11.1 Some systems automatically delete all user les when a user logs off or aj o bt e r m i n a t e s ,u n l e s st h eu s e re x p l i c i t l yr e q u e s t st h a tt h e yb ek e p t . Other systems keep all les unless the user explicitly deletes them. Discuss the relative merits of each approach. 11.2 Why do some systems

merits of each approach. 11.2 Why do some systems keep track of the type of a le, while others leave it to the user and others simply do not implement multiple le types? Which system is better ? 11.3 Similarly, some systems support many types of structures for a les data, while others simply support a stream of bytes. What are the advantages and disadvantages of each approach? 11.4 Could you simulate a multilevel directory structure with a singlelevel directory structure in which arbitrarily

directory structure in which arbitrarily long names can be used? If your answer is yes, explain how you can do so, and contrast this scheme with the multilevel directory scheme. If your answer is no, explain what prevents your simulations success. How would your answer change if le names were limited to seven characters? 11.5 Explain the purpose of the open() andclose() operations. 11.6 In some systems, a subdirectory can be read and written by an authorized user, just as ordinary les can be. a.

authorized user, just as ordinary les can be. a. Describe the protection problems that could arise. b. Suggest a scheme for dealing with each of these protection problems. 11.7 Consider a system that supports 5,000 users. Suppose that you want to allow 4,990 of these users to be able to access one le. a. How would you specify this protection scheme in UNIX ?540 Chapter 11 FileSystem Interface b. Can you suggest another protection scheme that can be used more effectively for this purpose than the

be used more effectively for this purpose than the scheme provided by UNIX ? 11.8 Researchers have suggested that, instead of having an access list associated with each le (specifying which users can access the le, and how), we should have a user control list associated with each user (specifying which les a user can access, and how). Discuss the relative merits of these two schemes. Exercises 11.9 Consider a le system in which a le can be deleted and its disk space reclaimed while links to that

and its disk space reclaimed while links to that le still exist. What problems may occur if an e w l ei sc r e a t e di nt h es a m es t o r a g ea r e ao rw i t ht h es a m ea b s o l u t e path name? How can these problems be avoided? 11.10 The openle table is used to maintain information about les that are currently open. Should the operating system maintain a separate table for each user or maintain just one table that contains references to les that are currently being accessed by all

to les that are currently being accessed by all users? If the same le is being accessed by two different programs or users, should there be separate entries in the openle table? Explain. 11.11 What are the advantages and disadvantages of providing mandatory locks instead of advisory locks whose use is left to users discretion? 11.12 Provide examples of applications that typically access les according to the following methods: Sequential Random 11.13 Some systems automatically open a le when it

11.13 Some systems automatically open a le when it is referenced for the rst time and close the le when the job terminates. Discuss the advantages and disadvantages of this scheme compared with the more traditional one, where the user has to open and close the le explicitly. 11.14 If the operating system knew that a certain application was going to access le data in a sequential manner, how could it exploit this information to improve performance? 11.15 Give an example of an application that

11.15 Give an example of an application that could benet from operating system support for random access to indexed les. 11.16 Discuss the advantages and disadvantages of supporting links to les that cross mount points (that is, the le link refers to a le that is stored in a different volume). 11.17 Some systems provide le sharing by maintaining a single copy of a le. Other systems maintain several copies, one for each of the users sharing the le. Discuss the relative merits of each

the le. Discuss the relative merits of each approach.Bibliography 541 11.18 Discuss the advantages and disadvantages of associating with remote le systems (stored on le servers) a set of failure semantics different from that associated with local le systems. 11.19 What are the implications of supporting UNIX consistency semantics for shared access to les stored on remote le systems? Bibliographical Notes Database systems and their le structures are described in full in [Silberschatz et al.

are described in full in [Silberschatz et al. (2010)]. Am u l t i l e v e ld i r e c t o r ys t r u c t u r ew a s r s ti m p l e m e n t e do nt h e MULTICS system ([Organick (1972)]). Most operating systems now implement multilevel directory structures. These include Linux ([Love (2010)]), Mac OS X ([Singh (2007)]), Solaris ([McDougall and Mauro (2007)]), and all versions of Windows ([Russinovich and Solomon (2005)]). The network le system ( NFS), designed by Sun Microsystems, allows directory

designed by Sun Microsystems, allows directory structures to be spread across networked computer systems. NFS Version 4 is described in RFC3505 ( http:www.ietf.orgrfcrfc3530.txt ). A gen eral discussion of Solaris le systems is found in the Sun System Administration Guide: Devices and File Systems (http:docs.sun.comappdocsdoc8175093 ). DNS was rst proposed by [Su (1982)] and has gone through several revisions since. LDAP ,a l s ok n o w na sX . 5 0 9 ,i sad e r i v a t i v es u b s e to ft h eX

5 0 9 ,i sad e r i v a t i v es u b s e to ft h eX . 5 0 0 distributed directory protocol. It was dened by [Yeong et al. (1995)] and has been implemented on many operating systems. Bibliography [Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developers Library (2010). [McDougall and Mauro (2007)] R. McDougall and J. Mauro, Solaris Internals, Second Edition, Prentice Hall (2007). [Organick (1972)] E. I. Organick, The Multics System: An Examination of Its Structure ,M I TP r e s s(

An Examination of Its Structure ,M I TP r e s s( 1 9 7 2 ) . [Russinovich and Solomon (2005)] M. E. Russinovich and D. A. Solomon, Microsoft Windows Internals, Fourth Edition, Microsoft Press (2005). [Silberschatz et al. (2010)] A. Silberschatz, H. F. Korth, and S. Sudarshan, Database System Concepts, Sixth Edition, McGrawHill (2010). [Singh (2007)] A. Singh, Mac OS X Internals: A Systems Approach ,A d d i s o n  Wesley (2007). [Su (1982)] Z. Su, AD i s t r i b u t e dS y s t e mf o rI n t e r n

AD i s t r i b u t e dS y s t e mf o rI n t e r n e tN a m eS e r v i c e ,Network Working Group, Request for Comments: 830 (1982). [Yeong et al. (1995)] W. Yeong, T. Howes, and S. Kille, Lightweight Directory Access Protocol ,Network Working Group, Request for Comments: 1777 (1995).12CHAPTER File System Implementation As we saw in Chapter 11, the le system provides the mechanism for online storage and access to le contents, including data and programs. The le system resides permanently on

and programs. The le system resides permanently on secondary storage, which is designed to hold a large amount of data permanently. This chapter is primarily concerned with issues surrounding le storage and access on the most common secondarystorage medium, the disk. We explore ways to structure le use, to allocate disk space, to recover freed space, to track the locations of data, and to interface other parts of the operating system to secondary storage. Performance issues are considered

storage. Performance issues are considered throughout the chapter. CHAPTER OBJECTIVES To describe the details of implementing local le systems and directory structures. To describe the implementation of remote le systems. To discuss block allocation and freeblock algorithms and tradeoffs. 12.1 FileSystem Structure Disks provide most of the secondary storage on which le systems are maintained. Two characteristics make them convenient for this purpose: 1.Ad i s kc a nb er e w r i t t e ni np l a c

1.Ad i s kc a nb er e w r i t t e ni np l a c e ;i ti sp o s s i b l et or e a dab l o c kf r o mt h e disk, modify the block, and write it back into the same place. 2.Ad i s kc a na c c e s sd i r e c t l ya n yb l o c ko fi n f o r m a t i o ni tc o n t a i n s .T h u s ,i ti s simple to access any le either sequentially or randomly, and switching from one le to another requires only moving the readwrite heads and waiting for the disk to rotate. We discuss disk structure in great detail in

We discuss disk structure in great detail in Chapter 10. To improve IO efciency, IO transfers between memory and disk are performed in units of blocks .E a c hb l o c kh a so n eo rm o r es e c t o r s .D e p e n d i n g 543544 Chapter 12 FileSystem Implementation on the disk drive, sector size varies from 32 bytes to 4,096 bytes; the usual size is 512 bytes. File systems provide efcient and convenient access to the disk by allowing data to be stored, located, and retrieved easily. A le system

stored, located, and retrieved easily. A le system poses two quite different design problems. The rst problem is dening how the le system should look to the user. This task involves dening a le and its attributes, the operations allowed on a le, and the directory structure for organizing les. The second problem is creating algorithms and data structures to map the logical le system onto the physical secondarystorage devices. The le system itself is generally composed of many different levels.

is generally composed of many different levels. The structure shown in Figure 12.1 is an example of a layered design. Each level in the design uses the features of lower levels to create new features for use by higher levels. The IOcontrol level consists of device drivers and interrupt handlers to transfer information between the main memory and the disk system. A device driver can be thought of as a translator. Its input consists of high level commands such as retrieve block 123. Its output

commands such as retrieve block 123. Its output consists of lowlevel, hardwarespecic instructions that are use db yt h eh a r d w a r ec o n t r o l l e r ,w h i c h interfaces the IOdevice to the rest of the system. The device driver usually writes specic bit patterns to special locations in the IOcontrollers memory to tell the controller which device location to act on and what actions to take. The details of device drivers and the IOinfrastructure are covered in Chapter 13. The basic le

are covered in Chapter 13. The basic le system needs only to issue generic commands to the appropriate device driver to read and write physical blocks on the disk. Each physical block is identied by its numeric disk address (for example, drive 1, cylinder 73, track 2, sector 10). This layer also manages the memory buffers and caches that hold various lesystem, directory, and data blocks. A block in the buffer is allocated before the transfer of a disk block can occur. When the buffer is full,

a disk block can occur. When the buffer is full, the buffer manager must nd more buffer memory or freeapplication programs fileorganization module basic file system IO control deviceslogical file system Figure 12.1 Layered le system.12.1 FileSystem Structure 545 up buffer space to allow a requested IOto complete. Caches are used to hold frequently used lesystem metadata to improve performance, so managing their contents is critical for optimum system performance. The leorganization module knows

performance. The leorganization module knows about les and their logical blocks, as well as physical blocks. By knowing the type of le allocation used and the location of the le, the leorganization module can translate logical block addresses to physical block addresses for the basic le system to transfer. Each les logical blocks are numbered from 0 (or 1) through N.Since the physical blocks containing the data usually do not match the logical numbers, at r a n s l a t i o ni sn e e d e dt ol o

at r a n s l a t i o ni sn e e d e dt ol o c a t ee a c hb l o c k .T h e l e  o r g a n i z a t i o nm o d u l ea l s o includes the freespace manager, which tracks unallocated blocks and provides these blocks to the leorganization module when requested. Finally, the logical le system manages metadata information. Metadata includes all of the lesystem structure except the actual data (or contents of the les). The logical le system manages the directory structure to provide the leorganization

directory structure to provide the leorganization module with the information the latter needs, given a symbolic le name. It maintains le structure via lecontrol blocks. A le control block (FCB)(aninode inUNIX le systems) contains information about the le, including ownership, permissions, and location of the le contents. The logical le system is also responsible for protection, as discussed in Chaptrers 11 and 14. When a layered structure is used for lesystem implementation, duplica tion of

used for lesystem implementation, duplica tion of code is minimized. The IOcontrol and sometimes the basic lesystem code can be used by multiple le systems. Each le system can then have its own logical lesystem and leorganization modules. Unfortunately, layering can introduce more operatingsystem overhead, which may result in decreased performance. The use of layering, including the decision about how many layers to use and what each layer should do, is a major challenge in designing new

should do, is a major challenge in designing new systems. Many le systems are in use today, and most operating systems support more than one. For example, most CDROM sa r ew r i t t e ni nt h e ISO 9660 format, a standard format agreed on by CDROM manufacturers. In addition to removablemedia le systems, each operating system has one or more disk based le systems. UNIX uses the UNIX le system (UFS),w h i c hi sb a s e do nt h e Berkeley Fast File System ( FFS). Windows supports disk lesystem

File System ( FFS). Windows supports disk lesystem formats of FAT,FAT32, and NTFS (or Windows NTFile System), as well as CDROM and DVD lesystem formats. Although Linux supports over forty different le systems, the standard Linux le system is known as the extended le system ,w i t h the most common versions being ext3 and e xt4. There are also distributed le systems in which a le system on a server is mounted by one or more client computers across a network. Filesystem research continues to be an

a network. Filesystem research continues to be an active area of operatingsystem design and implementation. Google created its own le system to meet the companys specic storage and r etrieval needs, which include high performance access from many clients across a very large number of disks. Another interesting project is the FUSE le system, which provides exibility in lesystem development and use by implementing and executing le systems as userlevel rather than kernellevel code. Using FUSE ,au s

rather than kernellevel code. Using FUSE ,au s e rc a na d dan e w le system to a variety of operating systems and can use that le system to manage her les.546 Chapter 12 FileSystem Implementation 12.2 FileSystem Implementation As was described in Section 11.1.2, operating systems implement open() and close() systems calls for processes to request access to le contents. In this section, we delve into the structures and operations used to implement lesystem operations. 12.2.1 Overview Several

lesystem operations. 12.2.1 Overview Several ondisk and inmemory structures are used to implement a le system. These structures vary depending on the operating system and the le system, but some general principles apply. On disk, the le system may contain information about how to boot an operating system stored there, the total number of blocks, the number and location of free blocks, the directory structure, and individual les. Many of these structures are detailed throughout the remainder of

are detailed throughout the remainder of this chapter. Here, we describe them briey: Aboot control block (per volume) can contain information needed by the system to boot an operating system from that volume. If the disk does not contain an operating system, this block can be empty. It is typically the rst block of a volume. In UFS, it is called the boot block .I n NTFS ,i ti st h e partition boot sector . Avolume control block (per volume) contains volume (or partition) details, such as the

volume (or partition) details, such as the number of blocks in the partition, the size of the blocks, af r e e  b l o c kc o u n ta n df r e e  b l o c kp o i n t e r s ,a n daf r e e  FCB count and FCB pointers. In UFS,t h i si sc a l l e da superblock .I n NTFS ,i ti ss t o r e di nt h e master le table . Ad i r e c t o r ys t r u c t u r e( p e r l es y s t e m )i su s e dt oo r g a n i z et h e l e s .I n UFS, this includes le names and associated inode numbers. In NTFS ,i ti ss t o r e d in

inode numbers. In NTFS ,i ti ss t o r e d in the master le table. Ap e r   l e FCB contains many details about the le. It has a unique identier number to allow association with a directory entry. In NTFS , this information is actually stored within the master le table, which uses ar e l a t i o n a ld a t a b a s es t r u c t u r e ,w i t har o wp e r l e . The inmemory information is used for both lesystem management and performance improvement via cachin g. The data are loaded at mount time,

via cachin g. The data are loaded at mount time, updated during lesystem operations, and discarded at dismount. Several types of structures may be included. An inmemory mount table contains information about each mounted volume. An inmemory directorystructure cache holds the directory information of recently accessed directories. (For directories at which volumes are mounted, it can contain a pointer to the volume table.) The systemwide openle table contains a copy of the FCB of each open le, as

contains a copy of the FCB of each open le, as well as other information.12.2 FileSystem Implementation 547file permissions file dates (create, access, write) file owner, group, ACL file size file data blocks or pointers to file data blocks Figure 12.2 At y p i c a l l e  c o n t r o lb l o c k . The perprocess openle table contains a pointer to the appropriate entry in the systemwide openle table, as well as other information. Buffers hold lesystem blocks when they are being read from disk or

blocks when they are being read from disk or written to disk. To create a new le, an application program calls the logical le system. The logical le system knows the format of the directory structures. To create a new le, it allocates a new FCB.( A l t e r n a t i v e l y ,i ft h e l e  s y s t e mi m p l e m e n t a t i o n creates all FCBsa t l e  s y s t e mc r e a t i o nt i m e ,a n FCB is allocated from the set of free FCBs.) The system then reads the appropriate directory into memory,

then reads the appropriate directory into memory, updates it with the new le name and FCB, and writes it back to the disk. A typical FCBis shown in Figure 12.2. Some operating systems, including UNIX ,t r e a tad i r e c t o r ye x a c t l yt h es a m e as a leone with a type eld indicating that it is a directory. Other operating systems, including Windows, implement separate system calls for les and directories and treat directories as entities separate from les. Whatever the larger structural

separate from les. Whatever the larger structural issues, the logical le system can call the leorganization module to map the directory IOinto diskblock numbers, which are passed on to the basic le system and IOcontrol system. Now that a le has been created, it can be used for IO.F i r s t ,t h o u g h ,i t must be opened. The open() call passes a le name to the logical le system. The open() system call rst searches the systemwide openle table to see if the le is already in use by another

to see if the le is already in use by another process. If it is, a perprocess openle table entry is created pointing to the existing systemwide openle table. This algorithm can save substantial overhead. If the le is not already open, the directory structure is searched for the given le name. Parts of the directory structure are usually cached in memory to speed directory operations. Once the le is found, the FCBis copied into a systemwide openle table in memory. This table not only stores the

table in memory. This table not only stores the FCBbut also tracks the number of processes that have the le open. Next, an entry is made in the perproc ess openle table, with a pointer to the entry in the systemwide openle table and some other elds. These other elds may include a pointer to the current location in the le (for the next read() orwrite() operation) and the access mode in which the le is open. The open() call returns a pointer to the appropriate entry in the perprocess548 Chapter 12

appropriate entry in the perprocess548 Chapter 12 FileSystem Implementationdirectory structure directory structureopen (file name) kernel memory user space index(a)filecontrol block secondary storage data blocks perprocess openfile tablesystemwide openfile tableread (index) kernel memory user space (b)filecontrol block secondary storage Figure 12.3 Inmemory lesystem structures. (a) File open. (b) File read. lesystem table. All le operations are then performed via this pointer. The le name may

then performed via this pointer. The le name may not be part of the openle table, as the system has no use for it once the appropriate FCB is located on disk. It could be cached, though, to save time on subsequent opens of the same le. The name given to the entry varies. UNIX systems refer to it as a le descriptor ;W i n d o w sr e f e r st oi ta sa le handle . When a process closes the le, the per process table entry is removed, and the systemwide entrys open count is decremented. When all

entrys open count is decremented. When all users that have opened the le close it, any updated metadata is copied back to the diskbased directory structure, and the systemwide openle table entry is removed. Some systems complicate this scheme further by using the le system as an interface to other system aspects, such as networking. For example, in UFS,t h e systemwide openle table holds the inodes and other information for les and directories. It also holds similar information for network

It also holds similar information for network connections and devices. In this way, one mechanism can be used for multiple purposes. The caching aspects of lesystem structures should not be overlooked. Most systems keep all information about an open le, except for its actual data blocks, in memory. The BSD UNIX system is typical in its use of caches wherever disk IOcan be saved. Its average cache hit rate of 85 percent shows that these techniques are well worth implementing. The BSD UNIX system

are well worth implementing. The BSD UNIX system is described fully in Appendix A. The operating structures of a lesystem implementation are summarized in Figure 12.3.12.2 FileSystem Implementation 549 12.2.2 Partitions and Mounting The layout of a disk can have many variations, depending on the operating system. A disk can be sliced into multiple partitions, or a volume can span multiple partitions on multiple disks. The former layout is discussed here, while the latter, which is more

is discussed here, while the latter, which is more appropriately considered a form of RAID ,i s covered in Section 10.7. Each partition can be either raw, containing no le system, or cooked,  containing a le system. Raw disk is used where no le system is appropriate. UNIX swap space can use a raw partition, for example, since it uses its own format on disk and does not use a le system. Likewise, some databases use raw disk and format the data to suit their needs. Raw disk can also hold

data to suit their needs. Raw disk can also hold information needed by disk RAID systems, such as bit maps indicating which blocks are mirrored and which have changed and need to be mirrored. Similarly, raw disk can contain a miniature database holding RAID conguration information, such as which disks are members of each RAID set. Raw disk use is discussed in Section 10.5.1. Boot information can be stored in a separate partition, as described in Section 10.5.2. Again, it has its own format,

in Section 10.5.2. Again, it has its own format, because at boot time the system does not have the lesystem code loaded and therefore cannot interpret the lesystem format. Rather, boot information is usually a sequential series of blocks, loaded as an image into memory. Execution of the image starts at a predened location, such as the rst byte. This boot loader in turn knows enough about the lesystem structure to be able to nd and load the kernel and start it executing. It can contain more than

and start it executing. It can contain more than the instructions for how to boot as p e c i  co p e r a t i n gs y s t e m .F o ri n s t a n c e ,m a n ys y s t e m sc a nb e dualbooted , allowing us to install multiple operating systems on a single system. How does the system know which one to boot? A boot loader that understands multiple le systems and multiple operating systems can occupy the boot space. Once loaded, it can boot one of the operating systems available on the disk. The disk

operating systems available on the disk. The disk can have multiple partitions, each containing a different type of le system and ad i f f e r e n to p e r a t i n gs y s t e m . The root partition , which contains the operatingsystem kernel and some times other system les, is mounted at boot time. Other volumes can be automatically mounted at boot or manually mounted later, depending on the operating system. As part of a successful mount operation, the operating system veries that the device

the operating system veries that the device contains a valid le system. It does so by asking the device driver to read the device dir ectory and verifying that the directory has the expected format. If the format is invalid, the partition must have its consistency checked and possibly corrected, either with or without user intervention. Finally, the operating system notes in its inmemory mount table that a le system is mounted, along with the type of the le system. The details of this function

of the le system. The details of this function depend on the operating system. Microsoft Windowsbased systems mount each volume in a separate name space, denoted by a letter and a colon. To record that a le system is mounted atF:,f o re x a m p l e ,t h eo p e r a t i n gs y s t e mp l a c e sap o i n t e rt ot h e l es y s t e mi n a eld of the device structure corresponding to F:.W h e nap r o c e s ss p e c i  e s the driver letter, the operating system nds the appropriate lesystem pointer

system nds the appropriate lesystem pointer and traverses the directory structures on that device to nd the specied le550 Chapter 12 FileSystem Implementation or directory. Later versions of Windows can mount a le system at any point within the existing directory structure. On UNIX , l es y s t e m sc a nb em o u n t e da ta n yd i r e c t o r y .M o u n t i n gi s implemented by setting a ag in the inmemory copy of the inode for that directory. The ag indicates that the directory is a mount

The ag indicates that the directory is a mount point. A eld then points to an entry in the mount table, indicating which device is mounted there. The mount table entry contains a pointer to the superblock of the le system on that device. This scheme enables the operating system to traverse its directory structure, switching seamlessly among le systems of varying types. 12.2.3 Virtual File Systems The previous section makes it clear that modern operating systems must concurrently support multiple

systems must concurrently support multiple types of le systems. But how does an operating system allow multiple types of le systems to be integrated into a directory structure? And how can users seamlessly move between lesystem types as they navigate the lesystem space? We now discuss some of these implementation details. An obvious but suboptimal method of implementing multiple types of le systems is to write directory and le routines for each type. Instead, however, most operating systems,

type. Instead, however, most operating systems, including UNIX ,u s eo b j e c t  o r i e n t e dt e c h n i q u e st o simplify, organize, and modularize the implementation. The use of these methods allows very dissimilar lesystem types to be implemented within the same structure, including network le systems, such as NFS.U s e r sc a n access les contained within multiple le systems on the local disk or even on le systems available across the network. Data structures and procedures are used to

Data structures and procedures are used to isolate the basic system call functionality from the implementation details. Thus, the lesystem implementation consists of three major layers, as depicted schematically in Figure 12.4. The rst layer is the lesystem interface, based on the open() , read() ,write() ,a n d close() calls and on le descriptors. The second layer is called the virtual le system (VFS)layer. The VFSlayer serves two important functions: 1.It separates lesystemgeneric operations

1.It separates lesystemgeneric operations from their implementation by dening a clean VFS interface. Several implementations for the VFS interface may coexist on the same machine, allowing transparent access to different types of le systems mounted locally. 2.It provides a mechanism for uniquely representing a le throughout a network. The VFS is based on a lerepresentation structure, called a vnode ,t h a tc o n t a i n san u m e r i c a ld e s i g n a t o rf o ran e t w o r k  w i d eu n i q u

i g n a t o rf o ran e t w o r k  w i d eu n i q u e le. ( UNIX inodes are unique within only a single le system.) This networkwide uniqueness is required for support of network le systems. The kernel maintains one vnode structur ef o re a c ha c t i v en o d e(  l eo r directory). Thus, the VFS distinguishes local les from remote ones, and local les are further distinguished according to their lesystem types. The VFS activates lesystemspecic operations to handle local requests according to

operations to handle local requests according to their lesystem types and calls the NFS protocol procedures for12.2 FileSystem Implementation 551 local file system type 1 disklocal file system type 2 diskremote file system type 1 networkfilesystem interface VFS interface Figure 12.4 Schematic view of a virtual le system. remote requests. File handles are constructed from the relevant vnodes and are passed as arguments to these procedures. The layer implementing the lesystem type or the

The layer implementing the lesystem type or the remotelesystem protocol is the third layer of the architecture. Lets briey examine the VFSarchitecture in Linux. The four main object types dened by the Linux VFSare: The inode object ,w h i c hr e p r e s e n t sa ni n d i v i d u a l l e The le object ,w h i c hr e p r e s e n t sa no p e n l e The superblock object ,w h i c hr e p r e s e n t sa ne n t i r e l es y s t e m The dentry object ,w h i c hr e p r e s e n t sa ni n d i v i d u a ld i

i c hr e p r e s e n t sa ni n d i v i d u a ld i r e c t o r ye n t r y For each of these four object types, the VFSdenes a set of operations that may be implemented. Every object of one of these types contains a pointer to af u n c t i o nt a b l e .T h ef u n c t i o nt a b l el i s t st h ea d d r e s s e so ft h ea c t u a lf u n c t i o n s that implement the dened operations for that particular object. For example, an abbreviated APIfor some of the operations for the le object includes:

some of the operations for the le object includes: int open(. . .) Open a le. int close(...) Close an alreadyopen le. ssize tr e a d ( ... ) Read from a le. ssize tw r i t e ( ... ) W rite to a le. int mmap(. . .) Memorymap a le.552 Chapter 12 FileSystem Implementation An implementation of the le object for a specic le type is required to imple ment each function specied in the denition of the le object. (The complete denition of the le object is specied in the struct file operations ,w h i c h

specied in the struct file operations ,w h i c h is located in the le usrincludelinuxfs.h .) Thus, the VFS software layer can perform an operation on one of these objects by calling the appropriate function from the objects function table, without having to know in advance exactly what kind of object it is dealing with. The VFSdoes not know, or care, whether an inode represents a disk le, ad i r e c t o r y l e ,o rar e m o t e l e .T h ea p p r o p r i a t ef u n c t i o nf o rt h a t l e  s

r o p r i a t ef u n c t i o nf o rt h a t l e  s read() operation will always be at the same place in its function table, and the VFS software layer will call that function without caring how the data are actually read. 12.3 Directory Implementation The selection of directoryallocation and directorymanagement algorithms signicantly affects the efciency, performance, and reliability of the le system. In this section, we discuss the tradeoffs involved in choosing one of these algorithms. 12.3.1

in choosing one of these algorithms. 12.3.1 Linear List The simplest method of implementing a directory is to use a linear list of le names with pointers to the data blocks. This method is simple to program but timeconsuming to execute. To create a new le, we must rst search the directory to be sure that no existing le has the same name. Then, we add a new entry at the end of the directory. To delete a le, we search the directory for the named le and then release the space allocated to it. To

le and then release the space allocated to it. To reuse the directory entry, we can do one of several things. We can mark the entry as unused (by assigning it a special name, such as an allblank name, or by including a used unused bit in each entry), or we can attach it to a list of free directory entries. A third alternative is to copy the last entry in the directory into the freed location and to decrease the length of the directory. A linked list can also be used to decrease the time required

can also be used to decrease the time required to delete a le. The real disadvantage of a linear list of directory entries is that nding a le requires a linear search. Directory information is used frequently, and users will notice if access to it is slow. In fact, many operating systems implement a software cache to store the most recently used directory information. A cache hit avoids the need to constantly reread the information from disk. A sorted list allows a binary search and decreases

A sorted list allows a binary search and decreases the average search time. However, the requirement that the list be kept sorted may complicate creating and deleting les, since we may have to move substantial amounts of directory information to maintain a sorted directory. A more sophisticated tree data structure, such as a balanced tree, might help here. An advantage of the sorted list is that a sorted directory listing can be produced without a separate sort step. 12.3.2 Hash Table Another

a separate sort step. 12.3.2 Hash Table Another data structure used for a le directory is a hash table. Here, a linear list stores the directory entries, but a hash data structure is also used. The hash table takes a value computed from the le name and returns a pointer to the le12.4 Allocation Methods 553 name in the linear list. Therefore, it can greatly decrease the directory search time. Insertion and deletion are also fairly straightforward, although some provision must be made for

although some provision must be made for collisionssituations in which two le names hash to the same location. The major difculties with a hash table are its generally xed size and the dependence of the hash function on that size. For example, assume that we make a linearprobing hash table that holds 64 entries. The hash function converts le names into integers from 0 to 63 (for instance, by using the remainder of a division by 64). If we later try to create a 65th le, we must enlarge the

later try to create a 65th le, we must enlarge the directory hash tablesay, to 128 entries. As a result, we need an e wh a s hf u n c t i o nt h a tm u s tm a p l en a m e st ot h er a n g e0t o1 2 7 ,a n dw e must reorganize the existing directory entries to reect their new hashfunction values. Alternatively, we can use a chainedoverow hash table. Each hash entry can be a linked list instead of an individual value, and we can resolve collisions by adding the new entry to the linked list.

by adding the new entry to the linked list. Lookups may be somewhat slowed, because searching for a name might require stepping through a linked list of colliding table entries. Still, this method is likely to be much faster than a linear search through the entire directory. 12.4 Allocation Methods The directaccess nature of disks gives us exibility in the implementation of les. In almost every case, many les are stored on the same disk. The main problem is how to allocate space to these les so

problem is how to allocate space to these les so that disk space is utilized effectively and les can be accessed quickl y. Three major methods of allocating disk space are in wide use: contiguous, linked, and indexed. Each method has advantages and disadvantages. Although some systems support all three, it is more common for a system to use one method for all les within a lesystem type. 12.4.1 Contiguous Allocation Contiguous allocation requires that each le occupy a set of contiguous blocks on

that each le occupy a set of contiguous blocks on the disk. Disk addresses dene a linear ordering on the disk. With this ordering, assuming that only one job is accessing the disk, accessing block b 1a f t e rb l o c k bnormally requires no head movement. When head movement is needed (from the last sector of one c ylinder to the rst sector of the next cylinder), the head need only move from one track to the next. Thus, the number of disk seeks required for accessing contiguously allocated les is

for accessing contiguously allocated les is minimal, as is seek time when a seek is nally needed. Contiguous allocation of a le is dened by the disk address and length (in block units) of the rst block. If the le is nblocks long and starts at location b,then it occupies blocks b, b1 , b2 ,. . . , bn1. The directory entry for each le indicates the address of the starting block and the length of the area allocated for this le (Figure 12.5). Accessing a le that has been allocated contiguously is

a le that has been allocated contiguously is easy. For sequential access, the le system remembers the disk address of the last block referenced and, when necessary, reads the next block. For direct access to block iof a554 Chapter 12 FileSystem Implementationdirectory 0 1 2 3 4 5 6 7 8 910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31count f tr mail liststart 0 14 19 28 6length 2 3 6 4 2file count tr mail list f Figure 12.5 Contiguous allocation of disk space. le that starts at

allocation of disk space. le that starts at block b,we can immediately access block bi.Thus, both sequential and direct access can be supported by contiguous allocation. Contiguous allocation has some problems, however. One difculty is nding space for a new le. The system chosen to manage free space determines how this task is accomplished; these management systems are discussed in Section 12.5. Any management system can be used, but some are slower than others. The contiguousallocation problem

than others. The contiguousallocation problem can be seen as a particular application of the general dynamic storageallocation problem discussed in Section 8.3, which involves how to satisfy a request of size nfrom a list of free holes. First t and best t are the most common strategies used to select a free hole from the set of available holes. Simulations have shown that both rst t and best t are more efcient than worst t in terms of both time and storage utilization. Neither rst t nor best t

and storage utilization. Neither rst t nor best t is clearly best in terms of storage utilization, but rst t is generally faster. All these algorithms suffer from the problem of external fragmentation . As les are allocated and deleted, the free disk space is broken into little pieces. External fragmentation exists whenever free space is broken into chunks. It becomes a problem when the largest contiguous chunk is insufcient for a request; storage is fragmented into a number of holes, none of

is fragmented into a number of holes, none of which is large enough to store the data. Depending on the total amount of disk storage and the average le size, external fragmentation may be a minor or a major problem. One strategy for preventing loss of signicant amounts of disk space to external fragmentation is to copy an entire le system onto another disk. The original disk is then freed completely, creating one large contiguous free space. We then copy the les back onto the original disk by

then copy the les back onto the original disk by allocating contiguous space from this one large hole. This scheme effectively compacts all free space into one contiguous space, solving the fragmentation problem. The cost of this12.4 Allocation Methods 555 compaction is time, however, and the cost can be particularly high for large hard disks. Compacting these disks may take hours and may be necessary on aw e e k l yb a s i s .S o m es y s t e m sr e q u i r et h a tt h i sf u n c t i o nb ed o

sr e q u i r et h a tt h i sf u n c t i o nb ed o n e offline ,w i t h the le system unmounted. During this down time ,n o r m a ls y s t e mo p e r a t i o n generally cannot be permitted, so such compaction is avoided at all costs on production machines. Most modern systems that need defragmentation can perform it online during normal system operations, but the performance penalty can be substantial. Another problem with contiguous allocation is determining how much space is needed for a le.

is determining how much space is needed for a le. When the le is created, the total amount of space it will need must be found and allocated. How does the creator (program or person) know the size of the le to be created? In some cases, this determination may be fairly simple (copying an existing le, for example). In general, however, the size of an output le may be difcult to estimate. If we allocate too little space to a le, we may nd that the le cannot be extended. Especially with a bestt

the le cannot be extended. Especially with a bestt allocation strategy, the space on both sides of the le may be in use. Hence, we cannot make the le larger in place. Two possibilities then exist. First, the user program can be terminated, with an appropriate error message. The user must then allocate more space and run the program again. These repeated runs may be costly. To prevent them, the user will normally overestimate th ea m o u n to fs p a c en e e d e d ,r e s u l t i n g in

u n to fs p a c en e e d e d ,r e s u l t i n g in considerable wasted space. The other possibility is to nd a larger hole, copy the contents of the le to the new space, and release the previous space. This series of actions can be repeated as long as space exists, although it can be time consuming. The user need never be informed explicitly about what is happening, however; the system continues despite the problem, although more and more slowly. Even if the total amount of space needed for a le

Even if the total amount of space needed for a le is known in advance, preallocation may be inefcient. A le that will grow slowly over a long period (months or years) must be allocated enough space for its nal size, even though much of that space will be unused for a long time. The le therefore has a large amount of internal fragmentation. To minimize these drawbacks, some operating systems use a modied contiguousallocation scheme. Here, a contiguous chunk of space is allocated initially. Then,

chunk of space is allocated initially. Then, if that amount proves not to be large enough, another chunk of contiguous space, known as an extent ,i sa d d e d .T h el o c a t i o no fa l e  sb l o c k s is then recorded as a location and a block count, plus a link to the rst block of the next extent. On some systems, the owner of the le can set the extent size, but this setting results in inefciencies if the owner is incorrect. Internal fragmentation can still be a problem if the extents are too

can still be a problem if the extents are too large, and external fragmentation can become a problem as extents of varying sizes are allocated and deallocated. The commercial Veritas le system uses extents to optimize performance. Veritas is a highperformance replacement for the standard UNIX UFS. 12.4.2 Linked Allocation Linked allocation solves all problems of contiguous allocation. With linked allocation, each le is a linked list of disk blocks; the disk blocks may be scattered anywhere on

the disk blocks may be scattered anywhere on the disk. The directory contains a pointer to the rst556 Chapter 12 FileSystem Implementation01 2 3 45 7 8 9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31156file jeepstart 9directory end 25 1 1 12 Figure 12.6 Linked allocation of disk space. and last blocks of the le. For example, a le of ve blocks might start at block 9a n dc o n t i n u ea tb l o c k1 6 ,t h e nb l o c k1 ,t h e nb l o c k1 0 ,a n d n a l l yb l o c k2 5 (Figure

nb l o c k1 0 ,a n d n a l l yb l o c k2 5 (Figure 12.6). Each block contains a pointer to the next block. These pointers are not made available to the user. Thus, if each block is 512 bytes in size, and ad i s ka d d r e s s( t h ep o i n t e r )r e q u i r e s4b y t e s ,t h e nt h eu s e rs e e sb l o c k so f5 0 8 bytes. To create a new le, we simply create a new entry in the directory. With linked allocation, each directory entry has a pointer to the rst disk block of the le. This pointer

to the rst disk block of the le. This pointer is initialized to null (the endoflist pointer value) to signify an empty le. The size eld is also set to 0. A write to the le causes the freespace management system to nd a free block, and this new block is written to and is linked to the end of the le. To read a le, we simply read blocks by following the pointers from block to block. There is no external fragmentation with linked allocation, and any free block on the freespace list can be used to

free block on the freespace list can be used to satisfy a request. The size of a le need not be declared when the le is created. A le can continue to grow as long as free blocks are available. Consequently, it is never necessary to compact disk space. Linked allocation does have disadvantages, however. The major problem is that it can be used effectively only for sequentialaccess les. To nd the ith block of a le, we must start at the beginning of that le and follow the pointers until we get to

of that le and follow the pointers until we get to the ith block. Each access to a pointer requires a disk read, and some require a disk seek. Consequently, it is inefcient to support a directaccess capability for linkedallocation les. Another disadvantage is the space required for the pointers. If a pointer requires 4 bytes out of a 512byte block, then 0.78 percent of the disk is being used for pointers, rather than for information. Each le requires slightly more space than it would otherwise.

slightly more space than it would otherwise. The usual solution to this problem is to collect blocks into multiples, called clusters ,a n dt oa l l o c a t ec l u s t e r sr a t h e rt h a nb l o c k s .F o ri n s t a n c e ,t h e l es y s t e m12.4 Allocation Methods 557 may dene a cluster as four blocks and operate on the disk only in cluster units. Pointers then use a much smaller percentage of the les disk space. This method allows the logicaltophysical block mapping to remain simple but

block mapping to remain simple but improves disk throughput (because fewer diskhead seeks are required) and decreases the space needed for bloc ka l l o c a t i o na n df r e e  l i s tm a n a g e m e n t . The cost of this approach is an increase in internal fragmentation, because more space is wasted when a cluster is partially full than when a block is partially full. Clusters can be used to improve the diskaccess time for many other algorithms as well, so they are used in most le systems.

as well, so they are used in most le systems. Yet another problem of linked allocation is reliability. Recall that the les are linked together by pointers scattered all over the disk, and consider what would happen if a pointer were lost or damaged. A bug in the operatingsystem software or a disk hardware failure might result in picking up the wrong pointer. This error could in turn result in linking into the freespace list or into another le. One partial solution is to use doubly linked lists,

partial solution is to use doubly linked lists, and another is to store the le name and relative block number in each block. However, these schemes require even more overhead for each le. An important variation on linked allocation is the use of a leallocation table (FAT).T h i ss i m p l eb u te f  c i e n tm e t h o do fd i s k  s p a c ea l l o c a t i o nw a su s e d by the MSDOS operating system. A section of disk at the beginning of each volume is set aside to contain the table. The table

is set aside to contain the table. The table has one entry for each disk block and is indexed by block number. The FAT is used in much the same way as a linked list. The directory entry contains the block number of the rst block of the le. The table entry indexed by that block number contains the block number of the next block in the le. This chain continues until it reaches the last block, which has a special endofle value as the table entry. An unused block is indicated by a table value of 0.

unused block is indicated by a table value of 0. Allocating a new block to a l ei sas i m p l em a t t e ro f n d i n gt h e r s t0  v a l u e dt a b l ee n t r ya n dr e p l a c i n g the previous endofle value with the address of the new block. The 0 is then replaced with the endofle value. An illustrative example is the FATstructure shown in Figure 12.7 for a le consisting of disk blocks 217, 618, and 339. The FATallocation scheme can result in a signicant number of disk head seeks, unless

in a signicant number of disk head seeks, unless the FAT is cached. The disk head must move to the start of the volume to read the FAT and nd the location of the block in question, then move to the location of the block itself. In the worst case, both moves occur for each of the blocks. A benet is that randomaccess time is improved, because the disk head can nd the location of any block by reading the information in the FAT. 12.4.3 Indexed Allocation Linked allocation solves the externa

Allocation Linked allocation solves the externa lfragmentation and sizedeclaration prob lems of contiguous allocation. However, in the absence of a FAT,l i n k e d allocation cannot support efcient direct access, since the pointers to the blocks are scattered with the blocks themselves all over the disk and must be retrieved in order. Indexed allocation solves this problem by bringing all the pointers together into one location: the index block . Each le has its own index block, which is an

. Each le has its own index block, which is an array of diskblock addresses. The ithentry in the index block points to the ithblock of the le. The directory558 Chapter 12 FileSystem Implementation  directory entry test 217 start block name 0 217 618 339 618 339 number of disk blocks 1 FAT Figure 12.7 Fileallocation table. contains the address of the index block (Figure 12.8). To nd and read the ith block, we use the pointer in the ithindexblock entry. This scheme is similar to the paging scheme

entry. This scheme is similar to the paging scheme described in Section 8.5. When the le is created, all pointers in the index block are set to null. When the ithblock is rst written, a block is obtained from the freespace manager, and its address is put in the ith indexblock entry. Indexed allocation supports direct access, without suffering from external fragmentation, because any free block on the disk can satisfy a request for more space. Indexed allocation does suffer fro mw a s t e ds p a

allocation does suffer fro mw a s t e ds p a c e ,h o w e v e r .T h ep o i n t e rdirectory 01 2 3 4 5 7 89 1 0 11 12 13 14 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31156 9 16 1 10 25 1 1 1file jeepindex block 19 19 Figure 12.8 Indexed allocation of disk space.12.4 Allocation Methods 559 overhead of the index block is generally greater than the pointer overhead of linked allocation. Consider a common case in which we have a le of only one or two blocks. With linked allocation, we lose the

or two blocks. With linked allocation, we lose the space of only one pointer per block. With indexed allocation, an en tire index block must be allocated, even if only one or two pointers will be non null. This point raises the question of how large the index block should be. Every le must have an index block, so we want the index block to be as small as possible. If the index block is too small, however, it will not be able to hold enough pointers for a large le, and a mechanism will have to be

for a large le, and a mechanism will have to be available to deal with this issue. Mechanisms for this purpose include the following: Linked scheme . An index block is normally one disk block. Thus, it can be read and written directly by itself. To allow for large les, we can link together several index blocks. For example, an index block might contain a small header giving the name of the le and a set of the rst 100 diskblock addresses. The next address (the last word in the index block) is

next address (the last word in the index block) is null (for as m a l l l e )o ri sap o i n t e rt oa n o t h e ri n d e xb l o c k( f o ral a r g e l e ) . Multilevel index .Av a r i a n to fl i n k e dr e p r e s e n t a t i o nu s e sa r s t  l e v e li n d e x block to point to a set of secondlevel index blocks, which in turn point to the le blocks. To access a block, the operating system uses the rstlevel index to nd a secondlevel index block and then uses that block to nd the desired data

and then uses that block to nd the desired data block. This approach could be continued to a third or fourth level, depending on the desired maximum le size. With 4,096byte blocks, we could store 1,024 fourbyte pointers in an index block. Two levels of indexes allow 1,048,576 data blocks and a le size of up to 4 GB. Combined scheme .A n o t h e ra l t e r n a t i v e ,u s e di n UNIX based le systems, is to keep the rst, say, 15 pointers of the index block in the les inode. The rst 12 of these

index block in the les inode. The rst 12 of these pointers point to direct blocks ;t h a ti s ,t h e yc o n t a i n addresses of blocks that contain data of the le. Thus, the data for small les (of no more than 12 blocks) do not need a separate index block. If the block size is 4 KB,t h e nu pt o4 8 KBof data can be accessed directly. The next three pointers point to indirect blocks .T h e r s tp o i n t st oa single indirect block ,w h i c hi sa ni n d e xb l o c kc o n t a i n i n gn o td a t

ni n d e xb l o c kc o n t a i n i n gn o td a t ab u tt h ea d d r e s s e so f blocks that do contain data. The second points to a double indirect block , which contains the address of a block that contains the addresses of blocks that contain pointers to the actual data blocks. The last pointer contains the address of a triple indirect block .( A UNIX inode is shown in Figure 12.9.) Under this method, the number of blocks that can be allocated to a le exceeds the amount of space addressable

to a le exceeds the amount of space addressable by the 4byte le pointers used by many operating systems. A 32bit le pointer reaches only 232bytes, or 4 GB.M a n y UNIX and Linux implementations now support 64bit le pointers, which allows les and le systems to be several exbibytes in size. The ZFSle system supports 128bit le pointers. Indexedallocation schemes suffer from some of the same performance problems as does linked allocation. Specically, the index blocks can be cached in memory, but the

the index blocks can be cached in memory, but the data blocks may be spread all over a volume.560 Chapter 12 FileSystem Implementation direct blocks data datadatadatadatadata datadatadatadata               mode owners (2) timestamps (3) size block count single indirect double indirect triple indirect Figure 12.9 The UNIX inode. 12.4.4 Performance The allocation methods that we have discussed vary in their storage efciency and datablock access times. Both are important criteria in selecting the

Both are important criteria in selecting the proper method or methods for an operating system to implement. Before selecting an allocation method, we need to determine how the systems will be used. A system with mostly sequential access should not use the same method as a system with mostly random access. For any type of access, contiguous allocation requires only one access to get ad i s kb l o c k .S i n c ew ec a ne a s i l yk e e pt h ei n i t i a la d d r e s so ft h e l ei nm e m o r y ,

i t i a la d d r e s so ft h e l ei nm e m o r y , we can calculate immediately the disk address of the ithblock (or the next block) and read it directly. For linked allocation, we can also keep the address of the next block in memory and read it directly. This method is ne for sequential access; for direct access, however, an access to the ithblock might require idisk reads. This problem indicates why linked allocation should not be used for an application requiring direct access. As a result,

application requiring direct access. As a result, some systems support directaccess les by using contiguous allocation and sequentialaccess les by using linked allocation. For these systems, the type of access to be made must be declared when the le is created. A l ec r e a t e df o rs e q u e n t i a la c c e s sw i l lb el i n k e da n dc a n n o tb eu s e df o rd i r e c t access. A le created for direct access will be contiguous and can support both direct access and sequential access, but

both direct access and sequential access, but its maximum length must be declared when it is created. In this case, the operating system must have appropriate data structures and algorithms to support both allocation methods. Files can be converted from one type to another by the creation of a new le of the desired type, into which the contents of the old le are copied. The old le may then be deleted and the new le renamed.12.5 FreeSpace Management 561 Indexed allocation is more complex. If the

561 Indexed allocation is more complex. If the index block is already in memory, then the access can be made directly .H o w e v e r ,k e e p i n gt h ei n d e xb l o c ki n memory requires considerable space. If this memory space is not available, then we may have to read rst the index block and then the desired data block. For a twolevel index, two indexblock reads might be necessary. For an extremely large le, accessing a block near the end of the le would require reading in all the index

of the le would require reading in all the index blocks before the needed data block nally could be read. Thus, the performance of indexed allocation depends on the index structure, on the size of the le, and on the position of the block desired. Some systems combine contiguous allocation with indexed allocation by using contiguous allocation for small les (up to three or four blocks) and automatically switching to an indexed allocation if the le grows large. Since most les are small, and

the le grows large. Since most les are small, and contiguous allocation is efcient for small les, average performance can be quite good. Many other optimizations are in use. Given the disparity between CPU speed and disk speed, it is not unreasonable to add thousands of extra instructions to the operating system to save just a few diskhead movements. Furthermore, this disparity is increasing over time, to the point where hundreds of thousands of instructions could reasonably be used to optimize

instructions could reasonably be used to optimize head movements. 12.5 FreeSpace Management Since disk space is limited, we need to reuse the space from deleted les for new les, if possible. (Writeonce optical disks allow only one write to any given sector, and thus reuse is not physically possible.) To keep track of free disk space, the system maintains a freespace list .T h ef r e e  s p a c el i s tr e c o r d sa l l free disk blocksthose not allocated to some le or directory. To create a le,

allocated to some le or directory. To create a le, we search the freespace list for the required amount of space and allocate that space to the new le. This space is then removed from the freespace list. When a l ei sd e l e t e d ,i t sd i s ks p a c ei sa d d e dt ot h ef r e e  s p a c el i s t .T h ef r e e  s p a c el i s t , despite its name, may not be implemented as a list, as we discuss next. 12.5.1 Bit Vector Frequently, the freespace list is implemented as a bit map orbit vector .E a

list is implemented as a bit map orbit vector .E a c h block is represented by 1 bit. If the block is free, the bit is 1; if the block is allocated, the bit is 0. For example, consider a disk where blocks 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 17, 18, 25, 26, and 27 are free and the rest of the blocks are allocated. The freespace bit map would be 001111001111110001100000011100000 ... The main advantage of this approach is its relative simplicity and its efciency in nding the rst free block or

and its efciency in nding the rst free block or nconsecutive free blocks on the disk. Indeed, many computers supply bitmanipulation instructions that can be used effectively for that purpose. One technique for nding the rst free block on a system that uses a bitvector to allocate disk space is to sequentially check each word in the bit map to see whether that value is not 0, since a562 Chapter 12 FileSystem Implementation 0valued word contains only 0 bits and represents a set of allocated

only 0 bits and represents a set of allocated blocks. The rst non0 word is scanned for the rst 1 bit, which is the location of the rst free block. The calculation of the block number is (number of bits per word) (number of 0value words)  offset of rst 1 bit. Again, we see hardware features driving software functionality. Unfor tunately, bit vectors are inefcient un less the entire vector is kept in main memory (and is written to disk occasionally for recovery needs). Keeping it in main memory is

for recovery needs). Keeping it in main memory is possible for smaller disks but not necessarily for larger ones. A1 . 3  GBdisk with 512byte blocks would need a bit map of over 332 KBto track its free blocks, although clustering the blocks in groups of four reduces this number to around 83 KBper disk. A 1 TBdisk with 4 KBblocks requires 256 MBto store its bit map. Given that disk size constantly increases, the problem with bit vectors will continue to escalate as well. 12.5.2 Linked List

continue to escalate as well. 12.5.2 Linked List Another approach to freespace managemen ti st ol i n kt o g e t h e ra l lt h ef r e e disk blocks, keeping a pointer to the rst free block in a special location on the disk and caching it in memory. This rst block contains a pointer to the next free disk block, and so on. Recall our earlier example (Section 12.5.1), in which blocks 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 17, 18, 25, 26, and 27 were free and the rest of the blocks were allocated. In

free and the rest of the blocks were allocated. In this situation, we would keep a pointer to block 2 as the rst free block. Block 2 would contain a pointer to block 3, which would point to block 4, which would point to block 5, which would point to block 8, and so on (Figure 12.10). This scheme is not efcient; to traverse the list, we must read each block, which requires substantial IOtime. Fortunately,01 2 3 45 7 8 9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31156freespace

19 20 21 22 23 24 25 26 27 28 29 30 31156freespace list head Figure 12.10 Linked freespace list on disk.12.5 FreeSpace Management 563 however, traversing the free list is not a frequent action. Usually, the operating system simply needs a free block so that it can allocate that block to a le, so the rst block in the free list is used. The FATmethod incorporates freeblock accounting into the allocation data structure. No separate method is needed. 12.5.3 Grouping Am o d i  c a t i o no ft h ef r

12.5.3 Grouping Am o d i  c a t i o no ft h ef r e e  l i s ta p p r o a c hs t o r e st h ea d d r e s s e so f nfree blocks in the rst free block. The rst n1o ft h e s eb l o c k sa r ea c t u a l l yf r e e .T h el a s t block contains the addresses of another nfree blocks, and so on. The addresses of a large number of free blocks can now be found quickly, unlike the situation when the standard linkedlist approach is used. 12.5.4 Counting Another approach takes advantage of th e fact that,

approach takes advantage of th e fact that, generally, several contigu ous blocks may be allocated or freed simultaneously, particularly when space is allocated with the contiguousallocation algorithm or through clustering. Thus, rather than keeping a list of nfree disk addresses, we can keep the address of the rst free block and the number ( n) of free contiguous blocks that follow the rst block. Each entry in the freespace list then consists of a disk address and a count. Although each entry

of a disk address and a count. Although each entry requires more space than would a simple disk address, the overall list is shorter, as long as the count is generally greater than 1. Note that this method of tracking free space is similar to the extent method of allocating blocks. These entries can be stored in a balanced tree, rather than a linked list, for efcient lookup, insertion, and deletion. 12.5.5 Space Maps Oracles ZFS le system (found in Solaris and other operating systems) was

(found in Solaris and other operating systems) was designed to encompass huge numbers of les, directories, and even le systems (inZFS,w ec a nc r e a t e l e  s y s t e mh i e r a r c h i e s ) .O nt h e s es c a l e s ,m e t a d a t a IOcan have a large performance impact. Consider, for example, that if the freespace list is implemented as a bit map, bit maps must be modied both when blocks are allocated and when they are freed. Freeing 1 GBof data on a 1 TBdisk could cause thousands of blocks

data on a 1 TBdisk could cause thousands of blocks of bit maps to be updated, because those data blocks could be scattered over the entire disk. Clearly, the data structures for such a system could be large and inefcient. In its management of free space, ZFSuses a combination of techniques to control the size of data structures and minimize the IOneeded to manage those structures. First, ZFScreates metaslabs to divide the space on the device into chunks of manageable size. A given volume may

into chunks of manageable size. A given volume may contain hundreds of metaslabs. Each metaslab has an associated space map. ZFSuses the counting algorithm to store information about free blocks. Rather than write counting structures to disk, it uses logstructured lesystem techniques to record them. The space map is a log of all block activity (allocating and freeing), in time order, in counting format. When ZFSdecides to allocate or free space from a metaslab, it loads the associated space map

from a metaslab, it loads the associated space map into memory in a balancedtree structure (for very efcient operation), indexed by offset, and replays the log into that structure. The inmemory space map is then an accurate representation of the allocated and free space in the metaslab. ZFSalso condenses the map as564 Chapter 12 FileSystem Implementation much as possible by combining contiguous free blocks into a single entry. Finally, the freespace list is updated on disk as part of the

freespace list is updated on disk as part of the transactionoriented operations of ZFS.D u r i n gt h ec o l l e c t i o na n ds o r t i n gp h a s e ,b l o c kr e q u e s t sc a n still occur, and ZFSsatises these requests from the log. In essence, the log plus the balanced tree isthe free list. 12.6 Efciency and Performance Now that we have discussed various blockallocation and directory management options, we can further consider their effect on performance and efcient disk use. Disks tend to

on performance and efcient disk use. Disks tend to represent a major bottleneck in system performance, since they are the slowest main computer component. In this section, we discuss a variety of techniques used to improve the efciency and performance of secondary storage. 12.6.1 Efciency The efcient use of disk space depends heavily on the diskallocation and directory algorithms in use. For instance, UNIX inodes are preallocated on av o l u m e .E v e na ne m p t yd i s kh a sap e r c e n t a g

.E v e na ne m p t yd i s kh a sap e r c e n t a g eo fi t ss p a c el o s tt oi n o d e s . However, by preallocating the inodes and spreading them across the volume, we improve the le systems performance. This improved performance results from the UNIX allocation and freespace algorithms, which try to keep a les data blocks near that les inode block to reduce seek time. As another example, lets reconsider the clustering scheme discussed in Section 12.4, which improves leseek and letransfer

Section 12.4, which improves leseek and letransfer performance at the cost of internal fragmentation. To reduce this fragmentation, BSD UNIX varies the cluster size as a le grows. Large clusters are used where they can be lled, and small clusters are used for small les and the last cluster of a le. This system is described in Appendix A. The types of data normally kept in a les directory (or inode) entry also require consideration. Commonly, a last write date is recorded to supply information to

write date is recorded to supply information to the user and to determine wh ether the le needs to be backed up. Some systems also keep a last access date, so that a user can determine when the le was last read. The result of keeping this information is that, whenever the le is read, a eld in the directory structure must be written to. That means the block must be read into memory, a section changed, and the block written back out to disk, because operations on disks occur only in block (or

operations on disks occur only in block (or cluster) chunks. So any time a le is opened for reading, its directory entry must be read and written as well. This requirement can be inefcient for frequently accessed les, so we must weigh its benet against its performance cost when designing a le system. Generally, every data item associated with a l en e e d st ob ec o n s i d e r e df o ri t se f f e c to ne f  c i e n c ya n dp e r f o r m a n c e . Consider, for instance, how efciency is

a n c e . Consider, for instance, how efciency is affected by the size of the pointers used to access data. Most systems use either 32bit or 64bit pointers throughout the operating system. Using 32bit pointers limits the size of a le to 232,o r4 GB.U s i n g6 4  b i tp o i n t e r sa l l o w sv e r yl a r g e l es i z e s ,b u t6 4  b i tp o i n t e r sr e q u i r e12.6 Efciency and Performance 565 more space to store. As a result, the allocation and freespacemanagement methods (linked lists,

and freespacemanagement methods (linked lists, indexes, and so on) use more disk space. One of the difculties in choosing a pointer sizeor, indeed, any xed allocation size within an operating systemis planning for the effects of changing technology. Consider that the IBM PC XT had a 10 MBhard drive and an MSDOS le system that could support only 32 MB.( E a c h FAT entry was 12 bits, pointing to an 8 KBcluster.) As disk capacities increased, larger disks had to be split into 32 MBpartitions,

larger disks had to be split into 32 MBpartitions, because the le system could not track blocks beyond 32 MB.A sh a r dd i s k sw i t hc a p a c i t i e so fo v e r1 0 0 MBbecame common, the disk data structures and algorithms in MSDOS had to be modied to allow larger le systems. (Each FATentry was expanded to 16 bits and later to 32 bits.) The initial lesystem decisions were made for efciency reasons; however, with the advent of MSDOS Version 4, millions of computer users were inconvenienced

4, millions of computer users were inconvenienced when they had to switch to the new, larger le system. Solaris ZFSle system uses 128bit pointers, which theoretically should never need to be extended. (The minimum mass of a device capable of storing 2128bytes using atomiclevel storage would be about 272 trillion kilograms.) As another example, consider the evolution of the Solaris operating system. Originally, many data structures were of xed length, allocated at system startup. These structures

allocated at system startup. These structures included the process table and the openle table. When the process table became full, no more processes could be created. When the le table became full, no more les could be opened. The system would fail to provide services to users. Table sizes could be increased only by recompiling the kernel and rebooting the system. With later releases of Solaris, almost all kernel structures were allocated dynamically, eliminating these articial limits on system

eliminating these articial limits on system performance. Of course, the algorithms that manipulate these tables are more complicated, and the operating system is a little slower because it must dynamically allocate and deallocate table entries; but that price is the usual one for more general functionality. 12.6.2 Performance Even after the basic lesystem algorithms have been selected, we can still improve performance in several ways. As will be discussed in Chapter 13, most disk controllers

be discussed in Chapter 13, most disk controllers include local memory to form an onboard cache that is large enough to store entire tracks at a time. Once a seek is performed, the track is read into the disk cache starting at the sector under the disk head (reducing latency time). The disk cont roller then transfers any sector requests to the operating system. Once blocks make it from the disk controller into main memory, the operating system may cache the blocks there. Some systems maintain a

cache the blocks there. Some systems maintain a separate section of main memory for a buffer cache ,w h e r eb l o c k sa r ek e p tu n d e rt h ea s s u m p t i o nt h a tt h e yw i l lb eu s e d again shortly. Other systems cache le data using a page cache .T h ep a g e cache uses virtual memory techniques to cache le data as pages rather than as lesystemoriented blocks. Caching le data using virtual addresses is far more efcient than caching through physical disk blocks, as accesses interface

physical disk blocks, as accesses interface with virtual memory rather than the le system. Several systemsincluding Solaris, Linux, and Windows use page caching to cache both process pages and le data. This is known as unied virtual memory .566 Chapter 12 FileSystem Implementationmemorymapped IOIO using read( ) and write( ) page cache buffer cache file system Figure 12.11 IO without a unied buffer cache. Some versions of UNIX and Linux provide a unied buffer cache .T o illustrate the benets of

a unied buffer cache .T o illustrate the benets of the unied buffer cache, consider the two alternatives for opening and accessing a le. One approach is to use memory mapping (Section 9.7); the second is to use the standard system calls read() and write() .W i t h o u tau n i  e db u f f e rc a c h e ,w eh a v eas i t u a t i o ns i m i l a rt oF i g u r e 12.11. Here, the read() andwrite() system calls go through the buffer cache. The memorymapping call, however, requires using two cachesthe

call, however, requires using two cachesthe page cache and the buffer cache. A memory mapping proceeds by reading in disk blocks from the le system and storing them in the buffer cache. Because the virtual memory system does not interface with the buffer cache, the contents of the le in the buffer cache must be copied into the page cache. This situation, known as double caching ,r e q u i r e sc a c h i n g l e  s y s t e md a t at w i c e .N o to n l y does it waste memory but it also wastes

o to n l y does it waste memory but it also wastes signicant CPU and IOcycles due to the extra data movement within system memory. In addition, inconsistencies between the two caches can result in corrupt les. In contrast, when a unied buffer cache is provided, both memory mapping and the read() andwrite() system calls use the same page cache. This has the benet of avoiding double caching, and it allows the virtual memory system to manage lesystem data. The unied buffer cache is shown in Figure

data. The unied buffer cache is shown in Figure 12.12. Regardless of whether we are caching disk blocks or pages (or both), LRU (Section 9.4.4) seems a reasonable generalpurpose algorithm for block or page replacement. However, the evolution of the Solaris pagecaching algorithms reveals the difculty in choosing an a lgorithm. Solaris allows processes and the page cache to share unused memory. Versions earlier than Solaris 2.5.1 made no distinction between allocating pages to a process and

between allocating pages to a process and allocating them to the page cache. As a result, a system performing many IOoperations used most of the available memory for caching pages. Because of the high rates of IO,t h ep a g es c a n n e r( S e c t i o n9 . 1 0 . 2 )r e c l a i m e dp a g e sf r o mp r o c e s s e s  r a t h e r than from the page cachewhen free memory ran low. Solaris 2.6 and Solaris 7o p t i o n a l l yi m p l e m e n t e dp r i o r i t yp a g i n g ,i nw h i c ht h ep a g es c

o r i t yp a g i n g ,i nw h i c ht h ep a g es c a n n e rg i v e s12.6 Efciency and Performance 567memorymapped IOIO using read( ) and write( ) buffer cache file system Figure 12.12 IO using a unied buffer cache. priority to process pages over the page cache. Solaris 8 applied a xed limit to process pages and the lesystem page cache, preventing either from forcing the other out of memory. Solaris 9 and 10 again changed the algorithms to maximize memory use and minimize thrashing. Another issue

memory use and minimize thrashing. Another issue that can affect the performance of IOis whether writes to the le system occur synchronously or asynchronously. Synchronous writes occur in the order in which the disk subsystem receives them, and the writes are not buffered. Thus, the calling routine must wait for the data to reach the disk drive before it can proceed. In an asynchronous write ,t h ed a t aa r es t o r e di n the cache, and control returns to the caller. Most writes are

and control returns to the caller. Most writes are asynchronous. However, metadata writes, among others, ca nb es y n c h r o n o u s .O p e r a t i n g systems frequently include a ag in the open system call to allow a process to request that writes be performed synchronously. For example, databases use this feature for atomic transactions, to assure that data reach stable storage in the required order. Some systems optimize their page cache by using different replacement algorithms, depending

using different replacement algorithms, depending on the access type of the le. A le being read or written sequentially should not have its pages replaced in LRU order, because the most recently used page will be used last, or perhaps never again. Instead, sequential access can be optimized by techniques known as freebehind and readahead. Freebehind removes a page from the buffer as soon as the next page is requested. The previous pages are not likely to be used again and waste buffer space.

likely to be used again and waste buffer space. With readahead , a requested page and several subsequent pages are read and cached. These pages are likely to be requested after the current page is processed. Retrieving these data from the disk in one transfer and caching them saves a considerable amount of time. One might think that at r a c kc a c h eo nt h ec o n t r o l l e rw o u l de l i m i n a t et h en e e df o rr e a d  a h e a do na multiprogrammed system. However, because of the high

system. However, because of the high latency and overhead involved in making many small transfers from the track cache to main memory, performing a readahead remains benecial. The page cache, the le system, and the disk drivers have some interesting interactions. When data are written to a disk le, the pages are buffered in the cache, and the disk driver sorts its output queue according to disk address. These two actions allow the disk driver to minimize diskhead seeks and to568 Chapter 12

to minimize diskhead seeks and to568 Chapter 12 FileSystem Implementation write data at times optimized for disk rotation. Unless synchronous writes are required, a process writing to disk simply writes into the cache, and the system asynchronously writes the data to disk wh en convenient. The user process sees very fast writes. When data are read from a disk le, the block IOsystem does some readahead; however, writes are much more nearly asynchronous than are reads. Thus, output to the disk

than are reads. Thus, output to the disk through the le system is often faster than is input for large transfers, counter to intuition. 12.7 Recovery Files and directories are kept both in main memory and on disk, and care must be taken to ensure that a system failure does not result in loss of data or in data inconsistency. We deal with these issues in this section. We also consider how as y s t e mc a nr e c o v e rf r o ms u c haf a i l u r e . As y s t e mc r a s hc a nc a u s ei n c o n s i

. As y s t e mc r a s hc a nc a u s ei n c o n s i s t e n c i e sa m o n go n  d i s k l e  s y s t e md a t a structures, such as directory structures, freeblock pointers, and free FCB pointers. Many le systems apply changes to these structures in place. A typical operation, such as creating a le, can involve many structural changes within the le system on the disk. Directory structures are modied, FCBsa r e allocated, data blocks are allocated, and the free counts for all of these blocks are

and the free counts for all of these blocks are decreased. These changes can be interrupted by a crash, and inconsistencies among the structures can result. For example, the free FCBcount might indicate that an FCB had been allocated, but the directory structure might not point to the FCB.C o m p o u n d i n gt h i sp r o b l e mi st h ec a c h i n gt h a to p e r a t i n gs y s t e m sd o to optimize IOperformance. Some changes may go directly to disk, while others may be cached. If the cached

to disk, while others may be cached. If the cached changes do not reach disk before a crash occurs, more corruption is possible. In addition to crashes, bugs in lesystem implementation, disk controllers, and even user applications can corrupt a le system. File systems have varying methods to deal with corruption, depending on the lesystem data structures and algorithms. We deal with these issues next. 12.7.1 Consistency Checking Whatever the cause of corruption, a le system must rst detect the

of corruption, a le system must rst detect the problems and then correct them. For detection, a scan of all the metadata on each le system can conrm or deny the consistency of the system. Unfortunately, this scan can take minutes or hours and should occur every time the system boots. Alternatively, a le system can record its state within the lesystem metadata. At the start of any metadata change, a status bit is set to indicate that the metadata is in ux. If all updates to the metadata complete

is in ux. If all updates to the metadata complete successfully, the le system can clear that bit. If, however, the status bit remains set, a consistency checker is run. The consistency checker a systems program such as fsck inUNIX  compares the data in the directory structure with the data blocks on disk and tries to x any inconsistencies it nds. The allocation and freespace management algorithms dictate what types of problems the checker can nd and how successful it will be in xing them. For

nd and how successful it will be in xing them. For instance, if linked allocation is used and there is a link from any block to its next block, then the entire le can be12.7 Recovery 569 reconstructed from the data blocks, and the directory structure can be recreated. In contrast, the loss of a directory en try on an indexed allocation system can be disastrous, because the data blocks have no knowledge of one another. For this reason, UNIX caches directory entries for reads; but any write that

directory entries for reads; but any write that results in space allocation, or other metadata changes, is done synchronously, before the corresponding data blocks are written. Of course, problems can still occur if a synchronous write is interrupted by a crash. 12.7.2 LogStructured File Systems Computer scientists often nd that algorithms and technologies originally used in one area are equally useful in other areas. Such is the case with the database logbased recovery algorithms. These logging

logbased recovery algorithms. These logging algorithms have been applied successfully to the problem of consistency checking. The resulting implementations are known as logbased transactionoriented (orjournaling ) le systems. Note that with the consistencychecking approach discussed in the pre ceding section, we essentially allow structures to break and repair them on recovery. However, there are several problems with this approach. One is that the inconsistency may be irreparable. The

is that the inconsistency may be irreparable. The consistency check may not be able to recover the structures, resulting in loss of les and even entire directories. Consistency checking can require human intervention to resolve conicts, and that is inconvenient if no human is available. The system can remain unavailable until the human tells it how to proceed. Consistency checking also takes system and clock time. To check terabytes of data, hours of clock time may be required. The solution to

of clock time may be required. The solution to this problem is to apply logbased recovery techniques to lesystem metadata updates. Both NTFS and the Veritas le system use this method, and it is included in recent versions of UFS on Solaris. In fact, it is becoming common on many operating systems. Fundamentally, all metadata changes are written sequentially to a log. Each set of operations for performing a specic task is a transaction .O n c e the changes are written to this log, they are

c e the changes are written to this log, they are considered to be committed, and the system call can return to the user process, allowing it to continue execution. Meanwhile, these log entries are replayed across the actual le system structures. As the changes are made, a pointer is updated to indicate which actions have completed and which are still incomplete. When an entire committed transaction is completed, it is removed from the log le, which is actually a circular buffer. A circular

which is actually a circular buffer. A circular buffer writes to the end of its space and then continues at the beginning, overwriting older values as it goes. We would not want the buffer to write over data that had not yet been saved, so that scenario is avoided. The log may be in a separate section of the le system or even on a separate disk spindle. It is more efcient, but more complex, to have it under separate read and write heads, thereby decreasing head contention and seek times. If the

decreasing head contention and seek times. If the system crashes, the log le will contain zero or more transactions. Any transactions it contains were not completed to the le system, even though they were committed by the operating system, so they must now be completed. The transactions can be executed from th ep o i n t e ru n t i lt h ew o r ki sc o m p l e t e570 Chapter 12 FileSystem Implementation so that the lesystem structures remain consistent. The only problem occurs when a transaction

The only problem occurs when a transaction was abortedthat is, was not committed before the system crashed. Any changes from such a transaction that were applied to the le system must be undone, again preserving the consistency of the le system. This recovery is all that is needed after a crash, eliminating any problems with consistency checking. As i d eb e n e  to fu s i n gl o g g i n go nd i s km e t a d a t au p d a t e si st h a tt h o s e updates proceed much faster than when th ey are

e updates proceed much faster than when th ey are applied directly to the on disk data structures. The reason is found in the performance advantage of sequential IOover random IO.T h ec o s t l ys y n c h r o n o u sr a n d o mm e t a d a t a writes are turned into much less c ostly synchronous sequential writes to the logstructured le systems logging area. Those changes, in turn, are replayed asynchronously via random writes to the appropriate structures. The overall result is a signicant gain

structures. The overall result is a signicant gain in performance of metadataoriented operations, such as le creation and deletion. 12.7.3 Other Solutions Another alternative to consistency checking is employed by Network Appli ances WAFL le system and the Solaris ZFSle system. These systems never overwrite blocks with new data. Rather, a transaction writes all data and meta data changes to new blocks. When the transaction is complete, the metadata structures that pointed to the old versions of

structures that pointed to the old versions of these blocks are updated to point to the new blocks. The le system can then remove the old pointers and the old blocks and make them available for reuse. If the old pointers and blocks are kept, a snapshot is created; the snapshot is a view of the le system before the last update took place. This solution should require no consistency checking if the pointer update is done atomically. WAFL does have a consistency checker, however, so some failure

a consistency checker, however, so some failure scenarios can still cause metadata corruption. (See Section 12.9 for details of the WAFL le system.) ZFStakes an even more innovative approach to disk consistency. It never overwrites blocks, just like WAFL .H o w e v e r , ZFS goes further and provides checksumming of all metadata and data blocks. This solution (when combined with RAID )a s s u r e st h a td a t aa r ea l w a y sc o r r e c t . ZFStherefore has no consistency checker. (More

t . ZFStherefore has no consistency checker. (More details on ZFSare found in Section 10.7.6.) 12.7.4 Backup and Restore Magnetic disks sometimes fail, and care must be taken to ensure that the data lost in such a failure are not lost forever. To this end, system programs can be used to back up data from disk to another storage device, such as a magnetic tape or other hard disk. Recovery from the loss of an individual le, or of an entire disk, may then be a matter of restoring the data from

may then be a matter of restoring the data from backup. To minimize the copying needed, we can use information from each les directory entry. For instance, if the backup program knows when the last backup of a le was done, and the les last write date in the directory indicates that the le has not changed since that date, then the le does not need to be copied again. A typical backup schedule may then be as follows: Day 1 .C o p yt oab a c k u pm e d i u ma l l l e sf r o mt h ed i s k .T h i si

e d i u ma l l l e sf r o mt h ed i s k .T h i si sc a l l e da full backup .12.8 NFS 571 Day 2 .C o p yt oa n o t h e rm e d i u ma l l l e sc h a n g e ds i n c ed a y1 .T h i si sa n incremental backup . Day 3 . Copy to another medium all les changed since day 2. . . . Day N. Copy to another medium all les changed since day N1. Then go back to day 1. The new cycle can have its backup written over the previous set or onto a new set of backup media. Using this method, we can restore an entire

media. Using this method, we can restore an entire disk by starting restores with the full backup and continuing through each of the incremental backups. Of course, the larger the value of N,the greater the number of media that must be read for a complete restore. An added advantage of this backup cycle is that we can restore any le accidentally deleted during the cycle by retrieving the deleted le from the backup of the previous day. The length of the cycle is a compromise between the amount of

of the cycle is a compromise between the amount of backup medium needed and the number of days covered by a restore. To decrease the number of tapes that must be read to do a restore, an option is to perform a full backup and then each day back up all les that have changed since the full backup. In this way, a restore can be done via the most recent incremental backup and the full backup, with no other incremental backups needed. The tradeoff is that more les will be modied each day, so each

is that more les will be modied each day, so each successive incremental backup involves more les and more backup media. A user may notice that a particular le is missing or corrupted long after the damage was done. For this reason, we usually plan to take a full backup from time to time that will be saved forever. It is a good idea to store these permanent backups far away from the regular backups to protect against hazard, such as a re that destroys the computer and all the backups too. And if

the computer and all the backups too. And if the backup cycle reuses media, we must take care not to reuse the media too many timesif the media wear out, it might not be possible to restore any data from the backups. 12.8 NFS Network le systems are commonplace. They are typically integrated with the overall directory structure and interface of the client system. NFS is a good example of a widely used, well implemented clientserver network le system. Here, we use it as an example to explore the

Here, we use it as an example to explore the implementation details of network le systems. NFSis both an implementation and a specication of a software system for accessing remote les across LAN s( o re v e n WAN s).NFS is part of ONC ,w h i c h most UNIX vendors and some PCoperating systems support. The implementa tion described here is part of the Solaris operating system, which is a modied version of UNIX SVR4 . It uses either the TCP orUDPIP protocol (depending on572 Chapter 12 FileSystem

protocol (depending on572 Chapter 12 FileSystem Implementation localusr shared dir1usrU: S1: S2: dir2usr Figure 12.13 Three independent le systems. the interconnecting network). The sp ecication and the implementation are intertwined in our description of NFS.W h e n e v e rd e t a i li sn e e d e d ,w er e f e rt o the Solaris implementation; whenever th ed e s c r i p t i o ni sg e n e r a l ,i ta p p l i e st o the specication also. There are multiple versions of NFS,w i t ht h el a t e s tb

multiple versions of NFS,w i t ht h el a t e s tb e i n gV e r s i o n4 .H e r e , we describe Version 3, as that is the one most commonly deployed. 12.8.1 Overview NFSviews a set of interconnected workstations as a set of independent machines with independent le systems. The goal is to allow some degree of sharing among these le systems (on explicit request) in a transparent manner. Sharing is based on a clientserver relationship. A machine may be, and often is, both a client and a server.

may be, and often is, both a client and a server. Sharing is allowed between any pair of machines. To ensure machine independence, sharing of a remote le system affects only the client machine and no other machine. So that a remote directory will be accessible in a transparent manner from a particular machinesay, from M1a client of that machine must rst carry out a mount operation. The semantics of the operation involve mounting a remote directory over a directory of a local le system. Once the

over a directory of a local le system. Once the mount operation is completed, the mounted directory looks like an integral subtree of the local le system, replacing the subtree descending from the local directory. The local directory becomes the name of the root of the newly mounted directory. Specication of the remote directory as an argument for the mount operation is not done transparently; the location (or host name) of the remote directory has to be provided. However, from then on, users on

to be provided. However, from then on, users on machine M1can access les in the remote directory in a totally transparent manner. To illustrate le mounting, consider the le system depicted in Figure 12.13, where the triangles represent subtrees of directories that are of interest. The gure shows three independent le systems of machines named U,S1,a n d S2.A tt h i sp o i n t ,o ne a c hm a c h i n e ,o n l yt h el o c a l l e sc a nb ea c c e s s e d .F i g u r e 12.14(a) shows the effects of

s s e d .F i g u r e 12.14(a) shows the effects of mounting S1:usrshared over U:usrlocal . This gure depicts the view users on Uhave of their le system. After the mount is complete, they can access any le within the dir1 directory using the12.8 NFS 573 local dir1 dir1usrU: U: (a) (b)localusr Figure 12.14 Mounting in NFS. (a) Mounts. (b) Cascading mounts. prex  usrlocaldir1 .T h eo r i g i n a ld i r e c t o r y usrlocal on that machine is no longer visible. Subject to accessrights accreditation,

visible. Subject to accessrights accreditation, any le system, or any directory within a le system, can be mounted remotely on top of any local directory. Diskless workstations can even mount their own roots from servers. Cascading mounts are also permitted in some NFS implementations. That is, a le system can be mounted over another le system that is remotely mounted, not local. A machine is affected by only those mounts that it has itself invoked. Mounting a remote le system does not give the

Mounting a remote le system does not give the client access to other le systems that were, by chance, mounted over the former le system. Thus, the mount mechanism does not exhibit a transitivity property. In Figure 12.14(b), we illustrate cascading mounts. The gure shows the result of mounting S2:usrdir2 over U:usrlocaldir1 ,w h i c hi sa l r e a d y remotely mounted from S1.Users can access les within dir2 onUusing the prex usrlocaldir1 .I fas h a r e d l es y s t e mi sm o u n t e do v e rau s

a r e d l es y s t e mi sm o u n t e do v e rau s e r sh o m e directories on all machines in a network, the user can log into any workstation and get their home environment. This property permits user mobility. One of the design goals of NFS was to operate in a heterogeneous envi ronment of different machines, operating systems, and network architectures. The NFS specication is independent of these media. This independence is achieved through the use of RPC primitives built on top of an

the use of RPC primitives built on top of an external data representation ( XDR)p r o t o c o lu s e db e t w e e nt w oi m p l e m e n t a t i o n  i n d e p e n d e n t interfaces. Hence, if the systems heterogeneous machines and le systems are properly interfaced to NFS, l es y s t e m so fd i f f e r e n tt y p e sc a nb em o u n t e db o t h locally and remotely. The NFS specication distinguishes between the services provided by a mount mechanism and the actual remotele access services.

mechanism and the actual remotele access services. Accordingly, two separate protocols are specied for these services: a mount protocol and a protocol for remote le accesses, the NFS protocol .T h ep r o t o c o l sa r es p e c i  e da s sets of RPCs. These RPCsa r et h eb u i l d i n gb l o c k su s e dt oi m p l e m e n tt r a n s p a r e n t remote le access.574 Chapter 12 FileSystem Implementation 12.8.2 The Mount Protocol The mount protocol establishes the initial logical connection between

establishes the initial logical connection between a server and a client. In Solaris, each machine has a server process, outside the kernel, performing the protocol functions. Am o u n to p e r a t i o ni n c l u d e st h en a m eo ft h er e m o t ed i r e c t o r yt ob e mounted and the name of the server machine storing it. The mount request is mapped to the corresponding RPC and is forwarded to the mount server running on the specic server machine. The server maintains an export list that

machine. The server maintains an export list that species local le systems that it exports for mounting, along with names of machines that are permitted to mount them. (In Solaris, this list is the etcdfsdfstab ,w h i c hc a nb ee d i t e do n l yb yas u p e r u s e r . )T h es p e c i  c a t i o n can also include access rights, such as read only. To simplify the maintenance of export lists and mount tables, a distributed naming scheme can be used to hold this information and make it available

to hold this information and make it available to appropriate clients. Recall that any directory within an exported le system can be mounted remotely by an accredited machine. A component unit is such a directory. When the server receives a mount request that conforms to its export list, it returns to the client a le handle that serves as the key for further accesses to les within the mounted le system. The le handle contains all the information that the server needs to distinguish an individual

that the server needs to distinguish an individual le it stores. In UNIX terms, the le handle consists of a lesystem identier and an inode number to identify the exact mounted directory within the exported le system. The server also maintains a list of the client machines and the corresponding currently mounted directories. This list is used mainly for administrative purposesfor instance, for notifying all clients that the server is going down. Only through addition and deletion of entries in

Only through addition and deletion of entries in this list can the server state be affected by the mount protocol. Usually, a system has a static mounting preconguration that is established at boot time ( etcvfstab in Solaris); however, this layout can be modied. In addition to the actual mount procedure, the mount protocol includes several other procedures, such as unmount and return export list. 12.8.3 The NFS Protocol The NFS protocol provides a set of RPCsf o rr e m o t e l eo p e r a t i o

a set of RPCsf o rr e m o t e l eo p e r a t i o n s .T h e procedures support the following operations: Searching for a le within a directory Reading a set of directory entries Manipulating links and directories Accessing le attributes Reading and writing les These procedures can be invoked only after a le handle for the remotely mounted directory has been established. The omission of open and close operations is intentional. A prominent feature of NFS servers is that they are stateless.

feature of NFS servers is that they are stateless. Servers do not maintain information about their clients from one access to another. No parallels to12.8 NFS 575 UNIX s openles table or le structures exist on the server side. Consequently, each request has to provide a full set of arguments, including a unique le identier and an absolute offset inside the le for the appropriate operations. The resulting design is robust; no special measures need be taken to recover as e r v e ra f t e rac r a s

be taken to recover as e r v e ra f t e rac r a s h .F i l eo p e r a t i o n sm u s tb ei d e m p o t e n tf o rt h i sp u r p o s e , that is, the same operation performed multiple times has the same effect as if it were only performed once. To achieve idempotence, every NFS request has a sequence number, allowing the server to determine if a request has been duplicated or if any are missing. Maintaining the list of clients that we mentioned seems to violate the statelessness of the server.

seems to violate the statelessness of the server. However, this list is not essential for the correct operation of the client or the server, and hence it does not need to be restored after a server crash. Consequently, it may include inconsistent data and is treated as only a hint. Af u r t h e ri m p l i c a t i o no ft h es t a t e l e s s  s e r v e rp h i l o s o p h ya n dar e s u l to ft h e synchrony of an RPC is that modied data (including indirection and status blocks) must be committed

indirection and status blocks) must be committed to the servers disk before results are returned to the client. That is, a client can cache write blocks, but when it ushes them to the server, it assumes that they have reached the servers disks. The server must write all NFS data synchronously. Thus, a server crash and recovery will be invisible to a client; all blocks that the server is managing for the client will be intact. The resulting performance penalty can be large, because the advantages

penalty can be large, because the advantages of caching are lost. Performance can be increased by using storage with its own nonvolatile cache (usually batterybackedup memory). The disk controller acknowledges the disk write when the write is stored in the nonvolatile cache. In essence, the host sees a very fast synchronous write. These blocks remain intact even after a system crash and are written from this stable storage to disk periodically. As i n g l e NFS write procedure call is guaranteed

i n g l e NFS write procedure call is guaranteed to be atomic and is not intermixed with other write calls to the same le. The NFS protocol, however, does not provide concurrencycontrol mechanisms. A write() system call may be broken down into several RPC writes, because each NFS write or read call can contain up to 8 KBof data and UDP packets are limited to 1,500 bytes. As a result, two users writing to the same remote le may get their data intermixed. The claim is that, because lock management

The claim is that, because lock management is inherently stateful, a service outside the NFS should provide locking (and Solaris does). Users are advised to coordinate access to shared les using mechanisms outside the scope of NFS. NFS is integrated into the operating system via a VFS.A sa ni l l u s t r a t i o n of the architecture, lets trace how an operation on an alreadyopen remote le is handled (follow the example in Figure 12.15). The client initiates the operation with a regular system

initiates the operation with a regular system call. The operatingsystem layer maps this call to a VFSoperation on the appropriate vnode. The VFSlayer identies the le as a remote one and invokes the appropriate NFS procedure. An RPC call is made to the NFS service layer at the remote server. This call is reinjected to the VFS layer on the remote system, which nds that it is local and invokes the appropriate lesystem operation. This path is retraced to return the result. An advantage of this

to return the result. An advantage of this architecture is that the client and the server are identical; thus, a machine may be a client, or a server, or both. The actual service on each server is performed by kernel threads.576 Chapter 12 FileSystem Implementation diskdisksystemcalls interfaceclient server other types of file systemsUNIX file systemUNIX file systemNFS client RPCXDR networkRPCXDRNFS serverVFS interface VFS interface Figure 12.15 Schematic view of the NFS architecture. 12.8.4

Schematic view of the NFS architecture. 12.8.4 PathName Translation Pathname translation inNFS involves the parsing of a path name such as usrlocaldir1file.txt into separate directory entries, or components: (1)usr,( 2 )local ,a n d( 3 ) dir1 .P a t h  n a m et r a n s l a t i o ni sd o n eb yb r e a k i n gt h e path into component names and performing a separate NFSlookup call for every pair of component name and dire ctory vnode. Once a mount point is crossed, every component lookup causes a

point is crossed, every component lookup causes a separate RPC to the server. This expensive pathnametraversal scheme is needed, since the layout of each clients logical name space is unique, dictated by the mounts the client has performed. It would be much more efcient to hand a server a path name and receive a target vnode once a moun t point is encountered. At any point, however, there might be another mount point for the particular client of which the stateless server is unaware. So that

of which the stateless server is unaware. So that lookup is fast, a directorynamelookup cache on the client side holds the vnodes for remote directory names. This cache speeds up references to les with the same initial path name. The directory cache is discarded when attributes returned from the server do not match the attributes of the cached vnode. Recall that some implementations of NFS allow mounting a remote le system on top of another alreadymounted remote le system (a cascading mount).

remote le system (a cascading mount). When a client has a cascading mount, more than one server can be involved in a pathname traversal. However, when a client does a lookup on ad i r e c t o r yo nw h i c ht h es e r v e rh a sm o u n t e da l es y s t e m ,t h ec l i e n ts e e st h e underlying directory instead of the mounted directory.12.9 Example: The WAFL File System 577 12.8.5 Remote Operations With the exception of opening and closing les, there is an almost onetoone correspondence

les, there is an almost onetoone correspondence between the regular UNIX system calls for le operations and the NFS protocol RPCs. Thus, a remote le operation can be translated directly to the corresponding RPC.C o n c e p t u a l l y , NFS adheres to the remoteservice paradigm; but in practice, buffering and caching techniques are employed for the sake of performance. No direct corr espondence exists between a remote operation and an RPC. Instead, le blocks and le attributes are fetched by the

le blocks and le attributes are fetched by the RPCsa n da r ec a c h e dl o c a l l y .F u t u r er e m o t eo p e r a t i o n su s et h ec a c h e dd a t a , subject to consistency constraints. There are two caches: the leattribute (inodeinformation) cache and the leblocks cache. When a le is opened, the kernel checks with the remote server to determine whether to fetch or revalidate the cached attributes. The cached le blocks are used only if the corresponding cached attributes are up to date.

corresponding cached attributes are up to date. The attribute cache is updated whenever new attributes arrive from the server. Cached attributes are, by default, discarded after 60 seconds. Both readahead and delayedwrite techniques ar e used between the server and the client. Clients do not free delayedwrite blocks until the server conrms that the data have been written to disk. Delayedwrite is retained even when a le is opened concurrently, in conicting modes. Hence, UNIX semantics (Section

in conicting modes. Hence, UNIX semantics (Section 11.5.3.1) are not preserved. Tuning the system for performance makes it difcult to characterize the consistency semantics of NFS.N e w l e sc r e a t e do nam a c h i n em a yn o tb e visible elsewhere for 30 seconds. Furthermore, writes to a le at one site may or may not be visible at other sites that have this le open for reading. New opens of a le observe only the changes that have already been ushed to the server. Thus, NFS provides neither

ushed to the server. Thus, NFS provides neither strict emulation of UNIX semantics nor the session semantics of Andrew (Section 11.5.3.2). In spite of these drawbacks, the utility and good performance of the mechanism make it the most widely used multivendordistributed system in operation. 12.9 Example: The WAFL File System Because disk IOhas such a huge impact on system performance, lesystem design and implementation command quite a lot of attention from system designers. Some le systems are

from system designers. Some le systems are general purpose, in that they can provide reasonable performance and functionality for a wide variety of le sizes, le types, and IOloads. Others are optimized for specic tasks in an attempt to provide better performance in those areas than generalpurpose le systems. Thewriteanywhere le layout (WAFL )from Network Appliance is an example of this sort of optimization. WAFL is a powerful, elegant le system optimized for random writes. WAFL is used

system optimized for random writes. WAFL is used exclusively on network le servers produced by Network Appliance and is meant for use as a distributed le system. It can provide les to clients via the NFS,CIFS,ftp,a n d http protocols, although it was designed just for NFS and CIFS.W h e nm a n yc l i e n t su s et h e s ep r o t o c o l st ot a l kt oa l e server, the server may see a very large demand for random reads and an even larger demand for random writes. The NFSand CIFS protocols cache

for random writes. The NFSand CIFS protocols cache data from read operations, so writes are of the greatest concern to leserver creators.578 Chapter 12 FileSystem Implementation WAFL is used on le servers that include an NVRAM cache for writes. The WAFL designers took advantage of running on a specic architecture to optimize the le system for random IO,w i t has t a b l e  s t o r a g ec a c h ei nf r o n t . Ease of use is one of the guiding principles of WAFL .I t sc r e a t o r sa l s od e s

of WAFL .I t sc r e a t o r sa l s od e s i g n e di t to include a new snapshot functionality that creates multiple readonly copies of the le system at different points in time, as we shall see. The le system is similar to the Berkeley Fast File System, with many modications. It is blockbased and uses inodes to describe les. Each inode contains 16 pointers to blocks (or indirect blocks) belonging to the le described by the inode. Each le system has a root inode. All of the metadata lives in

has a root inode. All of the metadata lives in les. All inodes are in one le, the freeblock map in another, and the freeinode map in a third, as shown in Figure 12.16. Because these are standard les, the data blocks are not limited in location and can be placed anywhere. If a le system is expanded by addition of disks, the lengths of the metadata les are automatically expanded by the le system. Thus, a WAFL le system is a tree of blocks with the root inode as its base. To take a snapshot, WAFL

root inode as its base. To take a snapshot, WAFL creates a copy of the root inode. Any le or metadata updates after that go to new blocks rather than overwriting their existing blocks. The new root inode points to metadata and data changed as a result of these writes. Meanwhile, the snapshot (the old root inode) still points to the old blocks, which have not been updated. It therefore provides access to the le system just as it was at the instant the snapshot was madeand takes very little disk

the snapshot was madeand takes very little disk space to do so. In essence, the extra disk space occupied by a snapshot consists of just the blocks that have been modied since the snapshot was taken. An important change from more standard le systems is that the freeblock map has more than one bit per block. It is a bitmap with a bit set for each snapshot that is using the block. When all snapshots that have been using the block are deleted, the bit map for that block is all zeros, and the block

bit map for that block is all zeros, and the block is free to be reused. Used blocks are never overwritten, so writes are very fast, because aw r i t ec a no c c u ra tt h ef r e eb l o c kn e a r e s tt h ec u r r e n th e a dl o c a t i o n .T h e r ea r e many other performance optimizations in WAFL as well. Many snapshots can exist simultaneously, so one can be taken each hour of the day and each day of the month. A user with access to these snapshots can access les as they were at any of

snapshots can access les as they were at any of the times the snapshots were taken. The snapshot facility is also useful for backups, testing, versioning, and so on. free block map free inode map file in the file system...root inode inode file  Figure 12.16 The WAFL le layout.12.9 Example: The WAFL File System 579 block A B C D Eroot inode (a) Before a snapshot. block A B C D Eroot inode (b) After a snapshot, before any blocks change.new snapshot block A B C D D Eroot inode (c) After block D has

block A B C D D Eroot inode (c) After block D has changed to D.new snapshot Figure 12.17 Snapshots in WAFL. WAFL s snapshot facility is very efcient in that it does not even require that copyonwrite copies of each data block be taken before the block is modied. Other le systems provide snapshots, but frequently with less efciency. WAFL snapshots are depicted in Figure 12.17. Newer versions of WAFL actually allow readwrite snapshots, known as clones .C l o n e sa r ea l s oe f  c i e n t ,u s i n

.C l o n e sa r ea l s oe f  c i e n t ,u s i n gt h es a m et e c h n i q u e sa ss h a p s h o t s .I n this case, a readonly snapshot captures the state of the le system, and a clone refers back to that readonly snapshot. Any writes to the clone are stored in new blocks, and the clones pointers are updated to refer to the new blocks. The original snapshot is unmodied, still giving a view into the le system as it was before the clone was updated. Clones can also be promoted to replace the

Clones can also be promoted to replace the original le system; this involves throwing out all of the old pointers and any associated old blocks. Clones are useful for testing and upgrades, as the original version is left untouched and the clone deleted when the test is done or if the upgrade fails. Another feature that naturally results from the WAFL le system implemen tation is replication, the duplication and synchronization of a set of data over a network to another system. First, a snapshot

a network to another system. First, a snapshot of a WAFL le system is duplicated to another system. When another snapshot is taken on the source system, it is relatively easy to update the remote system just by sending over all blocks contained in the new snapshot. These blocks are the ones that have changed580 Chapter 12 FileSystem Implementation between the times the two snapshots were taken. The remote system adds these blocks to the le system and updates its pointers, and the new system then

and updates its pointers, and the new system then is a duplicate of the source system as of the time of the second snapshot. Repeating this process maintains the remote system as a nearly uptodate copy of the rst system. Such replication is used for disaster recovery. Should the rst system be destroyed, most of its data are available for use on the remote system. Finally, we should note that the ZFSle system supports similarly efcient snapshots, clones, and replication. 12.10 Summary The le

clones, and replication. 12.10 Summary The le system resides permanently on secondary storage, which is designed to hold a large amount of data permanently. The most common secondarystorage medium is the disk. Physical disks may be segmented into partitions to control media use and to allow multiple, possibly varying, le systems on a single spindle. These le systems are mounted onto a logical le system architecture to make them available for use. File systems are often implemented in a layered

File systems are often implemented in a layered or modular structure. The lower levels deal with the physical properties of storage devices. Upper levels deal with symbolic le names and logical properties of les. Intermediate levels map the logical le c oncepts into physical device properties. Any lesystem type can have different structures and algorithms. A VFS layer allows the upper layers to deal with each lesystem type uniformly. Even remote le systems can be integrated into the systems

le systems can be integrated into the systems directory structure and acted on by standard system calls via the VFSinterface. The various les can be allocated space on the disk in three ways: through contiguous, linked, or indexed allocation. Contiguous allocation can suffer from external fragmentation. Direct access is very inefcient with linked allocation. Indexed allocation may require substantial overhead for its index block. These algorithms can be optimized in many ways. Contiguous space

can be optimized in many ways. Contiguous space can be enlarged through extents to increase exibility and to decrease external fragmentation. Indexed allocation can be done in clusters of multiple blocks to increase throughput and to reduc et h en u m b e ro fi n d e xe n t r i e sn e e d e d . Indexing in large clusters is similar to contiguous allocation with extents. Freespace allocation methods also inuence the efciency of diskspace use, the performance of the le system, and the reliability

performance of the le system, and the reliability of secondary storage. The methods used include bit vectors and linked lists. Optimizations include grouping, counting, and the FAT,w h i c hp l a c e st h el i n k e dl i s ti no n ec o n t i g u o u s area. Directorymanagement routines must consider efciency, performance, and reliability. A hash table is a commonly used method, as it is fast and efcient. Unfortunately, damage to the table or a system crash can result in inconsistency between the

crash can result in inconsistency between the directory information and the disks contents. Ac o n s i s t e n c yc h e c k e rc a nb eu s e dt or e p a i rt h ed a m a g e .O p e r a t i n g  s y s t e m backup tools allow disk data to be copied to tape, enabling the user to recover from data or even disk loss due to hardware failure, operating system bug, or user error.Practice Exercises 581 Network le systems, such as NFS,u s ec l i e n t  s e r v e rm e t h o d o l o g yt o allow users to

s e r v e rm e t h o d o l o g yt o allow users to access les and directories from remote machines as if they were on local le systems. System calls on the client are translated into network protocols and retranslated into lesystem operations on the server. Networking and multipleclient access create challenges in the areas of data consistency and performance. Due to the fundamental role that le systems play in system operation, their performance and reliability are crucial. Techniques such as

and reliability are crucial. Techniques such as log structures and caching help improve performance, while log structures and RAID improve reliability. The WAFL le system is an example of optimization of performance to match a specic IOload. Practice Exercises 12.1 Consider a le currently consisting of 100 blocks. Assume that the le control block (and the index block, in the case of indexed allocation) is already in memory. Calculate how many disk IO operations are required for contiguous,

disk IO operations are required for contiguous, linked, and indexed (singlelevel) allocation strategies, if, for one block, the following conditions hold. In the contiguousallocation case, assume that there is no room to grow at the beginning but there is room to grow at the end. Also assume that the block information to be added is stored in memory. a. The block is added at the beginning. b. The block is added in the middle. c. The block is added at the end. d. The block is removed from the

added at the end. d. The block is removed from the beginning. e. The block is removed from the middle. f. The block is removed from the end. 12.2 What problems could occur if a system allowed a le system to be mounted simultaneously at more than one location? 12.3 Why must the bit map for le allocation be kept on mass storage, rather than in main memory? 12.4 Consider a system that supports the strategies of contiguous, linked, and indexed allocation. What criteria should be used in deciding

What criteria should be used in deciding which strategy is best utilized for a particular le? 12.5 One problem with contiguous allocation is that the user must preallo cate enough space for each le. If the le grows to be larger than the space allocated for it, special actions must be taken. One solution to this problem is to dene a le structure consisting of an initial contiguous area (of a specied size). If this area is lled, the operating system automatically denes an overow area that is

system automatically denes an overow area that is linked to the initial contiguous area. If the overow area is lled, another overow area is allocated. Compare this implementation of a le with the standard contiguous and linked implementations.582 Chapter 12 FileSystem Implementation 12.6 How do caches help improve performance? Why do systems not use more or larger caches if they are so useful? 12.7 Why is it advantageous to the user for an operating system to dynami cally allocate its internal

system to dynami cally allocate its internal tables? What are the penalties to the operating system for doing so? 12.8 Explain how the VFS layer allows an operating system to support multiple types of le systems easily. Exercises 12.9 Consider a le system that uses a modifed contiguousallocation scheme with support for extents. A le is a collection of extents, with each extent corresponding to a contiguous set of blocks. A key issue in such systems is the degree of variability in the size of the

is the degree of variability in the size of the extents. What are the advantages and disadvantages of the following schemes? a. All extents are of the same size, and the size is predetermined. b. Extents can be of any size and are allocated dynamically. c. Extents can be of a few xed sizes, and these sizes are predeter mined. 12.10 Contrast the performance of the three techniques for allocating disk blocks (contiguous, linked, and indexed) for both sequential and random le access. 12.11 What are

sequential and random le access. 12.11 What are the advantages of the variant of linked allocation that uses a FATto chain together the blocks of a le? 12.12 Consider a system where free space is kept in a freespace list. a. Suppose that the pointer to the freespace list is lost. Can the system reconstruct the freespace list? Explain your answer. b. Consider a le system similar to the one used by UNIX with indexed allocation. How many disk IO operations might be required to read the contents of

might be required to read the contents of a small local le at abc ? Assume that none of the disk blocks is currently being cached. c. Suggest a scheme to ensure that the pointer is never lost as a result of memory failure. 12.13 Some le systems allow disk storage to be allocated at different levels of granularity. For instance, a le system could allocate 4 KBof disk space as a single 4 KBblock or as eight 512byte blocks. How could we take advantage of this exibility to improve performance? What

of this exibility to improve performance? What modications would have to be made to the freespace management scheme in order to support this feature? 12.14 Discuss how performance optimizations for le systems might result in difculties in maintaining the consistency of the systems in the event of computer crashes.Programming Problems 583 12.15 Consider a le system on a disk that has both logical and physical block sizes of 512 bytes. Assume that the information about each le is already in

that the information about each le is already in memory. For each of the three allocation strategies (contiguous, linked, and indexed), answer these questions: a. How is the logicaltophysical address mapping accomplished in this system? (For the indexed allocation, assume that a le is always less than 512 blocks long.) b. If we are currently at logical block 10 (the last block accessed was block 10) and want to access logical block 4, how many physical blocks must be read from the disk? 12.16

physical blocks must be read from the disk? 12.16 Consider a le system that uses inodes to represent les. Disk blocks are 8 KBin size, and a pointer to a disk block requires 4 bytes. This le system has 12 direct disk blocks, as well as single, double, and triple indirect disk blocks. What is the maximum size of a le that can be stored in this le system? 12.17 Fragmentation on a storage device can be eliminated by recompaction of the information. Typical disk devices do not have relocation or

Typical disk devices do not have relocation or base registers (such as those used when memory is to be compacted), so how can we relocate les? Give three reasons why recompacting and relocation of les are often avoided. 12.18 Assume that in a particular augmentation of a remoteleaccess protocol, each client maintains a name cache that caches translations from le names to corresponding le handles. What issues should we take into account in implementing the name cache? 12.19 Explain why logging

the name cache? 12.19 Explain why logging metadata updates ensures recovery of a le system after a lesystem crash. 12.20 Consider the following backup scheme: Day 1 .C o p yt oab a c k u pm e d i u ma l l l e sf r o mt h ed i s k . Day 2 . Copy to another medium all les changed since day 1. Day 3 . Copy to another medium all les changed since day 1. This differs from the schedule given in Section 12.7.4 by having all subsequent backups copy all les modied since the rst full backup. What are the

les modied since the rst full backup. What are the benets of this system over the one in Section 12.7.4? What are the drawbacks? Are restore operations made easier or more difcult? Explain your answer. Programming Problems The following exercise examines th er e l a t i o n s h i pb e t w e e n l e sa n d inodes on a UNIX or Linux system. On these systems, les are repre sented with inodes. That is, an inode is a le (and vice versa). You can complete this exercise on the Linux virtual machine

this exercise on the Linux virtual machine that is provided with this text. You can also complete the exercise on any Linux, UNIX ,o r584 Chapter 12 FileSystem Implementation Mac OS X system, but it will require creating two simple text les named file1.txt andfile3.txt whose contents are unique sentences. 12.21 In the source code available with this text, open file1.txt and examine its contents. Next, obtain the inode number of this le with the command ls li file1.txt This will produce output

command ls li file1.txt This will produce output similar to the following: 16980 rwrr 2 os os 22 Sep 14 16:13 file1.txt where the inode number is boldfaced. (The inode number of file1.txt is likely to be different on your system.) The UNIX lncommand creates a link between a source and target le. This command works as follows: ln [s] source file target file UNIX provides two types of links: (1) hard links and (2) soft links . Ah a r dl i n kc r e a t e sas e p a r a t et a r g e t l et h a th a

e a t e sas e p a r a t et a r g e t l et h a th a st h es a m ei n o d ea st h e source le. Enter the following command to create a hard link between file1.txt andfile2.txt : ln file1.txt file2.txt What are the inode values of file1.txt and file2.txt ?A r et h e y the same or different? Do the two les have the sameor different contents? Next, edit file2.txt and change its contents. After you have done so, examine the contents of file1.txt .A r et h ec o n t e n t so f file1.txt andfile2.txt the

h ec o n t e n t so f file1.txt andfile2.txt the same or different? Next, enter the following command which removes file1.txt : rm file1.txt Does file2.txt still exist as well? Now examine the manpages for both the rmandunlink commands. Afterwards, remove file2.txt by entering the command strace rm file2.txt The strace command traces the execution of system calls as the command rm file2.txt is run. What system call is used for removing file2.txt ? A soft link (or symbolic link) creates a new le

? A soft link (or symbolic link) creates a new le that points to the name of the le it is linking to. In the source code available with this text, create a soft link to file3.txt by entering the following command: ln s file3.txt file4.txt After you have done so, obtain the inode numbers of file3.txt and file4.txt using the command ls li file.txtBibliography 585 Are the inodes the same, or is eac hu n i q u e ?N e x t ,e d i tt h ec o n t e n t s offile4.txt .H a v et h ec o n t e n t so f

n t s offile4.txt .H a v et h ec o n t e n t so f file3.txt been altered as well? Last, delete file3.txt . After you have done so, explain what happens when you attempt to edit file4.txt . Bibliographical Notes The MSDOS FAT system is explained in [Norton and Wilton (1988)]. The internals of the BSD UNIX system are covered in full in [McKusick and NevilleNeil (2005)]. Details concerning le systems for Linux can be found in [Love (2010)]. The Google le system is described in [Ghemawat et al.

Google le system is described in [Ghemawat et al. (2003)]. FUSE can be found at http:fuse.sourceforge.net . Logstructured le organizations for enhancing both performance and consistency are discussed in [Rosenblum and Ousterhout (1991)], [Seltzer et al. (1993)], and [Seltzer et al. (1995)]. Algorithms such as balanced trees (and much more) are covered by [Knuth (1998)] and [Cormen et al. (2009)]. [Silvers (2000)] discusses implementing the page cache in the NetBSD operating system. The ZFSsource

in the NetBSD operating system. The ZFSsource code for space maps can be found at http:src.opensolaris.orgsourcexrefonnvonnvgateusrsrcutscommon fszfsspace map.c . The network le system ( NFS)i sd i s c u s s e di n[ C a l l a g h a n( 2 0 0 0 ) ] . NFS Ver sion 4 is a standard described at http:www.ietf.orgrfcrfc3530.txt .[ O u s t e r  hout (1991)] discusses the role of distributed state in networked le systems. Logstructured designs for networked le systems are proposed in [Hartman and

networked le systems are proposed in [Hartman and Ousterhout (1995)] and [Thekkath et al. (1997)]. NFS and the UNIX le system ( UFS)a r ed e s c r i b e di n[ V a h a l i a( 1 9 9 6 ) ]a n d[ M a u r oa n dM c D o u g a l l (2007)]. The NTFS le system is explained in [Solomon (1998)]. The Ext3 le system used in Linux is described in [Mauerer (2008)] and the WAFL le system is covered in [Hitz et al. (1995)]. ZFS documentation can be found athttp:www.opensolaris.orgoscommunityZFSdocs .

athttp:www.opensolaris.orgoscommunityZFSdocs . Bibliography [Callaghan (2000)] B. Callaghan, NFS Illustrated ,A d d i s o n  W e s l e y( 2 0 0 0 ) . [Cormen et al. (2009)] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms, Third Edition, MIT Press (2009). [Ghemawat et al. (2003)] S. Ghemawat, H. Gobioff, and S.T. Leung, The Google File System ,Proceedings of the ACM Symposium on Operating Systems Principles (2003). [Hartman and Ousterhout (1995)] J. H.

(2003). [Hartman and Ousterhout (1995)] J. H. Hartman and J. K. Ousterhout, The Zebra Striped Network File System ,ACM Transactions on Computer Systems , Volume 13, Number 3 (1995), pages 274310. [Hitz et al. (1995)] D. Hitz, J. Lau, and M. Malcolm, File System Design for an NFS File Server Appliance ,T e c h n i c a lr e p o r t ,N e t A p p( 1 9 9 5 ) .586 Chapter 12 FileSystem Implementation [Knuth (1998)] D. E. Knuth, The Art of Computer Programming, Volume 3: Sorting and Searching, Second

Volume 3: Sorting and Searching, Second Edition, AddisonWesley (1998). [Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developers Library (2010). [Mauerer (2008)] W. Mauerer, Professional Linux Kernel Architecture ,J o h nW i l e y and Sons (2008). [Mauro and McDougall (2007)] J. Mauro and R. McDougall, Solaris Internals: Core Kernel Architecture ,P r e n t i c eH a l l( 2 0 0 7 ) . [McKusick and NevilleNeil (2005)] M. K. McKusick and G. V . NevilleNeil, The Design and

K. McKusick and G. V . NevilleNeil, The Design and Implementation of the FreeBSD UNIX Operating System ,A d d i s o n Wesley (2005). [Norton and Wilton (1988)] P. N o r t o n a n d R . Wi l t o n , The New Peter Norton Programmers Guide to the IBM PC  PS2 ,M i c r o s o f tP r e s s( 1 9 8 8 ) . [Ousterhout (1991)] J. Ousterhout. The Role of Distributed State .In CMU Computer Science: a 25th Anniversary Commemorative ,R .F .R a s h i d ,E d . ,A d d i s o n  Wesley (1991). [Rosenblum and

d . ,A d d i s o n  Wesley (1991). [Rosenblum and Ousterhout (1991)] M. Rosenblum and J. K. Ousterhout, The Design and Implementation of a LogStructured File System ,Proceedings of the ACM Symposium on Operating Systems Principles (1991), pages 115. [Seltzer et al. (1993)] M. I. Seltzer, K. Bostic, M. K. McKusick, and C. Staelin, An Implementation of a LogStructured File System for UNIX ,USENIX Winter (1993), pages 307326. [Seltzer et al. (1995)] M. I. Seltzer, K. A. Smith, H. Balakrishnan, J.

M. I. Seltzer, K. A. Smith, H. Balakrishnan, J. Chang, S. McMains, and V . N. Padmanabhan, File System Logging Versus Clustering: AP e r f o r m a n c eC o m p a r i s o n ,USENIX Winter (1995), pages 249264. [Silvers (2000)] C. Silvers, UBC: An Efcient Unied IO and Memory Caching Subsystem for NetBSD ,USENIX Annual Technical ConferenceFREENIX Track (2000). [Solomon (1998)] D. A. Solomon, Inside Windows NT, Second Edition, Microsoft Press (1998). [Thekkath et al. (1997)] C. A. Thekkath, T. Mann,

[Thekkath et al. (1997)] C. A. Thekkath, T. Mann, and E. K. Lee, Frangipani: AS c a l a b l eD i s t r i b u t e dF i l eS y s t e m ,Symposium on Operating Systems Principles (1997), pages 224237. [Vahalia (1996)] U. Vahalia, Unix Internals: The New Frontiers ,P r e n t i c eH a l l (1996).13CHAPTER IO Systems The two main jobs of a computer are IOand processing. In many cases, the main job is IO,a n dt h ep r o c e s s i n gi sm e r e l yi n c i d e n t a l .F o ri n s t a n c e ,w h e n we

c i d e n t a l .F o ri n s t a n c e ,w h e n we browse a web page or edit a le, our immediate interest is to read or enter some information, not to compute an answer. The role of the operating system in computer IO is to manage and control IO operations and IO devices. Although related topics appear in other chapters, here we bring together the pieces to paint a complete picture ofIO.F i r s t ,w ed e s c r i b et h eb a s i c so f IOhardware, because the nature of the hardware interface

because the nature of the hardware interface places constraints on the internal facilities of the operating system. Next, we discuss the IOservices provided by the operating system and the embodiment of th ese services in the application IOinterface. Then, we explain how the operating system bridges the gap between the hardware interface and the application interface. We also discuss the UNIX System V STREAMS mechanism, which enables an application to assemble pipelines of driver code

application to assemble pipelines of driver code dynamically. Finally, we discuss the performance aspects of IO and the principles of operatingsystem design that improve IOperformance. CHAPTER OBJECTIVES To explore the structure of an operating systems IOsubsystem. To discuss the principles and complexities of IOhardware. To explain the performance aspects of IOhardware and software. 13.1 Overview The control of devices connected to the computer is a major concern of operatingsystem designers.

is a major concern of operatingsystem designers. Because IO devices vary so widely in their function and speed (consider a mouse, a hard disk, and a tape robot), varied methods are needed to control them. These methods form the IOsubsystem of the kernel, which separates the r est of the kernel from the complexities of managing IOdevices. 587588 Chapter 13 IO Systems IOdevice technology exhibits two conicting trends. On the one hand, we see increasing standardization of software and hardware

standardization of software and hardware interfaces. This trend helps us to incorporate improved device generations into existing computers and operating systems. On the other hand, we see an increasingly broad variety ofIOdevices. Some new devices are so unlike previous devices that it is a challenge to incorporate them into our computers and operating systems. This challenge is met by a combination of hardware and software techniques. The basic IO hardware elements, such as ports, buses, and

IO hardware elements, such as ports, buses, and device controllers, accommodate a wide variety of IOdevices. To encapsulate the details and oddities of different devices, the kernel of an operating system is structured to use devicedriver modules. The device drivers present a uniform device access interface to the IOsubsystem, much as system calls provide a standard interface between the application and the operating system. 13.2 IO Hardware Computers operate a great many kinds of d evices. Most

operate a great many kinds of d evices. Most t into the general categories of storage devices (disks, tapes), transmission devices (network con nections, Bluetooth), and humaninterface devi ces (screen, keyboard, mouse, audio in and out). Other devices are more specialized, such as those involved in the steering of a jet. In these aircraft, a human gives input to the ight computer via a joystick and foot pedals, and the computer sends output commands that cause motors to move rudders and aps and

that cause motors to move rudders and aps and fuels to the engines. Despite the incredible variety of IOdevices, though, we need only a few concepts to understand how the devices are attached and how the software can control the hardware. Ad e v i c ec o m m u n i c a t e sw i t hac o m p u t e rs y s t e mb ys e n d i n gs i g n a l so v e r ac a b l eo re v e nt h r o u g ht h ea i r .T h ed evice communicates with the machine via a connection point, or port for example, a serial port. If

point, or port for example, a serial port. If devices share a common set of wires, the connection is called a bus. A bus is a set of wires and a rigidly dened protocol that species a set of messages that can be sent on the wires. In terms of the electron ics, the messages are conveyed by patterns of electrical voltages applied to the wires with dened timings. When device Ahas a cable that plugs into device B,and device Bhas a cable that plugs into device C,and device Cplugs into a port on the

into device C,and device Cplugs into a port on the computer, this arrangement is called a daisy chain .Ad a i s yc h a i nu s u a l l yo p e r a t e sa sab u s . Buses are used widely in computer architecture and vary in their signaling methods, speed, throughput, and connection methods. A typical PCbus structure appears in Figure 13.1. In the gure, a PCI bus (the common PC system bus) connects the processormemory subsystem to fast devices, and anexpansion bus connects relatively slow devices,

anexpansion bus connects relatively slow devices, such as the keyboard and serial and USB ports. In the upperright portion of the gure, four disks are connected together on a Small Computer System Interface (SCSI )bus plugged into a SCSI controller. Other common buses used to interconnect main parts of ac o m p u t e ri n c l u d e PCIExpress (PCIe),w i t ht h r o u g h p u to fu pt o1 6 GBper second, and HyperTransport ,w i t ht h r o u g h p u to fu pt o2 5 GBper second. Acontroller is a

h p u to fu pt o2 5 GBper second. Acontroller is a collection of electronics that can operate a port, a bus, or a device. A serialport controller is a simple device controller. It is a single chip (or portion of a chip) in the computer that controls the signals on the13.2 IO Hardware 589 expansion busPCI busdisk disk diskdisk diskdisk disk disk SCSI controller SCSI bus cache memoryprocessor bridgememory controllermonitor IDE disk controllerexpansion bus interfacegraphics controller keyboard

bus interfacegraphics controller keyboard parallel portserial port Figure 13.1 A typical PC bus structure. wires of a serial port. By contrast, a SCSI bus controller is not simple. Because the SCSI protocol is complex, the SCSI bus controller is often implemented as as e p a r a t ec i r c u i tb o a r d( o ra host adapter )t h a tp l u g si n t ot h ec o m p u t e r .I t typically contains a processor, microcode, and some private memory to enable it to process the SCSI protocol messages. Some

it to process the SCSI protocol messages. Some devices have their own builtin controllers. If you look at a disk drive, you will see a circuit board attached to one side. This board is the disk controller. It implements the disk side of the protocol for some kind of connection SCSI orSerial Advanced Technology Attachment (SATA ),f o ri n s t a n c e .I th a sm i c r o c o d ea n dap r o c e s s o rt od om a n y tasks, such as badsector mapping, prefetching, buffering, and caching. How can the

prefetching, buffering, and caching. How can the processor give commands and data to a controller to accomplish an IOtransfer? The short answer is that the controller has one or more registers for data and control signals. The processor communicates with the controller by reading and writing bit patterns in these registers. One way in which this communication can occur is through the use of special IO instructions that specify the transfer of a byte or word to an IO port address. The

of a byte or word to an IO port address. The IOinstruction triggers bus lines to select the proper device and to move bits into or out of a device register .A l t e r n a t i v e l y ,t h ed e v i c ec o n t r o l l e r can support memorymapped IO.I nt h i sc a s e ,t h ed e v i c e  c o n t r o lr e g i s t e r s are mapped into the address space of the processor. The CPU executes IO requests using the standard datatransfer instructions to read and write the devicecontrol registers at their

and write the devicecontrol registers at their mapped locations in physical memory. Some systems use both techniques. For instance, PCsu s e IOinstructions to control some devices and memorymapped IO to control others. Figure 13.2 shows the usual IOport addresses for PCs. The graphics controller has IOports for basic control operations, but the controller has a large memory590 Chapter 13 IO SystemsIO address range (hexadecimal) 00000F 020021 040043 20020F 2F82FF 32032F 37837F 3D03DF 3F03F7

040043 20020F 2F82FF 32032F 37837F 3D03DF 3F03F7 3F83FFdevice DMA controller interrupt controller timer game controller serial port (secondary) harddisk controller parallel port graphics controller diskettedrive controller serial port (primary) Figure 13.2 Device IO port locations on PCs (partial). mapped region to hold screen contents. Th ep r o c e s ss e n d so u t p u tt ot h es c r e e n by writing data into the memorymapped region. The controller generates the screen image based on the

controller generates the screen image based on the contents of this memory. This technique is simple to use. Moreover, writing millions of bytes to the graphics memory is faster than issuing millions of IO instructions. But the ease of writing to a memorymapped IO controller is offset by a disadvantage. Because a common type of software fault is a write through an incorrect pointer to an unintended region of memory, a memorymapped device register is vulnerable to accidental modication. Of

is vulnerable to accidental modication. Of course, prote cted memory helps to reduce this risk. An IOport typically consists of four registers, called the status, control, datain, and dataout registers. The datain register is read by the host to get input. The dataout register is written by the host to send output. The status register contains bits that can be read by the host. These bits indicate states, such as whether the current command has completed, whether a byte is available to be read

completed, whether a byte is available to be read from the datain register, and whether a device error has occurred. The control register can be written by the host to start a command or to change the mode of a device. For instance, a certain bit in the control register of a serial port chooses between fullduplex and halfduplex communication, another bit enables parity checking, a third bit sets the word length to 7 or 8 bits, and other bits select one of the speeds supported by the serial port.

one of the speeds supported by the serial port. The data registers are typically 1 to 4 bytes in size. Some controllers have FIFO chips that can hold several bytes of input or output data to expand the capacity of the controller beyond the size of the data register. A FIFO chip can hold a small burst of data until the device or host is able to receive those data.13.2 IO Hardware 591 13.2.1 Polling The complete protocol for interaction between the host and a controller can be intricate, but the

host and a controller can be intricate, but the basic handshaking notion is simple. We explain handshaking with an example. Assume that 2 bits are used to coordinate the producerconsumer relationship between the controller and the host. The controller indicates its state through the busy bit in the status register. (Recall that to setab i tm e a n st ow r i t ea1i n t ot h eb i ta n dt o clear ab i tm e a n st ow r i t e a0i n t oi t . )T h ec o n t r o l l e rs e t st h e busy bit when it is

o n t r o l l e rs e t st h e busy bit when it is busy working and clears thebusy bit when it is ready to accept the next command. The host signals its wishes via the commandready bit in the command register. The host sets the commandready bit when a command is available for the controller to execute. For this example, the host writes output through a port, coordinating with the controller by handshaking as follows. 1.The host repeatedly reads the busy bit until that bit becomes clear. 2.The

the busy bit until that bit becomes clear. 2.The host sets the write bit in the command register and writes a byte into thedataout register. 3.The host sets the commandready bit. 4.When the controller notices that the commandready bit is set, it sets the busy bit. 5.The controller reads the c ommand register and sees the write command. It reads the dataout register to get the byte and does the IOto the device. 6.The controller clears the commandready bit, clears the error bit in the status

bit, clears the error bit in the status register to indicate that the device IOsucceeded, and clears the busy bit to indicate that it is nished. This loop is repeated for each byte. In step 1, the host is busywaiting orpolling :i ti si nal o o p ,r e a d i n gt h e status register over and over until the busy bit becomes clear. If the controller and device are fast, this method is a reasonable one. But if the wait may be long, the host should probably switch to another task. How, then, does the

switch to another task. How, then, does the host know when the controller has become idle? For some devices, the host must service the device quickly, or data will be lost. For instance, when data are streaming in on a serial port or from a keyboard, the small buffer on the controller will overow and data will be lost if the host waits too long before returning to read the bytes. In many computer architectures, three CPUinstruction cycles are sufcient to poll a device: read ad e v i c er e g i s

to poll a device: read ad e v i c er e g i s t e r , logicaland to extract a status bit, and branch if not zero. Clearly, the basic polling operation is efcient. But polling becomes inefcient when it is attempted repeatedly yet rarely nds a device ready for service, while other useful CPU processing remains undone. In such instances, it may be more efcient to arrange for the hardware controller to notify the CPU when the device becomes ready for service, rather than to require the CPU to poll

service, rather than to require the CPU to poll repeatedly for an IOcompletion. The hardware mechanism that enables a device to notify the CPU is called an interrupt .592 Chapter 13 IO Systems device driver initiates IO CPU receiving interrupt, transfers control to interrupt handler CPU resumes processing of interrupted taskCPU 1IO controller CPU executing checks for interrupts between instructions 5 interrupt handler processes data, returns from interruptinitiates IO 32 4 7input ready, output

interruptinitiates IO 32 4 7input ready, output complete, or error generates interrupt signal 6 Figure 13.3 Interruptdriven IO cycle. 13.2.2 Interrupts The basic interrupt mechanism works as follows. The CPU hardware has a wire called the interruptrequest line that the CPU senses after executing every instruction. When the CPU detects that a controller has asserted a signal on the interruptrequest line, the CPU performs a state save and jumps to the interrupthandler routine at a xed address in

the interrupthandler routine at a xed address in memory. The interrupt handler determines the cause of the interrupt, performs the necessary processing, performs a state restore, and executes a return from interrupt instruction to return the CPU to the execution state prior to the interrupt. We say that the device controller raises an interrupt by asserting a signal on the interrupt request line, the CPU catches the interrupt and dispatches it to the interrupt handler, and the handler clears the

the interrupt handler, and the handler clears the interrupt by servicing the device. Figure 13.3 summarizes the interruptdriven IOcycle. We stress interrupt management in this chapter because even singleuser modern systems manage hundreds of interrupts per second and servers hundreds of thousands per second. The basic interrupt mechanism just described enables the CPU to respond to an asynchronous event, as when a d evice controller becomes ready for service. In a modern operating system,

ready for service. In a modern operating system, however, we need more sophisticated interrupthandling features.13.2 IO Hardware 593 1.We need the ability to defer interrupt handling during critical processing. 2.We need an efcient way to dispatch to the proper interrupt handler for ad e v i c ew i t h o u t r s tp o l l i n ga l lt h ed e v i c e st os e ew h i c ho n er a i s e dt h e interrupt. 3.We need multilevel interrupts, so that the operating system can distin guish between high and

operating system can distin guish between high and lowpriority interrupts and can respond with the appropriate degree of urgency. In modern computer hardware, these three features are provided by the CPU and by the interruptcontroller hardware . Most CPUsh a v et w oi n t e r r u p tr e q u e s tl i n e s .O n ei st h e nonmaskable interrupt , which is reserved for events such as unrecoverable memory errors. The second interrupt line is maskable :i tc a nb et u r n e do f fb yt h e CPU before

:i tc a nb et u r n e do f fb yt h e CPU before the execution of critical instruction sequences that must not be interrupted. The maskable interrupt is used by devic ec o n t r o l l e r st or e q u e s ts e r v i c e . The interrupt mechanism accepts an address a number that selects a specic interrupthandling routine from a small set. In most architectures, this address is an offset in a table called the interrupt vector .T h i sv e c t o rc o n t a i n s the memory addresses of specialized

o n t a i n s the memory addresses of specialized interrupt handlers. The purpose of a vectored interrupt mechanism is to r educe the need for a single interrupt handler to search all possible sources of interrupts to determine which one needs service. In practice, however, computers have more devices (and, hence, interrupt handlers) than they have addr ess elements in the interrupt vector. Ac o m m o nw a yt os o l v et h i sp r o b l e mi st ou s e interrupt chaining ,i nw h i c h each element

s e interrupt chaining ,i nw h i c h each element in the interrupt vector points to the head of a list of interrupt handlers. When an interrupt is raised, the handlers on the corresponding list are called one by one, until one is found that can service the request. This structure is a compromise between the o verhead of a huge interrupt table and the inefciency of dispatching to a single interrupt handler. Figure 13.4 illustrates the design of the interrupt vector for the Intel Pentium

of the interrupt vector for the Intel Pentium processor. The events from 0 to 31, which are nonmaskable, are used to signal various error conditions. The events from 32 to 255, which are maskable, are used for purposes such as devicegenerated interrupts. The interrupt mechanism also implements a system of interrupt priority levels .T h e s el e v e l se n a b l et h e CPU to defer the handling of lowpriority interrupts without masking all interrupts and makes it possible for a high priority

and makes it possible for a high priority interrupt to preempt the execution of a lowpriority interrupt. Am o d e r no p e r a t i n gs y s t e mi n t e r a c t sw i t ht h ei n t e r r u p tm e c h a n i s mi n several ways. At boot time, the operating system probes the hardware buses to determine what devices are present and installs the corresponding interrupt handlers into the interrupt vector. During IO,t h ev a r i o u sd e v i c ec o n t r o l l e r s raise interrupts when they are ready

r o l l e r s raise interrupts when they are ready for service. These interrupts signify that output has completed, or that input data are available, or that a failure has been detected. The interrupt mechanism is also used to handle a wide variety of exceptions , such as dividing by 0, accessing a protected or nonexistent memory address, or attempting to execute a privileged instruction from user mode. The events that trigger interrupts have a co mmon property: they are occurrences that induce

co mmon property: they are occurrences that induce the operating system to execute an urgent, selfcontained routine.594 Chapter 13 IO Systemsdescription vector number 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1931 32255divide error debug exception null interrupt breakpoint INTOdetected overflow bound range exception invalid opcode device not available double fault coprocessor segment overrun (reserved) invalid task state segment segment not present stack fault general protection page fault

present stack fault general protection page fault (Intel reserved, do not use) floatingpoint error alignment check machine check (Intel reserved, do not use) maskable interrupts Figure 13.4 Intel Pentium processor eventvector table. An operating system has other good uses for an efcient hardware and software mechanism that saves a small amount of processor state and then calls a privileged routine in the kernel. For example, many operating systems use the interrupt mechanism for virtual memory

use the interrupt mechanism for virtual memory paging. A page fault is an exception that raises an interrupt. The interrupt suspends the current process and jumps to the pagefault handler in the kernel. This handler saves the state of the process, moves the process to the wait queue, performs pagecache management, schedules an IOoperation to fetch the page, schedules another process to resume execution, and then returns from the interrupt. Another example is found in the implementation of system

example is found in the implementation of system calls. Usually, ap r o g r a mu s e sl i b r a r yc a l l st oi s s u es y s t e mc a l l s .T h el i b r a r yr o u t i n e sc h e c k the arguments given by the application, build a data structure to convey the arguments to the kernel, and then execute a special instruction called a software interrupt ,o r trap.T h i si n s t r u c t i o nh a sa no p e r a n dt h a ti d e n t i  e s the desired kernel service. When a proce ss executes the trap

kernel service. When a proce ss executes the trap instruction, the interrupt hardware saves the state of the user code, switches to kernel mode, and dispatches to the kernel routine that imple ments the requested service. The trap is given a relatively low interrupt priority compared with those assigned to device interruptsexecuting a system call on behalf of an application is less urgent than servicing a device controller before its FIFO queue overows and loses data. Interrupts can also be used

and loses data. Interrupts can also be used to manage the ow of control within the kernel. For example, consider one example of the processing required to complete13.2 IO Hardware 595 ad i s kr e a d .O n es t e pi st oc o p yd a t af r o mk e r n e ls p a c et ot h eu s e rb u f f e r . This copying is time consuming but not urgentit should not block other highpriority interrupt handling. Another step is to start the next pending IO for that disk drive. This step has higher priority. If the

disk drive. This step has higher priority. If the disks are to be used efciently, we need to start the next IOas soon as the previous one completes. Consequently, a pair of interrupt handlers implements the kernel code that completes a disk read. The highpriority handler records the IOstatus, clears the device interrupt, starts the next pending IO,a n dr a i s e sal o w  p r i o r i t y interrupt to complete the work. Later, when the CPU is not occupied with high priority work, the lowpriority

occupied with high priority work, the lowpriority interrupt will be dispatched. The corresponding handler completes the userlevel IOby copying data from kernel buffers to the application space and then calling the scheduler to place the application on the ready queue. At h r e a d e dk e r n e la r c h i t e c t u r ei sw e l ls u i t e dt oi m p l e m e n tm u l t i p l e interrupt priorities and to enforce the precedence of interrupt handling over background processing in kernel and

handling over background processing in kernel and application routines. We illustrate this point with the Solaris kernel. In Solaris, interrupt handlers are executed as kernel threads. A range of high priorities is reserved for these threads. These priorities give interrupt hand lers precedence over application code and kernel housekeeping and implement the priority relationships among interrupt handlers. The priorities cause the Solaris thread scheduler to preempt low priority interrupt

thread scheduler to preempt low priority interrupt handlers in favor of higherpriority ones, and the threaded implementation enables multiprocessor hardware to run several interrupt handlers concurrently. We describe the interrupt architecture of Windows XP and UNIX in Chapter 19 and Appendix A, respectively. In summary, interrupts are used throughout modern operating systems to handle asynchronous events and to trap to supervisormode routines in the kernel. To enable the most urgent work to be

the kernel. To enable the most urgent work to be done rst, modern computers use a system of interrupt priorities. Device controllers, hardware faults, and system calls all raise interrupts to trigger kernel routines. Because interrupts are used so heavily for timesensitive processing, efcient interrupt handling is required for good system performance. 13.2.3 Direct Memory Access For a device that does large transfers, such as a disk drive, it seems wasteful to use an expensive generalpurpose

seems wasteful to use an expensive generalpurpose processor to watch status bits and to feed data into a controller register one byte at a timea process termed programmed IO(PIO). Many computers avoid burdening the main CPU with PIOby ofoading some of this work to a specialpurpose processor called a directmemoryaccess (DMA )controller. To initiate a DMA transfer, the host writes a DMA command block into memory. This block contains a pointer to the source of a transfer, a pointer to the

to the source of a transfer, a pointer to the destination of the transfer, and a count of the number of bytes to be transferred. The CPU writes the address of this command block to the DMA controller, then goes on with other work. The DMA controller proceeds to operate the memory bus directly, placing addresses on the bus to perform transfers without the help of the main CPU.As i m p l e DMA controller is a standard component in all modern computers, from smartphones to mainframes.596 Chapter 13

from smartphones to mainframes.596 Chapter 13 IO Systems Handshaking between the DMA controller and the device controller is performed via a pair of wires called DMA request and DMA acknowledge . The device controller places a signal on the DMA request wire when a word of data is available for transfer. This signal causes the DMA controller to seize the memory bus, place the desired address on the memoryaddress wires, and place a signal on the DMA acknowledge wire. When the device controller

DMA acknowledge wire. When the device controller receives the DMA acknowledge signal, it transfers the word of data to memory and removes the DMA request signal. When the entire transfer is nished, the DMA controller interrupts the CPU. This process is depicted in Figure 13.5. When the DMA controller seizes the memory bus, the CPU is momentarily prevented from accessing main memory, although it can still access data items in its primary and secondary caches. Although this cycle stealing can slow

caches. Although this cycle stealing can slow down the CPU computation, ofoading the datatransfer work to a DMA controller generally improves the total system performance. Some computer architectures use physical memory addresses for DMA ,b u to t h e r sp e r f o r m direct virtual memory access (DVMA ), using virtual addresses that undergo translation to physical addresses. DVMA can perform a transfer between two memorymapped d evices without the intervention of the CPU or the use of main

the intervention of the CPU or the use of main memory. On protectedmode kernels, the operating system generally prevents processes from issuing device commands directly. This discipline protects data from accesscontrol violations and also protects the system from erroneous use of device controllers that could cause a system crash. Instead, the operating system exports functions that a sufciently privileged process can use to access lowlevel operations on the underlyin gh a r d w a r e .O nk e r

on the underlyin gh a r d w a r e .O nk e r n e l sw i t h o u t memory protection, processes can access device controllers directly. This direct access can be used to achieve high performance, since it can avoid kernel communication, context switches, and layers of kernel software. Unfortunately, IDE disk controllerx DMAbus interrupt controllerbufferx memory CPU memory bus PCI buscacheCPU 5. DMA controller transfers bytes to buffer X, increasing memory address and decreasing C until C H11005 0

memory address and decreasing C until C H11005 0 1. device driver is told to transfer disk data to buffer at address X 2. device driver tells disk controller to transfer C bytes from disk to buffer at address X 6. when C H11005 0, DMA interrupts CPU to signal transfer completion 3. disk controller initiates DMA transfer 4. disk controller sends each byte to DMA controllerdisk diskdisk disk Figure 13.5 Steps in a DMA transfer.13.3 Application IO Interface 597 it interferes with system security

Interface 597 it interferes with system security and stability. The trend in generalpurpose operating systems is to protect memory and devices so that the system can try to guard against erroneous or malicious applications. 13.2.4 IO Hardware Summary Although the hardware aspects of IOare complex when considered at the level of detail of electronicshardwar ed e s i g n ,t h ec o n c e p t st h a tw eh a v e just described are sufcient to enable us to understand many IO features of operating

us to understand many IO features of operating systems. Lets review the main concepts: Ab u s Ac o n t r o l l e r An IOport and its registers The handshaking relationship between the host and a device controller The execution of this handshaking in a polling loop or via interrupts The ofoading of this work to a DMA controller for large transfers We gave a basic example of the handshaking that takes place between a device controller and the host earlier in this section. In reality, the wide

host earlier in this section. In reality, the wide variety of available devices poses a problem for operatingsystem implementers. Each kind of device has its own set of capabilities, controlbit denitions, and protocols for interacting with the hostand they are all different. How can the operating system be designed so that we can attach new devices to the computer without rewriting the operating system? And when the devices vary so widely, how can the operating system give a convenient, uniform

the operating system give a convenient, uniform IO interface to applications? We address those questions next. 13.3 Application IO Interface In this section, we discuss structuring techniques and interfaces for the operating system that enable IOdevices to be treated in a standard, uniform way. We explain, for instance, how an application can open a le on a disk without knowing what kind of disk it is and how new disks and other devices can be added to a computer without disruption of the

be added to a computer without disruption of the operating system. Like other complex softwareengineerin gp r o b l e m s ,t h ea p p r o a c hh e r e involves abstraction, encapsulation, and software layering. Specically, we can abstract away the detailed differences in IOdevices by identifying a few general kinds. Each general kind is accessed through a standardized set of functionsan interface .T h ed i f f e r e n c e sa r ee n c a p s u l a t e di nk e r n e lm o d u l e s called device

l a t e di nk e r n e lm o d u l e s called device drivers that internally are customtailored to specic devices but that export one of the standard interfaces. Figure 13.6 illustrates how the IOrelated portions of the kernel are structured in software layers. The purpose of the devicedriver layer is to hide the differences among device controllers from the IO subsystem of the kernel, much as the IO system calls encapsulate the behavior of devices in a few generic classes that hide hardware

in a few generic classes that hide hardware differences from applications. Making the IOsubsystem598 Chapter 13 IO Systems kernelhardware softwareSCSI device driverkeyboard device drivermouse device driver        PCI bus device driverfloppy device driverATAPI device driver SCSI device controllerkeyboard device controllermouse device controllerPCI bus device controllerfloppy device controllerATAPI device controller SCSI deviceskeyboard mouse PCI busfloppy disk drivesATAPI devices (disks, tapes,

busfloppy disk drivesATAPI devices (disks, tapes, drives)kernel IO subsystem Figure 13.6 Ak e r n e lI  Os t r u c t u r e . independent of the hardware simplies the job of the operatingsystem developer. It also benets the hardware manufacturers. They either design new devices to be compatible with an existing host controller interface (such asSATA ), or they write device drivers to interface the new hardware to popular operating systems. Thus, we can attach new peripherals to a computer without

can attach new peripherals to a computer without waiting for the operatingsystem vendor to develop support code. Unfortunately for devicehardware manufacturers, each type of operating system has its own standards for the devicedriver interface. A given device may ship with multiple device driversfor instance, drivers for Windows, Linux, AIX,a n dM a c OS X .D e v i c e sv a r yo nm a n yd i m e n s i o n s ,a si l l u s t r a t e di n Figure 13.7. Characterstream or block .Ac h a r a c t e r  s

Characterstream or block .Ac h a r a c t e r  s t r e a md e v i c et r a n s f e r sb y t e so n e by one, whereas a block device transfers a block of bytes as a unit. Sequential or random access .As e q u e n t i a ld e v i c et r a n s f e r sd a t ai na x e d order determined by the device, whereas the user of a randomaccess device can instruct the device to seek to any of the available data storage locations. Synchronous or asynchronous . A synchronous device performs data transfers with

A synchronous device performs data transfers with predictable response times, in coordination with other aspects of the system. An asynchronous device exhibits irregular or unpredictable response times not coor dinated with other computer events. Sharable or dedicated .As h a r a b l ed e v i c ec a nb eu s e dc o n c u r r e n t l yb y several processes or threads; a dedicated device cannot.13.3 Application IO Interface 599aspect variation example terminal disk modem CDROM tape keyboard tape

terminal disk modem CDROM tape keyboard tape keyboard CDROM graphics controller diskdatatransfer mode access method transfer schedule sharing IO directioncharacter block sequential random synchronous asynchronous dedicated sharable read only write only readwritelatency seek time transfer rate delay between operationsdevice speed Figure 13.7 Characteristics of IO devices. Speed of operation .D e v i c es p e e d sr a n g ef r o maf e wb y t e sp e rs e c o n dt o af e wg i g a b y t e sp e rs e c

rs e c o n dt o af e wg i g a b y t e sp e rs e c o n d . Readwrite, read only, or write only .S o m ed e v i c e sp e r f o r mb o t hi n p u t and output, but others support only one data transfer direction. For the purpose of application access, many of these differences are hidden by the operating system, and the devices are grouped into a few conventional types. The resulting styles of device access have been found to be useful and broadly applicable. Although the exact system calls may

applicable. Although the exact system calls may differ across operating systems, the device categories are fairly standard. The major access conventions include block IO,c h a r a c t e r  s t r e a m IO,m e m o r y  m a p p e d l e access, and network sockets. Operating systems also provide special system calls to access a few additional devices, such as a timeofday clock and a timer. Some operating systems provide a set of system calls for graphical display, video, and audio devices. Most

graphical display, video, and audio devices. Most operating systems also have an escape (orback door )t h a tt r a n s p a r  ently passes arbitrary commands from an application to a device driver. In UNIX ,t h i ss y s t e mc a l li s ioctl() (for IOcontrol ). The ioctl() system call enables an application to access any functionality that can be implemented by any device driver, without the need to invent a new system call. The ioctl() system call has three arguments. The rst is a le descriptor

has three arguments. The rst is a le descriptor that connects the application to the driver by referring to a hardware device managed by that driver. The second is an integer that se lects one of the commands implemented in the driver. The third is a pointer to an arbitrary data structure in memory that enables the application and driver to communicate any necessary control information or data.600 Chapter 13 IO Systems 13.3.1 Block and Character Devices Theblockdevice interface captures all the

Devices Theblockdevice interface captures all the aspects necessary for accessing disk drives and other blockoriented devices. T he device is expected to understand commands such as read() and write() .I fi ti sar a n d o m  a c c e s sd e v i c e , it is also expected to have a seek() command to specify which block to transfer next. Applications normally access such a device through a lesystem interface. We can see that read() ,write() ,a n d seek() capture the essential behaviors of

,a n d seek() capture the essential behaviors of blockstorage devices, so that applications are insulated from the lowlevel differences among those devices. The operating system itself, as well as special applications such as database management systems, may prefer to access a block device as a simple linear array of blocks. This mode of access is sometimes called raw IO.I ft h e application performs its own buffering, then using a le system would cause extra, unneeded buffering. Likewise, if an

cause extra, unneeded buffering. Likewise, if an application provides its own locking of le blocks or regions, then any operatingsystem locking services would be redundant at the least and contradictory at the worst. To avoid these conicts, rawdevice access passes control of the device directly to the application, letting the operating system step out of the way. Unfortunately, no operatingsystem services are then performed on this device. A compromise that is becoming common is for the

A compromise that is becoming common is for the operating system to allow a mode of operation on a le that disables buffering and locking. In the UNIX world, this is called direct IO. Memorymapped le access can be layered on top of blockdevice drivers. Rather than offering read and write operations, a memorymapped interface provides access to disk storage via an array of bytes in main memory. The system call that maps a le into memory returns the virtual memory address that contains a copy of

the virtual memory address that contains a copy of the le. The actual data transfers are performed only when needed to satisfy access to th e memory image. Because the transfers are handled by the same mechanism as that used for demandpaged virtual memory access, memorymapped IO is efcient. Memory mapping is also convenient for programmersaccess to a memorymapped le is as simple as reading from and writing to memory. Operating systems that offer virtual memory commonly use the mapping interface

virtual memory commonly use the mapping interface for kernel services. For instance, to execute a program, the operating system maps the executable into memory and then transfers control to the entry address of the executable. The mapping interface is also commonly used for kernel access to swap space on disk. A keyboard is an example of a device that is accessed through a character stream interface .T h eb a s i cs y s t e mc a l l si nt h i si n t e r f a c ee n a b l ea na p p l i c a t i o n

n t e r f a c ee n a b l ea na p p l i c a t i o n toget() orput() one character. On top of this interface, libraries can be built that offer lineatatime access, with buffering and editing services (for example, when a user types a backspace, th ep r e c e d i n gc h a r a c t e ri sr e m o v e d from the input stream). This style of access is convenient for input devices such as keyboards, mice, and modems that produce data for input spontaneously  that is, at times that cannot necessarily be

that is, at times that cannot necessarily be predicted by the application. This access style is also good for output devices such as printers and audio boards, which naturally t the concept of a linear stream of bytes. 13.3.2 Network Devices Because the performance and addressing characteristics of network IOdiffer signicantly from those of disk IO,m o s to p e r a t i n gs y s t e m sp r o v i d ean e t w o r k13.3 Application IO Interface 601 IOinterface that is different from the read()

601 IOinterface that is different from the read() write() seek() interface used for disks. One interface available in many operating systems, including UNIX and Windows, is the network socket interface. Think of a wall socket for electricity: any electrical appliance can be plugged in. By analogy, the system calls in the socket interface enable an application to create a socket, to connect a local socket to a remote address (which plugs this application into a socket created by another

this application into a socket created by another application), to listen for any remote application to plug into the local socket, and to send and receive packets over the connection. T o support the implementation of servers, the socket interface also provides a function called select() that manages a set of sockets. A call to select() returns information about which sockets have a packet waiting to be received and which sockets have room to accept a packet to be sent. The use of select()

to accept a packet to be sent. The use of select() eliminates the polling and busy waiting that would otherwise be necessary for network IO.T h e s ef u n c t i o n se n c a p s u l a t et h e essential behaviors of networks, greatly facilitating the creation of distributed applications that can use any underlying network hardware and protocol stack. Many other approaches to interprocess communication and network communication have been implemented. For in stance, Windows provides one interface

For in stance, Windows provides one interface to the network interface car da n das e c o n di n t e r f a c et ot h en e t w o r k protocols. In UNIX ,w h i c hh a sal o n gh i s t o r ya sap r o v i n gg r o u n df o rn e t w o r k technology, we nd halfduplex pipes, fullduplex FIFO s, fullduplex STREAMS , message queues, and sockets. Information on UNIX networking is given in Section A.9. 13.3.3 Clocks and Timers Most computers have hardware clocks and timers that provide three basic

clocks and timers that provide three basic functions: Give the current time. Give the elapsed time. Set a timer to trigger operation Xat time T. These functions are used heavily by the operating system, as well as by time sensitive applications. Unfortunately, the system calls that implement these functions are not standardized across operating systems. The hardware to measure elapsed time and to trigger operations is called aprogrammable interval timer .I tc a nb es e tt ow a i tac e r t a i na

timer .I tc a nb es e tt ow a i tac e r t a i na m o u n to ft i m e and then generate an interrupt, and it can be set to do this once or to repeat the process to generate periodic interrupts. The scheduler uses this mechanism to generate an interrupt that will preempt a process at the end of its time slice. The disk IOsubsystem uses it to invoke the periodic ushing of dirty cache buffers to disk, and the network subsystem uses it to cancel operations that are proceeding too slowly because of

that are proceeding too slowly because of network congestion or failures. The operating system may also provide an interface for user processes to use timers. The operating system can support more timer requests than the number of timer hardware channels by simulating virtual clocks. To do so, the kernel (or the timer device driver) maintains a list of in terrupts wanted by its own routines and by user requests, sorted in earliesttimerst order. It sets the timer for the602 Chapter 13 IO Systems

It sets the timer for the602 Chapter 13 IO Systems earliest time. When the timer interrupts, the kernel signals the requester and reloads the timer with the next earliest time. On many computers, the interrupt rate generated by the hardware clock is between 18 and 60 ticks per second. This resolution is coarse, since a modern computer can execute hundreds of millions of instructions per second. The precision of triggers is limited by the coarse resolution of the timer, together with the overhead

of the timer, together with the overhead of maintaining virtual clocks. Furthermore, if the timer ticks are used to maintain the system timeofday clock, the system clock can drift. In most computers, the hardware clock is constructed from a high frequency counter. In some computers, the value of this counter can be read from a device register, in which case the counter can be considered a high resolution clock. Although this clock does not generate interrupts, it offers accurate measurements of

interrupts, it offers accurate measurements of time intervals. 13.3.4 Nonblocking and Asynchronous IO Another aspect of the systemcall interface relates to the choice between blocking IO and nonblocking IO.W h e na na p p l i c a t i o ni s s u e sa blocking system call, the execution of the application is suspended. The application is moved from the operating systems run queue to a wait queue. After the system call completes, the application is moved back to the run queue, where it is eligible

moved back to the run queue, where it is eligible to resume execution. When it resumes execution, it will receive the values returned by the system call. The physical actions performed by IOdevices are generally asynchronousthey take a varying or unpredictable amount of time. Nevertheless, most operating systems use blocking system calls for the application interface, because blocking application code is easier to understand than nonblocking application code. Some userlevel processes need

application code. Some userlevel processes need nonblocking IO. One example is a user interface that receives keyboard and mouse input while processing and displaying data on the screen. Anoth er example is a video application that reads frames from a le on disk while simultaneously decompressing and displaying the output on the display. One way an application writer can overlap execution with IOis to write am u l t i t h r e a d e da p p l i c a t i o n .S o m et h r e a d sc a np e r f o r mb

a t i o n .S o m et h r e a d sc a np e r f o r mb l o c k i n gs y s t e mc a l l s , while others continue executing. Some operating systems provide nonblocking IO system calls. A nonblocking call does not halt the execution of the application for an extended time. Instead, it returns quickly, with a return value that indicates how many bytes were transferred. An alternative to a nonblocking system call is an asynchronous system call. An asynchronous call returns immediately, without waiting

call returns immediately, without waiting for the IOto complete. The application continues to execute its code. The completion of the IOat some future time is communicated to the application, either through the setting of some variable in the address space of the application or through the triggering of a signal or software interrupt or a callback routine that is executed outside the linear control ow of the application. The difference between nonblocking and asynchronous system calls is that a

and asynchronous system calls is that a nonblocking read() returns immediately with whatever data are availablethe full number of bytes requested, fewer, or none at all. An asynchronous read() call requests a transfer that will be performed in its entirety but will complete at some future time. These two IOmethods are shown in Figure 13.8.13.3 Application IO Interface 603 requesting process waiting hardware data transferhardware data transferdevice driver device driver interrupt

data transferdevice driver device driver interrupt handlerrequesting process kernel user (a) (b)time timeuser kernel interrupt handler Figure 13.8 Two IO methods: (a) synchronous and (b) asynchronous. Asynchronous activities occur throughout modern operating systems. Frequently, they are not exposed to users or applications but rather are contained within the operatingsystem operation. Disk and network IOare useful examples. By default, when an application issues a network send request or a disk

issues a network send request or a disk write request, the operating system notes the request, buffers the IO,a n dr e t u r n st ot h ea p p l i c a t i o n .W h e np o s s i b l e ,t oo p t i m i z eo v e r a l l system performance, the operating system completes the request. If a system failure occurs in the interim, the application will lose any inight requests. Therefore, operating systems usually put a limit on how long they will buffer ar e q u e s t .S o m ev e r s i o n so f UNIX ush

ar e q u e s t .S o m ev e r s i o n so f UNIX ush their disk buffers every 30 seconds, for example, or each request is ushed within 30 seconds of its occurrence. Data consistency within applications is maintained by the kernel, which reads data from its buffers before issuing IOrequests to devices, assuring that data not yet written are nevertheless returned to a requesting reader. Note that multiple threads performing IO to the same le might not receive consistent data, depending on how the

not receive consistent data, depending on how the kernel implements its IO.I nt h i ss i t u a t i o n ,t h et h r e a d s may need to use locking protocols. Some IOrequests need to be performed immediately, so IOsystem calls usually have a way to indicate that a given request, or IOto a specic device, should be performed synchronously. Ag o o de x a m p l eo fn o n b l o c k i n gb e h a v i o ri st h e select() system call for network sockets. This system call takes an argument that species a

This system call takes an argument that species a maximum waiting time. By setting it to 0, an application can poll for network activity without blocking. But using select() introduces extra overhead, because theselect() call only checks whether IO is possible. For a data transfer, select() must be followed by some kind of read() orwrite() command. Av a r i a t i o no nt h i sa p p r o a c h ,f o u n di nM a c h ,i sab l o c k i n gm u l t i p l e  r e a dc a l l . It species desired reads for

l e  r e a dc a l l . It species desired reads for several devices in one system call and returns as soon as any one of them completes. 13.3.5 Vectored IO Some operating systems provide another major variation of IO via their applications interfaces. vectored IOallows one system call to perform multiple IOoperations involving multiple locations. For example, the UNIX readv604 Chapter 13 IO Systems system call accepts a vector of multiple buffers and either reads from a source to that vector or

and either reads from a source to that vector or writes from that vector to a destination. The same transfer could be caused by several individual invocations of system calls, but this scatter gather method is useful for a variety of reasons. Multiple separate buffers can have their contents transferred via one system call, avoiding contextswitching and systemcall overhead. Without vectored IO,t h ed a t am i g h t r s tn e e dt ob et r a n s f e r r e dt oal a r g e rb u f f e ri n the right

f e r r e dt oal a r g e rb u f f e ri n the right order and then transmitted, which is inefcient. In addition, some versions of scattergather provide atomicity, assuring that all the IOis done without interruption (and avoiding corruption of data if other threads are also performing IOinvolving those buffers). When possible, programmers make use of scattergather IOfeatures to increase throughput and decrease system overhead. 13.4 Kernel IO Subsystem Kernels provide many services related to IO.S

Kernels provide many services related to IO.S e v e r a ls e r v i c e s  s c h e d u l i n g , buffering, caching, spooling, device reservation, and error handlingare provided by the kernels IOsubsystem and build on the hardware and device driver infrastructure. The IOsubsystem is also responsible for protecting itself from errant processes and malicious users. 13.4.1 IO Scheduling To schedule a set of IOrequests means to determine a good order in which to execute them. The order in which

order in which to execute them. The order in which applications issue system calls rarely is the best choice. Scheduling can improve overall system performance, can share device access fairly among processes, and can reduce the average waiting time forIOto complete. Here is a simple example to illustrate. Suppose that a disk arm is near the beginning of a disk and that three applications issue blocking read calls to that disk. Application 1 requests a block near the end of the disk, application

a block near the end of the disk, application 2 requests one near the beginning, and application 3 requests one in the middle of the disk. The operating system can reduce the distance that the disk arm travels by serving the applications in the order 2, 3, 1. Rearranging the order of service in this way is the essence of IOscheduling. Operatingsystem developers implement scheduling by maintaining a wait queue of requests for each device. When an application issues a blocking IO system call, the

application issues a blocking IO system call, the request is placed on the queue for that device. The IOscheduler rearranges the order of the queue to improve the overall system efciency and the average response time experien ced by applications. The operating system may also try to be fair, so that no one application receives especially poor service, or it may give priority service for delaysensitive requests. For instance, requests from the virtual memory subsystem may take priority over

virtual memory subsystem may take priority over application requests. Several scheduling algorithms for disk IOare detailed in Section 10.4. When a kernel supports asynchronous IO,i tm u s tb ea b l et ok e e pt r a c k of many IOrequests at the same time. For this purpose, the operating system might attach the wait queue to a devicestatus table .T h ek e r n e lm a n a g e st h i s table, which contains an entry for each IOdevice, as shown in Figure 13.9.13.4 Kernel IO Subsystem 605 device:

Figure 13.9.13.4 Kernel IO Subsystem 605 device: keyboard status: idle device: laser printer status: busy device: mouse status: idle device: disk unit 1 status: idle device: disk unit 2 status: busy ...request for laser printer address: 38546 length: 1372 request for disk unit 2 file: xxx operation: read address: 43046 length: 20000request for disk unit 2 file: yyy operation: write address: 03458 length: 500Figure 13.9 Devicestatus table. Each table entry indicates the devices type, address, and

entry indicates the devices type, address, and state (not functioning, idle, or busy). If the device is busy with a request, the type of request and other parameters will be stored in the table entry for that device. Scheduling IOoperations is one way in which the IOsubsystem improves the efciency of the computer. Another way is by using storage space in main memory or on disk via buffering, caching, and spooling. 13.4.2 Buffering Abuffer , of course, is a memory area that stores data being

of course, is a memory area that stores data being transferred between two devices or between a device and an a pplication. Buffering is done for three reasons. One reason is to cope with a speed mismatch between the producer and consumer of a data stream. Suppose, for example, that a le is being received via modem for storage on the hard disk. The modem is about a thousand times slower than the hard disk. So a buffer is created in main memory to accumulate the bytes received from the modem.

to accumulate the bytes received from the modem. When an entire buffer of data has arrived, the buffer can be written to disk in a single operation. Since the disk write is not instantaneous and the modem still needs a place to store additional incoming data, two buffers ar e used. After the modem lls the rst buffer, the disk write is requested. The modem then starts to ll the second buffer while the rst buffer is written to disk. By the time the modem has lled the second buffer, the disk write

modem has lled the second buffer, the disk write from the rst one should have completed, so the modem can switch back to the rst buffer while the disk writes the second one. This double buffering decouples the producer of data from the consumer, thus relaxing timing requiremen ts between them. The need for this decoupling is illustrated in Figure 13.10, which lists the enormous differences in device speeds for typical computer hardware. As e c o n du s eo fb u f f e r i n gi st op r o v i d ea d

n du s eo fb u f f e r i n gi st op r o v i d ea d a p t a t i o n sf o rd e v i c e st h a t have different datatransfer sizes. Such disparities are especially common in computer networking, where buffers are used widely for fragmentation and reassembly of messages. At the sending side, a large message is fragmented606 Chapter 13 IO Systems 0.1 10 0.001 10E6 000001 0001 10000.0modem mouse keyboardhard diskFireWireSCSI busGigabit EthernetSerial A T A (SA T A300)Infiniband (QDR 12X)PCI Express

A T A (SA T A300)Infiniband (QDR 12X)PCI Express 2.0 ( 32)HyperT ransport (32pair)system bus Figure 13.10 Sun Enterprise 6000 devicetransfer rates (logarithmic). into small network packets. The packets are sent over the network, and the receiving side places them in a reassembly buffer to form an image of the source data. A third use of buffering is to support copy semantics for application IO. An example will clarify the meaning of copy semantics. Suppose that an application has a buffer of

Suppose that an application has a buffer of data that it wishes to write to disk. It calls the write() system call, providing a pointer to the buffer and an integer specifying the number of bytes to write. After the system call returns, what happens if the application changes the contents of the buffer? With copy semantics ,t h e version of the data written to disk is guaranteed to be the version at the time of the application system call, independent of any subsequent changes in the

call, independent of any subsequent changes in the applications buffer. A simple way in which the operating system can guarantee copy semantics is for the write() system call to copy the application data into a kernel buffer before returning control to the application. The disk write is performed from the kernel buffer, so that subsequent changes to the application buffer have no effect. Copyin go fd a t ab e t w e e nk e r n e lb u f f e r sa n d application data space is common in operating

n d application data space is common in operating systems, despite the overhead that this operation introduces, because of the clean semantics. The same effect can be obtained more efciently by clever use of virtual memory mapping and copyonwrite page protection. 13.4.3 Caching Acache is a region of fast memory that holds copies of data. Access to the cached copy is more efcient than access to the original. For instance, the instructions13.4 Kernel IO Subsystem 607 of the currently running

Kernel IO Subsystem 607 of the currently running process are stored on disk, cached in physical memory, and copied again in the CPUs secondary and primary caches. The difference between a buffer and a cache is that a buffer may hold the only existing copy of a data item, whereas a cache, by denition, holds a copy on faster storage of an item that resides elsewhere. Caching and buffering are distinct functions, but sometimes a region of memory can be used for both purposes. For instance, to

can be used for both purposes. For instance, to preserve copy semantics and to enable efcient scheduling of disk IO,t h eo p e r a t i n gs y s t e m uses buffers in main memory to hold disk data. These buffers are also used as ac a c h e ,t oi m p r o v et h e IOefciency for les that are shared by applications or that are being written and reread rapidly. When the kernel receives a le IOrequest, the kernel rst accesses the buf fer cache to see whether that region of the le is already available

whether that region of the le is already available in main memory. If it is, a physical disk IO can be avoided or deferred. Also, disk writes are accumulated in the buffer cache for several seconds, so that large transfers are gathered to allow efcient write schedules. This strategy of delaying writes to improve IOefciency is discussed, in the context of remote le access, in Section 17.9.2. 13.4.4 Spooling and Device Reservation Aspool is a buffer that holds output for a device, such as a

a buffer that holds output for a device, such as a printer, that cannot accept interleaved data streams. Although a printer can serve only one job at a time, several applications may wish to prin tt h e i ro u t p u tc o n c u r r e n t l y , without having their output mixed together. The operating system solves this problem by intercepting all output to the printer. Each applications output is spooled to a separate disk le. When an application nishes printing, the spooling system queues the

nishes printing, the spooling system queues the corresponding spool le for output to the printer. The spooling system copies the queued spool les to the printer one at a time. In some operating systems, spooling is managed by a system daemon process. In others, it is handled by an inkernel thread. In either case, the operating system provides a control interface that enables users and system administrators to display the queue, remove unwanted jobs b efore those jobs print, suspend printing

jobs b efore those jobs print, suspend printing while the printer is serviced, and so on. Some devices, such as tape drives and printers, cannot usefully multiplex the IO requests of multiple concurrent applications. Spooling is one way operating systems can coordinate concurrent output. Another way to deal with concurrent device access is to provide explicit facilities for coordination. Some operating systems (including VMS )p r o v i d es u p p o r tf o re x c l u s i v ed e v i c ea c c e s s

o r tf o re x c l u s i v ed e v i c ea c c e s s by enabling a process to allocate an idle device and to deallocate that device when it is no longer needed. Other operating systems enforce a limit of one open le handle to such a device. Many operating systems provide functions that enable processes to coordinat ee x c l u s i v ea c c e s sa m o n gt h e m s e l v e s .F o r instance, Windows provides system calls to wait until a device object becomes available. It also has a parameter to the

becomes available. It also has a parameter to the OpenFile() system call that declares the types of access to be permitted to other concurrent threads. On these systems, it is up to the applications to avoid deadlock. 13.4.5 Error Handling An operating system that uses protected memory can guard against many kinds of hardware and application errors, so that a complete system failure is608 Chapter 13 IO Systems not the usual result of each minor mechanical malfunction. Devices and IO transfers

mechanical malfunction. Devices and IO transfers can fail in many ways, either for transient reasons, as when a network becomes overloaded, or for permanent reasons, as when a disk controller becomes defective. Operating systems can often compensate effectively for transient failures. For instance, a disk read() failure results in a read() retry, and a network send() error results in a resend() ,i ft h ep r o t o c o ls os p e c i  e s . Unfortunately, if an important component experiences a

if an important component experiences a permanent failure, the operating system is unlikely to recover. As a general rule, an IOsystem call will return one bit of information about the status of the call, signifying either success or failure. In the UNIX operating system, an additional integer variable named errno is used to return an error codeone of about a hundred valuesindicating the general nature of the failure (for example, argument out of range, bad pointer, or le not open). By contrast,

range, bad pointer, or le not open). By contrast, some hardware can provide highly detailed error information, although many current operating systems are not designed to convey this information to the application. For instance, a failure of a SCSI device is reported by the SCSI protocol in three levels of detail: a sense key that identies the general nature of the failure, such as a hardware error or an illegal request; an additional sense code that states the category of failure, such as a bad

that states the category of failure, such as a bad command parameter or a selftest failure; and an additional sensecode qualier that gives even more detail, such as which command parameter was in error or which hardware subsystem failed its selftest. Further, many SCSI devices maintain internal pages of error log information that can be requested by the hostbut seldom are. 13.4.6 IO Protection Errors are closely related to the issue of protection. A user process may accidentally or purposely

A user process may accidentally or purposely attempt to disrupt the normal operation of a system by attempting to issue illegal IOinstructions. We can use various mechanisms to ensure that such disruptions cannot take place in the system. To prevent users from performing illegal IO,w ed e  n ea l l IOinstructions to be privileged instructions. Thus, users cannot issue IOinstructions directly; they must do it through the operating system. To do IO,au s e rp r o g r a m executes a system call to

IO,au s e rp r o g r a m executes a system call to request that the operating system perform IOon its behalf (Figure 13.11). The operating system, executing in monitor mode, checks that the request is valid and, if it is, does the IOrequested. The operating system then returns to the user. In addition, any memorymapped and IOport memory locations must be protected from user access by the memoryprotection system. Note that a kernel cannot simply deny all user access. Most graphics games and video

all user access. Most graphics games and video editing and playback software need direct access to memorymapped graphics controller memory to speed the performance of the graphics, for example. The kernel might in this case provide a locking mechanism to allow a section of graphics memory (representing a window on screen) to be allocated to one process at a time. 13.4.7 Kernel Data Structures The kernel needs to keep state information about the use of IOcomponents. It does so through a variety

use of IOcomponents. It does so through a variety of inkernel data structures, such as the openle13.4 Kernel IO Subsystem 609 kernel 2 perform IOcase n system call nread 3 return to user1 trap to monitor user program            Figure 13.11 Use of a system call to perform IO. table structure from Section 12.1. The kernel uses many similar structures to track network connections, characterdevice communications, and other IO activities. UNIX provides lesystem access to a variety of entities, such

lesystem access to a variety of entities, such as user les, raw devices, and the address spaces of processes. Although each of these entities supports a read() operation, the semantics differ. For instance, to read a user le, the kernel needs to probe the buffer cache before deciding whether to perform a disk IO. To read a raw disk, the kernel needs to ensure that the request size is a multiple of the disk sector size and is aligned on a sector boundary. To read a process image, it is merely

boundary. To read a process image, it is merely necessary to copy data from memory. UNIX encapsulates these difference sw i t h i nau n i f o r ms t r u c t u r e by using an objectoriented technique. The openle record, shown in Figure 13.12, contains a dispatch table that holds pointers to the appropriate routines, depending on the type of le. Some operating systems use objectoriented methods even more exten sively. For instance, Windows uses a messagepassing implementation for IO. An IOrequest

messagepassing implementation for IO. An IOrequest is converted into a message that is sent through the kernel to the IOmanager and then to the device driver, each of which may change the message contents. For output, the message contains the data to be written. For input, the message contains a buffer to receive the data. The messagepassing approach can add overhead, by comparison with procedural techniques that use shared data structures, but it simplies the structure and design of the IO

but it simplies the structure and design of the IO system and adds exibility.610 Chapter 13 IO Systemsactiveinode table network information tableperprocess openfile table userprocess memorysystemwide openfile table kernel memory     filesystem record inode pointer pointer to read and write functions pointer to select function pointer to ioctl function pointer to close function networking (socket) record pointer to network info pointer to read and write functions pointer to select function

and write functions pointer to select function pointer to ioctl function pointer to close functionfile descriptor Figure 13.12 UNIX IO kernel structure. 13.4.8 Kernel IO Subsystem Summary In summary, the IOsubsystem coordinates an extensive collection of services that are available to applications and to other parts of the kernel. The IO subsystem supervises these procedures: Management of the name space for les and devices Access control to les and devices Operation control (for example, a

les and devices Operation control (for example, a modem cannot seek() ) Filesystem space allocation Device allocation Buffering, caching, and spooling IOscheduling Devicestatus monitoring, error handling, and failure recovery Devicedriver conguration and initialization The upper levels of the IO subsystem access devices via the uniform interface provided by the device drivers.13.5 Transforming IO Requests to Hardware Operations 611 13.5 Transforming IO Requests to Hardware Operations Earlier, we

IO Requests to Hardware Operations Earlier, we described the handshakin gb e t w e e nad e v i c ed r i v e ra n dad e v i c e controller, but we did not explain how the operating system connects an application request to a set of network wires or to a specic disk sector. Consider, for example, reading a le from disk. The application refers to the data by a le name. Within a disk, the le system maps from the le name through the lesystem directories to obtain the space allocation of the le. For

to obtain the space allocation of the le. For instance, in MSDOS ,t h en a m em a p st oan u m b e rt h a ti n d i c a t e sa ne n t r yi nt h e leaccess table, and that table entry tells which disk blocks are allocated to the le. In UNIX ,t h en a m em a p st oa ni n o d en u m b e r ,a n dt h ec o r r e s p o n d i n g inode contains the spaceallocation information. But how is the connection made from the le name to the disk controller (the hardware port address or the memorymapped controller

port address or the memorymapped controller registers)? One method is that used by MSDOS ,ar e l a t i v e l ys i m p l eo p e r a t i n gs y s t e m . The rst part of an MSDOS le name, preceding the colon, is a string that identies a specic har dware device. For example, C:is the rst part of every le name on the primary hard disk. The fact that C:represents the primary hard disk is built into the operating system; C:is mapped to a specic port address through a device table. Because of the colon

through a device table. Because of the colon separator, the device name space is separate from the lesystem name space. This separation makes it easy for the operating system to associate extra functionality with each device. For instance, it is easy to invoke spooling on any les written to the printer. If, instead, the device name space is incorporated in the regular lesystem name space, as it is in UNIX ,t h en o r m a l l e  s y s t e mn a m es e r v i c e sa r ep r o v i d e d automatically.

e r v i c e sa r ep r o v i d e d automatically. If the le system provides ownership and access control to all le names, then devices have owners and access control. Since les are stored on devices, such an interface provides access to the IOsystem at two levels. Names can be used to access the devices themselves or to access the les stored on the devices. UNIX represents device names in the regular lesystem name space. Unlike anMSDOS le name, which has a colon separator, a UNIX path name has no

has a colon separator, a UNIX path name has no clear separation of the device portion. In fact, no part of the path name is the name of a device. UNIX has a mount table that associates prexes of path names with specic device names. To resolve a path name, UNIX looks up the name in the mount table to nd the longest matching prex; the corresponding entry in the mount table gives the device name. This device name also has the form of a name in the lesystem name space. When UNIX looks up this name

lesystem name space. When UNIX looks up this name in the lesystem directory structures, it nds not an inode number but a major, minor device number. The major device n umber identies a device driver that should be called to handle IOto this device. The minor device number is passed to the device driver to index into ad e v i c et a b l e .T h ec o r r e s p o n d i n g devicetable entry gives the port address or the memorymapped address of the device controller. Modern operating systems gain

device controller. Modern operating systems gain signicant exibility from the multiple stages of lookup tables in the path between a request and a physical device controller. The mechanisms that pass requests between applications and drivers are general. Thus, we can introduce new devices and drivers into a computer without recompiling the kernel. In fact, some operating systems have the ability to load device drivers on demand. At boot time, the system612 Chapter 13 IO Systems send request to

system612 Chapter 13 IO Systems send request to device driver, block process if appropriate monitor device, interrupt when IO completedprocess request, issue commands to controller, configure controller to block until interruptedrequest IO system call noyesIO completed, input data available, or output completeduser process kernel IO subsystem kernel IO subsystem device driver device controller timeinterrupt handlertransfer data (if appropriate) to process, return completion or error code

to process, return completion or error code determine which IO completed, indicate state change to IO subsystem receive interrupt, store data in devicedriver buffer if input, signal to unblock device driver IO completed, generate interruptreturn from system call interruptdevicecontroller commandscan already satisfy request? Figure 13.13 The life cycle of an IO request. rst probes the hardware buses to determin ew h a td e v i c e sa r ep r e s e n t .I tt h e n loads in the necessary drivers,

e n t .I tt h e n loads in the necessary drivers, either immediately or when rst required by an IOrequest. We next describe the typical life cycle of a blocking read request, as depicted in Figure 13.13. The gure suggests that an IOoperation requires a great many steps that together consume a tremendous number of CPU cycles. 1.Ap r o c e s si s s u e sab l o c k i n g read() system call to a le descriptor of a le that has been opened previously. 2.The systemcall code in the kernel checks the

2.The systemcall code in the kernel checks the parameters for correctness. In the case of input, if the data are already available in the buffer cache, the data are returned to the process, and the IOrequest is completed.13.6 STREAMS 613 3.Otherwise, a physical IOmust be performed. The process is removed from the run queue and is placed on the wait queue for the device, and the IO request is scheduled. Eventually, the IOsubsystem sends the request to the device driver. Depen ding on the

request to the device driver. Depen ding on the operating system, the request is sent via a subroutine call or an inkernel message. 4.The device driver allocates kernel buf fer space to receive the data and schedules the IO.E v e n t u a l l y ,t h ed r i v e rs e n d sc o m m a n d st ot h ed e v i c e controller by writing into the devicecontrol registers. 5.The device controller operates the device hardware to perform the data transfer. 6.The driver may poll for status and data, or it may

driver may poll for status and data, or it may have set up a DMA transfer into kernel memory. We assume that the transfer is managed by a DMA controller, which generates an interrupt when the transfer completes. 7.The correct interrupt handler rece ives the interrupt via the interrupt vector table, stores any necessary data, signals the device driver, and returns from the interrupt. 8.The device driver receives th es i g n a l ,d e t e r m i n e sw h i c h IOrequest has completed, determines the

sw h i c h IOrequest has completed, determines the requests status, and signals the kernel IO subsystem that the request has been completed. 9.The kernel transfers data or return codes to the address space of the requesting process and moves the process from the wait queue back to the ready queue. 10. Moving the process to the ready queue un blocks the process. When the scheduler assigns the process to the CPU,t h ep r o c e s sr e s u m e se x e c u t i o n at the completion of the system call.

c u t i o n at the completion of the system call. 13.6 STREAMS UNIX System V has an interesting mechanism, called STREAMS ,t h a te n a b l e s an application to assemble pipelines of driver code dynamically. A stream is af u l l  d u p l e xc o n n e c t i o nb e t w e e nad e v i c ed r i v e ra n dau s e r  l e v e lp r o c e s s .I t consists of a stream head that interfaces with the user process, a driver end that controls the device, and zero or more stream modules between the stream head

or more stream modules between the stream head and the driver end. Each of th ese components contains a pair of queues a read queue and a write queue. Message passing is used to transfer data between queues. The STREAMS structure is shown in Figure 13.14. Modules provide the functionality of STREAMS processing; they are pushed onto a stream by use of the ioctl() system call. For example, a process can open a serialport device via a stream and can push on a module to handle input editing. Because

push on a module to handle input editing. Because messages are ex changed between queues in adjacent modules, a queue in one module may overow an adjacent queue. To prevent this from occurring, a queue may support ow control .W i t h o u t o wc o n t r o l , aq u e u ea c c e p t sa l lm e s s a g e sa n di m m e d i a t e l ys e n d st h e mo nt ot h eq u e u e in the adjacent module without buffering them. A queue that supports ow614 Chapter 13 IO Systems user process devicestream head driver

IO Systems user process devicestream head driver endread queue write queueSTREAMS modulesread queue read queue read queuewrite queue write queue write queue Figure 13.14 The STREAMS structure. control buffers messages and does not accept messages without sufcient buffer space. This process involves exchanges of control messages between queues in adjacent modules. Au s e rp r o c e s sw r i t e sd a t at oad e v i c eu s i n ge i t h e rt h e write() orputmsg() system call. The write() system

write() orputmsg() system call. The write() system call writes raw data to the stream, whereas putmsg() allows the user process to specify a message. Regardless of the system call used by the user process, the stream head copies the data into a message and delivers it to the queue for the n ext module in line. This copying of messages continues until the message is copied to the driver end and hence the device. Similarly, the user process reads data from the stream head using either theread()

data from the stream head using either theread() orgetmsg() system call. If read() is used, the stream head gets am e s s a g ef r o mi t sa d j a c e n tq u e u ea n dr e t u r n so r d i n a r yd a t a( a nu n s t r u c t u r e d byte stream) to the process. If getmsg() is used, a message is returned to the process. STREAMS IO is asynchronous (or nonblocking) except when the user process communicates with the stream head. When writing to the stream, the user process will block, assuming the

stream, the user process will block, assuming the next queue uses ow control, until there is room to copy the message. Likewise, the user process will block when reading from the stream until data are available. As mentioned, the driver endlike the stream head and moduleshas ar e a da n dw r i t eq u e u e .H o w e v e r ,t h ed r i v e re n dm u s tr e s p o n dt oi n t e r r u p t s , such as one triggered when a frame is ready to be read from a network. Unlike the stream head, which may block

a network. Unlike the stream head, which may block if it is unable to copy a message to the next queue in line, the driver end must handle all incoming data. Drivers must support ow control as well. However, if a devices buffer is full, the13.7 Performance 615 device typically resorts to dropping incoming messages. Consider a network card whose input buffer is full. The network card must simply drop further messages until there is enough buffer space to store incoming messages. The benet of

space to store incoming messages. The benet of using STREAMS is that it provides a framework for a modular and incremental approach t ow r i t i n gd e v i c ed r i v e r sa n dn e t w o r k protocols. Modules may be used by different streams and hence by different devices. For example, a networking module may be used by both an Ethernet network card and a 802.11 wireless network card. Furthermore, rather than treating characterdevice IOas an unstructured byte stream, STREAMS allows support for

byte stream, STREAMS allows support for message boundaries and control information when communicating between modules. Most UNIX variants support STREAMS ,a n di ti st h ep r e f e r r e d method for writing protocols and device drivers. For example, System V UNIX and Solaris implement the socket mechanism using STREAMS . 13.7 Performance IOis a major factor in system performance. It places heavy demands on the CPU to execute devicedriver code and to schedule processes fairly and efciently as

and to schedule processes fairly and efciently as they block and unblock. The resulting context switches stress the CPU and its hardware caches. IOalso exposes any inefciencies in the interrupthandling mechanisms in the kernel. In addition, IOloads down the memory bus during data copies between controllers and p hysical memory and again during copies between kernel buffers and application data space. Coping gracefully with all these demands is one of the major concerns of a computer architect.

one of the major concerns of a computer architect. Although modern computers can handle many thousands of interrupts per second, interrupt handling is a re latively expensive task. Each interrupt causes the system to perform a state change, to execute the interrupt handler, and then to restore state. Programmed IOcan be more efcient than interruptdriven IO,i ft h en u m b e ro fc y c l e ss p e n ti nb u s yw a i t i n gi sn o te x c e s s i v e .A n IO completion typically unblocks a process,

e .A n IO completion typically unblocks a process, leading to the full overhead of a context switch. Network trafc can also cause a high contextswitch rate. Consider, for instance, a remote login from one machine to another. Each character typed on the local machine must be transported to the remote machine. On the local machine, the character is typed; a keyboard interrupt is generated; and the character is passed through the interrupt handler to the device driver, to the kernel, and then to

to the device driver, to the kernel, and then to the user process. The user process issues a network IO system call to send the character to the remote machine. The character then ows into the local kernel, through the network layers that construct a network packet, and into the network devic ed r i v e r .T h en e t w o r kd e v i c ed r i v e rt r a n s f e r s the packet to the network controller, w hich sends the character and generates an interrupt. The interrupt is passed back up through

interrupt. The interrupt is passed back up through the kernel to cause the network IOsystem call to complete. Now, the remote systems network hardware receives the packet, and an interrupt is generated. The character is unpacked from the network protocols and is given to the appropriate network daemon. The network daemon identies which remote login session is involved and passes the packet to the appropriate subdaemon for that session. Throughout this ow, there are616 Chapter 13 IO Systems

this ow, there are616 Chapter 13 IO Systems network kerneluser processkerneldevice driverinterrupt handled device drivernetwork adapterinterrupt generatedinterrupt handledinterrupt generatedsystem call completes sending systemstate save state save state savecontext switch context switchhard ware hard ware receiving systemcharacter typed context switchcontext switchcontext switch context switch kernelnetwork daemonnetwork subdaemonkerneldevice driverinterrupt generatednetwork adapternetwork

driverinterrupt generatednetwork adapternetwork packet received Figure 13.15 Intercomputer communications. context switches and state switches (Figure 13. 15). Usually, the receiver echoes the character back to the sender; that approach doubles the work. To eliminate the context switches involved in moving each character between daemons and the kernel, the Solaris developers reimplemented the telnet daemon using inkernel threads. Sun estimated that this improvement increased the maximum number

that this improvement increased the maximum number of network logins from a few hundred to a few thousand on a large server. Other systems use separate frontend processors for terminal IOto reduce the interrupt burden on the main CPU.F o ri n s t a n c e ,a terminal concentrator can multiplex the trafc from hundreds of remote terminals into one port on a large computer. An IOchannel is a dedicated, specialpurpose CPU found in mainframes and in other highend systems. The job of a channel is to

other highend systems. The job of a channel is to ofoad IOwork from the main CPU. The idea is that the channels keep the data owing smoothly, while the main CPU remains free to process the data. Like the device controllers and DMA controllers found in smaller computers, a channel can process more general and sophisticated programs, so channels can be tuned for particular workloads.13.7 Performance 617 We can employ several principles to improve the efciency of IO: Reduce the number of context

the efciency of IO: Reduce the number of context switches. Reduce the number of times that data must be copied in memory while passing between device and application. Reduce the frequency of interrupts by using large transfers, smart con trollers, and polling (if busy waiting can be minimized). Increase concurrency by using DMA knowledgeable controllers or chan nels to ofoad simple data copying from the CPU. Move processing primitives into hardware, to allow their operation in device controllers

to allow their operation in device controllers to be concurrent with CPU and bus operation. Balance CPU,m e m o r ys u b s y s t e m ,b u s ,a n d IOperformance, because an overload in any one area will cause idleness in others. IOdevices vary greatly in complexity. For instance, a mouse is simple. The mouse movements and button clicks ar ec o n v e r t e di n t on u m e r i cv a l u e st h a t are passed from hardware, through the mouse device driver, to the application. By contrast, the

driver, to the application. By contrast, the functionality provided by the Windows disk device driver is complex. It not only manages individual disks but also implements RAID arrays (Section 10.7). To do so, it converts an applications read or write request into a coordinated set of disk IOoperations. Moreover, it implements sophisticated errorhandling and datarecovery algorithms and takes many steps to optimize disk performance. Where should the IOfunctionality be implementedin the device hard

IOfunctionality be implementedin the device hard ware, in the device driver, or in application software? Sometimes we observe the progression depicted in Figure 13.16. application code kernel code devicedriver code devicecontroller code (hardware) device code (hardware)new algorithm increased flexibilityincreased abstractionincreased development costincreased efficiencyincreased time (generations) Figure 13.16 Device functionality progression.618 Chapter 13 IO Systems Initially, we implement

Chapter 13 IO Systems Initially, we implement experimental IO algorithms at the application level, because application code is exib le and application bugs are unlikely to cause system crashes. Furthermore, by developing code at the applica tion level, we avoid the need to reboot or reload device drivers after every change to the code. An application level implementation can be inefcient, however, because of the overhead of co ntext switches and because the application cannot take advantage of

because the application cannot take advantage of internal kernel data structures and kernel functionality (such as efcient inke rnel messaging, threading, and locking). When an applicationlevel algorithm has demonstrated its worth, we may reimplement it in the kernel. This can improve performance, but the devel opment effort is more challenging, because an operatingsystem kernel is al a r g e ,c o m p l e xs o f t w a r es y s t e m .M o r e o v e r ,a ni n  k e r n e li m p l e m e n t a  tion

e r ,a ni n  k e r n e li m p l e m e n t a  tion must be thoroughly debugged to avoid data corruption and system crashes. The highest performance may be obtained through a specialized imple mentation in hardware, either in the device or in the controller. The disadvantages of a hardware implementation include the difculty and expense of making further improvements or of xing bugs, the increased development time (months rather than days), and the decreased exibility. For instance, a hardware

the decreased exibility. For instance, a hardware RAID controller may not provide any means for the kernel to inuence the order or location of individual block reads and writes, even if the kernel has special information about the workload that would enable it to improve the IOperformance. 13.8 Summary The basic hardware elements involved in IOare buses, device controllers, and the devices themselves. The work of moving data between devices and main memory is performed by the CPU as programmed

main memory is performed by the CPU as programmed IOor is ofoaded to a DMA controller. The kernel module that controls a device is a device driver. The systemcall interface provided to applications is designed to handle several basic categories of hardware, including block devices, character devices, memorymapped les, network sockets, and programmed interval timers. The system calls usually block the processes that issue them, but nonblocking and asynchronous calls are used by the kernel itself

asynchronous calls are used by the kernel itself and by applications that must not sleep while waiting for an IOoperation to complete. The kernels IO subsystem provides numerous services. Among these areIOscheduling, buffering, caching, spooling, device reservation, and error handling. Another service, name translation, makes the connections between hardware devices and the symbolic le names used by applications. It involves several levels of mapping that translate from characterstring names, to

that translate from characterstring names, to specic device drivers and device addresses, and then to physical addresses of IOports or bus controllers. This mapping may occur within the lesystem name space, as it does in UNIX ,o ri nas e p a r a t ed e v i c en a m es p a c e ,a si td o e si n MSDOS . STREAMS is an implementation and methodology that provides a frame work for a modular and incremental approach to writing device drivers andExercises 619 network protocols. Through streams, drivers

619 network protocols. Through streams, drivers can be stacked, with data passing through them sequentially and bidirectionally for processing. IOsystem calls are costly in terms of CPU consumption because of the many layers of software between a physic al device and an application. These layers imply overhead from several sources: context switching to cross the kernels protection boundary, signal an d interrupt handling to service the IO devices, and the load on the CPU and memory system to

and the load on the CPU and memory system to copy data between kernel buffers and application space. Practice Exercises 13.1 State three advantages of placing functionality in a device controller, rather than in the kernel. State three disadvantages. 13.2 The example of handshaking in Section 13.2 used two bits: a busy bit and a commandready bit. Is it possible to implement this handshaking with only one bit? If it is, describe the protocol. If it is not, explain why one bit is insufcient. 13.3

it is not, explain why one bit is insufcient. 13.3 Why might a system use interruptdriven IOto manage a single serial port and polling IOto manage a frontend processor, such as a terminal concentrator? 13.4 Polling for an IOcompletion can waste a large number of CPU cycles if the processor iterates a busywaiting loop many times before the IO completes. But if the IOdevice is ready for service, polling can be much more efcient than is catching and dispatching an interrupt. Describe ah y b r i ds

dispatching an interrupt. Describe ah y b r i ds t r a t e g yt h a tc o m b i n e sp o l l i n g ,s l e e p i n g ,a n di n t e r r u p t sf o r IOdevice service. For each of these three strategies (pure polling, pure interrupts, hybrid), describe a computing environment in which that strategy is more efcient than is either of the others. 13.5 How does DMA increase system concurrency? How does it complicate hardware design? 13.6 Why is it important to scale up systembus and device speeds as CPU

to scale up systembus and device speeds as CPU speed increases? 13.7 Distinguish between a STREAMS driver and a STREAMS module. Exercises 13.8 When multiple interrupts from different devices appear at about the same time, a priority scheme could be used to determine the order in which the interrupts would be serviced. Discuss what issues need to be considered in assigning priorities to different interrupts. 13.9 What are the advantages and disadvantages of supporting memory mapped IOto device

of supporting memory mapped IOto device control registers?620 Chapter 13 IO Systems 13.10 Consider the following IOscenarios on a singleuser PC: a. A mouse used with a graphical user interface b. A tape drive on a multitasking operating system (with no device preallocation available) c. A disk drive containing user les d. A graphics card with direct bus connection, accessible through memorymapped IO For each of these scenarios, would you design the operating system to use buffering, spooling,

the operating system to use buffering, spooling, caching, or a combination? Would you use polled IOor interruptdriven IO?G i v er e a s o n sf o ry o u rc h o i c e s . 13.11 In most multiprogrammed systems, user programs access memory through virtual addresses, while the operating system uses raw phys ical addresses to access memory. What are the implications of this design for the initiation of IO operations by the user program and their execution by the operating system? 13.12 What are the

by the operating system? 13.12 What are the various kinds of performance overhead associated with servicing an interrupt? 13.13 Describe three circumstances under which blocking IOshould be used. Describe three circumstances under which nonblocking IOshould be used. Why not just implement nonblocking IO and have processes busywait until their devices are ready? 13.14 Typically, at the completion of a device IO,as i n g l ei n t e r r u p ti sr a i s e d and appropriately handled by the host

sr a i s e d and appropriately handled by the host processor. In certain settings, however, the code that is to be executed at the completion of the IOcan be broken into two separate pieces. The rst piece executes immediately after the IOcompletes and schedules a second interrupt for the remaining piece of code to be executed at a later time. What is the purpose of using this strategy in the design of interrupt handlers? 13.15 Some DMA controllers support direct virtual memory access, where the

support direct virtual memory access, where the targets of IO operations are specied as virtual addresses and at r a n s l a t i o nf r o mv i r t u a lt op h y s i c a la d d r e s si sp e r f o r m e dd u r i n g the DMA .H o wd o e st h i sd e s i g nc o m p l i c a t et h ed e s i g no ft h e DMA controller? What are the advantages of providing such functionality? 13.16 UNIX coordinates the activities of the kernel IO components by manipulating shared inkernel data structures, whereas

shared inkernel data structures, whereas Windows uses objectoriented message passing between kernel IOcomponents. Discuss three pros and three cons of each approach. 13.17 Write (in pseudocode) an implementation of virtual clocks, including the queueing and management of timer requests for the kernel and applications. Assume that the hardware provides three timer channels. 13.18 Discuss the advantages and disadvantages of guaranteeing reliable transfer of data between modules in the STREAMS

transfer of data between modules in the STREAMS abstraction.Bibliography 621 Bibliographical Notes [Vahalia (1996)] provides a good overview of IOand networking in UNIX . [McKusick and NevilleNeil (2005)] detail the IO structures and methods employed in FreeBSD.T h eu s ea n dp r o g r a m m i n go ft h ev a r i o u si n t e r p r o c e s s  communication and network protocols in UNIX are explored in [Stevens (1992)]. [Hart (2005)] covers Windows programming. [Intel (2011)] provides a good

programming. [Intel (2011)] provides a good source for Intel processors. [Rago (1993)] provides a good discussion of STREAMS .[ H e n n e s s ya n dP a t t e r s o n( 2 0 1 2 ) ] describe multiprocessor systems and cacheconsistency issues. Bibliography [Hart (2005)] J. M. Hart, Windows System Programming, Third Edition, Addison Wesley (2005). [Hennessy and Patterson (2012)] J. Hennessy and D. Patterson, Computer Archi tecture: A Quantitative Approach, Fifth Edition, Morgan Kaufmann (2012).

Approach, Fifth Edition, Morgan Kaufmann (2012). [Intel (2011)] Intel 64 and IA32 Architectures Software Developers Manual, Com bined Volumes: 1, 2A, 2B, 3A and 3B .I n t e lC o r p o r a t i o n( 2 0 1 1 ) . [McKusick and NevilleNeil (2005)] M. K. McKusick and G. V . NevilleNeil, The Design and Implementation of the FreeBSD UNIX Operating System ,A d d i s o n Wesley (2005). [Rago (1993)] S. Rago, UNIX System V Network Programming ,A d d i s o n  W e s l e y (1993). [Stevens (1992)] R. Stevens,

W e s l e y (1993). [Stevens (1992)] R. Stevens, Advanced Programming in the UNIX Environment , AddisonWesley (1992). [Vahalia (1996)] U. Vahalia, Unix Internals: The New Frontiers ,P r e n t i c eH a l l (1996).Part Five Protection and Security Protection mechanisms control access to a system by limiting the types of le access permitted to users. In addition, protection must ensure that only processes that have ga ined proper authorization from the operating system can operate on memory

from the operating system can operate on memory segments, the CPU,a n do t h e r resources. Protection is provided by a mechanis mt h a tc o n t r o l st h ea c c e s so f programs, processes, or users to the resources dened by a computer system. This mechanism must provide a means for specifying the controls to be imposed, together with a means of enforcing them. Security ensures the authentication of system users to protect the integrity of the information stored in the system (both data and

information stored in the system (both data and code), as well as the physical resources of the computer system. The security system prevents unauthorized access, malicious destruction or alteration of data, and accidental introdu ction of inconsistency.14CHAPTER Protection The processes in an operating system must be protected from one anothers activities. To provide such protection, we can use various mechanisms to ensure that only processes that have gained proper authorization from the

that have gained proper authorization from the operating system can operate on the les, memory segments, CPU,a n do t h e rr e s o u r c e s of a system. Protection refers to a mechanism for controlling the access of programs, processes, or users to the resources dened by a computer system. This mechanism must provide a means for specifying the controls to be imposed, together with a means of enforcemen t. We distinguish between protection and security, which is a measure of condence that the

security, which is a measure of condence that the integrity of a system and its data will be preserved. In this cha pter, we focus on protection. Security assurance is a much broader topic, and we address it in Chapter 15. CHAPTER OBJECTIVES To discuss the goals and principles of protection in a modern computer system. To explain how protection domains, combined with an access matrix, are used to specify the resources a process may access. To examine capability and languagebased protection

To examine capability and languagebased protection systems. 14.1 Goals of Protection As computer systems have become more sophisticated and pervasive in their applications, the need to protect their integrity has also grown. Protection was originally conceived as an adjunct to multiprogramming operating systems, so that untrustworthy users might safely share a common logical name space, such as a directory of les, or share a common physical name space, such as memory. Modern protection concepts

space, such as memory. Modern protection concepts have evolved to increase the reliability of any complex system that makes use of shared resources. We need to provide protection for several reasons. The most obvious is the need to prevent the mischievous, inten tional violation of an access restriction 625626 Chapter 14 Protection by a user. Of more general importance, however, is the need to ensure that each program component active in a system uses system resources only in ways consistent

uses system resources only in ways consistent with stated policies. This requirement is an absolute one for a reliable system. Protection can improve reliability by detecting latent errors at the interfaces between component subsystems. Early detection of interface errors can often prevent contamination of a healthy subsystem by a malfunctioning subsystem. Also, an unprotected resource cannot defend against use (or misuse) by an unauthorized or incompetent user. A protectionoriented system

or incompetent user. A protectionoriented system provides means to distinguish between authorized and unauthorized usage. The role of protection in a computer system is to provide a mechanism for the enforcement of the policies governing resource use. These policies can be established in a variety of ways. Some are xed in the design of the system, while others are formulated by the management of a system. Still others are dened by the individual users to protect their own les and programs. A

users to protect their own les and programs. A protection system must have the exibility to enforce a variety of policies. Policies for resource use may vary by application, and they may change over time. For these reasons, protection is no longer the concern solely of the designer of an operating system. The application programmer needs to use protection mechanisms as well, to guard resources created and supported by an application subsystem against misuse. In this chapter, we describe the

against misuse. In this chapter, we describe the protection mechanisms the operating system should provide, but application designers can use them as well in design ing their own protection software. Note that mechanisms are distinct from policies .M e c h a n i s m sd e t e r m i n e how something will be done; policies decide what will be done. The separation of policy and mechanism is important for exibility. Policies are likely to change from place to place or time to time. In the worst

from place to place or time to time. In the worst case, every change in policy would require a change in the underlying mechanism. Using general mechanisms enables us to avoid such a situation. 14.2 Principles of Protection Frequently, a guiding principle can be used throughout a project, such as the design of an operating system. Following this principle simplies design decisions and keeps the system consistent and easy to understand. A key, timetested guiding principle for protection is the

timetested guiding principle for protection is the principle of least privilege .I t dictates that programs, users, and even systems be given just enough privileges to perform their tasks. Consider the analogy of a security guard with a passkey. If this key allows the guard into just the public areas that she guards, then misuse of the key will result in minimal damage. If, however, the passkey allows access to all areas, then damage from its being lost, stolen, misused, copied, or otherwise

being lost, stolen, misused, copied, or otherwise compromised will be much greater. An operating system following the principle of least privilege implements its features, programs, system calls, and data structures so that failure or compromise of a component does the minimum damage and allows the minimum damage to be done. The overow of a buffer in a system daemon might cause the daemon process to fail, for example, but should not allow the execution of code from the daemon process s stack

execution of code from the daemon process s stack that would enable a remote14.3 Domain of Protection 627 user to gain maximum privileges and access to the entire system (as happens too often today). Such an operating system also provides system calls and services that allow applications to be written with negrained access controls. It provides mechanisms to enable privileges when they are needed and to disable them when they are not needed. Also benecial is the creation of audit trails for all

benecial is the creation of audit trails for all privileged function access. The audit trail allows the programmer, system administrator, or lawenforcement ofcer to trace all protection and security activities on the system. Managing users with the principle of least privilege entails creating a separate account for each user, with just the privileges that the user needs. An operator who needs to mount tapes and back up les on the system has access to just those commands and les needed to

access to just those commands and les needed to accomplish the job. Some systems implement rolebased access control ( RBAC )t op r o v i d et h i sf u n c t i o n a l i t y . Computers implemented in a computing facility under the principle of least privilege can be limited to running specic services, accessing specic remote hosts via specic services, and doing so during specic times. Typically, these restrictions are implemented through enabling or disabling each service and through using

or disabling each service and through using access control lists, as described in Sections Section 11.6.2 and Section 14.6. The principle of least privilege can help produce a more secure computing environment. Unfortunately, it fr equently does not. For example, Windows 2000 has a complex protection scheme at its core and yet has many security holes. By comparison, Solaris is considered relatively secure, even though it is a variant of UNIX ,w h i c hh i s t o r i c a l l yw a sd e s i g n e dw

i c hh i s t o r i c a l l yw a sd e s i g n e dw i t hl i t t l ep r o t e c t i o n in mind. One reason for the difference may be that Windows 2000 has more lines of code and more services than Solaris and thus has more to secure and protect. Another reason could be that the protection scheme in Windows 2000 is incomplete or protects the wrong aspects of the operating system, leaving other areas vulnerable. 14.3 Domain of Protection Ac o m p u t e rs y s t e mi sac o l l e c t i o no fp r o c

t e rs y s t e mi sac o l l e c t i o no fp r o c e s s e sa n do b j e c t s .B y objects, we mean both hardware objects (such as the CPU,m e m o r ys e g m e n t s ,p r i n t e r s ,d i s k s ,a n d tape drives) and software objects (such as les, programs, and semaphores). Each object has a unique name that differe ntiates it from all other objects in the system, and each can be accessed only through welldened and meaningful operations. Objects are essentially abstract data types. The

Objects are essentially abstract data types. The operations that are possible may depend on the object. For example, on a CPU,w ec a no n l ye x e c u t e .M e m o r ys e g m e n t sc a nb er e a da n dw r i t t e n , whereas a CDROM orDVDROM can only be read. Tape drives can be read, written, and rewound. Data les can be created, opened, read, written, closed, and deleted; program les can b er e a d ,w r i t t e n ,e x e c u t e d ,a n dd e l e t e d . Ap r o c e s ss h o u l db ea l l o w e dt

t e d . Ap r o c e s ss h o u l db ea l l o w e dt oa c c e s so n l yt h o s er e s o u r c e sf o rw h i c hi t has authorization. Furthermore, at any time, a process should be able to access only those resources that it currently requires to complete its task. This second requirement, commonly referred to as the needtoknow principle ,i su s e f u l in limiting the amount of damage a faulty process can cause in the system.628 Chapter 14 Protection For example, when process pinvokes procedure

For example, when process pinvokes procedure A(),t h ep r o c e d u r es h o u l db e allowed to access only its own variables and the formal parameters passed to it; it should not be able to access all the variables of process p.Similarly, consider the case in which process pinvokes a compiler to compile a particular le. The compiler should not be able to access les arbitrarily but should have access only to a welldened subset of les (such as the source le, listing le, and so on) related to the

source le, listing le, and so on) related to the le to be compiled. Conversely, the compiler may have private les used for accounting or optimization purposes that process pshould not be able to access. The needtoknow principle is similar to the principle of least privilege discussed in Section 14.2 in that the goals of protection are to minimize the risks of possible security violations. 14.3.1 Domain Structure To facilitate the scheme just described, a process operates within a protection

described, a process operates within a protection domain ,w h i c hs p e c i  e st h er e s o u r c e st h a tt h ep r o c e s sm a ya c c e s s .E a c h domain denes a set of objects and the types of operations that may be invoked on each object. The ability to execute an operation on an object is an access right .Ad o m a i ni sac o l l e c t i o no fa c c e s sr i g h t s ,e a c ho fw h i c hi sa no r d e r e d pair objectname, rightsset .F o re x a m p l e ,i fd o m a i n Dhas the access

o re x a m p l e ,i fd o m a i n Dhas the access right file F,read,write ,t h e nap r o c e s se x e c u t i n gi nd o m a i n Dcan both read and write le F.It cannot, however, perform any other operation on that object. Domains may share access rights. For example, in Figure 14.1, we have three domains: D1,D2,a n d D3.T h ea c c e s sr i g h t O4,print is shared by D2 and D3,i m p l y i n gt h a tap r o c e s se x e c u t i n gi ne i t h e ro ft h e s et w od o m a i n sc a n print object O4.

ft h e s et w od o m a i n sc a n print object O4. Note that a process must be executing in domain D1to read and write object O1,w h i l eo n l yp r o c e s s e si nd o m a i n D3may execute object O1. The association between a process and a domain may be either static ,i f the set of resources available to the process is xed throughout the processs lifetime, or dynamic .A sm i g h tb ee x p e c t e d ,e s t a b l i s h i n gd y n a m i cp r o t e c t i o n domains is more complicated than

r o t e c t i o n domains is more complicated than establishing static protection domains. If the association between processes and domains is xed, and we want to adhere to the needtoknow principle, th en a mechanism must be available to change the content of a domain. The reason stems from the fact that a process may execute in two different phases and may, for example, need read access in one phase and write access in another. If a domain is static, we must dene the domain to include both read

we must dene the domain to include both read and w rite access. However, this arrangement provides more rights than are needed in each of the two phases, since we have read access in the phase where we need only write access, and vice versa.D1 H11021 O3, read, write H11022 H11021 O1, read, write H11022 H11021 O2, execute H11022H11021 O1, execute H11022 H11021 O3, read H11022H11021 O2, write H11022H11021 O4, print H11022D2 D3 Figure 14.1 System with three protection domains.14.3 Domain of

with three protection domains.14.3 Domain of Protection 629 Thus, the needtoknow principle is violated. We must allow the contents of a domain to be modied so that the domain always reects the minimum necessary access rights. If the association is dynamic, a mechanism is available to allow domain switching ,e n a b l i n gt h ep r o c e s st os w i t c hf r o mo n ed o m a i nt oa n o t h e r .W em a y also want to allow the content of a domain to be changed. If we cannot change the content of a

be changed. If we cannot change the content of a domain, we can provide the same effect by creating a new domain with the changed content and switching to that new domain when we want to change the domain content. Ad o m a i nc a nb er e a l i z e di nav a r i e t yo fw a y s : Each user may be a domain. In this case, the set of objects that can be accessed depends on the identity of the user. Domain switching occurs when the user is changedgenerally when one user logs out and another user logs

when one user logs out and another user logs in. Each process may be a domain. In this case, the set of objects that can be accessed depends on the identity of the process. Domain switching occurs when one process sends a message to another process and then waits for ar e s p o n s e . Each procedure may be a domain. In this case, the set of objects that can be accessed corresponds to the local variables dened within the procedure. Domain switching occurs when a procedure call is made. We

switching occurs when a procedure call is made. We discuss domain switching in greater detail in Section 14.4. Consider the standard dualmode (monitoruser mode) model of operatingsystem execution. When a process executes in monitor mode, it can execute privileged instructions and thus gain complete control of the computer system. In contrast, when a process executes in user mode, it can invoke only nonprivileged instructions. Consequently, it can execute only within its predened memory space.

can execute only within its predened memory space. These two modes protect the operating system (executing in monitor domain) from the user processes (executing in user domain). In a multiprogrammed operating system, two protection domains are insufcient, since users also want to be protected from one another. Therefore, a more elaborate scheme is needed. We illustrate such a scheme by examining two inuential operating systems UNIX and MULTICS to see how they implement these concepts. 14.3.2 An

see how they implement these concepts. 14.3.2 An Example: UNIX In the UNIX operating system, a domain is associated with the user. Switching the domain corresponds to changing the user identication temporarily. This change is accomplished through the le system as follows. An owner identication and a domain bit (known as the setuid bit )a r ea s s o c i a t e dw i t h each le. When the setuid bit is on, and a user executes that le, the user IDis set to that of the owner of the le. When the bit is

to that of the owner of the le. When the bit is off, however, the user ID does not change. For example, when a user A(that is, a user with user ID A)s t a r t se x e c u t i n ga l eo w n e db y B,whose associated domain bit is off,t h e user IDof the process is set to A.When the setuid bit is on,t h e user IDis set to630 Chapter 14 Protection that of the owner of the le: B.When the process exits, this temporary user ID change ends. Other methods are used to change domains in operating systems

are used to change domains in operating systems in which user IDsare used for domain denition, because almost all systems need to provide such a mechanism. This mechanism is used when an otherwise privileged facility needs to be made available to the general user population. For instance, it might be desirable to allow users to access a network without letting them write their own networking programs. In such a case, on a UNIX system, the setuid bit on a networking program would be set, causing

bit on a networking program would be set, causing the user IDto change when the program was run. The user IDwould change to that of a user with network access privilege (such as root ,t h em o s tp o w e r f u l user ID). One problem with this method is that if a user manages to create a le with user IDroot and with its setuid bit on,t h a tu s e rc a nb e c o m e root and do anything and everything on the system. The setuid mechanism is discussed further in Appendix A. An alternative to this

further in Appendix A. An alternative to this method used in some other operating systems is to place privileged programs in a special directory. The operating system is designed to change the user IDof any program run from this directory, either to the equivalent of root or to the user IDof the owner of the directory. This eliminates one security problem, which occu rs when intruders create programs to manipulate the setuid feature and hide the programs in the system for later use (using

the programs in the system for later use (using obscure le or directory names). This method is less exible than that used in UNIX ,h o w e v e r . Even more restrictive, and thus more protective, are systems that simply do not allow a change of user ID.I nt h e s ei n s t a n c e s ,s p e c i a lt e c h n i q u e sm u s t be used to allow users access to privileged facilities. For instance, a daemon process may be started at boot time and run as a special user ID.U s e r st h e n run a separate

a special user ID.U s e r st h e n run a separate program, which sends requests to this process whenever they need to use the facility. This method is used by the TOPS20 operating system. In any of these systems, great care must be taken in writing privileged programs. Any oversight can result in a total lack of protection on the system. Generally, these programs are the rst to be attacked by people trying to break into a system. Unfortunately, the attackers are frequently successful. For

the attackers are frequently successful. For example, security has been breached on many UNIX systems because of the setuid feature. We discuss security in Chapter 15. 14.3.3 An Example: MULTICS In the MULTICS system, the protection domains are organized hierarchically into a ring structure. Each ring corresponds to a single domain (Figure 14.2). The rings are numbered from 0 to 7. Let Diand Djbe any two domain rings. Ifji,t h e n Diis a subset of Dj. That is, a process executing in domain Dj

of Dj. That is, a process executing in domain Dj has more privileges than does a process executing in domain Di.Ap r o c e s s executing in domain D0has the most privileges. If only two rings exist, this scheme is equivalent to the monitoruser mode of execution, where monitor mode corresponds to D0and user mode corresponds to D1. MULTICS has a segmented address space; each segment is a le, and each segment is associated with one of the rings. A segment description includes an entry that identies

description includes an entry that identies the ring number. In addition, it includes three access bits14.3 Domain of Protection 631ring 0 ring 1 ring N  1    Figure 14.2 MULTICS ring structure. to control reading, writing, and execution. The association between segments and rings is a policy decision with which we are not concerned here. Acurrentringnumber counter is associated with each process, iden tifying the ring in which the process is executing currently. When a process is executing in

currently. When a process is executing in ring i,i tc a n n o ta c c e s sas e g m e n ta s s o c i a t e dw i t hr i n g j(ji). It can access a segment associated with ring k(ki). The type of access, however, is restricted according to the access bits associated with that segment. Domain switching in MULTICS occurs when a process crosses from one ring to another by calling a procedure in a different ring. Obviously, this switch must be done in a controlled manner; otherwise, a process could

in a controlled manner; otherwise, a process could start executing in ring 0, and no protection would be provided. To allow controlled domain switching, we modify the ring eld of the segment descriptor to include the following: Access bracket .Ap a i ro fi n t e g e r s , b1and b2,s u c ht h a t b1b2. Limit .A ni n t e g e r b3such that b3b2. List of gates .I d e n t i  e st h ee n t r yp o i n t s( o r gates )a tw h i c ht h es e g m e n t s may be called. If a process executing in ring icalls

be called. If a process executing in ring icalls a procedure (or segment) with access bracket (b1,b2), then the call is allowed if b1ib2,a n dt h ec u r r e n tr i n gn u m b e ro f the process remains i.O t h e r w i s e ,at r a pt ot h eo p e r a t i n gs y s t e mo c c u r s ,a n d the situation is handled as follows: Ifib1,t h e nt h ec a l li sa l l o w e dt oo c c u r ,b e c a u s ew eh a v eat r a n s f e rt oa ring (or domain) with fewer privileges. However, if parameters are passed that

privileges. However, if parameters are passed that refer to segments in a lower ring (that is, segments not accessible to the called procedure), then these segments must be copied into an area that can be accessed by the called procedure. Ifib2,t h e nt h ec a l li sa l l o w e dt oo c c u ro n l yi f b3is greater than or equal toiand the call has been directed to one of the designated entry points in632 Chapter 14 Protection the list of gates. This scheme allows processes with limited access

This scheme allows processes with limited access rights to call procedures in lower rings that have more access rights, but only in a carefully controlled manner. The main disadvantage of the ring (or hierarchical) structure is that it does not allow us to enforce the needtoknow principle. In particular, if an object must be accessible in domain Djbut not accessible in domain Di,t h e nw em u s t have ji.B u tt h i sr e q u i r e m e n tm e a n st h a te v e r ys e g m e n ta c c e s s i b l ei

st h a te v e r ys e g m e n ta c c e s s i b l ei n Diis also accessible in Dj. The MULTICS protection system is generally more complex and less efcient than are those used in current operating systems. If protection interferes with the ease of use of the system or signicantly decreases system performance, then its use must be weighed carefully against the purpose of the system. For instance, we would want to have a complex protection system on a computer used by a university to process

on a computer used by a university to process students grades and also used by students for classwork. A similar protection system would not be suited to a computer being used for number crunching, in which performance is of utmost importance. We would prefer to separate the mechanism from the protection policy, allowing the same system to have complex or simple protection depending on the needs of its users. To separate mechanism from policy, we require a more general model of protection. 14.4

require a more general model of protection. 14.4 Access Matrix Our general model of protection can be viewed abstractly as a matrix, called anaccess matrix .T h er o w so ft h ea c c e s sm a t r i xr e p r e s e n td o m a i n s ,a n dt h e columns represent objects. Each entry in the matrix consists of a set of access rights. Because the column denes objects explicitly, we can omit the object name from the access right. The entry access (i,j)d e  n e st h es e to fo p e r a t i o n s that a

e  n e st h es e to fo p e r a t i o n s that a process executing in domain Dican invoke on object Oj. To illustrate these concepts, we consider the access matrix shown in Figure 14.3. There are four domains and four objectsthree les ( F1,F2,F3)a n do n e laser printer. A process executing in domain D1can read les F1and F3.A process executing in domain D4has the same privileges as one executing inobject printer read read execute read writeread writeread printF1 D1 D2 D3 D4F2 F3domain Figure 14.3

printF1 D1 D2 D3 D4F2 F3domain Figure 14.3 Access matrix.14.4 Access Matrix 633 domain D1;b u ti na d d i t i o n ,i tc a na l s ow r i t eo n t o l e s F1and F3.T h el a s e r printer can be accessed only by a process executing in domain D2. The accessmatrix scheme provides us with the mechanism for specifying a variety of policies. The mechanism consists of implementing the access matrix and ensuring that the semantic properties we have outlined hold. More specically, we must ensure that a

hold. More specically, we must ensure that a process executing in domain Dican access only those objects specied in row i,a n dt h e no n l ya sa l l o w e db yt h e accessmatrix entries. The access matrix can implement policy decisions concerning protection. The policy decisions involve which rights should be included in the ( i,j)th entry. We must also decide the domain in which each process executes. This last policy is usually decided by the operating system. The users normally decide the

operating system. The users normally decide the contents of the accessmatrix entries. When au s e rc r e a t e san e wo b j e c t Oj, the column Ojis added to the access matrix with the appropriate initialization entries, as dictated by the creator. The user may decide to enter some rights in some entries in column jand other rights in other entries, as needed. The access matrix provides an appropriate mechanism for dening and implementing strict control for both static and dynamic association

control for both static and dynamic association between processes and domains. When we switch a process from one domain to another, we are executing an operation ( switch )o na no b j e c t( t h ed o m a i n ) .W ec a n control domain switching by including domains among the objects of the access matrix. Similarly, when we change the content of the access matrix, we are performing an operation on an object: the access matrix. Again, we can control these changes by including the access matrix

these changes by including the access matrix itself as an object. Actually, since each entry in the access matrix can be modied individually, we must consider each entry in the access matrix as an object to be protected. Now, we need to consider only the operations possible on these new objects (domains and the access matrix) and decide how we want processes to be able to execute these operations. Processes should be able to switch from one domain to another. Switching from domain Dito domain

to another. Switching from domain Dito domain Djis allowed if and only if the access right switch access (i,j). Thus, in Figure 14.4, a process executing in domain D2can switchlaser printer read read execute read writeread writeread print switchswitch switch switchF1 D1D1 D2D2 D3D3 D4D4 F2 F3object domain Figure 14.4 Access matrix of Figure 14.3 with domains as objects.634 Chapter 14 Protection to domain D3or to domain D4.Ap r o c e s si nd o m a i n D4can switch to D1,a n d one in domain D1can

a i n D4can switch to D1,a n d one in domain D1can switch to D2. Allowing controlled change in the contents of the accessmatrix entries requires three additional operations: copy ,owner ,a n d control .W ee x a m i n e these operations next. The ability to copy an access right from one domain (or row) of the access matrix to another is denoted by an asterisk () appended to the access right. The copy right allows the access right to be copied only within the column (that is, for the object) for

within the column (that is, for the object) for which the right is dened. For example, in Figure 14.5(a), a process executing in domain D2can copy the read operation into any entry associated with le F2.H e n c e ,t h ea c c e s sm a t r i xo fF i g u r e1 4 . 5 ( a )c a nb e modied to the access matrix shown in Figure 14.5(b). This scheme has two additional variants: 1.Ar i g h ti sc o p i e df r o m access (i,j)t oaccess (k,j); it is then removed from access (i,j). This action is a of a right,

from access (i,j). This action is a of a right, rather than a copy. 2.Propagation of the copy right may be limited. That is, when the right Ris copied from access (i,j)t oaccess (k,j), only the right R(not R) is created. A process executing in domain Dkcannot further copy the right R. As y s t e mm a ys e l e c to n l yo n eo ft h e s et h r e e copy rights, or it may provide all three by identifying them as separate rights: copy ,transfer ,a n d limited copy . We also need a mechanism to allow

d limited copy . We also need a mechanism to allow addition of new rights and removal of some rights. The owner right controls these operations. If access (i,j)i n c l u d e s theowner right, then a process executing in domain Dican add and removeobject readwrite execute execute execute executeF1 D1 D2 D3F2 F3 domain (a)object readwrite execute execute execute execute readF1 D1 D2 D3F2 F3 domain (b) Figure 14.5 Access matrix with copy rights.14.4 Access Matrix 635object read ownerwriteowner

Access Matrix 635object read ownerwriteowner execute read owner write executeF1 D1 D2 D3F2 F3 domain (a)object owner read writewriteowner execute read owner writeF1 D1 D2 D3F2 F3 domain (b)write write Figure 14.6 Access matrix with owner rights. any right in any entry in column j.F o re x a m p l e ,i nF i g u r e1 4 . 6 ( a ) ,d o m a i n D1 is the owner of F1and thus can add and delete any valid right in column F1. Similarly, domain D2is the owner of F2and F3and thus can add and remove any

owner of F2and F3and thus can add and remove any valid right within these two columns. Thus, the access matrix of Figure 14.6(a) can be modied to the access matrix shown in Figure 14.6(b). The copy and owner rights allow a process to change the entries in a column. A mechanism is also needed to change the entries in a row. The control right is applicable only to domain objects. If access (i,j) includes the control right, then a process executing in domain Dican remove any access right from row

in domain Dican remove any access right from row j. For example, suppose that, in Figure 14.4, we include the control right in access (D2,D4). Then, a process executing in domain D2 could modify domain D4,a ss h o w ni nF i g u r e1 4 . 7 . The copy and owner rights provide us with a mechanism to limit the propagation of access rights. However, they do not give us the appropriate tools for preventing the propagation (or disclosure) of information. The problem of guaranteeing that no information

The problem of guaranteeing that no information initially held in an object can migrate outside of its execution environment is called the connement problem .T h i sp r o b l e m is in general unsolvable (see the bibliographical notes at the end of the chapter). These operations on the domains and the access matrix are not in them selves important, but they illustrate the ability of the accessmatrix model to allow us to implement and control dynamic protection requirements. New objects and new

protection requirements. New objects and new domains can be created dynamically and included in the636 Chapter 14 Protectionlaser printer read read execute write writeread print switchswitch switchswitch controlF1 D1D1 D2D2 D3D3 D4D4 F2 F3object domain Figure 14.7 Modied access matrix of Figure 14.4. accessmatrix model. However, we have shown only that the basic mechanism exists. System designers and users must make the policy decisions concerning which domains are to have access to which

which domains are to have access to which objects in which ways. 14.5 Implementation of the Access Matrix How can the access matrix be implemented effectively? In general, the matrix will be sparse; that is, most of the entries will be empty. Although data structure techniques are available for r epresenting sparse matrices, they are not particularly useful for this application, because of the way in which the protection facility is used. Here, we rst describe several methods of implementing the

rst describe several methods of implementing the access matrix and then compare the methods. 14.5.1 Global Table The simplest implementation of the access matrix is a global table consisting of a set of ordered triples domain, object, rightsset .W h e n e v e ra n operation Mis executed on an object Ojwithin domain Di,t h eg l o b a lt a b l e is searched for a triple Di,Oj,Rk,w i t h MRk.I ft h i st r i p l ei sf o u n d ,t h e operation is allowed to continue; otherwise, an exception (or

allowed to continue; otherwise, an exception (or error) condition is raised. This implementation suffers from several drawbacks. The table is usually large and thus cannot be kept in main memory, so additional IOis needed. Virtual memory techniques are often used for managing this table. In addition, it is difcult to take advantage of special groupings of objects or domains. For example, if everyone can read a particular object, this object must have a separate entry in every domain. 14.5.2

must have a separate entry in every domain. 14.5.2 Access Lists for Objects Each column in the access matrix can be implemented as an access list for one object, as described in Section 11.6.2. Obviously, the empty entries can be discarded. The resulting list for each object consists of ordered pairs domain, rightsset ,w h i c hd e  n ea l ld o m a i n sw i t han o n e m p t ys e to fa c c e s sr i g h t s for that object. This approach can be extended easily to dene a list plus a default set of

easily to dene a list plus a default set of access rights. When an operation Mon an object Ojis attempted in domain14.5 Implementation of the Access Matrix 637 Di,w es e a r c ht h ea c c e s sl i s tf o ro b j e c t Oj,l o o k i n gf o ra ne n t r y Di,Rkwith MRk.I ft h ee n t r yi sf o u n d ,w ea l l o wt h eo p e r a t i o n ;i fi ti sn o t ,w ec h e c kt h e default set. If Mis in the default set, we allow the access. Otherwise, access is denied, and an exception condition occurs. For

is denied, and an exception condition occurs. For efciency, we may check the default set rst and then search the access list. 14.5.3 Capability Lists for Domains Rather than associating the columns of the access matrix with the objects as access lists, we can associate each row with its domain. A capability list for ad o m a i ni sal i s to fo b j e c t st o g e t h e rw i t ht h eo p e r a t i o n sa l l o w e do nt h o s e objects. An object is often represented b yi t sp h y s i c a ln a m eo

often represented b yi t sp h y s i c a ln a m eo ra d d r e s s ,c a l l e d acapability .T oe x e c u t eo p e r a t i o n Mon object Oj,t h ep r o c e s se x e c u t e st h e operation M,s p e c i f y i n gt h ec a p a b i l i t y( o rp o i n t e r )f o ro b j e c t Ojas a parameter. Simple possession of the capability means that access is allowed. The capability list is associated with a domain, but it is never directly accessible to a process executing in that domain. Rather, the capability

executing in that domain. Rather, the capability list is itself a protected object, maintained by the operating system and accessed by the user only indirectly. Capabilitybased protection relies on the fact that the capabilities are never allowed to migrate into any address space directly accessible by a user process (where they could be modied). If all capabilities are secure, the object they protect is also secure against unauthorized access. Capabilities were originally proposed as a kind of

Capabilities were originally proposed as a kind of secure pointer, to meet the need for resource protection that was foreseen as multiprogrammed computer systems came of age. The idea of an inherently protected pointer provides a foundation for protection that can be extended up to the application level. To provide inherent protection, we must distinguish capabilities from other kinds of objects, and they must be interpreted by an abstract machine on which higherlevel programs run. Capabilities

on which higherlevel programs run. Capabilities are usually distinguished from other data in one of two ways: Each object has a tagto denote whether it is a capability or accessible data. The tags themselves must not be directly accessible by an application program. Hardware or rmware support may be used to enforce this restriction. Although only one bit is necessary to distinguish between capabilities and other objects, more bits are often used. This extension allows all objects to be tagged

This extension allows all objects to be tagged with their types by the hardware. Thus, the hardware can distinguish integers, oatingpoint numbers, pointers, Booleans, characters, instructions, capabilities, and uninitialized values by their tags. Alternatively, the address space associated with a program can be split into two parts. One part is accessible to the program and contains the programs normal data and instructions. The other part, containing the capability list, is accessible only by

the capability list, is accessible only by the operating system. A segmented memory space (Section 8.4) is useful to support this approach. Several capabilitybased protection systems have been developed; we describe them briey in Section 14.8. The Mach operating system also uses a version of capabilitybased protection; it is described in Appendix B.638 Chapter 14 Protection 14.5.4 A LockKey Mechanism The lockkey scheme is a compromise between access lists and capability lists. Each object has a

lists and capability lists. Each object has a list of unique bit patterns, called locks .S i m i l a r l y ,e a c h domain has a list of unique bit patterns, called keys .Ap r o c e s se x e c u t i n gi na domain can access an object only if that domain has a key that matches one of the locks of the object. As with capability lists, the list of keys for a domain must be managed by the operating system on behalf of the domain. Users are not allowed to examine or modify the list of keys (or

allowed to examine or modify the list of keys (or locks) directly. 14.5.5 Comparison As you might expect, choosing a technique for implementing an access matrix involves various tradeoffs. Using a global table is simple; however, the table can be quite large and often cannot take advantage of special groupings of objects or domains. Access lists correspond directly to the needs of users. When a user creates an object, he can specify which domains can access the object, as well as what operations

can access the object, as well as what operations are allowed. However, because accessright information for a particular domain is not localized, determining the set of access rights for each domain is difcult. In addition, every access to the object must be checked, requiring a search of the access list. In a large system with long access lists, this search can be time consuming. Capability lists do not correspond directly to the needs of users, but they are useful for localizing information

but they are useful for localizing information for a given process. The process attempting access must present a capability for that access. Then, the protection system needs only to verify that the capability is valid. Revocation of capabilities, however, may be inefcient (Section 14.7). The lockkey mechanism, as mention ed, is a compromise between access lists and capability lists. The mechanism can be both effective and exible, depending on the length of the keys. The keys can be passed

on the length of the keys. The keys can be passed freely from domain to domain. In addition, access privileges can be effectively revoked by the simple technique of changing some of the locks associated with the object (Section 14.7). Most systems use a combination of access lists and capabilities. When a process rst tries to access an object, the access list is searched. If access is denied, an exception condition occurs. Otherwise, a capability is created and attached to the process.

capability is created and attached to the process. Additional references use the capability to demonstrate swiftly that access is allowed. After the last access, the capability is destroyed. This strategy is used in the MULTICS system and in the CAL system. As an example of how such a strategy works, consider a le system in which each le has an associated access list. When a process opens a le, the directory structure is searched to nd the le, access permission is checked, and buffers are

le, access permission is checked, and buffers are allocated. All this information is recorded in a new entry in a le table associated with the process. The operation returns an index into this table for the newly opened le. All operations on the le are made by specication of the index into the le table. The entry in the le table then points to the le and its buffers. When the le is closed, the letable entry is deleted. Since the le table is maintained by the operating system, the user cannot

by the operating system, the user cannot accidentally corrupt it. Thus, the user can access on ly those les that have been opened.14.6 Access Control 639 Since access is checked when the le is opened, protection is ensured. This strategy is used in the UNIX system. The right to access must still be checked on each access, and the letable entry has a capability only for the allowed operations. If a le is opened for reading, then a capability for read access is placed in the letable entry. If an

read access is placed in the letable entry. If an attempt is made to write onto the le, the system identies this protection violation by comparing the requested operation with the capability in the letable entry. 14.6 Access Control In Section 11.6.2, we described how access controls can be used on les within a l es y s t e m .E a c h l ea n dd i r e c t o r yi sa s s i g n e da no w n e r ,ag r o u p ,o rp o s s i b l y al i s to fu s e r s ,a n df o re a c ho ft h o s ee n t i t i e s ,a c c e

df o re a c ho ft h o s ee n t i t i e s ,a c c e s s  c o n t r o li n f o r m a t i o ni s assigned. A similar function can be added to other aspects of a computer system. A good example of this is found in Solaris 10. Solaris 10 advances the protection available in the operating system by explicitly adding the principle of least privilege via rolebased access control (RBAC ).T h i sf a c i l i t yr e v o l v e sa r o u n dp r i v i l e g e s .Ap r i v i l e g ei st h er i g h tt o execute a

.Ap r i v i l e g ei st h er i g h tt o execute a system call or to use an option within that system call (such as opening a l ew i t hw r i t ea c c e s s ) .P r i v i l e g e sc a nb ea s s i g n e dt op r o c e s s e s ,l i m i t i n gt h e m to exactly the access they need to perform their work. Privileges and programs can also be assigned to roles .U s e r sa r ea s s i g n e dr o l e so rc a nt a k er o l e sb a s e d on passwords to the roles. In this way, a user can take a role that

roles. In this way, a user can take a role that enables a privilege, allowing the user to run a program to accomplish a specic task, as depicted in Figure 14.8. This implementation of privileges decreases the security risk associated with superusers and setuid programs.user 1 role 1 privileges 1 executes with role 1 privilegesprivileges 2 process Figure 14.8 Rolebased access control in Solaris 10.640 Chapter 14 Protection Notice that this facility is similar to the access matrix described in

is similar to the access matrix described in Section 14.4. This relationship is further explored in the exercises at the end of the chapter. 14.7 Revocation of Access Rights In a dynamic protection system, we may sometimes need to revoke access rights to objects shared by different users. Various questions about revocation may arise: Immediate versus delayed .D o e sr e v o c a t i o no c c u ri m m e d i a t e l y ,o ri si t delayed? If revocation is delayed, can we nd out when it will take

is delayed, can we nd out when it will take place? Selective versus general .W h e na na c c e s sr i g h tt oa no b j e c ti sr e v o k e d , does it affect all the users who have an access right to that object, or can we specify a select group of users whose access rights should be revoked? Partial versus total .C a nas u b s e to ft h er i g h t sa s s o c i a t e dw i t ha no b j e c tb e revoked, or must we revoke all access rights for this object? Temporary versus permanent . Can access be

object? Temporary versus permanent . Can access be revoked permanently (that is, the revoked access right will never again be available), or can access be revoked and later be obtained again? With an accesslist scheme, revocation is easy. The access list is searched for any access rights to be revoked, and they are deleted from the list. Revocation is immediate and can be general or selective, total or partial, and permanent or temporary. Capabilities, however, present a much more difcult

Capabilities, however, present a much more difcult revocation problem, as mentioned earlier. Since the capabilities are distributed throughout the system, we must nd them before we can revoke them. Schemes that implement revocation for capabilities include the following: Reacquisition .P e r i o d i c a l l y ,c a p a b i l i t i e sa r ed e l e t e df r o me a c hd o m a i n .I f ap r o c e s sw a n t st ou s eac a p a b i l i t y ,i tm a y n dt h a tt h a tc a p a b i l i t yh a sb e e n

y n dt h a tt h a tc a p a b i l i t yh a sb e e n deleted. The process may then try to reacquire the capability. If access has been revoked, the process will not be able to reacquire the capability. Backpointers . A list of pointers is maintained with each object, pointing to all capabilities associated with that object. When revocation is required, we can follow these pointers, changing the capabilities as necessary. This scheme was adopted in the MULTICS system. It is quite general, but its

the MULTICS system. It is quite general, but its implementation is costly. Indirection . The capabilities point indirectly, not directly, to the objects. Each capability points to a unique entry in a global table, which in turn points to the object. We implement revocation by searching the global table for the desired entry and deleting it. Then, when an access is attempted, the capability is found to point to an illegal table entry. Table entries can be reused for other capabilities without

can be reused for other capabilities without difculty, since both the capability and the table entry contain the unique name of the object. The object for a14.8 CapabilityBased Systems 641 capability and its table entry must match. This scheme was adopted in the CAL system. It does not allow selective revocation. Keys .Ak e yi sau n i q u eb i tp a t t e r nt h a tc a nb ea s s o c i a t e dw i t hac a p a b i l i t y . This key is dened when the capability is created, and it can be neither

the capability is created, and it can be neither modied nor inspected by the process that owns the capability. A master key is associated with each object; it can be dened or replaced with thesetkey operation. When a capability is created, the current value of the master key is associated with the capability. When the capability is exercised, its key is compared with the master key. If the keys match, the operation is allowed to continue; otherwise, an exception condition is raised. Revocation

an exception condition is raised. Revocation replaces the master key with a new value via the setkey operation, invalidating all previous capabilities for this object. This scheme does not allow selective revocation, since only one master key is associated with each object. If we associate a list of keys with each object, then selective revocation ca nb ei m p l e m e n t e d .F i n a l l y ,w ec a ng r o u p all keys into one global table of keys. A capability is valid only if its key matches

A capability is valid only if its key matches some key in the global table. We implement revocation by removing the matching key from the table. With this scheme, a key can be associated with several objects, and several keys can be associated with each object, providing maximum exibility. In keybased schemes, the operations of dening keys, inserting them into lists, and deleting them from lists should not be available to all users. In particular, it would be reasonable to allow only the owner

it would be reasonable to allow only the owner of an object to set the keys for that object. This choice, however, is a policy decision that the protection system can implement but should not dene. 14.8 CapabilityBased Systems In this section, we survey two capabilitybased protection systems. These systems differ in their complexity and in the types of policies that can be implemented on them. Neither system is widely used, but both provide interesting proving grounds for protection theories.

proving grounds for protection theories. 14.8.1 An Example: Hydra Hydra is a capabilitybased protection system that provides considerable exibility. The system implements a xed set of possible access rights, including such basic forms of access as the right to read, write, or execute a memory segment. In addition, a user (of the protection system) can declare other rights. The interpretation of userdened rights is performed solely by the users program, but the system provides access protection

program, but the system provides access protection for the use of these rights, as well as for the use of systemdened rights. These facilities constitute a signicant development in protection technology. Operations on objects are dened pr ocedurally. The procedures that implement such operations are themselves a f orm of object, and they are accessed indirectly by capabilities. The names of userdened procedures must be identied to the protection system if it is to deal with objects of the user

system if it is to deal with objects of the user dened type. When the denition of an object is made known to Hydra, the names of operations on the type become auxiliary rights .A u x i l i a r yr i g h t s642 Chapter 14 Protection can be described in a capability for an instance of the type. For a process to perform an operation on a typed object, the capability it holds for that object must contain the name of the operation being invoked among its auxiliary rights. This restriction enables

its auxiliary rights. This restriction enables discrimination of access rights to be made on an instancebyinstance and processbyprocess basis. Hydra also provides rights amplication .T h i ss c h e m ea l l o w sap r o c e d u r e to be certied as trustworthy to act on a formal parameter of a specied type on behalf of any process that holds a right to execute the procedure. The rights held by a trustworthy procedure are independent of, and may exceed, the rights held by the calling process.

exceed, the rights held by the calling process. However, such a procedure must not be regarded as universally trustworthy (the procedure is not allowed to act on other types, for instance), and the trustworthiness must not be extended to any other procedures or program segments that might be executed by a process. Amplication allows implementation procedures access to the representa tion variables of an abstract data type. If a process holds a capability to a typed object A,for instance, this

capability to a typed object A,for instance, this capability may include an auxiliary right to invoke some operation Pbut does not include any of the socalled kernel rights, such as read, write, or execute, on the segment that represents A.Such a capability gives a process a means of indirect access (through the operation P)t ot h e representation of A,but only for specic purposes. When a process invokes the operation Pon an object A,however, the capability for access to Amay be amplied as

the capability for access to Amay be amplied as control passes to the code body ofP.This amplication may be necessary to allow Pthe right to access the storage segment representing Aso as to implement the operation that Pdenes on the abstract data type. The code body of Pmay be allowed to read or to write to the segment of Adirectly, even though the calling process cannot. On return from P,the capability for Ais restored to its original, unamplied state. This case is a typical one in which the

state. This case is a typical one in which the rights held by a process for access to a protected segment must change dy namically, depending on the task to be performed. The dynamic adjustment of rights is performed to guarantee consistency of a programmerdened abstraction. Amplication of rights can be stated explicitly in the declaration of an abstract type to the Hydra operating system. When a user passes an object as an argument to a procedure, we may need to ensure that the procedure cannot

we may need to ensure that the procedure cannot modify the object. We can implement this restriction readily by passing an access right that does not have the modication (write) right. However, if amplication may occur, the right to modify may be reinstated. Thus, the userprotection requirement can be circumvented. In general, of course, a user may trust that a procedure performs its task correctly. This assumption is not always correct, however, because of hardware or software errors. Hydra

because of hardware or software errors. Hydra solves this problem by restricting amplications. The procedurecall mechanism of Hydra was designed as a direct solution to the problem of mutually suspicious subsystems .T h i sp r o b l e mi sd e  n e d as follows. Suppose that a program can be invoked as a service by a number of different users (for example, a sort routine, a compiler, a game). When users invoke this service program, they take the risk that the program will malfunction and will

risk that the program will malfunction and will either damage the given data or retain some access right to the data to be used (without authority) later. Similarly, the service program may have some private les (for accounting purposes, for example) that should not14.8 CapabilityBased Systems 643 be accessed directly by the calling user program. Hydra provides mechanisms for directly dealing with this problem. AH y d r as u b s y s t e mi sb u i l to nt o po fi t sp r o t e c t i o nk e r n e

i l to nt o po fi t sp r o t e c t i o nk e r n e la n dm a yr e q u i r e protection of its own components. A subsystem interacts with the kernel through calls on a set of kerneldened primitives that dene access rights to resources dened by the subsystem. The subsystem designer can dene policies for use of these resources by user processes, but the policies are enforced by use of the standard access protection provided by the capability system. Programmers can make direct use of the protection

Programmers can make direct use of the protection system after acquaint ing themselves with its features in the appropriate reference manual. Hydra provides a large library of systemdened procedures that can be called by user programs. Programmers can explicitly incorporate calls on these system procedures into their program code or can use a program translator that has been interfaced to Hydra. 14.8.2 An Example: Cambridge CAP System A different approach to capabilitybased protection has been

approach to capabilitybased protection has been taken in the design of the Cambridge CAP system. CAPs capability system is simpler and supercially less powerful than that of Hydra. However, closer examination shows that it, too, can be used to provide secure protection of userdened objects. CAP has two kinds of capabilities. The ordinary kind is called a data capability . It can be used to provide access to objects, but the only rights provided are the standard read, write, and execute of the

are the standard read, write, and execute of the individual storage segments associated with the object. Data capabilities are interpreted by microcode in the CAP machine. The second kind of capability is the socalled software capability ,w h i c h is protected, but not interpreted, by the CAP microcode. It is interpreted by a protected (that is, privileged) procedure, which may be written by an application programmer as part of a subsystem. A particular kind of rights amplication is associated

kind of rights amplication is associated with a protec ted procedure. When executing the code body of such a procedure, a process temporarily acquires the right to read or write the contents of a software capability itself. This specic kind of rights amplication corresponds to an implementation of the seal and unseal primitives on capabilities. Of course, this privilege is still subject to type verication to ensure that only software capabilities for a specied abstract type are passed to any

for a specied abstract type are passed to any such procedure. Universal trust is not placed in any code other than the CAP machines microcode. (See the bibliographical notes at the end of the chapter for references.) The interpretation of a software capability is left completely to the sub system, through the protected procedures it contains. This scheme allows a variety of protection policies to be implemented. Although programmers can dene their own protected procedures (any of which might be

own protected procedures (any of which might be incorrect), the security of the overall system cannot be compromised. The basic protection system will not allow an unveried, userdened, protected procedure access to any storage segments (or capabilities) that do not belong to the protection environment in which it resides. The most serious consequence of an insecure protected procedure is a protection breakdown of the subsystem for which that procedure has responsibility.644 Chapter 14 Protection

has responsibility.644 Chapter 14 Protection The designers of the CAP system have noted that the use of software capabilities allowed them to realize considerable economies in formulating and implementing protection policies commensurate with the requirements of abstract resources. However, subsystem designers who want to make use of this facility cannot simply study a reference manual, as is the case with Hydra. Instead, they must learn the principles and techniques of protection, since the

principles and techniques of protection, since the system provides them with no library of procedures. 14.9 LanguageBased Protection To the degree that protection is provided in existing computer systems, it is usually achieved through an operatingsystem kernel, which acts as a security agent to inspect and validate each attempt to access a protected resource. Since comprehensive access validation may be a source of considerable overhead, either we must give it hardware support to reduce the

we must give it hardware support to reduce the cost of each validation, or we must allow the system designer to compromise the goals of protection. Satisfying all these goals is difcult if the exibility to implement protection policies is restricted by the support mechanisms provided or if protection environments are made larger than necessary to secure greater operational efciency. As operating systems have become more complex, and particularly as they have attempted to provide higherlevel user

as they have attempted to provide higherlevel user interfaces, the goals of protection have become much more rened. The designers of protection systems have drawn heavily on ideas that originated in programming languages and especially on the concepts of abstract data types and objects. Protection systems are now concerned not only with the identity of a resource to which access is attempted but also with the functional nature of that access. In the newest protection systems, concern for the

In the newest protection systems, concern for the function to be invoked extends beyond as e to fs y s t e m  d e  n e df u n c t i o n s ,s u c ha ss t a n d a r d l e  a c c e s sm e t h o d s ,t o include functions that may be userdened as well. Policies for resource use may also vary, depending on the application, and they may be subject to change over time. For these reasons, protection can no longer be considered a matter of concern only to the designer of an operating system. It should

to the designer of an operating system. It should also be available as a tool for use by the application designer, so that resources of an application subsystem can be guarded against tampering or the inuence of an error. 14.9.1 CompilerBased Enforcement At this point, programming languages enter the picture. Specifying the desired control of access to a shared resource in a system is making a declarative statement about the resource. This kin d of statement can be integrated into a language by

of statement can be integrated into a language by an extension of its typing facility. When protection is declared along with data typing, the designer of each subsystem can specify its requirements for protection, as well as its need for use of other resources in a system. Such a specication should be given directly as a program is composed, and in the language in which the program itself is stated. This approach has several signicant advantages:14.9 LanguageBased Protection 645 1.Protection

LanguageBased Protection 645 1.Protection needs are simply declared, rather than programmed as a sequence of calls on procedures of an operating system. 2.Protection requirements can be stated independently of the facilities provided by a particular operating system. 3.The means for enforcement need not be provided by the designer of a subsystem. 4.Ad e c l a r a t i v en o t a t i o ni sn a t u r a lb e c a u s ea c c e s sp r i v i l e g e sa r ec l o s e l y related to the linguistic concept

r ec l o s e l y related to the linguistic concept of data type. Av a r i e t yo ft e c h n i q u e sc a nb ep r o v i d e db yap r o g r a m m i n g  l a n g u a g e implementation to enforce protection, but any of these must depend on some degree of support from an underlying machine and its operating system. For example, suppose a language is used to generate code to run on the Cambridge CAP system. On this system, every storage reference made on the underlying hardware occurs indirectly

made on the underlying hardware occurs indirectly through a capability. This restriction prevents any process from accessing a resource outside of its protection environment at any time. However, a program may impose arbitrary restrictions on how ar e s o u r c ec a nb eu s e dd u r i n ge x e c u t i o no fap a r t i c u l a rc o d es e g m e n t . We can implement such restrictions most readily by using the software capabilities provided by CAP.Al a n g u a g ei m p l e m e n t a t i o nm i g

a n g u a g ei m p l e m e n t a t i o nm i g h tp r o v i d e standard protected procedures to interpret software capabilities that would realize the protection policies that could be specied in the language. This scheme puts policy specication at the disposal of the programmers, while freeing them from implementing its enforcement. Even if a system does not provide a protection kernel as powerful as those of Hydra or CAP,m e c h a n i s m sa r es t i l la v a i l a b l ef o ri m p l e m e n t

es t i l la v a i l a b l ef o ri m p l e m e n t i n gp r o t e c t i o n specications given in a programming language. The principal distinction is that the security of this protection will not be as great as that supported by ap r o t e c t i o nk e r n e l ,b e c a u s et h em e c h a n i s mm u s tr e l yo nm o r ea s s u m p t i o n s about the operational state of the system. A compiler can separate references for which it can certify that no protection violation could occur from those

no protection violation could occur from those for which a violation might be possible, and it can treat them differently. The security provided by this form of protection rests on the assumption that the code generated by the compiler will not be modied prior to or during its execution. What, then, are the relative merits of enforcement based solely on a kernel, as opposed to enforcement provided largely by a compiler? Security .E n f o r c e m e n tb yak e r n e lp r o v i d e sag r e a t e rd

e n tb yak e r n e lp r o v i d e sag r e a t e rd e g r e eo fs e c u r i t y of the protection system itself than does the generation of protection checking code by a compiler. In a compilersupported scheme, security rests on correctness of the translator, on some underlying mechanism of storage management that protects the segments from which compiled code is executed, and, ultimately, on the security of les from which a program is loaded. Some of these considerations also apply to a software

of these considerations also apply to a software supported protection kernel, but to a lesser degree, since the kernel may reside in xed physical storage segments and may be loaded only from ad e s i g n a t e d l e .W i t hat a g g e d  c a p a b i l i t ys y s t e m ,i nw h i c ha l la d d r e s s646 Chapter 14 Protection computation is performed either by hardware or by a xed microprogram, even greater security is possible. Hardwaresupported protection is also relatively immune to protection

protection is also relatively immune to protection violations that might occur as a result of either hardware or system software malfunction. Flexibility .T h e r ea r el i m i t st ot h e e x i b i l i t yo fap r o t e c t i o nk e r n e li n implementing a userdened policy, although it may supply adequate facilities for the system to provide enforcement of its own policies. With a programming language, protection policy can be declared and enforcement provided as needed by an implementation.

provided as needed by an implementation. If a language does not provide sufcient exibility, it can be extended or replaced with less disturbance than would be caused by the modication of an operatingsystem kernel. Efciency . The greatest efciency is obtaine dw h e ne n f o r c e m e n to fp r o t e c  tion is supported directly by hardware (or microcode). Insofar as software support is required, languagebased enforcement has the advantage that static access enforcement can be veried offline at

static access enforcement can be veried offline at compile time. Also, since an intelligent compiler can tailor the enforcement mechanism to meet the specied need, the xed overhead of kernel calls can often be avoided. In summary, the specication of protection in a programming language allows the highlevel description of policies for the allocation and use of resources. A language implementation can provide software for protection enforcement when automatic hardwa resupported checking is

when automatic hardwa resupported checking is unavailable. In addition, it can interpret protection spec ications to generate calls on whatever protection system is provided by the hardware and the operating system. One way of making protection available to the application program is through the use of a software capability that could be used as an object of computation. Inherent in this concept is the idea that certain program components might have the privilege of creating or examining these

have the privilege of creating or examining these software capabilities. A capabilitycreating program would be able to execute a primitive operation that would seal a data structure, rendering the latters contents inaccessible to any program components that did not hold either the seal or the unseal privilege. Such components might copy the data structure or pass its address to other program components, but they could not gain access to its contents. The reason for introducing such software

contents. The reason for introducing such software capabilities is to bring a protection mechanism into the programming language. The only problem with the concept as proposed is that the use of the seal andunseal operations takes ap r o c e d u r a la p p r o a c ht os p e c i f y i n gp r otection. A nonprocedural or declarative notation seems a preferable way to make protection available to the application programmer. What is needed is a safe, dynamic acce sscontrol mechanism for distribut

dynamic acce sscontrol mechanism for distribut ing capabilities to system resources among user processes. To contribute to the overall reliability of a system, the accesscontrol mechanism should be safe to use. To be useful in practice, it should also be reasonably efcient. This requirement has led to the development of a number of language constructs that allow the programmer to declare various restrictions on the use of a specic managed resource. (See the bibliographic al notes for appropriate

(See the bibliographic al notes for appropriate references.) These constructs provide mechanisms for three functions:14.9 LanguageBased Protection 647 1.Distributing capabilities safely and efciently among customer processes. In particular, mechanisms ensure that a user process will use the managed resource only if it was granted a capability to that resource. 2.Specifying the type of operations that a particular process may invoke on an allocated resource (for example, a reader of a le should

resource (for example, a reader of a le should be allowed only to read the le, whereas a writer should be able both to read and to write). It should not be necessary to grant the same set of rights to every user process, and it should be impossible for a process to enlarge its set of access rights, except with the authorization of the accesscontrol mechanism. 3.Specifying the order in which a particular process may invoke the various operations of a resource (for example, a le must be opened

of a resource (for example, a le must be opened before it can be read). It should be possible to give two processes different restrictions on the order in which they can invoke the operations of the allocated resource. The incorporation of protection concepts into programming languages, as a practical tool for system design, is in its infancy. Protection will likely become am a t t e ro fg r e a t e rc o n c e r nt ot h ed e s i g n e r so fn e ws y s t e m sw i t hd i s t r i b u t e d

so fn e ws y s t e m sw i t hd i s t r i b u t e d architectures and increasingly stringent requirements on data security. Then the importance of suitable language notations in which to express protection requirements will be recognized more widely. 14.9.2 Protection in Java Because Java was designed to run in a distrib uted environment, the Java virtual machineor JVMhas many builtin protection mechanisms. Java programs are composed of classes ,e a c ho fw h i c hi sac o l l e c t i o no fd a t

a c ho fw h i c hi sac o l l e c t i o no fd a t a e l d s and functions (called methods ) that operate on those elds. The JVM loads a class in response to a request to create instances (or objects) of that class. One of the most novel and useful features of Java is its support for dynamically loading untrusted classes over a network and for executing mutually distrusting classes within the same JVM. Because of these capabilities, protection is a paramount concern. Classes running in the same

a paramount concern. Classes running in the same JVM may be from different sources and may not be equally trusted. As a result, enforcing protection at the granularity of the JVM process is insufcient. Intuitively, whether a request to open a le should be allowed will generally depend on which class has requested the open. The operating system lacks this knowledge. Thus, such protection decisions are handled within the JVM.W h e nt h e JVM loads a class, it assigns the class to a protection

a class, it assigns the class to a protection domain that gives the permissions of that class. The protection domain to which the class is assigned depends on the URL from which the class was loaded and any digital signatures on the class le. (Digital signatures are covered in Section 15.4.1.3.) Ac o n  g u r a b l ep o l i c y l ed e t e r m i n e st h ep e r m i s s i o n sg r a n t e dt ot h ed o m a i n (and its classes). For example, classes loaded from a trusted server might be placed in a

loaded from a trusted server might be placed in a protection domain that allows them to access les in the users home directory, whereas classes loaded from an untrusted server might have no le access permissions at all.648 Chapter 14 Protection It can be complicated for the JVMto determine what class is responsible for a request to access a protected resource .A c c e s s e sa r eo f t e np e r f o r m e di n d i r e c t l y , through system libraries or other classes. For example, consider a

or other classes. For example, consider a class that is not allowed to open network connections. It could call a system library to request the load of the contents of a URL.T h e JVM must decide whether or not to open a network connection for this request. But which class should be used to determine if the connection should be allowed, the application or the system library? The philosophy adopted in Java is to require the library class to explicitly permit a network connection. More generally,

permit a network connection. More generally, in order to access a protected resource, some method in the calling sequence that resulted in the request must explicitly assert the privilege to access the resource. By doing so, this method takes responsibility for the request. Presumably, it will also perform whatever checks are necessary to ensure the safety of the request. Of course, not every method is allowed to assert a privilege; a method can assert a privilege only if its class is in a

can assert a privilege only if its class is in a protection domain that is itself allowed to exercise the privilege. This implementation approach is called stack inspection .E v e r yt h r e a d in the JVM has an associated stack of its ongoing method invocations. When ac a l l e rm a yn o tb et r u s t e d ,am e t h o de x e c u t e sa na c c e s sr e q u e s tw i t h i na doPrivileged block to perform the access to a protected resource directly or indirectly. doPrivileged() is a static method

or indirectly. doPrivileged() is a static method in the AccessController class that is passed a class with a run() method to invoke. When the doPrivileged block is entered, the stack frame for this method is annotated to indicate this fact. Then, the contents of the block are executed. When an access to a protected resource is subsequently requested, either by this method or a method it calls, a call to checkPermissions() is used to invoke stack inspection to determine if the request should be

inspection to determine if the request should be allowed. The inspection examines stack frames on the calling threads stack, starting from the most recently added frame and working toward the oldest. If a stack frame is rst found that has the doPrivileged() annotation, then checkPermissions() returns immediately and silently, allowing the access. If a stack frame is rst found for which access is disallowed based on the protection domain of the methods class, then checkPermissions() throws an

methods class, then checkPermissions() throws an AccessControlException .I ft h es t a c k inspection exhausts the stack without nding either type of frame, then whether access is allowed depends on the implementation (for example, some implementations of the JVM may allow access, while other implementations may not). Stack inspection is illustrated in Figure 14.9. Here, the gui() method of ac l a s si nt h e untrusted applet protection domain performs two operations, rst a get() and then an

performs two operations, rst a get() and then an open() .T h ef o r m e ri sa ni n v o c a t i o no ft h e get() method of a class in the URL loader protection domain, which is permitted to open() sessions to sites in the lucent.com domain, in particular ap r o x ys e r v e r proxy.lucent.com for retrieving URLs. For this reason, the untrusted applets get() invocation will succeed: the checkPermissions() call in the networking library encounters the stack frame of the get() method, which

the stack frame of the get() method, which performed its open() in a doPrivileged block. However, the untrusted applets open() invocation will result in an exception, because thecheckPermissions() call nds no doPrivileged annotation before encountering the stack frame of the gui() method.14.10 Summary 649untrusted appletprotection domain: socket permission: class:none gui:  get(url); open(addr); networkin g any open(Addr a):  checkPermission (a, connect); connect (a); get(URL u):  doPrivile ged

connect); connect (a); get(URL u):  doPrivile ged  open(proxy.lucent.com:80);  H11021request u from proxy H11022 .lucent.com:80, connectURL loader Figure 14.9 Stack inspection. Of course, for stack inspection to work, a program must be unable to modify the annotations on its own stack frame or to otherwise manipulate stack inspection. This is one of the most important differences between Java and many other languages (including C). A Java program cannot directly access memory; it can manipulate

cannot directly access memory; it can manipulate only an object for which it has a reference. References cannot be forged, and manipulations are made only through well dened interfaces. Compliance is enfor ced through a sophisticated collection of loadtime and runtime checks. As a result, an object cannot manipulate its run time stack, because it cannot get a reference to the stack or other components of the protection system. More generally, Javas loadtime and runtime checks enforce type safety

loadtime and runtime checks enforce type safety of Java classes. Type safety ensures that classes cannot treat integers as pointers, write past the end of an array, or otherwise access memory in arbitrary ways. Rather, a program can access an object only via the methods dened on that object by its class. This is the foundation of Java protection, since it enables a class to effectively encapsulate and protect its data and methods from other classes loaded in the same JVM. For example, a variable

loaded in the same JVM. For example, a variable can be dened as private so that only the class that contains it can access it or protected so that it can be accessed only by the class that contains it, subclasses of that class, or classes in the same package. Type safety ensures that these restrictions can be enforced. 14.10 Summary Computer systems contain many objects, and they need to be protected from misuse. Objects may be hardware (such as memory, CPU time, and IOdevices) or software (such

memory, CPU time, and IOdevices) or software (such as les, programs, and semaphores). An access right is permission to perform an operation on an object. A domain is a set of access rights. Processes execute in domains and may use any of the access rights in the domain to access and manipulate objects. During its lifetime, a process may be either bound to a protection domain or allowed to switch from one domain to another.650 Chapter 14 Protection The access matrix is a general model of

Protection The access matrix is a general model of protection that provides a mechanism for protection without imposing a particular protection policy on the system or its users. The separation of policy and mechanism is an important design property. The access matrix is sparse. It is normally implemented either as access lists associated with each object or as capability lists associated with each domain. We can include dynamic protection in the accessmatrix model by considering domains and the

accessmatrix model by considering domains and the access matrix itself as objects. Revocation of access rights in a dynamic protection model is typically easier to implement with an accesslist scheme than with a capability list. Real systems are much more limited than the general model and tend to provide protection only for les. UNIX is representative, providing read, write, and execution protection separately for the owner, group, and general public for each le. MULTICS uses a ring structure

public for each le. MULTICS uses a ring structure in addition to le access. Hydra, the Cambridge CAP system, and Mach are capability systems that extend protection to userdened software objects. Solaris 10 implements the principle of least privilege via rolebased access control, a form of the access matrix. Languagebased protection provides nergrained arbitration of requests and privileges than the operating system is able to provide. For example, a single Java JVM can run several threads, each

a single Java JVM can run several threads, each in a different protection class. It enforces the resource requests through sophisticated stack inspection and via the type safety of the language. Practice Exercises 14.1 What are the main differences between capability lists and access lists? 14.2 AB u r r o u g h sB 7 0 0 0  B 6 0 0 0 MCP le can be tagged as sensitive data. When such a le is deleted, its storage area is overwritten by some random bits. For what purpose would such a scheme be

bits. For what purpose would such a scheme be useful? 14.3 In a ringprotection system, level 0 has the greatest access to objects, and level n(where n0) has fewer access rights. The access rights of ap r o g r a ma tap a r t i c u l a rl e v e li nt h er i n gs t r u c t u r ea r ec o n s i d e r e da set of capabilities. What is the relationship between the capabilities of ad o m a i na tl e v e l jand a domain at level ito an object (for ji)? 14.4 The RC 4000 system, among others, has dened a

14.4 The RC 4000 system, among others, has dened a tree of processes (called a process tree) such that all the descendants of a process can be given resources (objects) and access rights by their ancestors only. Thus, a descendant can never have the ability to do anything that its ancestors cannot do. The root of the tree is the operating system, which has the ability to do anything. Assume that the set of access rights is represented by an access matrix, A. A (x,y)d e  n e st h ea c c e s sr i

matrix, A. A (x,y)d e  n e st h ea c c e s sr i g h t so f process xto object y.Ifxis a descendant of z,what is the relationship between A(x,y)a n d A(z,y)f o ra na r b i t r a r yo b j e c t y? 14.5 What protection problems may arise if a shared stack is used for parameter passing?Exercises 651 14.6 Consider a computing environment where a unique number is associ ated with each process and each object in the system. Suppose that we allow a process with number nto access an object with number

with number nto access an object with number monly ifnm.What type of protection structure do we have? 14.7 Consider a computing environment where a process is given the privilege of accessing an object only ntimes. Suggest a scheme for implementing this policy. 14.8 If all the access rights to an object are deleted, the object can no longer be accessed. At this point, the object should also be deleted, and the space it occupies should be returned to the system. Suggest an efcient implementation

to the system. Suggest an efcient implementation of this scheme. 14.9 Why is it difcult to protect a system in which users are allowed to do their own IO? 14.10 Capability lists are usually kept within the address space of the user. How does the system ensure that the user cannot modify the contents of the list? Exercises 14.11 Consider the ringprotection scheme in MULTICS .I fw ew e r et oi m p l e  ment the system calls of a typical operating system and store them in a segment associated with

system and store them in a segment associated with ring 0, what should be the values stored in the ring eld of the segment descriptor? What happens during a system call when a process executing in a highernumbered ring invokes a procedure in ring 0? 14.12 The accesscontrol matrix can be used to determine whether a process can switch from, say, domain A to domain B and enjoy the access privileges of domain B. Is this approach equivalent to including the access privileges of domain B in those of

the access privileges of domain B in those of domain A? 14.13 Consider a computer system in which computer games can be played by students only between 10 P. M . and 6 A.M. ,b yf a c u l t ym e m b e r s between 5 P. M . and 8 A.M. ,a n db yt h ec o m p u t e rc e n t e rs t a f fa ta l l times. Suggest a scheme for implementing this policy efciently. 14.14 What hardware features does a computer system need for efcient capability manipulation? Can these features be used for memory protection?

Can these features be used for memory protection? 14.15 Discuss the strengths and weaknesses of implementing an access matrix using access lists that are associated with objects. 14.16 Discuss the strengths and weaknesses of implementing an access matrix using capabilities that are associated with domains. 14.17 Explain why a capabilitybased system such as Hydra provides greater exibility than the ringprotection scheme in enforcing protection policies.652 Chapter 14 Protection 14.18 Discuss the

Chapter 14 Protection 14.18 Discuss the need for rights amplication in Hydra. How does this practice compare with the crossring calls in a ringprotection scheme? 14.19 What is the needtoknow principle? Why is it important for a protec tion system to adhere to this principle? 14.20 Discuss which of the following systems allow module designers to enforce the needtoknow principle. a. The MULTICS ringprotection scheme b. Hydras capabilities c. JVMs stackinspection scheme 14.21 Describe how the Java

stackinspection scheme 14.21 Describe how the Java protection model would be compromised if a Java program were allowed to directly alter the annotations of its stack frame. 14.22 How are the accessmatrix facility and the rolebased accesscontrol facility similar? How do they differ? 14.23 How does the principle of least privilege aid in the creation of protection systems? 14.24 How can systems that implement the principle of least privilege still have protection failures that lead to security

have protection failures that lead to security violations? Bibliographical Notes The accessmatrix model of protection between domains and objects was developed by [Lampson (1969)] and [Lampson (1971)]. [Popek (1974)] and [Saltzer and Schroeder (1975)] provided excellent surveys on the subject of protection. [Harrison et al. (1976)] used a formal version of the access matrix model to enable them to prove properties of a protection system mathematically. The concept of a capability evolved from

The concept of a capability evolved from Iliffes and Jodeits codewords , which were implemented in the Rice University computer ([Iliffe and Jodeit (1962)]). The term capability was introduced by [Dennis and Horn (1966)]. The Hydra system was described by [Wulf et al. (1981)]. The CAP system was described by [Needham and Walker (1977)]. [Organick (1972)] discussed the MULTICS ringprotection system. Revocation was discussed by [Redell and Fabry (1974)], [Cohen and Jefferson (1975)], and

Fabry (1974)], [Cohen and Jefferson (1975)], and [Ekanadham and Bernstein (1979)]. The principle of separation of policy and mechanism was advocated by the designer of Hydra ([Levin et al. (1975)]). The connement problem was rst discussed by [Lampson (1973)] and was further examined by [Lipner (1975)]. The use of higherlevel languages for specifying access control was sug gested rst by [Morris (1973)], who proposed the use of the seal and unseal operations discussed in Section 14.9. [Kieburtz

operations discussed in Section 14.9. [Kieburtz and Silberschatz (1978)], [Kieburtz and Silberschatz (1983)], and [McGraw and Andrews (1979)] pro posed various language constructs for dealing with general dynamicresource management schemes. [Jones and Liskov (1978)] considered how a static accessBibliography 653 control scheme can be incorporated in a programming language that supports abstract data types. The use of minimal operatingsystem support to enforce protection was advocated by the

support to enforce protection was advocated by the Exokernel Project ([Ganger et al. (2002)], [Kaashoek et al. (1997)]). Extensibility of system code through languagebased protection mechanisms was discussed in [Bershad et al. (1995)]. Other tech niques for enforcing protection include sandboxing ([Goldberg et al. (1996)]) and software fault isolation ([Wahbe et al. (1993)]). The issues of lowering the overhead associated with protection costs and enabling userlevel access to networking devices

enabling userlevel access to networking devices were discussed in [McCanne and Jacobson (1993)] and [Basu et al. (1995)]. More detailed analyses of stack inspection, including comparisons with other approaches to Java security, can be found in [Wallach et al. (1997)] and [Gong et al. (1997)]. Bibliography [Basu et al. (1995)] A. Basu, V . Buch, W. Vogels, and T. von Eicken, UNet: AU s e r  L e v e lN e t w o r kI n t e r f a c ef o rP a r a l l e la n dD i s t r i b u t e dC o m p u t i n g ,

l e la n dD i s t r i b u t e dC o m p u t i n g , Proceedings of the ACM Symposium on Operating Systems Principles (1995). [Bershad et al. (1995)] B. N. Bershad, S. Savage, P . Pardyak, E. G. Sirer, M. Fiuczynski, D. Becker, S. Eggers, and C. Chambers, Extensibility, Safety and Performance in the SPIN Operating System ,Proceedings of the ACM Symposium on Operating Systems Principles (1995), pages 267284. [Cohen and Jefferson (1975)] E. S. Cohen and D. Jefferson, Protection in the Hydra

S. Cohen and D. Jefferson, Protection in the Hydra Operating System ,Proceedings of the ACM Symposium on Operating Systems Principles (1975), pages 141160. [Dennis and Horn (1966)] J. B. Dennis and E. C. V . Horn, Programming Seman tics for Multiprogrammed Computations ,Communications of the ACM ,V o l u m e 9, Number 3 (1966), pages 143155. [Ekanadham and Bernstein (1979)] K. Ekanadham and A. J. Bernstein, Con ditional Capabilities ,IEEE Transactions on Software Engineering ,V o l u m eS E  5 ,

on Software Engineering ,V o l u m eS E  5 , Number 5 (1979), pages 458464. [Ganger et al. (2002)] G. R. Ganger, D. R. Engler, M. F. Kaashoek, H. M. Briceno, R. Hunt, and T. Pinckney, Fast and Flexible ApplicationLevel Networking on Exokernel Systems ,ACM Transactions on Computer Systems ,V o l u m e2 0 ,N u m b e r 1( 2 0 0 2 ) ,p a g e s4 9  8 3 . [Goldberg et al. (1996)] I. Goldberg, D. Wagner, R. Thomas, and E. A. Brewer, AS e c u r eE n v i r o n m e n tf o rU n t r u s t e dH e l p e rA p

r o n m e n tf o rU n t r u s t e dH e l p e rA p p l i c a t i o n s ,Proceedings of the 6th Usenix Security Symposium (1996). [Gong et al. (1997)] L. Gong, M. Mueller, H. Prafullchandra, and R. Schemers, Going Beyond the Sandbox: An Overview of the New Security Architecture in the Java Development Kit 1.2 ,Proceedings of the USENIX Symposium on Internet Technologies and Systems (1997).654 Chapter 14 Protection [Harrison et al. (1976)] M. A. Harrison, W. L. Ruzzo, and J. D. Ullman, Protec tion

W. L. Ruzzo, and J. D. Ullman, Protec tion in Operating Systems ,Communications of the ACM ,V o l u m e1 9 ,N u m b e r8 (1976), pages 461471. [Iliffe and Jodeit (1962)] J. K. Iliffe and J. G. Jodeit, A Dynamic Storage Alloca tion System ,Computer Journal ,V o l u m e5 ,N u m b e r3( 1 9 6 2 ) ,p a g e s2 0 0  2 0 9 . [Jones and Liskov (1978)] A. K. Jones and B. H. Liskov, AL a n g u a g eE x t e n s i o n for Expressing Constraints on Data Access ,Communications of the ACM ,V o l u m e 21,

Access ,Communications of the ACM ,V o l u m e 21, Number 5 (1978), pages 358367. [Kaashoek et al. (1997)] M. F. Kaashoek, D. R. Engler, G. R. Ganger, H. M. Briceno, R. Hunt, D. Mazieres, T. Pinckney, R. Grimm, J. Jannotti, and K. Macken zie, Application Performance and Flexibility on Exokernel Systems ,Pro ceedings of the ACM Symposium on Operating Systems Principles (1997), pages 5265. [Kieburtz and Silberschatz (1978)] R. B. Kieburtz and A. Silberschatz, Capabil ity Managers ,IEEE

and A. Silberschatz, Capabil ity Managers ,IEEE Transactions on Software Engineering ,V o l u m eS E  4 ,N u m b e r 6 (1978), pages 467477. [Kieburtz and Silberschatz (1983)] R. B. Kieburtz and A. Silberschatz, Access Right Expressions ,ACM Transactions on Programming Languages and Systems , Volume 5, Number 1 (1983), pages 7896. [Lampson (1969)] B. W. Lampson, Dynamic Protection Structures ,Proceedings of the AFIPS Fall Joint Computer Conference (1969), pages 2738. [Lampson (1971)] B. W.

(1969), pages 2738. [Lampson (1971)] B. W. Lampson, Protection ,Proceedings of the Fifth Annual Princeton Conference on Information Systems Science (1971), pages 437443. [Lampson (1973)] B. W. Lampson, AN o t eo nt h eC o n  n e m e n tP r o b l e m , Communications of the ACM ,V o l u m e1 0 ,N u m b e r1 6( 1 9 7 3 ) ,p a g e s6 1 3  6 1 5 . [Levin et al. (1975)] R. Levin, E. S. Cohen, W. M. Corwin, F. J. Pollack, and W. A. Wulf, PolicyMechanism Separation in Hydra ,Proceedings of the ACM

Separation in Hydra ,Proceedings of the ACM Symposium on Operating Systems Principles (1975), pages 132140. [Lipner (1975)] S. Lipner, AC o m m e n to nt h eC o n  n e m e n tP r o b l e m ,Operating System Review ,V o l u m e9 ,N u m b e r5( 1 9 7 5 ) ,p a g e s1 9 2  1 9 6 . [McCanne and Jacobson (1993)] S. McCanne and V . Jacobson, The BSD Packet Filter: A New Architecture for Userlevel Packet Capture ,USENIX Winter (1993), pages 259270. [McGraw and Andrews (1979)] J. R. McGraw and G. R.

[McGraw and Andrews (1979)] J. R. McGraw and G. R. Andrews, Access Control in Parallel Programs ,IEEE Transactions on Software Engineering ,V o l u m e SE5, Number 1 (1979), pages 19. [Morris (1973)] J. H. Morris, Protection in Programming Languages ,Commu nications of the ACM ,V o l u m e1 6 ,N u m b e r1( 1 9 7 3 ) ,p a g e s1 5  2 1 . [Needham and Walker (1977)] R. M. Needham and R. D. H. Walker, The Cambridge CAP Computer and Its Protection System ,Proceedings of the Sixth Symposium on

System ,Proceedings of the Sixth Symposium on Operating System Principles (1977), pages 110.Bibliography 655 [Organick (1972)] E. I. Organick, The Multics System: An Examination of Its Structure ,M I TP r e s s( 1 9 7 2 ) . [Popek (1974)] G. J. Popek, Protection Structures ,Computer , Volume 7, Num ber 6 (1974), pages 2233. [Redell and Fabry (1974)] D. D. Redell and R. S. Fabry, Selective Revocation of Capabilities ,Proceedings of the IRIA International Workshop on Protection in Operating

International Workshop on Protection in Operating Systems (1974), pages 197210. [Saltzer and Schroeder (1975)] J. H. Saltzer and M. D. Schroeder, The Protec tion of Information in Computer Systems ,Proceedings of the IEEE (1975), pages 12781308. [Wahbe et al. (1993)] R. Wahbe, S. Lucco, T. E. Anderson, and S. L. Graham, Efcient SoftwareBased Fault Isolation ,ACM SIGOPS Operating Systems Review ,V o l u m e2 7 ,N u m b e r5( 1 9 9 3 ) ,p a g e s2 0 3  2 1 6 . [Wallach et al. (1997)] D. S.

g e s2 0 3  2 1 6 . [Wallach et al. (1997)] D. S. Wallach, D. Balfanz, D. Dean, and E. W. Felten, Extensible Security Architectures for Java ,Proceedings of the ACM Symposium on Operating Systems Principles (1997), pages 116128. [Wulf et al. (1981)] W. A. Wulf, R. Levin, and S. P. Harbison, HydraC.mmp: An Experimental Computer System ,M c G r a w  H i l l( 1 9 8 1 ) .15CHAPTER Security Protection, as we discussed in Chapter 14, is strictly an internal problem: How do we provide controlled access

problem: How do we provide controlled access to programs and data stored in a computer system? Security , on the other hand, requires not only an adequate protection system but also consideration of the external environment within which the system operates. A protection system is ineffective if user authentication is compromised or a program is run by an unauthorized user. Computer resources must be guarded against unauthorized access, mali cious destruction or alteration, and accidental

cious destruction or alteration, and accidental introduction of inconsistency. These resources include information stored in the system (both data and code), as well as the CPU, memory, disks, tapes, and networking that are the com puter. In this chapter, we start by examining ways in which resources may be accidentally or purposely misused. We then explore a key security enabler cryptography . Finally , we look at mechanisms to guard against or detect attacks. CHAPTER OBJECTIVES To discuss

or detect attacks. CHAPTER OBJECTIVES To discuss security threats and attacks. To explain the fundamentals of encryption, authentication, and hashing. To examine the uses of cryptography in computing. To describe various countermeasures to security attacks. 15.1 The Security Problem In many applications, ensuring the security of the computer system is worth considerable effort. Large commercial systems containing payroll or other nancial data are inviting targets to thieves. Systems that contain

inviting targets to thieves. Systems that contain data pertain ing to corporate operations may be of interest to unscrupulous competitors. Furthermore, loss of such data, whether by accident or fraud, can seriously impair the ability of the corporation to function. In Chapter 14, we discussed mechanisms that the operating system can provide (with appropriate aid from the hardware) that allow users to protect 657658 Chapter 15 Security their resources, including programs an d data. These

resources, including programs an d data. These mechanisms work well only as long as the users conform to the intended use of and access to these resources. We say that a system is secure if its resources are used and accessed as intended under all circumstances. Unfortunately, total security cannot be achieved. Nonetheless, we must have mechanisms to make security breaches ar a r eo c c u r r e n c e ,r a t h e rt h a nt h en o r m . Security violations (or misuse) of the system can be

violations (or misuse) of the system can be categorized as inten tional (malicious) or accidental. It is easier to protect against accidental misuse than against malicious misuse. For the most part, protection mechanisms are the core of protection from acciden ts. The following list includes several forms of accidental and malicious security violations. We should note that in our dis cussion of security, we use the terms intruder and cracker for those attempting to breach security. In addition,

those attempting to breach security. In addition, a threat is the potential for a security violation, such as the discovery of a vulnerability, whereas an attack is the attempt to break security. Breach of condentiality .T h i st y p eo fv i o l a t i o ni n v o l v e su n a u t h o r i z e d reading of data (or theft of information). Typically, a breach of conden tiality is the goal of an intruder. Capturing secret data from a system or ad a t as t r e a m ,s u c ha sc r e d i t  c a r di n f o

t r e a m ,s u c ha sc r e d i t  c a r di n f o r m a t i o no ri d e n t i t yi n f o r m a t i o nf o r identity theft, can result directly in money for the intruder. Breach of integrity .T h i sv i o l a t i o ni n v o l v e su n a u t h o r i z e dm o d i  c a t i o n of data. Such attacks can, for example, result in passing of liability to an innocent party or modication of the source code of an important commercial application. Breach of availability .T h i sv i o l a t i o ni n v o l v e

.T h i sv i o l a t i o ni n v o l v e su n a u t h o r i z e dd e s t r u c t i o no f data. Some crackers would rather wreak havoc and gain status or bragging rights than gain nancially. Website defacement is a common example of this type of security breach. Theft of service .T h i sv i o l a t i o ni n v o l v e su n a u t h o r i z e du s eo fr e s o u r c e s . For example, an intruder (or intrusion program) may install a daemon on as y s t e mt h a ta c t sa sa l es e r v e r . Denial of

t e mt h a ta c t sa sa l es e r v e r . Denial of service .T h i sv i o l a t i o ni n v o l v e sp r e v e n t i n gl e g i t i m a t eu s eo f the system. Denialofservice (DOS )attacks are sometimes accidental. The original Internet worm turned into a DOS attack when a bug failed to delay its rapid spread. We discuss DOS attacks further in Section 15.3.3. Attackers use several standard methods in their attempts to breach security. The most common is masquerading ,i nw h i c ho n ep a r t i c

is masquerading ,i nw h i c ho n ep a r t i c i p a n ti n a communication pretends to be someone else (another host or another person). By masquerading, attackers breach authentication ,t h ec o r r e c t n e s s of identication; they can then gain access that they would not normally be allowed or escalate their privilegesobtain privileges to which they would not normally be entitled. Another common attack is to replay a captured exchange of data. A replay attack consists of the malicious or

data. A replay attack consists of the malicious or fraudulent repeat of a valid data transmission. Sometimes the replay comprises the entire attack for example, in a repeat of a request to transfer money. But frequently it is done along with message modication ,a g a i nt oe s c a l a t ep r i v i l e g e s .C o n s i d e r15.1 The Security Problem 659 communication communication communicationcommunicationsender receiver attacker sender receiver attackersender receiver attacker Masquerading

attackersender receiver attacker Masquerading ManinthemiddleNormal Figure 15.1 Standard security attacks. the damage that could be done if a request for authentication had a legitimate users information replaced with an unauthorized users. Yet another kind of attack is the maninthemiddle attack , in which an attacker sits in the data ow of a communication, masquerading as the sender to the receiver, and vice versa. In a network communication, a maninthemiddle attack may be preceded by a session

maninthemiddle attack may be preceded by a session hijacking ,i nw h i c ha na c t i v ec o m m u n i c a t i o ns e s s i o ni s intercepted. Several attack methods are depicted in Figure 15.1. As we have already suggested, absolute protection of the system from malicious abuse is not possible, but the cost to the perpetrator can be made sufciently high to deter most intruders. In some cases, such as a denialof service attack, it is preferable to prevent t he attack but sufcient to detect the

to prevent t he attack but sufcient to detect the attack so that countermeasures can be taken. To protect a system, we must take security measures at four levels: 1.Physical . The site or sites containing the computer systems must be physically secured against armed or surreptitious entry by intruders. Both the machine rooms and the terminals or workstations that have access to the machines must be secured.660 Chapter 15 Security 2.Human .A u t h o r i z a t i o nm u s tb ed o n ec a r e f u l l

o r i z a t i o nm u s tb ed o n ec a r e f u l l yt oa s s u r et h a to n l y appropriate users have access to the system. Even authorized users, however, may be encouraged to let others use their access (in exchange for a bribe, for example). They may also be tricked into allowing access via social engineering .O n et y p eo fs o c i a l  e n g i n e e r i n ga t t a c k isphishing .H e r e ,al e g i t i m a t e  l o o k i n ge  m a i lo rw e bp a g em i s l e a d s au s e ri n t oe n t e r i

bp a g em i s l e a d s au s e ri n t oe n t e r i n gc o n  d e n t i a li n f o r m a t i o n .A n o t h e rt e c h n i q u ei s dumpster diving ,ag e n e r a lt e r mf o ra t t e m p t i n gt og a t h e ri n f o r m a t i o ni n order to gain unauthorized access to the computer (by looking through trash, nding phone books, or nding notes containing passwords, for example). These security problems are management and personnel issues, not problems pertaining to operating systems. 3.Operating

pertaining to operating systems. 3.Operating system .T h es y s t e mm u s tp r o t e c ti t s e l ff r o ma c c i d e n t a lo r purposeful security breaches. A runaway process could constitute an accidental denialofservice attack. A query to a service could reveal pass words. A stack overow could allow the launching of an unauthorized process. The list of possible breaches is almost endless. 4.Network . Much computer data in modern systems travels over private leased lines, shared lines like

over private leased lines, shared lines like the Internet, wireless connections, or dialup lines. Intercepting these data could be just as harmful as breaking into a computer, and interruption of communications could constitute a remote denialofservice attack, diminishing users use of and trust in the system. Security at the rst two levels must be maintained if operatingsystem security is to be ensured. A weakness at a high level of security (physical or human) allows circumvention of strict

(physical or human) allows circumvention of strict lowlevel (operatingsystem) security measures. Thus, the old adage that a chain is only as strong as its weakest link is especially true of system security. All of these aspects must be addressed for security to be maintained. Furthermore, the system must provide protection (Chapter 14) to allow the implementation of security features. Without the ability to authorize users and processes, to control their access, and to log their activities, it

their access, and to log their activities, it would be impossible for an operating system to implement security measures or to run securely. Hardware protection features are needed to support an overall protection scheme. For example, a system without memory protection cannot be secure. New hardware features are allowing systems to be made more secure, as we shall discuss. Unfortunately, little in security is straightforward. As intruders exploit security vulnerabilities, security

exploit security vulnerabilities, security countermeasures are created and deployed. This causes intruders to become more sophisticated in their attacks. For example, recent security incidents include the use of spyware to provide ac o n d u i tf o rs p a mt h r o u g hi n n o c e n ts y s t e m s( w ed i s c u s st h i sp r a c t i c ei n Section 15.2). This catandmouse game is likely to continue, with more security tools needed to block the escalatin gi n t r u d e rt e c h n i q u e sa n da c

gi n t r u d e rt e c h n i q u e sa n da c t i v i t i e s . In the remainder of this chapter, we address security at the network and operatingsystem levels. Security at the physical and human levels, although important, is for the most part beyond the scope of this text. Security within the operating system and between operating systems is implemented in several15.2 Program Threats 661 ways, ranging from passwords for authentication through guarding against viruses to detecting intrusions. We

against viruses to detecting intrusions. We start with an exploration of security threats. 15.2 Program Threats Processes, along with the kernel, are the only means of accomplishing work on a computer. Therefore, writing a program that creates a breach of security, or causing a normal process to change its behavior and create a breach, is a common goal of crackers. In fact, even most nonprogram security events have as their goal causing a program threat. For example, while it is useful to log in

threat. For example, while it is useful to log in to a system without authorization, it is quite a lot more useful to leave behind abackdoor daemon that provides information or allows easy access even if the original exploit is blocked. In this section, we describe common methods by which programs cause security breaches. Note that there is considerable variation in the naming conventions for security holes and that we use the most common or descriptive terms. 15.2.1 Trojan Horse Many systems

terms. 15.2.1 Trojan Horse Many systems have mechanisms for allowing programs written by users to be executed by other users. If these programs are executed in a domain that provides the access rights of the executing user, the other users may misuse these rights. A texteditor program, for example, may include code to search the le to be edited for certain keywords. If any are found, the entire le may be copied to a special area accessible to the creator of the text editor. Ac o d es e g m e n

creator of the text editor. Ac o d es e g m e n tt h a tm i s u s e si t se n v i r o n m e n ti sc a l l e da Trojan horse .L o n g search paths, such as are common on UNIX systems, exacerbate the Trojan horse problem. The search path lists the set of directories to search when an ambiguous program name is given. The path is searched for a le of that name, and the le is executed. All the directories in such a search path must be secure, or a Trojan horse could be slipped into the users path and

horse could be slipped into the users path and executed accidentally. For instance, consider the use of the .character in a search path. The . tells the shell to include the current directory in the search. Thus, if a user has .in her search path, has set her current directory to a friends directory, and enters the name of a normal system command, the command may be executed from the friends directory. The program will run within the users domain, allowing the program to do anything that the

allowing the program to do anything that the user is allowed to do, including deleting the users les, for instance. Av a r i a t i o no ft h eT r o j a nh o r s ei sap r o g r a mt h a te m u l a t e sal o g i np r o g r a m . An unsuspecting user starts to log in at a terminal and notices that he has apparently mistyped his password. He tries again and is successful. What has happened is that his authentication key and password have been stolen by the login emulator, which was left running on

by the login emulator, which was left running on the terminal by the thief. The emulator stored away the password, printed out a login error message, and exited; the user was then provided with a genuine login prompt. This type of attack can be defeated by having the operating system print a usage message at the end of an interactive session or by a nontrappable key sequence,662 Chapter 15 Security such as the controlaltdelete combination used by all modern Windows operating systems. Another

by all modern Windows operating systems. Another variation on the Trojan horse is spyware .S p y w a r es o m e t i m e s accompanies a program that the user has chosen to install. Most frequently, it comes along with freeware or shareware programs, but sometimes it is included with commercial software. The goal of spyware is to download ads to display on the users system, create popup browser windows when certain sites are visited, or capture information from the users system and return it to a

from the users system and return it to a central site. This latter practice is an example of a general category of attacks known as covert channels , in which surreptitious communication occurs. For example, the installation of an innocuousseeming program on a Windows system could result in the loading of a spyware daemon. The spyware could contact a central site, be given a message and a list of recipient addresses, and deliver a spam message to those users from the Windows machine. This

to those users from the Windows machine. This process continues until the user discovers the spyware. Frequently, the spyware is not discovered. In 2010, it was estimated that 90 percent of spam was being delivered by this method. This theft of service is not even considered a crime in most countries! Spyware is a micro example of a macro problem: violation of the principle of least privilege. Under most circumstances, a user of an operating system does not need to install network daemons. Such

does not need to install network daemons. Such daemons are installed via two mistakes. First, a user may run with more privileges than necessary (for example, as the administrator), allowing programs that she runs to have more access to the system than is necessary. This is a case of human errora common security weakness. Second, an operating system may allow by default more privileges than a normal user needs. This is a case of poor operatingsystem design decisions. An operating system (and,

design decisions. An operating system (and, indeed, software in general) should allow negrained control of access and security, but it must also be easy to manage and understand. Inconvenient or inadequate security measures are bound to be circumvented, causing an o verall weakening of the security they were designed to implement. 15.2.2 Trap Door The designer of a program or system might leave a hole in the software that only she is capable of using. This type of security breach (or trap door

using. This type of security breach (or trap door )w a ss h o w ni n the movie War Games .F o ri n s t a n c e ,t h ec o d em i g h tc h e c kf o ras p e c i  cu s e r IDor password, and it might circumvent normal security procedures. Programmers have been arrested for embezzling from banks by including rounding errors in their code and having the occasional halfcent credited to their accounts. This account crediting can add up to a large amount of money, considering the number of transactions

of money, considering the number of transactions that a large bank executes. A clever trap door could be included in a compiler. The compiler could generate standard object code as well as a trap door, regardless of the source code being compiled. This activity is particularly nefarious, since a search of the source code of the program will not reveal any problems. Only the source code of the compiler would contain the information. Trap doors pose a difcult problem because, to detect them, we

pose a difcult problem because, to detect them, we have to analyze all the source code for all components of a system. Given that software systems may consist of millions of lines of code, this analysis is not done frequently, and frequently it is not done at all!15.2 Program Threats 663 15.2.3 Logic Bomb Consider a program that initiates a security incident only under certain circumstances. It would be hard to detect because under normal operations, there would be no security hole. However,

there would be no security hole. However, when a predened set of parameters was met, the security hole would be created. This scenario is known as a logic bomb .Ap r o g r a m m e r ,f o re x a m p l e ,m i g h tw r i t ec o d et od e t e c tw h e t h e rh e was still employed; if that check failed, a daemon could be spawned to allow remote access, or code could be launched to cause damage to the site. 15.2.4 Stack and Buffer Overow The stack or bufferoverow attack is the most common way for an

bufferoverow attack is the most common way for an attacker outside the system, on a network or dialup connection, to gain unauthorized access to the target system. An authorized user of the system may also use this exploit for privilege escalation. Essentially, the attack exploits a bug in a program. The bug can be a simple case of poor programming, in which the programmer neglected to code bounds checking on an input eld. In this case, the attacker sends more data than the program was

the attacker sends more data than the program was expecting. By using trial and error, or by examining the source code of the attacked program if it is available, the attacker determines the vulnerability and writes a program to do the following: 1.Overow an input eld, commandline argument, or input bufferfor example, on a network daemonuntil it writes into the stack. 2.Overwrite the current return address on the stack with the address of the exploit code loaded in step 3. 3.Write a simple set

code loaded in step 3. 3.Write a simple set of code for the next space in the stack that includes the commands that the attacker wishes to executefor instance, spawn as h e l l . The result of this attack programs execution will be a root shell or other privileged command execution. For instance, if a webpage form expec ts a user name to be entered into a eld, the attacker could send the user name, plus extra characters to overow the buffer and reach the stack, plus a new return address to load

reach the stack, plus a new return address to load onto the stack, plus the code the attacker wants to run. When the bufferreading subroutine returns from execution, the return address is the exploit code, and the code is run. Lets look at a bufferoverow exploit in more detail. Consider the simple C program shown in Figure 15.2. This program creates a character array of size BUFFER SIZE and copies the contents of the parameter provided on the command line argv[1] . As long as the size of this

command line argv[1] . As long as the size of this parameter is less than BUFFER SIZE (we need one byte to store the null terminator), this program works properly. But consider what happens if the parameter provided on the command line is longer than BUFFER SIZE .I nt h i ss c e n a r i o ,t h e strcpy() function will begin copying from argv[1] until it encounters a null terminator (0) or until the program crashes. Thus, this program suffers from a potential bufferoverow problem in which copied

a potential bufferoverow problem in which copied data overow the buffer array.664 Chapter 15 Security include stdio.h  define BUFFER SIZE 256 int main(int argc, char argv[])  char buffer[BUFFER SIZE]; if (argc  2) return 1; else  strcpy(buffer,argv[1]); return 0;   Figure 15.2 Cp r o g r a mw i t hb u f f e r  o v e r  o wc o n d i t i o n . Note that a careful programmer could have performed bounds checking on the size of argv[1] by using the strncpy() function rather than strcpy() , replacing

function rather than strcpy() , replacing the line strcpy(buffer, argv[1]); with strncpy(buffer, argv[1], sizeof(buffer)1); .U n f o r t u n a t e l y ,g o o db o u n d sc h e c k i n gi s the exception rather than the norm. Furthermore, lack of bounds checking is not the only possible cause of the behavior of the program in Figure 15.2. The program could instead have been carefully designed to compromise the integrity of the system. We now consider the possible security vulnerabilities of a

the possible security vulnerabilities of a buffer overow. When a function is invoked in a typic al computer architecture, the variables dened locally to the function (sometimes known as automatic variables ), the parameters passed to the function, and the address to which control returns once the function exits are stored in a stack frame .T h el a y o u tf o rat y p i c a ls t a c k frame is shown in Figure 15.3. Examining the stack frame from top to bottom, we rst see the parameters passed to

top to bottom, we rst see the parameters passed to the function, followed by any automatic variables declared in the function. We next see the frame pointer ,w h i c hi s the address of the beginning of the stack frame. Finally, we have the return parameter(s)bottom grows topautomatic variablessaved frame pointer frame pointer return address Figure 15.3 The layout for a typical stack frame.15.2 Program Threats 665 address, which species where to return control once the function exits. The frame

return control once the function exits. The frame pointer must be saved on the stack, as the value of the stack pointer can vary during the function call. The saved frame pointer allows relative access to parameters and automatic variables. Given this standard memory layout, a cracker could execute a buffer overow attack. Her goal is to replace the return address in the stack frame so that it now points to the code segment containing the attacking program. The programmer rst writes a short code

program. The programmer rst writes a short code segment such as the following: include stdio.h int main(int argc, char argv[])  execvp( binsh, binsh, NULL); return 0;  Using the execvp() system call, this code segment creates a shell process. If the program being attacked runs with systemwide permissions, this newly created shell will gain complete access to the system. Of course, the code segment could do anything allowed by the privileges of the attacked process. This code segment is then

of the attacked process. This code segment is then compiled so that the assembly language instructions can be modied. The primary modication is to remove unnecessary features in the code, thereby reducing the code size so that it can t into a stack frame. This assembled code fragment is now a binary sequence that will be at the heart of the attack. Refer again to the program shown in Figure 15.2. Lets assume that when themain() function is called in that program, the stack frame appears as shown

in that program, the stack frame appears as shown in Figure 15.4(a). Using a debugger, the programmer then nds the buffer(0)buffer(1)buffer(BUFFERSIZE  1)saved frame pointer copiedreturn address modified shell codeNO OP address of modified shell code          (a) (b) Figure 15.4 Hypothetical stack frame for Figure 15.2, (a) before and (b) after.666 Chapter 15 Security address of buffer[0] in the stack. That address is the location of the code the attacker wants executed. The binary sequence is

attacker wants executed. The binary sequence is appended with the necessary amount of NOOP instructions (for NOOP eration )t o l lt h es t a c kf r a m eu p to the location of the return address, and the location of buffer[0] ,t h en e w return address, is added. The attack is complete when the attacker gives this constructed binary sequence as input to the process. The process then copies the binary sequence from argv[1] to position buffer[0] in the stack frame. Now, when control returns from

in the stack frame. Now, when control returns from main() ,i n s t e a do fr e t u r n i n gt ot h el o c a t i o n specied by the old value of the return address, we return to the modied shell code, which runs with the access rights of the attacked process! Figure 15.4(b) contains the modied shell code. There are many ways to exploit potential bufferoverow problems. In this example, we considered the possibility that the program being attacked the code shown in Figure 15.2ran with systemwide

the code shown in Figure 15.2ran with systemwide permissions. However, the code segment that runs once the value of the return address has been modied might perform any type of malicious act, such as deleting les, opening network ports for further exploitation, and so on. This example bufferoverow attack r eveals that considerable knowledge and programming skill are needed to recognize exploitable code and then to exploit it. Unfortunately, it does not take great programmers to launch security

does not take great programmers to launch security attacks. Rather, one cracker can determine the bug and then write an exploit. Anyone with rudimentary computer skills and access to the exploit as o  c a l l e d script kiddie can then try to launch the attack at target systems. The bufferoverow attack is especially pernicious because it can be run between systems and can travel over allowed communication channels. Such attacks can occur within protocols that are expected to be used to

within protocols that are expected to be used to communicate with the target machine, and they can therefore be hard to detect and prevent. They can even bypass the security added by rewalls (Section 15.7). One solution to this problem is for the CPU to have a feature that disallows execution of code in a stack section of memory. Recent versions of Suns SPARC chip include this setting, and recent versions of Solaris enable it. The return address of the overowed routine can still be modied; but

of the overowed routine can still be modied; but when the return address is within the stack and the c ode there attempts to execute, an exception is generated, and the program is halted with an error. Recent versions of AMD and Intel x86 chips include the NXfeature to prevent this type of attack. The use of the feature is supported in several x86 operating systems, including Linux and Windows XP SP2 .T h eh a r d w a r ei m p l e m e n t a t i o n involves the use of a new bit in the page

t i o n involves the use of a new bit in the page tables of the CPUs. This bit marks the associated page as nonexecutable, so that instructions cannot be read from it and executed. As this feature becomes mor e prevalent, bufferoverow attacks should greatly diminish. 15.2.5 Viruses Another form of program threat is a virus .Av i r u si saf r a g m e n to fc o d ee m b e d  ded in a legitimate program. Viruses are selfreplicating and are designed to infect other programs. They can wreak havoc in

to infect other programs. They can wreak havoc in a system by modifying or destroying les and causing system crashes and program malfunctions. As with most penetration attacks, viruses are very specic to architectures, oper ating systems, and applications. Viruses are a particular problem for users of15.2 Program Threats 667 PCs.UNIX and other multiuser operating systems generally are not susceptible to viruses because the executable programs are protected from writing by the operating system.

protected from writing by the operating system. Even if a virus does infect such a program, its powers usually are limited because other aspects of the system are protected. Viruses are usually borne via email, with spam the most common vector. They can also spread when users download viral programs from Internet lesharing services or exchange infected disks. Another common form of virus transmission uses Microsoft Ofce les, such as Microsoft Word documents. These documents can contain macros

Word documents. These documents can contain macros (or Visual Basic programs) that programs in the Ofce suite (Word, PowerPoint, and Excel) will execute automatically. Bec ause these programs run under the users own account, the macros can run largely unconstrained (for example, deleting user les at will). Commonly, the virus will also email itself to others in the users contact list. Here is a code sample that shows how simple it is to write a Visual Basic macro that a virus could use to format

Basic macro that a virus could use to format the hard drive of a Windows computer as soon as the le containing the macro was opened: Sub AutoOpen() Dim oFS Set oFS  CreateObject(Scripting.FileSystemObject) vs  Shell(c: command.com k format c:,vbHide) End Sub How do viruses work? Once a virus reaches a target machine, a program known as a virus dropper inserts the virus into the system. The virus dropper is usually a Trojan horse, executed for other reasons but installing the virus as its core

other reasons but installing the virus as its core activity. Once installed, the virus may do any one of a number of things. There are literally thousands of viruses, but they fall into several main categories. Note that many viruses belong to more than one category. File.As t a n d a r d l ev i r u si n f e c t sas y s t e mb ya p p e n d i n gi t s e l ft oa l e . It changes the start of the program so that execution jumps to its code. After it executes, it returns control to the program so

it executes, it returns control to the program so that its execution is not noticed. File viruses are sometimes known as parasitic viruses, as they leave no full les behind and leave the host program still functional. Boot . A boot virus infects the boot sector of the system, executing every time the system is booted and before the operating system is loaded. It watches for other bootable media and infe cts them. These viruses are also known as memory viruses, because they do not appear in the

memory viruses, because they do not appear in the le system. Figure 15.5 shows how a boot virus works. Macro .M o s tv i r u s e sa r ew r i t t e ni nal o w  l e v e ll a n g u a g e ,s u c ha sa s s e m b l y or C. Macro viruses are written in a highlevel language, such as Visual Basic. These viruses are triggered wh en a program capable of executing the macro is run. For example, a macro virus could be contained in a spreadsheet le. Source code .As o u r c ec o d ev i r u sl o o k sf o rs o u

.As o u r c ec o d ev i r u sl o o k sf o rs o u r c ec o d ea n dm o d i  e si tt o include the virus and to help spread the virus.668 Chapter 15 Security whenever new removable RW disk is installed, it infects that as wellit has a logic bomb to wreak havoc at a certain datevirus replaces original boot block with itself at system boot, virus decreases physical memory, hides in memory above new limit virus attaches to disk read write interrupt, monitors all disk activity it blocks any attempts

monitors all disk activity it blocks any attempts of other programs to write the boot sectorvirus copies boot sector to unused location X Figure 15.5 Ab o o t  s e c t o rc o m p u t e rv i r u s . Polymorphic . A polymorphic virus changes each time it is installed to avoid detection by antivirus software. The changes do not affect the viruss functionality but rather change the viruss signature. A virus signature is ap a t t e r nt h a tc a nb eu s e dt oi d e n t i f yav i r u s ,t y p i c a l

s e dt oi d e n t i f yav i r u s ,t y p i c a l l yas e r i e so fb y t e st h a t make up the virus code. Encrypted .A ne n c r y p t e dv i r u si n c l u d e sd e cryption code along with the encrypted virus, again to avoid detection .T h ev i r u s r s td e c r y p t sa n dt h e n executes. Stealth .T h i st r i c k yv i r u sa t t e m p t st oa v o i dd e t e c t i o nb ym o d i f y i n gp a r t s of the system that could be used to detect it. For example, it could modify theread system

it. For example, it could modify theread system call so that if the le it has modied is read, the original form of the code is returned rather than the infected code. Tu n n e l i n g . This virus attempts to bypass detection by an antivirus scanner by installing itself in the interrupthandler chain. Similar viruses install themselves in device drivers.15.3 System and Network Threats 669 Multipartite .Av i r u so ft h i st y p ei sa b l et oi n f e c tm u l t i p l ep a r t so fas y s t e m ,

n f e c tm u l t i p l ep a r t so fas y s t e m , including boot sectors, memory, and les. This makes it difcult to detect and contain. Armored .A na r m o r e dv i r u si sc o d e dt om a k ei th a r df o ra n t i v i r u s researchers to unravel and understand. It can also be compressed to avoid detection and disinfection. In addition, vir us droppers and other full les that are part of a virus infestation are frequently hidden via le attributes or unviewable le names. This vast variety of

or unviewable le names. This vast variety of viruses has continued to grow. For example, in 2004 an e wa n dw i d e s p r e a dv i r u sw a sd e t e c t e d .I te x p l o i t e dt h r e es e p a r a t eb u g s for its operation. This virus started by in fecting hundreds of Windows servers (including many trusted sites) running Microsoft Internet Information Server (IIS). Any vulnerable Microsoft Explorer web browser visiting those sites received a browser virus with any download. The browser

a browser virus with any download. The browser virus installed several backdoor programs, including a keystroke logger ,w h i c hr e c o r d s everything entered on the keyboard (in cluding passwords and creditcard numbers). It also installed a daemon to allow unlimited remote access by an intruder and another that allowed an intruder to route spam through the infected desktop computer. Generally, viruses are the most disruptive security attacks, and because they are effective, they will

attacks, and because they are effective, they will continue to be written and to spread. An active securityrelated debate within the computing community concerns the exis tence of a monoculture ,i nw h i c hm a n ys y s t e m sr u nt h es a m eh a r d w a r e , operating system, and application software. This monoculture supposedly consists of Microsoft products. One question is whether such a monoculture even exists today. Another question is whether, if it does, it increases the threat of and

if it does, it increases the threat of and damage caused by viruses and other security intrusions. 15.3 System and Network Threats Program threats typically use a breakdown in the protection mechanisms of a system to attack programs. In contrast, system and network threats involve the abuse of services and network connections. S ystem and network threats create as i t u a t i o ni nw h i c ho p e r a t i n g  s y s t e mr e s o u r c e sa n du s e r l e sa r em i s u s e d . Sometimes, a system

e r l e sa r em i s u s e d . Sometimes, a system and network attack is used to launch a program attack, and vice versa. The more open an operating system isthe more services it has enabled and the more functions it allowsthe more likely it is that a bug is available to exploit. Increasingly, operating systems strive to be secure by default . For example, Solaris 10 moved from a model in which many services ( FTP, telnet, and others) were enabled by default when the system was installed to a

by default when the system was installed to a model in which almost all services are disabled at installation time and must specically be enabled by system administrators. Such changes reduce the systems attack surface the set of ways in which an attacker can try to break into the system. In the remainder of this section, we discuss some examples of system and network threats, including worms, port scanning, and denialofservice670 Chapter 15 Security attacks. It is important to note that

15 Security attacks. It is important to note that masquerading and replay attacks are also commonly launched over networks between systems. In fact, these attacks are more effective and harder to counter when multiple systems are involved. For example, within a computer, the operating system usually can determine the sender and receiver of a message. E ven if the sender changes to the IDof someone else, there may be a record of that IDchange. When multiple systems are involved, especially

When multiple systems are involved, especially systems controlled by attackers, then such tracing is much more difcult. In general, we can say that sharing secrets (to prove identity and as keys to encryption) is required for authentication and encryption, and sharing secrets is easier in environments (such as a single operating system) in which secure sharing methods exist. These methods i nclude shared memory and interpro cess communications. Creating secure communication and authentication is

secure communication and authentication is discussed in Section 15.4 and Section 15.5. 15.3.1 Worms Aworm is a process that uses the spawn mechanism to duplicate itself. The worm spawns copies of itself, using up system resources and perhaps locking out all other processes. On computer networks, worms are particularly potent, since they may reproduce themselves among systems and thus shut down an entire network. Such an event occurred in 1988 to UNIX systems on the Internet, causing the loss of

UNIX systems on the Internet, causing the loss of system and systemadministrator time worth millions of dollars. At the close of the workday on November 2, 1988, Robert Tappan Morris, Jr., a rstyear Cornell graduate student, unleashed a worm program on one or more hosts connected to the Internet. Targeting Sun Microsystems Sun 3 workstations and VAX computers running variants of Version 4 BSD UNIX ,t h e worm quickly spread over great distances. Within a few hours of its release, it had consumed

Within a few hours of its release, it had consumed system resources to the point of bringing down the infected machines. Although Morris designed the selfreplicating program for rapid reproduc tion and distribution, some of the features of the UNIX networking environment provided the means to propagate the worm throughout the system. It is likely that Morris chose for initial infection an Internet host left open for and accessible to outside users. From there, the worm program exploited aws in

From there, the worm program exploited aws in the UNIX operating systems security routines and took advantage of UNIX utilities that simplify resource sharing in localarea networks to gain unauthorized access to thousands of other connected sites. Morriss methods of attack are outlined next. The worm was made up of two programs, a grappling hook (also called abootstrap orvector )p r o g r a ma n dt h em a i np r o g r a m .N a m e d l1.c ,t h e grappling hook consisted of 99 lines of C code

h e grappling hook consisted of 99 lines of C code compiled and run on each machine it accessed. Once established on the computer system under attack, the grappling hook connected to the machine where it originated and uploaded ac o p yo ft h em a i nw o r mo n t ot h e hooked system (Figure 15.6). The main program proceeded to search for other machines to which the newly infected system could connect easily. In these actions, Morris exploited the UNIX networking utility rshfor easy remote task

UNIX networking utility rshfor easy remote task execution. By setting up special les that list hostlogin name pairs, users can omit entering a password each time15.3 System and Network Threats 671grappling hook worm target systemworm infected systemrsh attack finger attack sendmail attack request for worm worm sent Figure 15.6 The Morris Internet worm. they access a remote account on the pair ed list. The worm searched these special les for site names that would allow remote execution without a

names that would allow remote execution without a password. Where remote shells were established ,t h ew o r mp r o g r a mw a su p l o a d e da n d began executing anew. The attack via remote access was one of three infection methods built into the worm. The other two methods involved operatingsystem bugs in the UNIX finger andsendmail programs. The finger utility functions as an electronic telephone directory. The command finger usernamehostname returns a persons real and login names along

returns a persons real and login names along with other information that the user may have provided, such as ofce and home address and telephone number, research plan, or clever quotation. Finger runs as a background process (or daemon) at each BSD site and responds to queries throughout the Internet. The worm executed a bufferoverow attack on finger .T h ep r o g r a m queried finger with a 536byte string crafted to exceed the buffer allocated for input and to overwrite the stack frame. Instead

input and to overwrite the stack frame. Instead of returning to the main routine where it resided before Morriss call, the finger daemon was routed to a procedure within the invading 536byte string now residing on the stack. The new procedure executed binsh , which, if successful, gave the worm a remote shell on the machine under attack. The bug exploited in sendmail also involved using a daemon process for malicious entry. sendmail sends, receives, and routes electronic mail. Debugging code in

and routes electronic mail. Debugging code in the utility permits testers to verify and display the state of the mail system. The debugging option was useful to system administrators and was often left on. Morris included in his attack arsenal a call to debug that instead of specifying a user address, as would be normal in testingissued as e to fc o m m a n d st h a tm a i l e da n de x e c u t e dac o p yo ft h eg r a p p l i n g  h o o k program. Once in place, the main worm systematically

Once in place, the main worm systematically attempted to discover user passwords. It began by trying simple cases of no password or passwords constructed of accountusername combinations, then used comparisons with an internal dictionary of 432 favorite password choices, and then went to the672 Chapter 15 Security nal stage of trying each word in the standard UNIX online dictionary as a possible password. This elaborate and efcient threestage passwordcracking algorithm enabled the worm to gain

algorithm enabled the worm to gain access to other user accounts on the infected system. The worm then searched for rsh data les in these newly broken accounts and used them as described previously to gain access to user accounts on remote systems. With each new access, the worm program searched for already active copies of itself. If it found one, the new copy exited, except in every seventh instance. Had the worm exited on all duplicate sightings, it might have remained undetected. Allowing

it might have remained undetected. Allowing every seventh duplicate to proceed (possibly to confound efforts to stop its spread by baiting with fakeworms) created a wholesale infestation of Sun and VAX systems on the Internet. The very features of the UNIX network environment that assisted in the worms propagation also helped to stop it sa d v a n c e .E a s eo fe l e c t r o n i cc o m m u  nication, mechanisms to copy source and binary les to remote machines, and access to both source code and

machines, and access to both source code and human expertise allowed cooperative efforts to develop solutions quickly. By the evening of t he next day, November 3, methods of halting the invading program were circulated to system administrators via the Internet. Within days, specic software patches for the exploited security aws were available. Why did Morris unleash the worm? The action has been characterized as both a harmless prank gone awry and a serious criminal offense. Based on the

awry and a serious criminal offense. Based on the complexity of the attack, it is unlikely that the worms release or the scope of its spread was unintentional. The worm program took elaborate steps to cover its tracks and to repel efforts to stop its spread. Yet the program contained no code aimed at damaging or destroying the systems on which it ran. The author clearly had the expertise to include such commands; in fact, data structures were present in the b ootstrap code that could have been

in the b ootstrap code that could have been used to transfer Trojanhorse or virus programs. The behavior of the program may lead to interesting observations, but it does not provide a sound basis for inferring motive. What is not open to speculation, however, is the legal outcome: a federal court convicted Morris and handed down a sentence of three years probation, 400 hours of community service, and a 10,000 ne. Morriss legal costs probably exceeded 100,000. Security experts continue to

exceeded 100,000. Security experts continue to evaluate m ethods to decrease or eliminate worms. A more recent event, though, s hows that worms are still a fact of life on the Internet. It also shows that as the Internet grows, the damage that even harmless worms can do also grows and can be signicant. This example occurred during August 2003. The fth version of the Sobig worm, more properly known as W32.Sobig.Fmm, was released by persons at this time unknown. It was the fastestspreading worm

time unknown. It was the fastestspreading worm released to date, at its peak infecting hundreds of thousands of computers and one in seventeen email messages on the Internet. It clogged email inboxes, slowed networks, and took a huge number of hours to clean up. Sobig.F was launched by being uploaded to a pornography newsgroup via an account created with a stolen credit card. It was disguised as a photo. The virus targeted Microsoft Windows systems and used its own SMTP engine to email itself to

and used its own SMTP engine to email itself to all the addresses found on an infected system. It used a variety of subject lines to help avoid detection, including Thank You! Your details, 15.3 System and Network Threats 673 and Re: Approved. It also used a random address on the host as the From:  address, making it difcult to determine from the message which machine was the infected source. Sobig.F included an attachment for the target email reader to click on, again with a variety of names.

reader to click on, again with a variety of names. If this payload was executed, it stored ap r o g r a mc a l l e d WINPPR32.EXE in the default Windows directory, along with at e x t l e .I ta l s om o d i  e dt h eW i n d o w sr e g i s t r y . The code included in the attachment was also programmed to periodically attempt to connect to one of twenty servers and download and execute a program from them. Fortunately, the servers were disabled before the code could be downloaded. The content of

the code could be downloaded. The content of the program from these servers has not yet been determined. If the code was malevolent, untold damage to a vast number of machines could have resulted. 15.3.2 Port Scanning Port scanning is not an attack but rather a means for a cracker to detect as y s t e m  sv u l n e r a b i l i t i e st oa t t a c k .P o r ts c a n n i n gt y p i c a l l yi sa u t o m a t e d , involving a tool that attempts to create a TCPIP connection to a specic port or a

to create a TCPIP connection to a specic port or a range of ports. For example, suppose there is a known vulnerability (or bug) in sendmail .Ac r a c k e rc o u l dl a u n c hap o r ts c a n n e rt ot r yt oc o n n e c t ,s a y , to port 25 of a particular system or to a range of systems. If the connection was successful, the cracker (or tool) could attempt to communicate with the answering service to determin ei ft h es e r v i c ew a si n d e e d sendmail and, if so, if it was the version with

d sendmail and, if so, if it was the version with the bug. Now imagine a tool in which each bug of every service of every operating system was encoded. The tool could attempt to connect to every port of one or more systems. For every service that answered, it could try to use each known bug. Frequently, the bugs are buffer overows, allowing the creation of ap r i v i l e g e dc o m m a n ds h e l lo nt h es y s t e m .F r o mt h e r e ,o fc o u r s e ,t h ec r a c k e r could install Trojan

o u r s e ,t h ec r a c k e r could install Trojan horses, backdoor programs, and so on. There is no such tool, but there are tools that perform subsets of that functionality. For example, nmap (from http:www.insecure.orgnmap )i s av e r yv e r s a t i l eo p e n  s o u r c eu t i l i t yf o rn e t w o r ke x p l o r a t i o na n ds e c u r i t y auditing. When pointed at a target, it will determine what services are running, including application names and versions. It can identify the host

names and versions. It can identify the host operating system. It can also provide information about defenses, such as what rewalls are defending the target. It does not exploit any known bugs. Because port scans are detectable (Section 15.6.3), they frequently are launched from zombie systems .S u c hs y s t e m sa r ep r e v i o u s l yc o m p r o m i s e d , independent systems that are serving their owners while being used for nefar ious purposes, including denialofservice attacks and spam

including denialofservice attacks and spam relay. Zombies make crackers particularly difcult to prosecute because determining the source of the attack and the person that launched it is challenging. This is one of many reasons for securing inconsequential systems, not just systems containing valuable information or services. 15.3.3 Denial of Service As mentioned earlier, denialofservice attacks are aimed not at gaining information or stealing resources but rather at disrupting legitimate use of

but rather at disrupting legitimate use of a system or facility. Most such attacks involve systems that the attacker has674 Chapter 15 Security not penetrated. Launching an attack that prevents legitimate use is frequently easier than breaking into a machine or facility. Denialofservice attacks are generally network based. They fall into two categories. Attacks in the rst category use so many facility resources that, in essence, no useful work can be done. For example, a website click could

can be done. For example, a website click could download a Java applet that proceeds to use all available CPU time or to pop up windows innitely. The second cate gory involves disrupting the network of the facility. There have been several successful denialofservice attacks of this kind against major websites. These attacks result from abuse of some of the fundamental functionality of TCPIP .F o ri n s t a n c e ,i ft h ea t t a c k e rs e n d st h ep a r t of the protocol that says Iw a n tt os

h ep a r t of the protocol that says Iw a n tt os t a r ta TCP connection, but never follows with the standard The connection is now complete, the result can be partially started TCP sessions. If enough of these sessions are launched, they can eat up all the network resources of the system, disabling any further legitimate TCP connections. Such attacks, which can last hours or days, have caused partial or full failure of attempts to use the target facility. The attacks are usually stopped at the

facility. The attacks are usually stopped at the network level until the operating systems can be updated to reduce their vulnerability. Generally, it is impossible to preven td e n i a l  o f  s e r v i c ea t t a c k s .T h ea t t a c k s use the same mechanisms as normal operation. Even more difcult to prevent and resolve are distributed denialofservice (DDOS )attacks. These attacks are launched from multiple sites at once, toward a common target, typically by zombies. DDOS attacks have

target, typically by zombies. DDOS attacks have become more common and are sometimes associated with blackmail attempts. A site comes under attack, and the attackers offer to halt the attack in exchange for money. Sometimes a site does not even know it is under attack. It can be difcult to determine whether a system slowdown is an attack or just a surge in system use. Consider that a successful advertising campaign that greatly increases trafc to a site could be considered a DDOS . There are

to a site could be considered a DDOS . There are other interesting aspects of DOS attacks. For example, if an authentication algorithm locks an account for a period of time after several incorrect attempts to access the account, then an attacker could cause all authentication to be blocked by purposely making incorrect attempts to access all accounts. Similarly, a rewall that automatically blocks certain kinds of trafc could be induced to block that trafc when it should not. These examples

that trafc when it should not. These examples suggest that programmers and systems managers need to fully understand the algorithms and technologies they are d eploying. Finally, computer science classes are notorious sources of accidental system DOS attacks. Consider the rst programming exercises in which students learn to create subprocesses or threads. A common bug involves spawning subprocesses innitely. The systems free memory and CPU resources dont stand a chance. 15.4 Cryptography as a

dont stand a chance. 15.4 Cryptography as a Security Tool There are many defenses against computer attacks, running the gamut from methodology to technology. The broadest tool available to system designers and users is cryptography. In this section, we discuss cryptography and its use in computer security. Note that the cryptography discussed here has been simplied for educational purposes; readers are cautioned against using any15.4 Cryptography as a Security Tool 675 of the schemes described

as a Security Tool 675 of the schemes described here in the real world. Good cryptography libraries are widely available and would make a good basis for production applications. In an isolated computer, the operating system can reliably determine the sender and recipient of all interprocess communication, since it controls all communication channels in the computer. In a network of computers, the situation is quite different. A networked computer receives bits from the wire with no immediate and

receives bits from the wire with no immediate and reliable way of determining what machine or application sent those bits. Similarly, the computer sends bits onto the network with no way of knowing who might eventually receive them. Additionally, when either sending or receiving, the system has no way of knowing if an eavesdropper listened to the communication. Commonly, network addresses are used to infer the potential senders and receivers of network messages. Network packets arrive with a

of network messages. Network packets arrive with a source address, such as an IPaddress. And when a computer sends a message, it names the intended receiver by spec ifying a destination address. However, for applications where security matters, we are asking for trouble if we assume that the source or destination address of a packet reliably determines who sent or received that packet. A rogue computer can send a message with a falsied source address, and numerous computers other than the one

address, and numerous computers other than the one specied by the destination address can (and typically do) receive a packet. For example, all of the routers on the way to the destination will receive the packet, too. How, then, is an operating system to decide whether to grant a request when it cannot trust the named source of the request? And how is it supposed to provide protection for a request or data when it cannot de termine who will receive the response or message contents it sends over

the response or message contents it sends over the network? It is generally considered infeasible to build a network of any scale in which the source and destination addresses of packets can be trusted in this sense. Therefore, the only alterna tive is somehow to eliminate the need to trust the network. This is the job of cryptography. Abstractly, cryptography is used to constrain the potential senders andor receivers of a message. Modern cryptography is based on secrets called keys that are

is based on secrets called keys that are selectively distributed to computers in a network and used to process messages. Cryptography enables a recipient of a message to verify that the message was created by some computer possessing a certain key. Similarly, a sender can encode its message so that only a computer with a certain key can decode the message. Unlike network addresses, however, keys are designed so that it is not computationally feasible to derive them from the messages they were

to derive them from the messages they were used to generate or from any other public information. Thus, they provide a much more trustworthy means of constraining senders and receivers of messages. Note that cryptography is a e l do fs t u d yu n t oi t s e l f ,w i t hl a r g ea n ds m a l lc o m p l e x i t i e sa n ds u b t l e t i e s . Here, we explore the most important aspects of the parts of cryptography that pertain to operating systems. 15.4.1 Encryption Because it solves a wide

15.4.1 Encryption Because it solves a wide variety of communication security problems, encryp tion is used frequently in many aspects of modern computing. It is used to send messages securely across across a network, as well as to protect database data, les, and even entire disks from having t heir contents read by unauthorized entities. An encryption algorithm enables th e sender of a message to ensure that676 Chapter 15 Security only a computer possessing a certain key can read the message, or

possessing a certain key can read the message, or ensure that the writer of data is the only reader of that data. Encryption of messages is an ancient practice, of course, and t here have been many encryption algorithms, dating back to ancient times. In this section, we describe important modern encryption principles and algorithms. An encryption algorithm consists of the following components: As e t Kof keys. As e t Mof messages. As e t Cof ciphertexts. An encrypting function E:K(MC). That is,

An encrypting function E:K(MC). That is, for each kK,Ekis a function for generating ciphertexts from messages. Both Eand Ekfor any k should be efciently computable functions. Generally, Ekis a randomized mapping from messages to ciphertexts. Ad e c r y p t i n gf u n c t i o n D:K(CM). That is, for each kK,Dkis a function for generating messages from ciphertexts. Both Dand Dkfor any kshould be efciently computable functions. An encryption algorithm must provide this essential property: given a

must provide this essential property: given a ciphertext cC,ac o m p u t e rc a nc o m p u t e msuch that Ek(m)conly if it possesses k. Thus, a computer holding kcan decrypt ciphertexts to the plaintexts used to produce them, but a computer not holding kcannot decrypt ciphertexts. Since ciphertexts are generally exposed (for example, sent on a network), it is important that it be infeasible to derive kfrom the ciphertexts. There are two main types of encryption algorithms: symmetric and

main types of encryption algorithms: symmetric and asymmetric. We discuss both types in the following sections. 15.4.1.1 Symmetric Encryption In a symmetric encryption algorithm ,t h es a m ek e yi su s e dt oe n c r y p ta n dt o decrypt. Therefore, the secrecy of kmust be protected. Figure 15.7 shows an example of two users communicating securely via symmetric encryption over an insecure channel. Note that the key exchange can take place directly between the two parties or via a trusted third

between the two parties or via a trusted third party (that is, a certicate authority), as discussed in Section 15.4.1.4. For the past several decades, the m ost commonly used symmetric encryp tion algorithm in the United States for civilian applications has been the dataencryption standard (DES)cipher adopted by the National Institute of Standards and Technology ( NIST ).DES works by taking a 64bit value and a 56bit key and performing a series of transformations that are based on substitution

of transformations that are based on substitution and permutation operations. Because DESworks on a block of bits at a time, is known as a block cipher ,a n di t st r a n s f o r m a t i o n sa r et y p i c a lo f block ciphers. With block ciphers, if the same key is used for encrypting an extended amount of data, it becomes vulnerable to attack. DESis now considered insecure for many applications because its keys can be exhaustively searched with moderate computing resources. (Note, though,

with moderate computing resources. (Note, though, that it is still frequently used.) Rather than giving up on DES,NIST created a modication called triple DES,i nw h i c ht h e DES algorithm is repeated three times (two encryptions and one decryption) on the same plaintext using two15.4 Cryptography as a Security Tool 677 key exchangemessage m message mencryption algorithm E decryption algorithm Dwrite encryption key k decryption key k readinsecure channelplaintextciphertext c  Ek(m)plaintext m

channelplaintextciphertext c  Ek(m)plaintext m  Dk(c)attacker Figure 15.7 As e c u r ec o m m u n i c a t i o no v e ra ni n s e c u r em e d i u m . or three keysfor example, cEk3(Dk2(Ek1(m))). When three keys are used, the effective key length is 168 bits. Triple DES is in widespread use today. In 2001, NIST adopted a new block cipher, called the advanced encryption standard (AES),t or e p l a c e DES.AES is another block cipher. It can use key lengths of 128, 192, or 256 bits and works on

key lengths of 128, 192, or 256 bits and works on 128bit blocks. Generally, the algorithm is compact and efcient. Block ciphers are not in themselves secure encryption schemes. In partic ular, they do not directly handle messages longer than their required block sizes. However, there are many modes of encryption that are based on stream ciphers, which can be used to securely encrypt longer messages. RC4 is perhaps the most common stream cipher. A stream cipher is designed to encrypt and decrypt

A stream cipher is designed to encrypt and decrypt a stream of bytes or bits rather than a block. This is useful when the length of a communication would make a block cipher too slow. The key is input into a pseudorandombit generator, which is an algorithm that attempts to produce random bits. The output of the generator when fed a key is a keystream. A keystream is an innite set of bits that can be used to encrypt a plaintext stream by simply XORing it with the plaintext. (XOR, for eXclusive OR

it with the plaintext. (XOR, for eXclusive OR is an operation that compares two input bits and generates one output bit. If the bits are the same, the result is 0. If the bits are different, the result is 1.) RC4 is used in encrypting steams of data, such as in WEP , the wireless LAN protocol. Unfortunately, RC4 as used in WEP (IEEE standard 802.11) has been found to be breakable in a reasonable amount of computer time. In fact, RC4itself has vulnerabilities.678 Chapter 15 Security 15.4.1.2

vulnerabilities.678 Chapter 15 Security 15.4.1.2 Asymmetric Encryption In an asymmetric encryption algorithm ,t h e r ea r ed i f f e r e n te n c r y p t i o na n d decryption keys. An entity preparing to r eceive encrypted communication creates two keys and makes one of them (called the public key) available to anyone who wants it. Any sender can use that key to encrypt a communication, but only the key creator can decrypt the communication. This scheme, known aspublickey encryption ,w a sab r

scheme, known aspublickey encryption ,w a sab r e a k t h r o u g hi nc r y p t o g r a p h y .N ol o n g e r must a key be kept secret and delivered securely. Instead, anyone can encrypt am e s s a g et ot h er e c e i v i n ge n t i t y ,a n dn om a t t e rw h oe l s ei sl i s t e n i n g ,o n l yt h a t entity can decrypt the message. As an example of how publickey encryption works, we describe an algorithm known as RSA,a f t e ri t si n v e n t o r s ,R i v e s t ,S h a m i r ,a n dA d l e m

t o r s ,R i v e s t ,S h a m i r ,a n dA d l e m a n . RSA is the most widely used asymmetric encryption algorithm. (Asymmetric algorithms based on elliptic curves are gaining ground, however, because the key length of such an algorithm can be shorter for the same amount of cryptographic strength.) InRSA,keis the public key ,a n d kdis the private key .Nis the product of two large, randomly chosen prime numbers pand q(for example, pand qare 512 bits each). It must be computationally infeasible

bits each). It must be computationally infeasible to derive kd,Nfrom ke,N,s o that keneed not be kept secret and can be widely disseminated. The encryption algorithm is Eke,N(m)mkemod N,w h e r e kesatises kekdmod ( p1)(q1) 1. The decryption algorithm is then Dkd,N(c)ckdmod N. An example using small values is shown in Figure 15.8. In this example, we make p7a n d q13. We then calculate N71391 and ( p1)(q1)72. We next select kerelatively prime to 72 and 72, yielding 5. Finally, we calculate

to 72 and 72, yielding 5. Finally, we calculate kdsuch that kekdmod 72 1, yielding 29. We now have our keys: the public key, ke,N5,91, and the private key, kd,N29,91. Encrypting the message 69 with the public key results in the message 62, which is then decoded by the receiver via the private key. The use of asymmetric encryption begins with the publication of the public key of the destination. For bidirectional communication, the source also must publish its public key. Publication can be as

must publish its public key. Publication can be as simple as handing over an electronic copy of the key, or it can be more complex. The private key (or secret key) must be zealously guarded, as anyone holding that key can decrypt any message created by the matching public key. We should note that the seemingly small difference in key use between asymmetric and symmetric cryptography is quite large in practice. Asymmetric cryptography is much more computationally expensive to execute. It is much

computationally expensive to execute. It is much faster for a computer to encode and dec ode ciphertext by using the usual symmetric algorithms than by using asymmetric algorithms. Why, then, use an asymmetric algorithm? In truth, these algorithms are not used for general purpose encryption of large amounts of data. However, they are used not only for encryption of small amounts of data but also for authentication, condentiality, and key distribution, as we show in the following sections.

as we show in the following sections. 15.4.1.3 Authentication We have seen that encryption offers a way of constraining the set of possible receivers of a message. Constraining the set of potential senders of a message is called authentication . Authentication is thus complementary to encryption.15.4 Cryptography as a Security Tool 679message 69 69695 mod 91 6229 mod 91write encryption key k5,91 decryption key k29,91 readinsecure channelplaintext 62 Figure 15.8 Encryption and decryption using

62 Figure 15.8 Encryption and decryption using RSA asymmetric cryptography. Authentication is also useful for proving t hat a message has not been modied. In this section, we discuss authentication as a constraint on possible senders of a message. Note that this sort of authentication is similar to but distinct from user authentication, which we discuss in Section 15.5. An authentication algorithm using symmetric keys consists of the follow ing components: As e t Kof keys. As e t Mof messages.

components: As e t Kof keys. As e t Mof messages. As e t Aof authenticators. Af u n c t i o n S:K(MA). That is, for each kK,Skis a function for generating authenticators from messages. Both Sand Skfor any kshould be efciently computable functions. Af u n c t i o n V:K(MAtrue ,false ). That is, for each kK,Vk is a function for verifying authenticators on messages. Both Vand Vkfor any kshould be efciently computable functions. The critical property that an authentication algorithm must possess is

that an authentication algorithm must possess is this: for a message m, a computer can generate an authenticator aAsuch that Vk(m,a)true only if it possesses k. Thus, a computer holding kcan680 Chapter 15 Security generate authenticators on messages so that any computer possessing kcan verify them. However, a computer not holding kcannot generate authenticators on messages that can be veried using Vk.S i n c ea u t h e n t i c a t o r sa r eg e n e r a l l y exposed (for example, sent on a

eg e n e r a l l y exposed (for example, sent on a network with the messages themselves), it must not be feasible to derive kfrom the authenticators. Practically, if Vk(m,a) true ,t h e nw ek n o wt h a t mhas not been modied, and that the sender of the message has k.I fw es h a r e kwith only one entity, then we know that the message originated from k. Just as there are two types of encryption algorithms, there are two main varieties of authentication algorithms. The rst step in understanding

algorithms. The rst step in understanding these algorithms is to explore hash functions. A hash function H(m) creates a small, xedsized block of data, known as a message digest orhash value , from a message m.H a s hf u n c t i o n sw o r kb yt a k i n gam e s s a g e ,s p l i t t i n gi ti n t ob l o c k s , and processing the blocks to produce an nbit hash. Hmust be collision resistant that is,itmustbe infeasibletond an mmsuch that H(m)H(m). Now, ifH(m)H(m), we know that mmthat is, we know

Now, ifH(m)H(m), we know that mmthat is, we know that the message has not been modied. Common messagedigest functions include MD5 ,n o w considered insecure, which produces a 128bit hash, and SHA1 ,w h i c ho u t p u t s a 160bit hash. Message digests are useful for detecting changed messages but are not useful as authenticators. For example, H(m)c a nb es e n ta l o n gw i t ha message; but if His known, then someone could modify mtomand recompute H(m), and the message modication would not be

H(m), and the message modication would not be detected. Therefore, we must authenticate H(m). The rst main type of authentication algorithm uses symmetric encryp tion. In a messageauthentication code (MAC ), a cryptographic checksum is generated from the message using a secret key. A MAC provides a way to securely authenticate short values. If we use it to authenticate H(m)f o ra n H that is collision resistant, then we obtain a way to securely authenticate long messages by hashing them rst.

authenticate long messages by hashing them rst. Note that kis needed to compute both Skand Vk,s oa n y o n ea b l et oc o m p u t eo n ec a nc o m p u t et h eo t h e r . The second main type of authentication algorithm is a digitalsignature algorithm ,a n dt h ea u t h e n t i c a t o r st h u sp r o d u c e da r ec a l l e d digital signatures . Digital signatures are very useful in that they enable anyone to verify the authenticity of the message. In a digitalsignature algorithm, it is

message. In a digitalsignature algorithm, it is computa tionally infeasible to derive ksfrom kv.T h u s , kvis the public key, and ksis the private key. Consider as an example the RSA digitalsignature algorithm. It is similar to the RSA encryption algorithm, but the key use is reversed. The digital signature of a message is derived by computing Sks(m)H(m)ksmod N. The key ksagain is a pair d,N,w h e r e Nis the product of two large, randomly chosen prime numbers pand q.T h ev e r i  c a t i o na

prime numbers pand q.T h ev e r i  c a t i o na l g o r i t h mi st h e n Vkv(m,a)?akvmod NH(m)), where kvsatises kvksmod ( p1)(q1)1. Note that encryption and authentication may be used together or sepa rately. Sometimes, for instance, we want authentication but not condentiality. For example, a company could provide a software patch and could signthat patch to prove that it came from the company and that it hasnt been modied. Authentication is a component of many aspects of security. For

is a component of many aspects of security. For example, digital signatures are the core of nonrepudiation ,w h i c hs u p p l i e sp r o o ft h a t an entity performed an action. A typical example of nonrepudiation involves15.4 Cryptography as a Security Tool 681 the lling out of electronic forms as an alternative to the signing of paper contracts. Nonrepudiation assures that a person lling out an electronic form cannot deny that he did so. 15.4.1.4 Key Distribution Certainly, a good part of

Key Distribution Certainly, a good part of the battle between cryptographers (those inventing ciphers) and cryptanalysts (those trying to break them) involves keys. With symmetric algorithms, both parties need the key, and no one else should have it. The delivery of the symmetric key is a huge challenge. Sometimes it is performed outofband say, via a paper document or a conversation. These methods do not scale well, ho wever. Also consider the keymanagement challenge. Suppose a user wanted to

keymanagement challenge. Suppose a user wanted to communicate with Nother users privately. That user would need Nkeys and, for more security, would need to change those keys frequently. These are the very reasons for efforts to create asymmetric key algorithms. Not only can the keys be exchanged in public, but a given user needs only one private key, no matter how many other people she wants to communicate with. There is still the matter of managing a public key for each recipient of the

of managing a public key for each recipient of the communication, but since public keys need not be secured, simple storage can be used for that key ring . Unfortunately, even the distribution of public keys requires some care. Consider the maninthemiddle attack shown in Figure 15.9. Here, the person who wants to receive an encrypted message sends out his public key, but an attacker also sends her badpublic key (which matches her private key). The person who wants to send the encrypted message

The person who wants to send the encrypted message knows no better and so uses the bad key to encrypt the message. The attacker then happily decrypts it. The problem is one of authenticationwhat we need is proof of who (or what) owns a public key. One way to solve that problem involves the use of digital certicates. A digital certicate is a public key digitally signed by a trusted party. The trusted party receives proof of identication from some entity and certies that the public key belongs to

entity and certies that the public key belongs to that entity. But how do we know we can trust the certier? These certicate authorities have their public keys included within web browsers (and othe rc o n s u m e r so fc e r t i  c a t e s )b e f o r et h e y are distributed. The certicate authorities can then vouch for other authorities (digitally signing the public keys of these other authorities), and so on, creating aw e bo ft r u s t .T h ec e r t i  c a t e sc a nb ed i s t r i b u t e di

e r t i  c a t e sc a nb ed i s t r i b u t e di nas t a n d a r dX . 5 0 9d i g i t a l certicate format that can be parsed by computer. This scheme is used for secure web communication, as we discuss in Section 15.4.3. 15.4.2 Implementation of Cryptography Network protocols are typically organized in layers ,l i k ea no n i o no rap a r f a i t , with each layer acting as a client of the one below it. That is, when one protocol generates a message to send to its protocol peer on another

a message to send to its protocol peer on another machine, it hands its message to the protocol below it in the networkprotocol stack for delivery to its peer on that machine. For example, in an IPnetwork, TCP (atransport layer protocol) acts as a client of IP(anetworklayer protocol): TCP packets are passed down to IPfor delivery to the IPpeer at the other end of the connection. IPencapsulates the TCP packet in an IPpacket, which it similarly passes down to the datalink layer to be transmitted

down to the datalink layer to be transmitted across the network to its peer on the682 Chapter 15 Security message m encryption algorithm E decryption algorithm Dwrite 3. E kbad(m) message m readencryption key kbad decryption key kddecryption algorithm Ddecryption key kbad 2. Public key kbad1. Public key keattacker Figure 15.9 Am a n  i n  t h e  m i d d l ea t t a c ko na s y m m e t r i cc r y p t o g r a p h y . destination computer. This IPpeer then delivers the TCP packet up to the TCP peer

then delivers the TCP packet up to the TCP peer on that machine. Cryptography can be inserted at almost any layer in the OSImodel. SSL (Section 15.4.3), for example, provides security at the transport layer. Network layer security generally has been standardized on IPSec,w h i c hd e  n e s IPpacket formats that allow the insertion of authenticators and the encryption of packet contents. IPSec uses symmetric encryption and uses the Internet Key Exchange (IKE)protocol for key exchange. IKEis

Key Exchange (IKE)protocol for key exchange. IKEis based on pubickey encryption. IPSec is becoming widely used as the basis for virtual private networks (VPN s),i n which all trafc between two IPSec endpoints is encrypted to make a private network out of one that may otherwise be public. Numerous protocols also have been developed for use by applications, such as PGPfor encrypting email, but then the applications themselves must be coded to implement security. Where is cryptographic protection

security. Where is cryptographic protection best placed in a protocol stack? In general, there is no denitive answer. O n the one hand, more protocols benet from protections placed lower in the stack. For example, since IPpackets encapsulate TCPpackets, encryption of IPpackets (using IPSec, for example) also15.4 Cryptography as a Security Tool 683 hides the contents of the encapsulated TCP packets. Similarly, authenticators onIPpackets detect the modication of contained TCP header information.

modication of contained TCP header information. On the other hand, protection at lower layers in the protocol stack may give insufcient protection to higherlayer protocols. For example, an application server that accepts connections encrypted with IPSec might be able to authenticate the client co mputers from which requests are received. However, to authenticate a user at a c lient computer, the server may need to use an applicationlevel protocolthe user may be required to type a password. Also

user may be required to type a password. Also consider the problem of email. Email delivered via the industrystandard SMTP protocol is stored and forwarded, frequently multiple times, before it is delivered. Each of these transmissions could go over a secure or an insecure network. For email to be secure, the email message needs to be encrypted so that its security is independent of the transports that carry it. 15.4.3 An Example: SSL SSL 3.0 is a cryptographic protocol that enables two

3.0 is a cryptographic protocol that enables two computers to communicate securelythat is, so that each can limit the sender and receiver of messages to the other. It is perhaps the most commonly used cryptographic protocol on the Internet today, since it is the standard protocol by which web browsers communicate securely with web servers. For completeness, we should note that SSLwas designed by Netscape and that i te v o l v e di n t ot h ei n d u s t r y s t a n d a r d TLSprotocol. In this

n d u s t r y s t a n d a r d TLSprotocol. In this discussion, we use SSLto mean both SSLand TLS. SSLis a complex protocol with many options. Here, we present only a single variation of it. Even then, we describe it i nav e r ys i m p l i  e da n da b s t r a c tf o r m , so as to maintain focus on its use of cryptographic primitives. What we are about to see is a complex dance in which asymmetric cryptography is used so that a client and a server can establish a secure session key that can be

can establish a secure session key that can be used for symmetric encryption of the session between the twoall of this while avoiding maninthemiddle and replay attacks. For added cryptographic strength, the session keys are forgotten once a session is completed. Another communication between the two will require generation of new session keys. The SSLprotocol is initiated by a client cto communicate securely with a server. Prior to the protocols use, the server sis assumed to have obtained a

use, the server sis assumed to have obtained a certicate, denoted cert s,f r o mc e r t i  c a t i o na u t h o r i t y CA.T h i sc e r t i  c a t ei sa structure containing the following: Various attributes ( attrs)o ft h es e r v e r ,s u c ha si t su n i q u e distinguished name and its common (DNS) name The identity of a asymmetric encryption algorithm E()for the server The public key keof this server Av a l i d i t yi n t e r v a l( interval )d u r i n gw h i c ht h ec e r t i  c a t es h o

)d u r i n gw h i c ht h ec e r t i  c a t es h o u l db ec o n s i d  ered valid A digital signature aon the above information made by the CAthat is, aSkCA(attrs,Eke,interval ) In addition, prior to the protocols use, the client is presumed to have obtained the public verication algorithm VkCAforCA.I nt h ec a s eo ft h eW e b , the users browser is shipped from its vendor containing the verication684 Chapter 15 Security algorithms and public keys of certain certication authorities. The user

keys of certain certication authorities. The user can add or delete these as she chooses. When cconnects to s,i ts e n d sa2 8  b y t er a n d o mv a l u e ncto the server, which responds with a random value nsof its own, plus its certicate cert s.T h ec l i e n t veries that VkCA(attrs,Eke,interval ,a ) true and that the current time is in the validity interval interval .I fb o t ho ft h e s et e s t sa r es a t i s  e d ,t h es e r v e r has proved its identity. Then the client generates a

proved its identity. Then the client generates a random 46byte premaster secret pmsand sends cpms Eke(pms)t ot h es e r v e r .T h es e r v e rr e c o v e r s pms Dkd(cpms ). Now both the client and the server are in possession of nc,ns, and pms,a n de a c hc a nc o m p u t eas h a r e d4 8  b y t e master secret msH(nc,ns, pms). Only the server and client can compute ms,s i n c eo n l yt h e yk n o w pms. Moreover, the dependence of msonncand nsensures that msis afresh value that is, a session

that msis afresh value that is, a session key that has not been used in a previous communication. At this point, the client and the server both compute the following keys from the ms: As y m m e t r i ce n c r y p t i o nk e y kcrypt cs for encrypting messages from the client to the server As y m m e t r i ce n c r y p t i o nk e y kcrypt sc for encrypting messages from the server to the client AMAC generation key kmac cs for generating authenticators on messages from the client to the server

on messages from the client to the server AMAC generation key kmac sc for generating authenticators on messages from the server to the client To send a message mto the server, the client sends cEkcrypt cs(m,Skmaccs(m)). Upon receiving c,t h es e r v e rr e c o v e r s m,aDkcrypt cs(c) and accepts mifVkmaccs(m,a)t r u e .S i m i l a r l y ,t os e n dam e s s a g e mto the client, the server sends cEkcrypt sc(m,Skmacsc(m)) and the client recovers m,aDkcrypt sc(c) and accepts mifVkmacsc(m,a)t r u e

sc(c) and accepts mifVkmacsc(m,a)t r u e . This protocol enables the server to limit the recipients of its messages to the client that generated pmsand to limit the senders of the messages it accepts to that same client. Similarly, the client can limit the recipients of the messages it sends and the senders of the me ssages it accepts to the party that knows kd (that is, the party that can decrypt cpms ). In many applications, such as web transactions, the client needs to verify the identity of

the client needs to verify the identity of the party that knows kd. This is one purpose of the certicate cert s.I np a r t i c u l a r ,t h e attrseld contains information that the client can use to determine the identityfor example, the15.5 User Authentication 685 domain nameof the server with which it is communicating. For applications in which the server also needs information about the client, SSLsupports an option by which a client can send a certicate to the server. In addition to its use

a certicate to the server. In addition to its use on the Internet, SSLis being used for a wide variety of tasks. For example, IPSecVPNsn o wh a v eac o m p e t i t o ri n SSL VPN s.IPSec is good for pointtopoint encryption of trafcsay, between two company ofces. SSL VPN s are more exible but not as efcient, so they might be used between an individual employee working remotely and the corporate ofce. 15.5 User Authentication Our earlier discussion of authentication involves messages and sessions.

of authentication involves messages and sessions. But what about users? If a system cannot authenticate a user, then authenticating that a message came from that user is pointless. Thus, a major security problem for operating systems is user authentication .T h ep r o t e c t i o ns y s t e md e p e n d s on the ability to identify the programs and processes currently executing, which in turn depends on the ability to identify each user of the system. Users normally identify themselves. How do

system. Users normally identify themselves. How do we determine whether a users identity is authentic? Generally, user authentication is based on one or more of three things: the users possession of something (a key or card), the users knowledge of something (a user identier and password), or an attribute of the user (ngerprint, retina pattern, or signature). 15.5.1 Passwords The most common approach to authenticating a user identity is the use of passwords .W h e nt h eu s e ri d e n t i  e sh

of passwords .W h e nt h eu s e ri d e n t i  e sh e r s e l fb yu s e r IDor account name, she is asked for a password. If the usersupplied password matches the password stored in the system, the system assumes that the account is being accessed by the owner of that account. Passwords are often used to protect objects in the computer system, in the absence of more complete protection schemes. They can be considered a special case of either keys or capabilities. For instance, a password may be

or capabilities. For instance, a password may be associated with each resource (such as a le). Whenever a request is made to use the resource, the password must be given. If the password is correct, access is granted. Different passwords may be associated with different access rights. For example, different passwords may be used for reading les, appending les, and updating les. In practice, most systems require only one password for a user to gain full rights. Although more passwords

user to gain full rights. Although more passwords theoretically would be more secure, such systems tend not to be implemented due to the classic tradeoff between security and convenience. If security makes something inconvenient, then the security is frequently bypassed or otherwise circumvented. 15.5.2 Password Vulnerabilities Passwords are extremely common because they are easy to understand and use. Unfortunately, passwords can often be guessed, accidentally exposed, sniffed (read by an

guessed, accidentally exposed, sniffed (read by an eavesdropper), or illegally transferred from an authorized user to an unauthorized one, as we show next.686 Chapter 15 Security There are two common ways to guess a password. One way is for the intruder (either human or program) to know the user or to have information about the user. All too frequently, people use obvious information (such as the names of their cats or spouses) as their passwords. The other way is to use brute force, trying

The other way is to use brute force, trying enumerationor all possible combinations of valid password characters (letters, numbers, and punctuation on some systems)until the password is found. Short passwords are especially vulnerable to this method. For example, a fourcharacter password provides only 10,000 variations. On average, guessing 5,000 times would produce a correct hit. A program that could try a password every millisecond would take only about 5 seconds to guess a fourcharacter

take only about 5 seconds to guess a fourcharacter password. Enumeration is less successful where systems allow longer passwords that include both uppercase and lowercase letters, along with numbers and all punctuation characters. Of course, users must take advantage of the large password space and must not, for example, use only lowercase letters. In addition to being guessed, passwords can be exposed as a result of visual or electronic monitoring. An intruder can look over the shoulder of a

An intruder can look over the shoulder of a user ( shoulder surng )w h e nt h eu s e ri sl o g g i n gi na n dc a nl e a r nt h ep a s s w o r d easily by watching the keyboard. Alternatively, anyone with access to the network on which a computer resides can seamlessly add a network monitor, allowing him to sniff ,o rw a t c h ,a l ld a t ab e i n gt r a n s f e r r e do nt h en e t w o r k , including user IDsa n dp a s s w o r d s .E n c r y p t i n gt h ed a t as t r e a mc o n t a i n i n gt

t i n gt h ed a t as t r e a mc o n t a i n i n gt h e password solves this problem. Even such a system could have passwords stolen, however. For example, if a le is used to contain the passwords, it could be copied for offsystem analysis. Or consider a Trojanhorse program installed on the system that captures every keystroke before sending it on to the application. Exposure is a particularly severe problem if the password is written down where it can be read or lost. Some systems force users to

can be read or lost. Some systems force users to select hardto remember or long passwords, or to change their password frequently, which may cause a user to record the password or to reuse it. As a result, such systems provide much less security than systems that allow users to select easy passwords! The nal type of password compromise, illegal transfer, is the result of human nature. Most computer installations have a rule that forbids users to share accounts. This rule is sometimes implemented

share accounts. This rule is sometimes implemented for accounting reasons but is often aimed at improving security. For instance, suppose one user IDis shared by several users, and a security breach occurs from that user ID.I ti si m p o s s i b l e to know who was using the IDat the time the break occurred or even whether the user was an authorized one. With one user per user ID,a n yu s e rc a nb e questioned directly about use of the account; in addition, the user might notice something

in addition, the user might notice something different about the account and detect the breakin. Sometimes, users break accountsharing rules to help friends or to circumvent accounting, and this behavior can result in a systems being accessed by unauthorized users possibly harmful ones. Passwords can be either generated by the system or selected by a user. Systemgenerated passwords may be difcult to remember, and thus users may write them down. As mentioned, how ever, userselected passwords are

As mentioned, how ever, userselected passwords are often easy to guess (the users name or favorite car, for example). Some systems will check a proposed password for ease of guessing or cracking before accepting15.5 User Authentication 687 it. Some systems also agepasswords, forcing users to change their passwords at regular intervals (every three months, for instance). This method is not foolproof either, because users can easily toggle between two passwords. The solution, as implemented on

two passwords. The solution, as implemented on some systems, is to record a password history for each user. For instance, the system could record the last Npasswords and not allow their reuse. Several variants on these simple password schemes can be used. For example, the password can be changed more frequently. At the extreme, the password is changed from session to session. A new password is selected (either by the system or by the user) at the end of each session, and that password must be

the end of each session, and that password must be used for the next session. In such a case, even if a password is used by an unauthorized person, that person can use it only once. When the legitimate user tries to use a nowinvalid password at the next session, he discovers the security violation. Steps can then be taken to repair the breached security. 15.5.3 Securing Passwords One problem with all these approaches is the difculty of keeping the password secret within the computer. How can the

password secret within the computer. How can the system store a password securely yet allow its use for authentication when the user presents her password? The UNIX system uses secure hashing to avoid the necessity of keeping its password list secret. Because the list is hashed rather than encrypted, it is impossible for the system to decrypt the stored value and determine the original password. Heres how this system works. Each user has a password. The system contains a function that is

a password. The system contains a function that is extremely difcultthe designers hope impossible to invert but is simple to compute. That is, given a value x,i ti se a s yt o compute the hash function value f(x). Given a function value f(x), however, it is impossible to compute x.T h i sf u n c t i o ni su s e dt oe n c o d ea l lp a s s w o r d s . Only encoded passwords are stored. When a user presents a password, it is hashed and compared against the stored encoded password. Even if the

against the stored encoded password. Even if the stored encoded password is seen, it cannot be decoded, so the password cannot be determined. Thus, the password le does not need to be kept secret. The aw in this method is that the system no longer has control over the passwords. Although the passwords are hashed, anyone with a copy of the password le can run fast hash routines against ithashing each word in ad i c t i o n a r y ,f o ri n s t a n c e ,a n dc o m p a r i n gt h er e s u l t sa g a

c e ,a n dc o m p a r i n gt h er e s u l t sa g a i n s tt h ep a s s w o r d s . If the user has selected a password that is also a word in the dictionary, the password is cracked. On sufciently fast computers, or even on clusters of slow computers, such a comparison may take only a few hours. Furthermore, because UNIX systems use a wellknown hashing algorithm, a cracker might keep a cache of passwords that have been cracked previously. For these reasons, systems include a salt,or recorded

these reasons, systems include a salt,or recorded random number, in the hashing algorithm. The salt value is added to the password to ensure that if two plaintext passwords are the same, they result in different hash values. In addition, the salt value makes hashing a dictionary ineffective, because each dictionary term would need to be combined with each salt value for comparison to the stored passwords. Newer versions of UNIX also store the hashed password entries in a l er e a d a b l eo n l

password entries in a l er e a d a b l eo n l yb yt h es u p e r u s e r .T h ep r o g r a m st h a tc o m p a r et h eh a s ht o688 Chapter 15 Security the stored value are run setuid to root, so they can read this le, but other users cannot. Another weakness in the UNIX password methods is that many UNIX systems treat only the rst eight characters as signicant. It is therefore extremely important for users to take advantage of the available password space. Complicating the issue further is the

space. Complicating the issue further is the fact that some systems do not allow the use of dictionary words as passwords. A good technique is to generate your password by using the rst letter of each word of an easily remembered phrase using both upper and lower characters with a number or punctuation mark thrown in for good measure. For example, the phrase My mothers name is Katherine might yield the password Mmn.isK! .T h ep a s s w o r di sh a r dt o crack but easy for the user to remember.

a r dt o crack but easy for the user to remember. A more secure system would allow more characters in its passwords. Indeed, a system might also allow passwords to include the space character, so that a user could create a passphrase . 15.5.4 OneTime Passwords To avoid the problems of password snifng and shoulder surng, a system can use a set of paired passwords .W h e nas e s s i o nb e g i n s ,t h es y s t e mr a n d o m l y selects and presents one part of a password pair; the user must

one part of a password pair; the user must supply the other part. In this system, the user is challenged and must respond with the correct answer to that challenge. This approach can be generalized to the use of an algorithm as a password. Such algorithmic passwords are not susceptible to reuse. That is, a user can type in a password, and no entity intercepting that password will be able to reuse it. In this scheme, the system and the user share a symmetric password. The password pwis never

a symmetric password. The password pwis never transmitted over a medium that allows exposure. Rather, the password is used as input to the function, along with a challenge chpresented by the system. The user then computes the function H(pw,ch). The result of this function is transmitted as the authenticator to the computer. Because the computer also knows pwand ch, it can perform the same computation. If the results match, the user is authenticated. The next time the user needs to be

authenticated. The next time the user needs to be authenticated, another chis generated, and the same steps ensue. This time, the authenticator is different. This onetime password system is one of only a few ways to prevent improper authentication due to password exposure. Onetime password systems are implemented in various ways. Commer cial implementations use hardware calculators with a display or a display and numeric keypad. These calculators generally take the shape of a credit card, a

generally take the shape of a credit card, a keychain dongle, or a USB device. Software running on computers or smartphones provides the user with H(pw,ch);pwcan be input by the user or generated by the calculator in synchronization with the computer. Sometimes, pwis just a personal identication number (PIN).T h eo u t p u t of any of these systems shows the onetime password. A onetime password generator that requires input by the user involves twofactor authentication . Two different types of

twofactor authentication . Two different types of components are needed in this case  for example, a onetime password generator that generates th ec o r r e c tr e s p o n s eo n l yi ft h e PIN is valid. Twofactor authentication offers far better authentication protection than singlefactor authentication because it requires something you have as well as something you know. 15.6 Implementing Security Defenses 689 Another variation on onetime passwords uses a code book ,o ronetime pad, which is a

uses a code book ,o ronetime pad, which is a list of singleuse passwords. Each password on the list is used once and then is crossed out or erased. The commonly used SKey system uses either a software calculator or a code book based on these calculations as a source of onetime passwords. Of course, the user must protect his code book, and it is helpful if the code book does not identify the system to which the codes are authenticators. 15.5.5 Biometrics Yet another variation on the use of

Biometrics Yet another variation on the use of passwords for authentication involves the use of biometric measures. Palm or handreaders are commonly used to secure physical accessfor example, access to a data center. These readers match stored parameters against what is being read from handreader pads. The parameters can include a temperature map, as well as nger length, nger width, and line patterns. These devices ar ec u r r e n t l yt o ol a r g ea n de x p e n s i v e to be used for normal

r g ea n de x p e n s i v e to be used for normal computer authentication. Fingerprint readers have become a ccurate and costeffective and should become more common in the future. These devices read nger ridge patterns and convert them into a sequence of nu mbers. Over time, they can store a set of sequences to adjust for the location of the nger on the reading pad and other factors. Software can then scan a nger on the pad and compare its features with these stored sequences to determine if

with these stored sequences to determine if they match. Of course, multiple users can have proles stored, and the scanner can differentiate among them. Av e r ya c c u r a t et w o  f a c t o ra u t h e n t i c a t i o ns c h e m ec a nr e s u l tf r o mr e q u i r i n g ap a s s w o r da sw e l la sau s e rn a m ea n d n g e r p r i n ts c a n .I ft h i si n f o r m a t i o n is encrypted in transit, the system can be very resistant to spoong or replay attack. Multifactor authentication is

or replay attack. Multifactor authentication is better still. Consider how strong authentica tion can be with a USB device that must be plugged into the system, a PIN,a n d a n g e r p r i n ts c a n .E x c e p tf o rh a v i n gt o place ones nger on a pad and plug the USBinto the system, this authentication method is no less convenient than that using normal passwords. Recall, though, that strong authentication by itself is not sufcient to guarantee the IDof the user. An authenticated session

the IDof the user. An authenticated session can still be hijacked if it is not encrypted. 15.6 Implementing Security Defenses Just as there are myriad threats to system and network security, there are many security solutions. The solutions range from improved user education, through technology, to writing bugfree software. Most security professionals subscribe to the theory of defense in depth ,w h i c hs t a t e st h a tm o r el a y e r so fd e f e n s ea r e better than fewer layers. Of

fd e f e n s ea r e better than fewer layers. Of course, this theory applies to any kind of security. Consider the security of a house without a door lock, with a door lock, and with a lock and an alarm. In this section, we look at the major methods, tools, and techniques that can be used to improve resistance to threats. 15.6.1 Security Policy The rst step toward improving the security of any aspect of computing is to have a security policy .P o l i c i e sv a r yw i d e l yb u tg e n e r a l l

l i c i e sv a r yw i d e l yb u tg e n e r a l l yi n c l u d eas t a t e m e n t690 Chapter 15 Security of what is being secured. For example, a policy might state that all outside accessible applications must have a code review before being deployed, or that users should not share their passwords, or that all connection points between a company and the outside must have port scans run every six months. Without ap o l i c yi np l a c e ,i ti si m p o s s i b l ef o ru s e r sa n da d m i n i s

m p o s s i b l ef o ru s e r sa n da d m i n i s t r a t o r st ok n o ww h a t is permissible, what is required, and what is not allowed. The policy is a road map to security, and if a site is trying to move from less secure to more secure, it needs a map to know how to get there. Once the security policy is in place, the people it affects should know it well. It should be their guide. The policy should also be a living document that is reviewed and updated periodically to ensure that it is

and updated periodically to ensure that it is still pertinent and still followed. 15.6.2 Vulnerability Assessment How can we determine whether a sec urity policy has been correctly imple mented? The best way is to execute a vulnerability assessment. Such assess ments can cover broad ground, from social engineering through risk assess ment to port scans. Risk assessment ,f o re x a m p l e ,a t t e m p t st ov a l u et h ea s s e t s of the entity in question (a program, a management team, a

in question (a program, a management team, a system, or a facility) and determine the odds that a security incident will affect the entity and decrease its value. When the odds of suffering a loss and the amount of the potential loss are known, a value can be placed on trying to secure the entity. The core activity of most vulnerability assessments is a penetration test , in which the entity is scanned for known vulnerabilities. Because this book is concerned with operating systems and the

book is concerned with operating systems and the software that runs on them, we concentrate on those aspects of vulnerability assessment. Vulnerability scans typically are done at times when computer use is relatively low, to minimize their impact. When appropriate, they are done on test systems rather than production systems, because they can induce unhappy behavior from the target systems or network devices. A scan within an individual system can check a variety of aspects of the system: Short

check a variety of aspects of the system: Short or easytoguess passwords Unauthorized privileged programs, such as setuid programs Unauthorized programs in system directories Unexpectedly longrunning processes Improper directory protections on user and system directories Improper protections on system data les, such as the password le, device drivers, or the operatingsystem kernel itself Dangerous entries in the program search path (for example, the Trojan horse discussed in Section 15.2.1)

the Trojan horse discussed in Section 15.2.1) Changes to system programs detected with checksum values Unexpected or hidden network daemons Any problems found by a security scan can be either xed automatically or reported to the managers of the system.15.6 Implementing Security Defenses 691 Networked computers are much more susceptible to security attacks than are standalone systems. Rather than attacks from a known set of access points, such as directly connected terminals, we face attacks from

directly connected terminals, we face attacks from an unknown and large set of access pointsa potentially severe security problem. To a lesser extent, systems connected to telephone lines via modems are also more exposed. In fact, the U.S. government considers a system to be only as secure as its most farreaching connection. For instance, a topsecret system may be accessed only from within a building also considered topsecret. The system loses its top secret rating if any form of communication

its top secret rating if any form of communication can occur outside that environment. Some government facilities take extreme security precautions. The connectors that plug a terminal into the secure computer are locked in a safe in the ofce when the terminal is not in use. A person must have proper IDto gain access to the building and her ofce, must know a physical lock combination, and must know authentication information for the computer itself to gain access to the computeran example of

itself to gain access to the computeran example of multifactor authentication. Unfortunately for system administrators and computersecurity profes sionals, it is frequently impossible to lock a machine in a room and disallow all remote access. For instance, the Internet currently connects millions of computers and has become a missioncritical , indispensable resource for many companies and individuals. If you consider the Internet a club, then, as in any club with millions of members, there are

as in any club with millions of members, there are many good members and some bad members. The bad members have many tools they can use to attempt to gain access to the interconnected computers, just as Morris did with his worm. Vulnerability scans can be applied to networks to address some of the problems with network security. The scans search a network for ports that respond to a request. If services are enabled that should not be, access to them can be blocked, or they can be disabled. The

them can be blocked, or they can be disabled. The scans then determine the details of the application listening on that port and try to determine if it has any known vulnerabilities. Testing those vulnerabilities can determine if the system is miscongured or lacks needed patches. Finally, though, consider the use of port scanners in the hands of a cracker rather than someone trying to improve security. These tools could help crackers nd vulnerabilities to attack. (Fortunately, it is possible to

to attack. (Fortunately, it is possible to detect port scans through anomaly detection, as we discuss next.) It is a general challenge to security that the same tools can be used for good and for harm. In fact, some people advocate security through obscurity ,s t a t i n gt h a tn ot o o l ss h o u l db e written to test security, because such tools can be used to nd (and exploit) security holes. Others believe that this approach to security is not a valid one, pointing out, for example, that

not a valid one, pointing out, for example, that crackers could write their own tools. It seems reasonable that security through obscurity be considered one of the layers of security only so long as it is not the only layer. For example, a company could publish its entire network conguration, but keeping that information secret makes it harder for intruders to know what to attack or to determine what might be detected. Even here, though, a company assuming that such information will remain a

assuming that such information will remain a secret has a false sense of security. 15.6.3 Intrusion Detection Securing systems and facilities is intimately linked to intrusion detection. Intru sion detection ,a si t sn a m es u g g e s t s ,s t r i v e st od e t e c ta t t e m p t e do rs u c c e s s f u l692 Chapter 15 Security intrusions into computer systems and to initiate appropriate responses to the intrusions. Intrusion detection encompasses a wide array of techniques that vary on a

a wide array of techniques that vary on a number of axes, including the following: The time at which detection occurs. Detection can occur in real time (while the intrusion is occurring) or after the fact. The types of inputs examined to detect intrusive activity. These may include usershell commands, process system calls, and network packet headers or contents. Some forms of intrusion might be detected only by correlating information from several such sources. The range of response

from several such sources. The range of response capabilities. Simple forms of response include alerting an administrator to the potential intrusion or somehow halting the potentially intrusive activityfor example, killing a process engaged in such activity. In a sophisticated form of response, a system might transparently divert an intruders activity to a honeypot afalseresource exposed to the attacker. The resource appears real to the attacker and enables the system to monitor and gain

and enables the system to monitor and gain information about the attack. These degrees of freedom in the design space for detecting intrusions have yielded a wide range of solutions, known as intrusiondetection systems (IDSs)and intrusionprevention systems (IDPs).IDSsystems raise an alarm when an intrusion is detected, while IDPsystems act as routers, passing trafc unless an intrusion is detected (at which point that trafc is blocked). But just what constitutes an intrusion? Dening a suitable

what constitutes an intrusion? Dening a suitable specication of intrusion turns out to be quite difcult, and thus automatic IDSsa n d IDPst o d a y typically settle for one of two less ambitious approaches. In the rst, called signaturebased detection ,s y s t e mi n p u to rn e t w o r kt r a f  ci se x a m i n e df o r specic behavior patterns (or signatures )k n o w nt oi n d i c a t ea t t a c k s .As i m p l e example of signaturebased detection is scanning network packets for the string

is scanning network packets for the string etcpasswd targeted for a UNIX system. Another example is virusdetection software, which scans binaries or network packets for known viruses. The second approach, typically called anomaly detection ,a t t e m p t s through various techniques to detect anomalous behavior within computer systems. Of course, not all anomalous system activity indicates an intrusion, but the presumption is that intrusions often induce anomalous behavior. An example of anomaly

induce anomalous behavior. An example of anomaly detection is monitoring system calls of a daemon process to detect whether the systemcall behavior deviates from normal patterns, possibly indicating that a buffer overow has been exploited in the daemon to corrupt its behavior. Another example is monitoring shell commands to detect anomalous commands for a given user or detecting an anomalous login time for a user, either of which may indicate that an attacker has succeeded in gaining access to

an attacker has succeeded in gaining access to that users account. Signaturebased detection and ano maly detection can be viewed as two sides of the same coin. Signaturebased detection attempts to characterize dangerous behaviors and to detect when one of these behaviors occurs, whereas anomaly detection attempts to characterize normal (or nondangerous) behaviors and to detect when something other than these behaviors occurs. These different approaches yield IDSsa n d IDPsw i t hv e r yd i f f e

yield IDSsa n d IDPsw i t hv e r yd i f f e r e n tp r o p e r  ties, however. In particular, anomaly detection can nd previously unknown15.6 Implementing Security Defenses 693 methods of intrusion (socalled zeroday attacks ). Signaturebased detection, in contrast, will identify only known attacks that can be codied in a rec ognizable pattern. Thus, new attacks that were not contemplated when the signatures were generated will evade signatur ebased detection. This problem is well known to

ebased detection. This problem is well known to vendors of virusdetection software, who must release new signatures with great frequency as new viruses are detected manually. Anomaly detection is not necessarily superior to signaturebased detection, however. Indeed, a signicant challenge for systems that attempt anomaly detection is to benchmark normal system behavior accurately. If the system has already been penetrated when it is benchmarked, then the intrusive activity may be included in the

then the intrusive activity may be included in the normal benchmark. Even if the system is bench marked cleanly, without inuence from intrusive behavior, the benchmark must give a fairly complete picture of normal behavior. Otherwise, the number offalse positives (false alarms) or, worse, false negatives (missed intrusions) will be excessive. To illustrate the impact of even a marginally high rate of false alarms, consider an installation consisting of a hundred UNIX workstations from which

of a hundred UNIX workstations from which securityrelevant events are recorded for purposes of intrusion detection. A small installation such as this could easily generate a million audit records per day. Only one or two might be worthy of an administrators investigation. If we suppose, optimistically, that each actual attack is reected in ten audit records, we can roughly compute the rate of occurrence of audit records reecting truly intrusive activity as follows: 2intrusions day10records

activity as follows: 2intrusions day10records intrusion 106records day0.00002 . Interpreting this as a probability of occurrence of intrusive records, we denote it as P(I); that is, event Iis the occurrence of a record reecting truly intrusive behavior. Since P(I)0.00002, we also know that P(I)1P(I) 0.99998. Now we let Adenote the raising of an alarm by an IDS.A na c c u r a t e IDS should maximize both P(IA)a n d P(IA)that is,theprobabilitiesthatan alarm indicates an intrusion and that no alarm

alarm indicates an intrusion and that no alarm indicates no intrusion. Focusing onP(IA)f o rt h em o m e n t ,w ec a nc o m p u t ei tu s i n g Bayes theorem : P(IA)P(I)P(AI) P(I)P(AI)P(I)P(AI) 0.00002 P(AI) 0.00002 P(AI)0.99998 P(AI) Now consider the impact of the falsealarm rate P(AI)o n P(IA). Even with a very good truealarm rate of P(AI)0.8, a seemingly good false alarm rate of P(AI)0.0001 yields P(IA)0.14. That is, fewer than one in every seven alarms indicates a real intrusion! In systems

alarms indicates a real intrusion! In systems where a security administrator investigates each alarm, a high rate of false alarmscalled a Christmas tree effect is exceedingly wasteful and will quickly teach the administrator to ignore alarms.694 Chapter 15 Security This example illustrates a general principle for IDSsa n d IDPs: for usability, they must offer an extremely low falsealarm rate. Achieving a sufciently low falsealarm rate is an especially serious challenge for anomalydetection

especially serious challenge for anomalydetection systems, as mentioned, because of the difculties of adequately benchmarking normal system behavior. However, research continues to improve anomaly detection techniques. Intrusion detection software is evolving to implement signatures, anomaly algorithms, and other algorithms and to combine the results to arrive at a more accurate anomalydetection rate. 15.6.4 Virus Protection As we have seen, viruses can and do wreak havoc on systems. Protection

can and do wreak havoc on systems. Protection from viruses thus is an important security concern. Antivirus programs are often used to provide this protection. Some of these programs are effective against only particular known viruses. They work by searching all the programs on as y s t e mf o rt h es p e c i  cp a t t e r no fi n s t r u c t i o n sk n o w nt om a k eu pt h ev i r u s . When they nd a known pattern, they remove the instructions, disinfecting the program. Antivirus programs may

disinfecting the program. Antivirus programs may have catalogs of thousands of viruses for which they search. Both viruses and antivirus software continue to become more sophisticated. Some viruses modify themselves as they in fect other software to avoid the basic patternmatch approach of antivirus programs. Antivirus programs in turn now look for families of patterns rather than a single pattern to identify a virus. In fact, some antivirus programs implement a variety of detection algorithms.

implement a variety of detection algorithms. They can decompress compressed viruses before checking for a signature. Some also look for process anomalies. A process opening an executable le for writing is suspicious, for example, unless it is a compiler. Another popular technique is to run a program in a sandbox ,w h i c hi sac o n t r o l l e do re m u l a t e d section of the system. The antivirus software analyzes the behavior of the code in the sandbox before letting it run unmonitored. Some

sandbox before letting it run unmonitored. Some antivirus programs also put up a complete shield rather than just scanning les within a le system. They search boot sectors, memory, inbound and outbound email, les as they are downloaded, les on removable devices or media, and so on. The best protection against computer vir uses is prevention, or the practice ofsafe computing .P u r c h a s i n gu n o p e n e ds o f t w a r ef r o mv e n d o r sa n da v o i d i n g free or pirated copies from

sa n da v o i d i n g free or pirated copies from public sources or disk exchange offer the safest route to preventing infection. How ever, even new copies of legitimate software applications are not immune to virus infection: in a few cases, disgruntled employees of a software company have infected the master copies of software programs to do economic harm to the company. For macro viruses, one defense is to exchange Microsoft Word documents in an alternative le format called rich text format

an alternative le format called rich text format (RTF).U n l i k et h en a t i v eW o r df o r m a t , RTFdoes not include the capability to attach macros. Another defense is to avoid opening any email attachments from unknown users. Unfortunately, history has shown that email vulnerabilities appear as fast as they are xed. For example, in 2000, the love bug virus became very widespread by traveling in email messages that pretended to be love notes sent by friends of the receivers. O nce a

notes sent by friends of the receivers. O nce a receiver opened the attached Visual Basic script, the virus propagated by sending itself to the rst addresses in the receivers email contact list. Fortunately, except for clogging email systems15.6 Implementing Security Defenses 695 THE TRIPWIRE FILE SYSTEM An example of an anomalydetection tool is the Tripwire le system integrity checking tool for UNIX ,d e v e l o p e da tP u r d u eU n i v e r s i t y .T r i p w i r eo p e r a t e so n the

e r s i t y .T r i p w i r eo p e r a t e so n the premise that many intrusions result in modication of system directories and les. For example, an attacker might modify the system programs, perhaps inserting copies with Trojan horses, or might insert new programs into directories commonly found in usershell search paths. Or an intruder might remove system log les to cover his tracks. Tripwire is a tool to monitor le systems for added, deleted, or changed les and to alert system administrators

or changed les and to alert system administrators to these modications. The operation of Tripwire is controlled by a conguration le tw.config that enumerates the directories and les to be monitored for changes, deletions, or additions. Each entry in this conguration le includes a selection mask to specify the le attributes (inode attributes) that will be monitored for changes. For example, the selection mask might specify that a les permissions be monitored but its access time be ignored. In

be monitored but its access time be ignored. In addition, the selection mask can instruct that the le be monitored for changes. Monitoring the hash of a le for changes is as good as monitoring the le itself, and storing hashes of les requires far less room than copying the les themselves. When run initially, Tripwire takes as input the tw.config le and computes a signature for each le or directory consisting of its monitored attributes (inode attributes and hash values). These signatures are

attributes and hash values). These signatures are stored in a database. When run subsequently, Tripwire inputs both tw.config and the previously stored database, recomputes the signature for each le or directory named in tw.config , and compares this signature with the signature (if any) in the previously computed database. Events reported to an administrator include any monitored le or directory whose signature differs from that in the database (a changed le), any le or directory in a monitored

(a changed le), any le or directory in a monitored directory for which a signature does not exist in the database (an added le), and any signature in the database for which the corresponding le or directory no longer exists (a deleted le). Although effective for a wide class of attacks, Tripwire does have limita tions. Perhaps the most obvious is the need to protect the Tripwire program and its associated les, especially the database le, from unauthorized mod ication. For this reason, Tripwire

mod ication. For this reason, Tripwire and its associated les should be stored on some tamperproof medium, such as a writeprotected disk or a secure server where logins can be tightly controlled. Unfortunately, this makes it less convenient to update the database after authorized updates to monitored directories and les. A second limitation is that some securityrelevant les for example, system log lesare supposed to change over time, and Tripwire does not provide a way to distinguish between an

does not provide a way to distinguish between an authorized and an unauthorized change. So, for example, an attack that modies (without deleting) a system log that would normally change anyway would escape Tripwires detection capabilities. The best Tripwire can do in this case is to detect certain obvious inconsistencies (for example, a shrinking log le). Free and commercial versions of Tripwire are available from http:tripwire.org and http:tripwire.com .696 Chapter 15 Security and users

.696 Chapter 15 Security and users inboxes, it was relatively harmless. It did, however, effectively negate the defensive strategy of opening attac hments only from people known to the receiver. A more effective defen se method is to avoid opening any email attachment that contains executable code. Some companies now enforce this as policy by removing all incoming attachments to email messages. Another safeguard, although it d oes not prevent infection, does permit early detection. A user must

does permit early detection. A user must begin by completely reformatting the hard disk, especially the boot sector, which is often targeted for viral attack. Only secure software is uploaded, and a signature of each program is taken via a secure messagedigest computation. The resulting le name and associated message digest list must then be kept free from unauthorized access. Periodically, or each time a program is run, the operating system recomputes the signature and compares it with the

recomputes the signature and compares it with the signature on the original list; any differences serve as a warning of possible infection. This technique can be combined with others. For example, a highoverhead antivirus scan, such as a sandbox, can be used; and if a program passes the test, a signature can be created for it. If the signatures match the next time the program is run, it does not need to be virusscanned again. 15.6.5 Auditing, Accounting, and Logging Auditing, accounting, and

Accounting, and Logging Auditing, accounting, and logging can decrease system performance, but they are useful in several areas, including security. Logging can be general or specic. All systemcall executions can be logged for analysis of program behavior (or misbehavior). More typic ally, suspicious events are logged. Authentication failures and authorization failures can tell us quite a lot about breakin attempts. Accounting is another potential tool in a security administrators kit. It can be

tool in a security administrators kit. It can be used to nd performance changes, which in turn can reveal security problems. One of the early UNIX computer breakins was detected by Cliff Stoll when he was examining accounting logs and spotted an anomaly. 15.7 Firewalling to Protect Systems and Networks We turn next to the question of how a trusted computer can be connected safely to an untrustworthy network. One solution is the use of a rewall to separate trusted and untrusted systems. A rewall

separate trusted and untrusted systems. A rewall is a computer, appliance, or router that sits between the trusted and the untrusted. A network rewall limits network access between the two security domains and monitors and logs all connections. It can also limit connections based on source or destination address, source or destination port, or direction of the connection. For instance, web servers use HTTP to communicate with web browsers. A rewall therefore may allow only HTTP to pass from all

therefore may allow only HTTP to pass from all hosts outside the rewall to the web server within the rewall. The Morris Internet worm used the finger protocol to break into computers, so finger would not be allowed to pass, for example. In fact, a network rewall can separate a network into multiple domains. Ac o m m o ni m p l e m e n t a t i o nh a st h eI n t e r n e ta st h eu n t r u s t e dd o m a i n ;a semitrusted and semisecure network, called the demilitarized zone (DMZ ), as another

called the demilitarized zone (DMZ ), as another domain; and a companys computers as a third domain (Figure15.7 Firewalling to Protect Systems and Networks 697Internet access from companys computers company computers DMZ access from Internetfirewall DMZaccess between DMZ and companys computersInternet Figure 15.10 Domain separation via rewall. 15.10). Connections are allowed from the Internet to the DMZ computers and from the company computers to the Internet but are not allowed from the

to the Internet but are not allowed from the Internet or DMZ computers to the company computers. Optionally, controlled communications may be allowed between the DMZ and one company computer or more. For instance, a web server on the DMZ may need to query a database server on the corporate network. With a rewall, however, access is contained, and any DMZ systems that are broken into still are unable to access the company computers. Of course, a rewall itself must be secure and attackproof.

a rewall itself must be secure and attackproof. Otherwise, its ability to secure connections can be compromised. Furthermore, rewalls do not prevent attacks that tunnel ,o rt r a v e lw i t h i np r o t o c o l so rc o n n e c t i o n s that the rewall allows. A bufferoverow attack to a web server will not be stopped by the rewall, for example, because the HTTP connection is allowed; it is the contents of the HTTP connection that house the attack. Likewise, denial ofservice attacks can affect

Likewise, denial ofservice attacks can affect rewalls as much as any other machines. Another vulnerability of rewalls is spoong , in which an unauthorized host pretends to be an authorized host by meeting some authorization criterion. For example, if a rewall rule allows a connection from a host and identies that host by its IPaddress, then another host could send packets using that same address and be allowed through the rewall. In addition to the most common network rewalls, there are other,

the most common network rewalls, there are other, newer kinds of rewalls, each with its pros and cons. A personal rewall is a software layer either included with the operating system or added as an application. Rather than limiting communication between security domains, it limits communication to (and possibly from) a given host. A user could add ap e r s o n a l r e w a l lt oh e r PCso that a Trojan horse would be denied access to the network to which the PCis connected, for example. An

to which the PCis connected, for example. An application proxy rewall understands the protocols that applications speak across the network. For example, SMTP is used for mail transfer. An application proxy accepts a connection just as an SMTP server would and then initiates a connection to the original destination SMTP server. It can monitor the trafc as it forwards the message, watching for and disabling illegal commands, attempts to exploit698 Chapter 15 Security bugs, and so on. Some rewalls

Chapter 15 Security bugs, and so on. Some rewalls are designed for one specic protocol. An XML rewall , for example, has the specic purpose of analyzing XML trafc and blocking disallowed or malformed XML .Systemcall rewalls sit between applications and the kernel, monitoring systemcall execution. For example, in Solaris 10, the least privilege feature implements a list of more than fty system calls that processes may or may not be allowed to make. A process that does not need to spawn other

make. A process that does not need to spawn other processes can have that ability taken away, for instance. 15.8 ComputerSecurity Classications The U.S. Department of Defense Trusted Computer System Evaluation Criteria specify four security classications in systems: A, B, C, and D. This specication is widely used to determine the security of a facility and to model security solutions, so we explore it here. The lowestlevel classication is division D, or minimal protection. Division D includes

D, or minimal protection. Division D includes only one class and is used for systems that have failed to meet the requirements of any of the other security classes. For instance, MSDOS and Windows 3.1 are in division D. Division C, the next level of security, pro vides discretionary protection and accountability of users and their actions through the use of audit capabilities. Division C has two levels: C1 and C2. A C1class system incorporates some form of controls that allow users to protect

some form of controls that allow users to protect private information and to keep other users from accidentally reading or destroying their data. A C1 environment is one in which cooperating users access data at the same levels of sensitivity. Most versions of UNIX are C1 class. The total of all protection systems within a computer system (hardware, software, rmware) that correctly enforce a security policy is known as a trusted computer base (TCB).T h e TCB of a C1 system controls access

(TCB).T h e TCB of a C1 system controls access between users and les by allowing the user to specify and control sharing of objects by named individuals or dened groups. In addition, the TCB requires that the users identify themselves before they start any activities that the TCBis expected to mediate. This identication is accomplished via a protected mechanism or password. The TCBprotects the authentication data so that they are inaccessible to unauthorized users. AC 2  c l a s ss y s t e ma d

unauthorized users. AC 2  c l a s ss y s t e ma d d sa ni n d i v i d u a l  l e v e la c c e s sc o n t r o lt ot h er e q u i r e  ments of a C1 system. For example, access rights of a le can be specied to the level of a single individual. In addition, the system administrator can selectively audit the actions of any one or more users based on individual identity. The TCB also protects itself from modication of its code or data structures. In addition, no information produced by a prior user

addition, no information produced by a prior user is available to another user who accesses a storage object that has been released back to the system. Some special, secure versions of UNIX have been certied at the C2 level. DivisionB mandatoryprotection systems have all the properties of a classC2 system. In addition, they attach a sensitivity label to each object in the system. The B1class TCB maintains these labels, which are used for decisions pertaining to mandatory access control. For

pertaining to mandatory access control. For example, a user at the condential level could not access a le at the more sensitive secret level. The TCB also denotes the sensitivity level at the top and bottom of each15.9 An Example: Windows 7 699 page of any humanreadable output. In addition to the normal username password authentication information, the TCB also maintains the clearance and authorizations of individual users and will support at least two levels of security. These levels are

at least two levels of security. These levels are hierarchical, so that a user may access any objects that carry sensitivity labels equal to or lower than his security clearance. For example, a secretlevel user could access a le at the condential level in the absence of other access controls. Processes are also isolated through the use of distinct address spaces. AB 2  c l a s ss y s t e me x t e n d st h es e n s i t i v i t yl a b e l st oe a c hs y s t e mr e s o u r c e , such as storage

a c hs y s t e mr e s o u r c e , such as storage objects. Physical devices are assigned minimum and maximum security levels that the system uses to enforce constraints imposed by the physical environments in which the devices are located. In addition, a B2 system supports covert channels and the auditing of events that could lead to the exploitation of a covert channel. A B3class system allows the creation of accesscontrol lists that denote users or groups not granted access to a given named

or groups not granted access to a given named object. The TCB also contains a mechanism to monitor events that may indicate a violation of security policy. The mechanism noties the security administrator and, if necessary, terminates the event in the least disruptive manner. The highestlevel classication is d ivision A. Architecturally, a classA1 system is functionally equivalent to a B3 system, but it uses formal design specications and verication techniques, granting a high degree of assurance

techniques, granting a high degree of assurance that the TCB has been implemented correctly. A system beyond class A1might be designed and developed in a trusted facility by trusted personnel. The use of a TCB merely ensures that the system can enforce aspects of a security policy; the TCB does not specify what the policy should be. Typically, ag i v e nc o m p u t i n ge n v i r o n m e n td e v e l o p sas e c u r i t yp o l i c yf o r certication and has the plan accredited by a security

and has the plan accredited by a security agency, such as the National Computer Security Center. Certain computing environments may require other certication, such as that supplied by TEMPEST ,w h i c hg u a r d sa g a i n s te l e c t r o n i c eavesdropping. For example, a TEMPEST certied system has terminals that are shielded to prevent electromagnetic elds from escaping. This shielding ensures that equipment outside the room or building where the terminal is housed cannot detect what

where the terminal is housed cannot detect what information is being displayed by the terminal. 15.9 An Example: Windows 7 Microsoft Windows 7 is a generalpurpose operating system designed to support a variety of security features and methods. In this section, we examine features that Windows 7 uses to perform security functions. For more information and background on Windows 7, see Chapter 19. The Windows 7 security model is based on the notion of user accounts . Windows 7 allows the creation

of user accounts . Windows 7 allows the creation of any number of user accounts, which can be grouped in any manner. Access to system objects can then be permitted or denied as desired. Users are identied to the system by a unique security ID. When a user logs on, Windows 7 creates a security access token that includes the security IDfor the user, security IDs for any groups of which the user is am e m b e r ,a n dal i s to fa n ys p e c i a lp r i v i l e g e st h a tt h eu s e rh a s .E x a m

r i v i l e g e st h a tt h eu s e rh a s .E x a m p l e s of special privileges include backing up les and directories, shutting down700 Chapter 15 Security the computer, logging on interactively, and changing the system clock. Every process that Windows 7 runs on behalf of a user will receive a copy of the access token. The system uses the security IDsi nt h ea c c e s st o k e nt op e r m i to r deny access to system objects whenever the user, or a process on behalf of the user, attempts to

or a process on behalf of the user, attempts to access the object. Authentication of a user account is typically accomplished via a user name and password, although the modular design of Windows 7 allows the development of custom authentication packages. For example, a retinal (or eye) scanner might b eu s e dt ov e r i f yt h a tt h eu s e ri sw h o she says she is. Windows 7 uses the idea of a subject to ensure that programs run by a user do not get greater access to the system than the user

not get greater access to the system than the user is authorized to have. Asubject is used to track and manage permissions for each program that a user runs. It is composed of the users access token and the program acting on behalf of the user. Since Windows 7 operates with a clientserver model, two classes of subjects are used to control access: simple subjects and server subjects. An example of a simple subject is the typical application program that a user executes after she logs on. The

that a user executes after she logs on. The simple subject is assigned a security context based on the security access token of the user. A server subject is a process implemented as a protected server th at uses the security context of the client when acting on the clients behalf. As mentioned in Section 15.7, auditing is a useful security technique. Windows 7 has builtin auditing that allows many common security threats to be monitored. Examples include failure auditing for login and logoff

include failure auditing for login and logoff events to detect random password breakins, success auditing for login and logoff events to detect login activity at strange hours, success and failure writeaccess auditing for executable les to track a virus outbreak, and success and failure auditing for le access to detect access to sensitive les. Windows added mandatory integrity control, which works by assigning an integrity label to each securable object and sub ject. In order for a given subject

object and sub ject. In order for a given subject to have access to an object, it must have the access requested in the discretionary accesscontrol list, and its integrity label must be equal to or higher than that of the secured object (for the given operation ). The integrity labels in Windows 7a r e( i na s c e n d i n go r d e r ) :u n t r u s t e d ,l o w ,m e d i u m ,h i g h ,a n ds y s t e m .I n addition, three access mask bits are permitted for integrity labels: NoReadUp, NoWriteUp,

for integrity labels: NoReadUp, NoWriteUp, and NoExecuteUp. NoWriteUp is automatically enforced, so a lowerintegrity subject cannot perform a write operation on a higherintegrity object. However, unless explictly bloc ked by the security descriptor, it can perform read or execute operations. For securable objects without an explicit integrity label, a default label of medium is assigned. The label for a given subject is assigned during logon. For instance, a nonadministrative use rw i l lh a v

instance, a nonadministrative use rw i l lh a v ea ni n t e g r i t yl a b e l of medium. In addition to integrity labels, Windows Vista also added User Account Control (UAC), which represents an administrative account (not the builtin Administrators account) with two separate tokens. One, for normal usage, has the builtin Administrators group disabled and has an integrity label of medium. The other, for elevated usage, has the builtin Administrators group enabled and an integrity label of high.

group enabled and an integrity label of high. Security attributes of an object in Windows 7 are described by a security descriptor .T h es e c u r i t yd e s c r i p t o rc o n t a i n st h es e c u r i t y IDof the owner of the object (who can change the access permissions), a group security IDused15.10 Summary 701 only by the POSIX subsystem, a discretionary accesscontrol list that identies which users or groups are allowed (and which are explicitly denied) access, and a system accesscontrol

denied) access, and a system accesscontrol list that controls which auditing messages the system will generate. Optionally, the system accesscontrol list can set the integrity of the object and identify which operations to block from lowerintegrity subjects: read, write (always enforced), or e xecute. For example, the security descriptor of the le foo.bar might have owner avi and this discretionary accesscontrol list: aviall access group csreadwrite access user cliffno access In addition, it

access user cliffno access In addition, it might have a system accesscontrol list that tells the system to audit writes by everyone, along with an integrity label of medium that denies read, write, and execute to lowerintegrity subjects. An accesscontrol list is composed of accesscontrol entries that contain the security IDof the individual and an access mask that denes all possible actions on the object, with a value of AccessAllowed or AccessDenied for each action. Files in Windows 7 may have

for each action. Files in Windows 7 may have the following access types: Read Data ,WriteData ,AppendData ,Execute ,ReadExtendedAttribute ,Write ExtendedAttribute ,ReadAttributes ,a n d WriteAttributes .W ec a ns e e how this allows a ne degree of control over access to objects. Windows 7 classies objects as ei ther container objects or noncontainer objects. Container objects ,s u c ha sd i r e c t o r i e s ,c a nl o g i c a l l yc o n t a i no t h e r objects. By default, when an object is

no t h e r objects. By default, when an object is crea ted within a container object, the new object inherits permissions from the parent object. Similarly, if the user copies a le from one directory to a new directory, the le will inherit the permissions of the destination directory. Noncontainer objects inherit no other permissions. Furthermore, if a permission is changed on a directory, the new permissions do not automatically apply to existing les and subdirectories; the user may explicitly

les and subdirectories; the user may explicitly apply them if he so desires. The system administrator can prohibit printing to a printer on the system for all or part of a day and can use the Windows 7 Performance Monitor to help her spot approaching problems. In general, Windows 7 does a good job of providing features to help ensure a secure computing environment. Many of these features are not enabled by default, however, which may be one reason for the myriad security breaches on Windows 7

for the myriad security breaches on Windows 7 systems. Another reason is the vast number of services Windows 7 starts at system boot time and the number of applications that typically are installed on a Windows 7 system. For a real multiuser environment, the system administrator should formulate a security plan and implement it, using the features that Windows 7 provides and other security tools. 15.10 Summary Protection is an internal problem. Security, in contrast, must consider both the

Security, in contrast, must consider both the computer system and the environmentpeople, buildings, businesses, valuable objects, and threatswithin which the system is used.702 Chapter 15 Security The data stored in the computer system must be protected from unautho rized access, malicious destruction or alteration, and accidental introduction of inconsistency. It is easier to protect against accidental loss of data consistency than to protect against malicious access to the data. Absolute

against malicious access to the data. Absolute protection of the information stored in a computer system from malicious abuse is not possible; but the cost to the perpetrator can be made sufciently high to deter most, if not all, attempts to access that information without proper authority. Several types of attacks can be launched against programs and against individual computers or the masses. Stack and bufferoverow techniques allow successful attackers to change their level of system access.

attackers to change their level of system access. Viruses and worms are selfperpetuating, sometimes infecting thousands of computers. Denialofservice attacks prevent legitimate use of target systems. Encryption limits the domain of receivers of data, while authentication limits the domain of senders. Encryption is used to provide condentiality of data being stored or transferred. Symmetric encryption requires a shared key, while asymmetric encryption provides a public key and a private key.

provides a public key and a private key. Authentication, when combined with hashing, can prove that data have not been changed. User authentication methods are used to identify legitimate users of a system. In addition to standard username and password protection, several authentication methods are used. Onetime passwords, for example, change from session to session to avoid replay attacks. Twofactor authentication requires two forms of authentication, such as a hardware calculator with an

such as a hardware calculator with an activation PIN. Multifactor authentication uses three or more forms. These methods greatly decrease the chance of authentication forgery. Methods of preventing or detecting sec urity incidents include intrusion detection systems, antivirus software, auditing and logging of system events, monitoring of system software changes, systemcall monitoring, and rewalls. Exercises 15.1 Bufferoverow attacks can be avoided by adopting a better program ming methodology

by adopting a better program ming methodology or by using special hardware support. Discuss these solutions. 15.2 A password may become known to other users in a variety of ways. Is there a simple method for detecting that such an event has occurred? Explain your answer. 15.3 What is the purpose of using a saltalong with the userprovided password? Where should the saltbe stored, and how should it be used? 15.4 The list of all passwords is kept within the operating system. Thus, if a user manages

the operating system. Thus, if a user manages to read this list, password protection is no longer provided. Suggest a scheme that will avoid this problem. (Hint: Use different internal and external representations.) 15.5 An experimental addition to UNIX allows a user to connect a watchdog program to a le. The watchdog is invoked whenever a programBibliographical Notes 703 requests access to the le. The watchdog then either grants or denies access to the le. Discuss two pros and two cons of using

to the le. Discuss two pros and two cons of using watchdogs for security. 15.6 The UNIX program COPS scans a given system for possible security holes and alerts the user to possible problems. What are two potential hazards of using such a system for security? How can these problems be limited or eliminated? 15.7 Discuss a means by which managers of systems connected to the Internet could design their systems to limit or eliminate the damage done by worms. What are the drawbacks of making the

by worms. What are the drawbacks of making the change that you suggest? 15.8 Argue for or against the judicial sentence handed down against Robert Morris, Jr., for his creation and execution of the Internet worm discussed in Section 15.3.1. 15.9 Make a list of six security concerns for a banks computer system. For each item on your list, state whether this concern relates to physical, human, or operatingsystem security. 15.10 What are two advantages of encrypting data stored in the computer

of encrypting data stored in the computer system? 15.11 What commonly used computer programs are prone to maninthe middle attacks? Discuss solutions for preventing this form of attack. 15.12 Compare symmetric and asymmetric encryption schemes, and discuss the circumstances under which a distributed system would use one or the other. 15.13 Why doesnt Dkd,N(Eke,N(m)) provide authentication of the sender? To what uses can such an encryption be put? 15.14 Discuss how the asymmetric encryption

put? 15.14 Discuss how the asymmetric encryption algorithm can be used to achieve the following goals. a. Authentication: the receiver knows that only the sender could have generated the message. b. Secrecy: only the receiver can decrypt the message. c. Authentication and secrecy: only the receiver can decrypt the message, and the receiver knows that only the sender could have generated the message. 15.15 Consider a system that generates 10 million audit records per day. Assume that, on average,

audit records per day. Assume that, on average, there are 10 attacks per day on this system and each attack is reected in 20 records. If the intrusiondetection system has a truealarm rate of 0.6 and a falsealarm rate of 0.0005, what percentage of alarms generated by the system correspond to real intrusions?704 Chapter 15 Security Bibliographical Notes General discussions concerning security are given by [Denning (1982)], [Peeger and Peeger (2006)] and [Tanenbaum (2010)]. Computer networking is

and [Tanenbaum (2010)]. Computer networking is discussed in [Kurose and Ross (2013)]. Issues concerning the design and verication of secure systems are dis cussed by [Rushby (1981)] and by [Silverman (1983)]. A security kernel for a multiprocessor microcomputer is described by [Schell (1983)]. A distributed secure system is described by [Rushby and Randell (1983)]. [Morris and Thompson (1979)] discuss password security. [Morshedian (1986)] presents methods to ght password pirates. Password

presents methods to ght password pirates. Password authentication with insecure communications is considered by [Lamport (1981)]. The issue of password cracking is examined by [Seely (1989)]. Computer breakins are discussed by [Lehmann (1987)] and by [Reid (1987)]. Issues related to trusting computer programs are discussed in [Thompson (1984)]. Discussions concerning UNIX security are offered by [Grampp and Morris (1984)], [Wood and Kochan (1985)], [Farrow (1986)], [Filipski and Hanko (1986)],

[Farrow (1986)], [Filipski and Hanko (1986)], [Hecht et al. (1988)], [Kramer (1988)], and [Garnkel et al. (2003)]. [Bershad and Pinkerton (1988)] present the watchdog extension to BSD UNIX . [Spafford (1989)] presents a detailed technical discussion of the Internet worm. The Spafford article appears with three others in a special section on the Morris Internet worm in Communications of the ACM (Volume 32, Number 6, June 1989). Security problems associated with the TCPIP protocol suite are

associated with the TCPIP protocol suite are described in [Bellovin (1989)]. The mechanisms commonly used to prevent such attacks are discussed in [Cheswick et al. (2003)]. Another approach to protecting networks from insider attacks is to secure topology or route discovery. [Kent et al. (2000)], [Hu et al. (2002)], [Zapata and Asokan (2002)], and [Hu and Perrig (2004)] present solutions for secure routing. [Savage et al. (2000)] examine the distributed denialofservice attack and propose

the distributed denialofservice attack and propose IPtraceback solutions to address the problem. [Perlman (1988)] proposes an approach to diagnose faults when the network contains malicious routers. Information about viruses and worms can be found at http:www.securelist.com , as well as in [Ludwig (1998)] and [Ludwig (2002)]. Another website containing uptodate security informa tion is http:www.eeye.comresourcessecuritycenterresearch .A paper on the dangers of a computer monoculture can be found

the dangers of a computer monoculture can be found at http:cryptome.orgcyberinsecurity.htm . [Dife and Hellman (1976)] and [Dife and Hellman (1979)] were the rst researchers to propose the use of the publickey encryption scheme. The algorithm presented in Section 15.4.1 is based on the publickey encryption scheme; it was developed by [Rivest et al. (1978)]. [C. Kaufman (2002)] and [Stallings (2011)] explore the use of cryptography in computer systems. Discussions concerning protection of digital

Discussions concerning protection of digital signatures are offered by [Akl (1983)], [Davies (1983)], [Denning (1983)], and [Denning (1984)]. Complete cryptography information is presented in [Schneier (1996)] and [Katz and Lindell (2008)]. The RSA algorithm is presented in [Rivest et al. (1978)]. Information about NIST sAES activities can be found at http:www.nist.govaes ;i n f o r m a t i o n about other cryptographic standards for the United States can also be foundBibliography 705 at that

States can also be foundBibliography 705 at that site. In 1999, SSL3.0 was modied slightly and presented in an IETF Request for Comments ( RFC)u n d e rt h en a m e TLS. The example in Section 15.6.3 illustrating the impact of falsealarm rate on the effectiveness of IDSsi sb a s e do n[ A x e l s s o n( 1 9 9 9 ) ] .T h ed e s c r i p t i o no f Tripwire in Section 15.6.5 is based on [Kim and Spafford (1993)]. Research into systemcallbased anomaly detection is described in [Forrest et al.

anomaly detection is described in [Forrest et al. (1996)]. The U.S. government is, of course, concerned about security. The Depart ment of Defense Trusted Computer System Evaluation Criteria ([DoD (1985)]), known also as the Orange Book, describes a set of security levels and the features that an operating system must have to qualify for each security rating. Reading it is a good starting point for understanding security concerns. The Microsoft Windows NTWorkstation Resource Kit ([Microsoft

Windows NTWorkstation Resource Kit ([Microsoft (1996)]) describes the security model of NTand how to use that model. Bibliography [Akl (1983)] S. G. Akl, Digital Signatures: A Tutorial Survey ,Computer ,V o l u m e 16, Number 2 (1983), pages 1524. [Axelsson (1999)] S. Axelsson, The BaseRate Fallacy and Its Implications for Intrusion Detection ,Proceedings of the ACM Conference on Computer and Communications Security (1999), pages 17. [Bellovin (1989)] S. M. Bellovin, Security Problems in the

(1989)] S. M. Bellovin, Security Problems in the TCPIP Protocol Suite ,Computer Communications Review ,V o l u m e1 9 : 2 ,( 1 9 8 9 ) ,p a g e s3 2  4 8 . [Bershad and Pinkerton (1988)] B. N. Bershad and C. B. Pinkerton, Watchdogs: Extending the Unix File System ,Proceedings of the Winter USENIX Conference (1988). [C. Kaufman (2002)] M. S. C. Kaufman, R. Perlman, Network Security: Private Communication in a Public World, Second Edition, Prentice Hall (2002). [Cheswick et al. (2003)] W.

Prentice Hall (2002). [Cheswick et al. (2003)] W. Cheswick, S. Bellovin, and A. Rubin, Firewalls and Internet Security: Repelling the Wily Hacker, Second Edition, AddisonWesley (2003). [Davies (1983)] D. W. Davies, Applying the RSA Digital Signature to Electronic Mail ,Computer ,V o l u m e1 6 ,N u m b e r2( 1 9 8 3 ) ,p a g e s5 5  6 2 . [Denning (1982)] D. E. Denning, Cryptography and Data Security ,A d d i s o n  Wesley (1982). [Denning (1983)] D. E. Denning, Protecting Public Keys and

(1983)] D. E. Denning, Protecting Public Keys and Signature Keys , Computer ,V o l u m e1 6 ,N u m b e r2( 1 9 8 3 ) ,p a g e s2 7  3 5 . [Denning (1984)] D. E. Denning, Digital Signatures with RSA and Other PublicKey Cryptosystems ,Communications of the ACM ,V o l u m e2 7 ,N u m b e r4 (1984), pages 388392. [Dife and Hellman (1976)] W. Dife and M. E. Hellman, New Directions in Cryptography ,IEEE Transactions on Information Theory ,V o l u m e2 2 ,N u m b e r6 (1976), pages 644654.706 Chapter

2 ,N u m b e r6 (1976), pages 644654.706 Chapter 15 Security [Dife and Hellman (1979)] W. Dife and M. E. Hellman, Privacy and Authen tication ,Proceedings of the IEEE (1979), pages 397427. [DoD (1985)] Trusted Computer System Evaluation Criteria .D e p a r t m e n t o f Defense (1985). [Farrow (1986)] R. Farrow, Security Issues and Strategies for Users ,UNIX World (April 1986), pages 6571. [Filipski and Hanko (1986)] A. Filipski and J. Hanko, Making UNIX Secure , Byte (April 1986), pages 113128.

UNIX Secure , Byte (April 1986), pages 113128. [Forrest et al. (1996)] S. Forrest, S. A. Hofmeyr, and T. A. Longstaff, AS e n s e of Self for UNIX Processes ,Proceedings of the IEEE Symposium on Security and Privacy (1996), pages 120128. [Garnkel et al. (2003)] S. Garnkel, G. Spafford, and A. Schwartz, Practical UNIX  Internet Security ,O  R e i l l yA s s o c i a t e s( 2 0 0 3 ) . [Grampp and Morris (1984)] F. T. Grampp and R. H. Morris, UNIX Oper atingSystem Security ,ATT Bell Laboratories

Oper atingSystem Security ,ATT Bell Laboratories Technical Journal ,V o l u m e6 3 , Number 8 (1984), pages 16491672. [Hecht et al. (1988)] M. S. Hecht, A. Johri, R. Aditham, and T. J. Wei, Experience Adding C2 Security Features to UNIX ,Proceedings of the Summer USENIX Conference (1988), pages 133146. [Hu and Perrig (2004)] Y.C. Hu and A. Perrig, SPV: A Secure Path Vector Routing Scheme for Securing BGP ,Proceedings of ACM SIGCOMM Conference on Data Communication (2004). [Hu et al. (2002)] Y.C.

Data Communication (2004). [Hu et al. (2002)] Y.C. Hu, A. Perrig, and D. Johnson, Ariadne: A Secure OnDemand Routing Protocol for Ad Hoc Networks ,Proceedings of the Annual International Conference on Mobile Computing and Networking (2002). [Katz and Lindell (2008)] J. Katz and Y. Lindell, Introduction to Modern Cryptog raphy ,C h a p m a nH a l l  C R CP r e s s( 2 0 0 8 ) . [Kent et al. (2000)] S. Kent, C. Lynn, and K. Seo, Secure Border Gateway Protocol (SecureBGP) ,IEEE Journal on Selected

Protocol (SecureBGP) ,IEEE Journal on Selected Areas in Communications ,V o l u m e 18, Number 4 (2000), pages 582592. [Kim and Spafford (1993)] G. H. Kim and E. H. Spafford, The Design and Implementation of Tripwire: A File System Integrity Checker ,T e c h n i c a lr e p o r t , Purdue University (1993). [Kramer (1988)] S. M. Kramer, Retaining SUID Programs in a Secure UNIX , Proceedings of the Summer USENIX Conference (1988), pages 107118. [Kurose and Ross (2013)] J. Kurose and K. Ross,

[Kurose and Ross (2013)] J. Kurose and K. Ross, Computer NetworkingA Top Down Approach, Sixth Edition, AddisonWesley (2013). [Lamport (1981)] L. Lamport, Password Authentication with Insecure Com munications ,Communications of the ACM ,V o l u m e2 4 ,N u m b e r1 1( 1 9 8 1 ) ,p a g e s 770772. [Lehmann (1987)] F. Lehmann, Computer BreakIns ,Communications of the ACM ,V o l u m e3 0 ,N u m b e r7( 1 9 8 7 ) ,p a g e s5 8 4  5 8 5 .Bibliography 707 [Ludwig (1998)] M. Ludwig, The Giant Black Book

[Ludwig (1998)] M. Ludwig, The Giant Black Book of Computer Viruses, Second Edition, American Eagle Publications (1998). [Ludwig (2002)] M. Ludwig, The Little Black Book of Email Viruses ,A m e r i c a n Eagle Publications (2002). [Microsoft (1996)] Microsoft Windows NT Workstation Resource Kit .M i c r o s o f t Press (1996). [Morris and Thompson (1979)] R. Morris and K. Thompson, Password Secu rity: A Case History ,Communications of the ACM ,V o l u m e2 2 ,N u m b e r1 1( 1 9 7 9 ) , pages

,V o l u m e2 2 ,N u m b e r1 1( 1 9 7 9 ) , pages 594597. [Morshedian (1986)] D. Morshedian, How to Fight Password Pirates ,Com puter ,V o l u m e1 9 ,N u m b e r1( 1 9 8 6 ) . [Perlman (1988)] R. Perlman, Network Layer Protocols with Byzantine Robustness . PhD thesis, Massachusetts Institute of Technology (1988). [Peeger and Peeger (2006)] C. Peeger and S. Peeger, Security in Computing, Fourth Edition, Prentice Hall (2006). [Reid (1987)] B. Reid, Reections on Some Recent Widespread Computer

Reid, Reections on Some Recent Widespread Computer BreakIns ,Communications of the ACM ,V o l u m e3 0 ,N u m b e r2( 1 9 8 7 ) ,p a g e s 103105. [Rivest et al. (1978)] R. L. Rivest, A. Shamir, and L. Adleman, On Digital Signatures and Public Key Cryptosystems ,Communications of the ACM ,V o l u m e 21, Number 2 (1978), pages 120126. [Rushby (1981)] J. M. Rushby, Design and Verication of Secure Systems , Proceedings of the ACM Symposium on Operating Systems Principles (1981), pages 1221.

Operating Systems Principles (1981), pages 1221. [Rushby and Randell (1983)] J. Rushby and B. Randell, AD i s t r i b u t e dS e c u r e System ,Computer ,V o l u m e1 6 ,N u m b e r7( 1 9 8 3 ) ,p a g e s5 5  6 7 . [Savage et al. (2000)] S. Savage, D. Wetherall, A. R. Karlin, and T. Anderson, Practical Network Support for IP Traceback ,Proceedings of ACM SIGCOMM Conference on Data Communication (2000), pages 295306. [Schell (1983)] R. R. Schell, AS e c u r i t yK e r n e lf o raM u l t i p r o

AS e c u r i t yK e r n e lf o raM u l t i p r o c e s s o rM i c r o c o m  puter ,Computer (1983), pages 4753. [Schneier (1996)] B. Schneier, Applied Cryptography, Second Edition, John Wiley and Sons (1996). [Seely (1989)] D. Seely, Password Cracking: A Game of Wits ,Communications of the ACM ,V o l u m e3 2 ,N u m b e r6( 1 9 8 9 ) ,p a g e s7 0 0  7 0 4 . [Silverman (1983)] J. M. Silverman, Reections on the Verication of the Security of an Operating System Kernel ,Proceedings of the ACM

an Operating System Kernel ,Proceedings of the ACM Symposium on Operating Systems Principles (1983), pages 143154. [Spafford (1989)] E. H. Spafford, The Internet Worm: Crisis and Aftermath , Communications of the ACM ,V o l u m e3 2 ,N u m b e r6( 1 9 8 9 ) ,p a g e s6 7 8  6 8 7 .708 Chapter 15 Security [Stallings (2011)] W. Stallings, Operating Systems, Seventh Edition, Prentice Hall (2011). [Tanenbaum (2010)] A. S. Tanenbaum, Computer Networks, Fifth Edition, Pren tice Hall (2010). [Thompson

Fifth Edition, Pren tice Hall (2010). [Thompson (1984)] K. Thompson, Reections on Trusting Trust ,Communica tions of ACM ,V o l u m e2 7 ,N u m b e r8( 1 9 8 4 ) ,p a g e s7 6 1  7 6 3 . [Wood and Kochan (1985)] P. Wo o d a n d S . K o c h a n , UNIX System Security , Hayden (1985). [Zapata and Asokan (2002)] M. Zapata and N. Asokan, Securing Ad Hoc Routing Protocols ,Proc. 2002 ACM Workshop on Wireless Security (2002), pages 110.Part Six Advanced Topics Virtualization permeates all aspects of

Topics Virtualization permeates all aspects of computing. Virtual machines are one instance of this trend. Generally, with a virtual machine, guest oper ating systems and applications run in an environment that appears to them to be native hardware. This environment behaves toward them as native hardware would but also protects, manages, and limits them. A distributed system is a collection of processors that do not share memory or a clock. Instead, each processor has its own local memory, and

each processor has its own local memory, and the processors communicate with on ea n o t h e rt h r o u g hc o m m u n i c a  tion lines such as localarea or widearea networks. Distributed systems offer several benets: they give users access to more of the resources maintained by the system, speed computation, and improve data avail ability and reliability.16CHAPTER Virtual Machines The term virtualization has many meanings, and aspects of virtualization permeate all aspects of computing.

virtualization permeate all aspects of computing. Virtual machines are one instance of this trend. Generally, with a virtual machine, guest operating systems and applications run in an environment that appears to them to be native hardware and that behaves toward them as native hardware would but that also protects, manages, and limits them. This chapter delves into the u ses, features, and implementation of virtual machines. Virtual machines can be implemented in several ways, and this chapter

be implemented in several ways, and this chapter describes these options. One option is to add virtual machine support to the kernel. Because that implementation method is the most pertinent to this book, we explore it most fully. Additionally, hardware features provided by theCPU and even by IOdevices can support virtual machine implementation, so we discuss how those features are used by the appropriate kernel modules. CHAPTER OBJECTIVES To explore the history and benets of virtual machines.

the history and benets of virtual machines. To discuss the various virtual machine technologies. To describe the methods used to implement virtualization. To show the most common hardware features that support virtualization and explain how they are used by operatingsystem modules. 16.1 Overview The fundamental idea behind a virtual machine is to abstract the hardware of a single computer (the CPU, memory, disk drives, network interface cards, and so forth) into several different execu tion

and so forth) into several different execu tion environments, thereby creating the illusion that each separate environment is running on its own private computer. This concept may seem similar to the layered approach of operating system implementation (see Section 2.7.2), and in some ways it is. In the case of virtualization, there is a layer that creates a virtual system on which operating systems or applications can run. 711712 Chapter 16 Virtual Machines Virtual machine implementations

Virtual Machines Virtual machine implementations involve several components. At the base is the host ,t h eu n d e r l y i n gh a r d w a r es y s t e mt h a tr u n st h ev i r t u a lm a c h i n e s . Thevirtual machine manager (VMM )(also known as a hypervisor )c r e a t e sa n d runs virtual machines by providing an interface that is identical to the host (except in the case of paravirtualization, discussed later). Each guest process is provided with a virtual copy of the host (Figure 16.1).

with a virtual copy of the host (Figure 16.1). Usually, the guest process is in fact an operating system. A single physical machine can thus run multiple operating systems concurrently, each in its own virtual machine. Take a moment to note that with virtualization, the denition of operating system once again blurs. For example, consider VMM software such as VMware ESX.T h i sv i r t u a l i z a t i o ns o f t w a r ei si n s t a l l e do nt h eh a r d w a r e ,r u n sw h e nt h e hardware

h eh a r d w a r e ,r u n sw h e nt h e hardware boots, and provides services to applications. The services include traditional ones, such as scheduling and memory management, along with new types, such as migration of applications between systems. Furthermore, the applications are in fact guest operating systems. Is the VMware ESX VMM an operating system that, in turn, runs other operating systems? Certainly it acts like an operating system. For clarity, however, we call the component that

For clarity, however, we call the component that provides virtual environments a VMM . The implementation of VMM s varies greatly. Options include the following: Hardwarebased solutions that provide support for virtual machine cre ation and management via rmware. These VMM s, which are commonly found in mainframe and large to midsized servers, are generally known astype 0 hypervisors .IBM LPARs and Oracle LDOMs are examples. Operatingsystemlike software built to provide virtualization, including

built to provide virtualization, including VMware ESX(mentioned above), Joyent Smart OS,a n dC i t r i xX e n S e r v e r . These VMMs are known as type 1 hypervisors . (a)processes hardwarekernel(b)programming interfaceprocesses processesprocesses kernel kernel kernel VM2 VM1 VM3 managerhardwarevirtual machine Figure 16.1 System models. (a) Nonvirtual machine. (b) Virtual machine.16.2 History 713 INDIRECTION All problems in computer science can be solved by another level of indirection

can be solved by another level of indirection DavidWheeler ...e x c e p tf o rt h ep r o b l e mo ft o om a n yl a y e r s of indirection. Kevlin Henney Generalpurpose operating systems that provide standard functions as well as VMM functions, including Microsoft Windows Server with HyperV and RedHat Linux with the KVM feature. Because such systems have a feature set similar to type 1 hypervisors, they are also known as type 1. Applications that run on standard operating systems but provide VMM

run on standard operating systems but provide VMM features to guest operating systems. These applications, which include VMware Workstation and Fusion, Parallels Desktop, and Oracle Virtual Box, are type 2 hypervisors . Paravirtualization ,at e c h n i q u ei nw h i c ht h eg u e s to p e r a t i n gs y s t e mi s modied to work in cooperation with the VMM to optimize performance. Programmingenvironment virtualization ,i nw h i c h VMM sd on o tv i r t u  alize real hardware but instead create

tv i r t u  alize real hardware but instead create an optimized virtual system. This technique is used by Oracle Java and Microsoft.Net. Emulators that allow applications written for one hardware environment to run on a very different hardware environment, such as a different type ofCPU. Application containment ,w h i c hi sn o tv i r t u a l i z a t i o na ta l lb u tr a t h e r provides virtualizationlike features by segregating applications from the operating system. Oracle Solaris Zones, BSD

the operating system. Oracle Solaris Zones, BSD Jails, and IBM AIX WPARs contain applications, making them more secure and manageable. The variety of virtualization techniques in use today is a testament to the breadth, depth, and importance of virtualization in modern computing. Virtualization is invaluable for datacenter operations, efcient application development, and software testin g, among many other uses. 16.2 History Virtual machines rst appeared commercially on IBM mainframes in 1972.

appeared commercially on IBM mainframes in 1972. Virtualization was provided by the IBM VM operating system. This system has evolved and is still available. In addition, many of its original concepts are found in other systems, making it worth exploring. IBM VM 370 divided a mainframe into multiple virtual machines, each running its own operating system. A major difculty with the VMapproach involved disk systems. Suppose that the physical machine had three disk drives but wanted to support seven

had three disk drives but wanted to support seven virtual machines. Clearly, it could not allocate a disk drive to each virtual machine. The solution was to provide virtual disks termed minidisks inIBMsVMoperating system. The minidisks are identical714 Chapter 16 Virtual Machines to the systems hard disks in all respects except size. The system implemented each minidisk by allocating as many tracks on the physical disks as the minidisk needed. Once the virtual machines were created, users could

the virtual machines were created, users could run any of the operating systems or software packages that were available on the underlying machine. For the IBM VM system, a user normally ran CMS a singleuser interactive operating system. For many years after IBM introduced this technology, virtualization remained in its domain. Most systems could not support virtualization. However, a formal denition of virtualization helped to establish system requirements and a target for functionality. The

requirements and a target for functionality. The virtualization requirements stated that: 1.AVMM provides an environment for programs that is essentially identical to the original machine. 2.Programs running within that environment show only minor perfor mance decreases. 3.The VMM is in complete control of system resources. These requirements of delity, performance, and safety still guide virtualiza tion efforts today. By the late 1990s, Intel 80x86 CPUsh a db e c o m ec o m m o n ,f a s t ,a n

CPUsh a db e c o m ec o m m o n ,f a s t ,a n dr i c h in features. Accordingly, developers launched multiple efforts to implement virtualization on that platform. Both Xen and VMware created technologies, still used today, to allow guest operating systems to run on the 80x86. Since that time, virtualization has expanded to include all common CPUs, many commercial and opensource tools, and many operating systems. For example, the opensource VirtualBox project ( http:www.virtualbox.org )p r o v i

project ( http:www.virtualbox.org )p r o v i d e sa program than runs on Intel x86 and AMD64 CPUsa n do nW i n d o w s ,L i n u x , Mac OS X,a n dS o l a r i sh o s to p e r a t i n gs y s t e m s .P o s s i b l eg u e s to p e r a t i n gs y s t e m s include many versions of Windows, Linux, Solaris, and BSD,i n c l u d i n ge v e n MSDOS and IBM OS2 . 16.3 Benets and Features Several advantages make virtualization attractive. Most of them are fundamen tally related to the ability to share the

fundamen tally related to the ability to share the same hardware yet run several different execution environments (that is, different operating systems) concurrently. One important advantage of virtualization is that the host system is protected from the virtual machines, just as the virtual machines are protected from each other. A virus inside a guest operating system might damage that operating system but is unlikely to affect the host or the other guests. Because each virtual machine is

the other guests. Because each virtual machine is almost completely isolated from all other virtual machines, there are almost no protection problems. Ap o t e n t i a ld i s a d v a n t a g eo fi s o l a t i o ni st h a ti tc a np r e v e n ts h a r i n go f resources. Two approaches to provide sharing have been implemented. First, it is possible to share a lesystem volume and thus to share les. Second, it is possible to dene a network of virtual machines, each of which can16.3 Benets and

virtual machines, each of which can16.3 Benets and Features 715 send information over the virtual communications network. The network is modeled after physical communication networks but is implemented in software. Of course, the VMM is free to allow any number of its guests to use physical resources, such as a physical network connection (with sharing provided by the VMM ), in which case the allowed guests could communicate with each other via the physical network. One feature common to most

the physical network. One feature common to most virtualization implementations is the ability to freeze, or suspend ,ar u n n i n gv i r t u a lm a c h i n e .M a n yo p e r a t i n gs y s t e m s provide that basic feature for processes, but VMM sg oo n es t e pf u r t h e ra n d allow copies and snapshots to be made of the guest. The copy can be used to create a new VMor to move a VMfrom one machine to another with its current state intact. The guest can then resume where it was, as if on its

guest can then resume where it was, as if on its original machine, creating a clone .T h es n a p s h o tr e c o r d sap o i n ti nt i m e ,a n dt h eg u e s t can be reset to that point if necessary (for example, if a change was made but is no longer wanted). Often, VMM sa l l o wm a n ys n a p s h o t st ob et a k e n .F o r example, snapshots might record a guests state every day for a month, making restoration to any of those snapshot states possible. These abilities are used to good

states possible. These abilities are used to good advantage in virtual environments. Av i r t u a lm a c h i n es y s t e mi sap e r f e c tv e h i c l ef o ro p e r a t i n g  s y s t e mr e s e a r c h and development. Normally, changing an operating system is a difcult task. Operating systems are large and complex programs, and a change in one part may cause obscure bugs to appear in some other part. The power of the operating system makes changing it particularly dangerous. Because the

changing it particularly dangerous. Because the operating system executes in kernel mode, a wrong change in a pointer could cause an error that would destroy the entire le system. Thus, it is necessary to test all changes to the operating system carefully. Furthermore, the operating system runs on and controls the entire machine, meaning that the system must be stopped and taken out of use while changes are made and tested. This period is commonly called systemdevelopment time .S i n c ei tm a k

called systemdevelopment time .S i n c ei tm a k e st h es y s t e mu n a v a i l a b l et ou s e r s ,s y s t e m  d e v e l o p m e n t time on shared systems is often scheduled late at night or on weekends, when system load is low. A virtualmachine system can eliminate much of this latter problem. System programmers are given their own virtual machine, and system develop ment is done on the virtual machine instead of on a physical machine. Normal system operation is disrupted only when a

Normal system operation is disrupted only when a completed and tested change is ready to be put into production. Another advantage of virtual machines for developers is that multiple operating systems can run concurrently on the developers workstation. This virtualized workstation allows for rapid porting and testing of programs in varying environments. In addition, multiple versions of a program can run, each in its own isolated operating system, within one system. Similarly, quality assurance

within one system. Similarly, quality assurance engineers can test their applications in multiple environments without buying, powering, and maintaining a computer for each environment. Am a j o ra d v a n t a g eo fv i r t u a lm a c h i n e si np r o d u c t i o nd a t a  c e n t e ru s ei s system consolidation ,w h i c hi n v o l v e st a k i n gt w oo rm o r es e p a r a t es y s t e m s and running them in virtual machines on one system. Such physicaltovirtual conversions result in

Such physicaltovirtual conversions result in resource optimization, since many lightly used systems can be combined to create one more heavily used system.716 Chapter 16 Virtual Machines Consider, too, that management tools that are part of the VMM allow system administrators to manage many more systems than they otherwise could. A virtual environment might include 100 physical servers, each running 20 virtual servers. Without virtualization, 2,000 servers would require several system

2,000 servers would require several system administrators. With virtualization and its tools, the same work can be managed by one or two administrators. One of the tools that make this possible istemplating ,i nw h i c ho n es t a n d a r dv i r t u a lm a c h i n ei m a g e ,i n c l u d i n ga n installed and congured guest operating system and applications, is saved and used as a source for multiple running VMs. Other features include managing the patching of all guests, backing up and

the patching of all guests, backing up and restoring the guests, and monitoring their resource use. Virtualization can improve not only resource utilization but also resource management. Some VMM si n c l u d ea live migration feature that moves a running guest from one physical server to another without interrupting its operation or active network connections. If a server is overloaded, live migration can thus free resources on the source host while not disrupting the guest. Similarly, when

while not disrupting the guest. Similarly, when host hardware must be repaired or upgraded, guests can be migrated to other servers, the evacuated host can be maintained, and then the guests can be migrated back. This operation occurs without downtime and without interruption to users. Think about the possible effects of virtualization on how applications are deployed. If a system can easily add, remove, and move a virtual machine, then why install applications on that system directly? Instead,

applications on that system directly? Instead, the application could be preinstalled on a tuned and customized operating system in a virtual machine. This method would offer several benets for application developers. Application management would become easier, less tuning would be required, and technical support of the application would be more straightforward. System administrators would nd the environment easier to manage as well. Installation would be simple, and redeploying the application

would be simple, and redeploying the application to another system would be much easier than the usual steps of uninstalling and reinstalling. For widespread adoption of this methodology to occur, though, the format of virtual machines must be standardized so that any virtual machine will run on any virtualization platform. The Open Virtual Machine Format is an attempt to provide such standardization, and it could succeed in unifying virtual machine formats. Virtualization has laid the

machine formats. Virtualization has laid the foundation for many other advances in computer facility implementation, management, and monitoring. Cloud computing , for example, is made possible by virtualization in which resources such as CPU,m e m o r y ,a n d IOare provided as services to customers using Internet technologies. By using APIs, a program can tell a cloud computing facility to create thousands of VMs, all running a specic guest operating system and application, which others can

operating system and application, which others can access via the Internet. Many multiuser games, photosharing sites, and other web services use this functionality. In the area of desktop computing, virtualization is enabling desktop and laptop computer users to connect remotely to virtual machines located in remote data centers and access their applications as if they were local. This practice can increase security, because no data are stored on local disks at the users site. The cost of the

on local disks at the users site. The cost of the users computing resource may also decrease. The user must have networking, CPU,a n ds o m em e m o r y ,b u ta l lt h a tt h e s es y s t e m components need to do is display an image of the guest as its runs remotely (via16.4 Building Blocks 717 ap r o t o c o ls u c ha s RDP). Thus, they need no t be expensive, highperformance components. Other uses of virtualization are sure to follow as it becomes more prevalent and hardware support continues

more prevalent and hardware support continues to improve. 16.4 Building Blocks Although the virtual machine concept is useful, it is difcult to implement. Much work is required to provide an exact duplicate of the underlying machine. This is especially a challenge on dualmode systems, where the underlying machine has only user mode and kernel mode. In this section, we examine the building blocks that are needed for efcient virtualization. Note that these building blocks are not required by type

these building blocks are not required by type 0 hypervisors, as discussed in Section 16.5.2. The ability to virtualize depends on the features provided by the CPU.I f the features are sufcient, then it is possible to write a VMM that provides ag u e s te n v i r o n m e n t .O t h e r w i s e ,v i r t u a l i z a t i o ni si m p o s s i b l e . VMM su s e several techniques to implement virtualization, including trapandemulate and binary translation. We discuss each of these techniques in this

We discuss each of these techniques in this section, along with the hardware support needed to support virtualization. One important concept found in most virtualization options is the imple mentation of a virtual CPU (VCPU ).T h e VCPU does not execute code. Rather, it represents the state of the CPU as the guest machine believes it to be. For each guest, the VMM maintains a VCPU representing that guests current CPU state. When the guest is contextswitched onto a CPU by the VMM ,i n f o r m a t

onto a CPU by the VMM ,i n f o r m a t i o n from the VCPU is used to load the right context, much as a generalpurpose operating system would use the PCB. 16.4.1 TrapandEmulate On a typical dualmode system, the virtual machine guest can execute only in user mode (unless extra hardware support is provided). The kernel, of course, runs in kernel mode, and it is not safe to allow userlevel code to run in kernel mode. Just as the physical machine h as two modes, however, so must the virtual machine.

two modes, however, so must the virtual machine. Consequently, we must have av i r t u a lu s e rm o d ea n dav i r t u a lk e r n e l mode, both of which run in physical user mode. Those actions that cause a transfer from user mode to kernel mode on a real machine (such as a system call, an interrupt, or an attempt to execute a privileged instruction) must also cause a transfer from virtual user mode to virtual kernel mode in the virtual machine. How can such a transfer be accomplished? The

How can such a transfer be accomplished? The procedure is as follows: When the kernel in the guest attempts to execute a privileged instruction, that is an error (because the system is in user mode) and causes a trap to the VMM in the real machine. The VMM gains control and executes (or emulates )t h e action that was attempted by the guest kernel on the part of the guest. It then returns control to the virtual machine. This is called the trapandemulate method and is shown in Figure 16.2. Most

method and is shown in Figure 16.2. Most virtualization products use this method to one extent or other. With privileged instructions, time becomes an issue. All nonprivileged instructions run natively on the hardware, providing the same performance718 Chapter 16 Virtual MachinesPrivileged Instruction Operating System VCPU VMMVMMGuest Kernel ModeUser Mode Emulate ActionReturn Trap UpdateUser Processes Figure 16.2 Trapandemulate virtualization implementation. for guests as native applications.

implementation. for guests as native applications. Privileged instructions create extra overhead, however, causing the guest to run more slowly than it would natively. In addition, the CPU is being multiprogrammed among many virtual machines, which can further slow down the virtual machines in unpredictable ways. This problem has been approached in various ways. IBM VM ,f o re x a m p l e , allows normal instructions for the virtual machines to execute directly on the hardware. Only the

to execute directly on the hardware. Only the privileged instructions (needed mainly for IO)m u s t be emulated and hence execute more slowly. In general, with the evolution of hardware, the performance of trapandemulate functionality has been improved, and cases in which it is needed have been reduced. For example, many CPUsn o wh a v ee x t r am o d e sa d d e dt ot h e i rs t a n d a r dd u a l  m o d e operation. The VCPU need not keep track of what mode the guest operating system is in,

of what mode the guest operating system is in, because the physical CPU performs that function. In fact, some CPUsp r o v i d eg u e s t CPU state management in hardware, so the VMM need not supply that functionality, removing the extra overhead. 16.4.2 Binary Translation Some CPUsd on o th a v eac l e a ns e p a r a t i o no fp r i v i l e g e da n dn o n p r i v i l e g e d instructions. Unfortunately for virtualization implementers, the Intel x86 CPU line is one of them. No thought was given

x86 CPU line is one of them. No thought was given to running virtualization on the x86 when it was designed. (In fact, the rst CPU in the familythe Intel 4004, released in 1971was designed to be the core of a calculator.) The chip has maintained backward compatibility throughout its lifetime, preventing changes that would have made vi rtualization easier through many generations. Lets consider an example of the problem. The command popf loads the ag register from the contents of the stack. If

the ag register from the contents of the stack. If the CPU is in privileged mode, all of the ags are replaced from the stack. If the CPU is in user mode, then only some ags are replaced, and others are ignored. Because no trap is generated ifpopf is executed in user mode, the trapandemulate procedure is rendered16.4 Building Blocks 719 useless. Other x86 instructions cause similar problems. For the purposes of this discussion, we will call this set of instructions special instructions .A sr e c

set of instructions special instructions .A sr e c e n t l y as 1998, Judi 1998 doesnt seem that recent using the trapandemulate method to implement virtualization on the x86 was considered impossible because of these special instructions. This previously insurmountable prob lem was solved with the implemen tation of the binary translation technique. Binary translation is fairly simple in concept but complex in implementation. The basic steps are as follows: 1.If the guest VCPU is in user mode,

as follows: 1.If the guest VCPU is in user mode, the guest can run its instructions natively on a physical CPU. 2.If the guest VCPU is in kernel mode, then the guest believes that it is running in kernel mode. The VMM examines every instruction the guest executes in virtual kernel mode by reading t he next few instructions that the guest is going to execute, based on the guests program counter. Instructions other than special instructions are run natively. Special instructions are translated

run natively. Special instructions are translated into a new set of instructions that perform the equivalent taskfor example changing the ags in the VCPU . Binary translation is shown in Figure 16.3. It is implemented by translation code within the VMM .T h ec o d er e a d sn a t i v eb i n a r yi n s t r u c t i o n sd y n a m i c a l l y from the guest, on demand, and generates native binary code that executes in place of the original code. The basic method of binary translation just described

basic method of binary translation just described would execute correctly but perform poorly. Fortunately, the vast majority of instructions would execute natively. But how could performance be improved for the other instructions? We can turn to a specic implementation of binary translation, the VMware method, to see one way of improving performance. Here, cachingUser Processes Special Instruction(VMM Reads Instructions) Operating System VCPU VMMVMMGuest Kernel ModeUser Mode Translate Execute

VMMVMMGuest Kernel ModeUser Mode Translate Execute TranslationReturn Update Figure 16.3 Binary translation virtualization implementation.720 Chapter 16 Virtual Machines provides the solution. The replacement code for each instruction that needs to be translated is cached. All later executions of that instruction run from the translation cache and need not be translated again. If the cache is large enough, this method can greatly improve performance. Lets consider another issue in virtualization:

Lets consider another issue in virtualization: memory management, specif ically the page tables. How can the VMM keep pagetable state both for guests that believe they are managing the page tables and for the VMM itself? A common method, used with both trapandemulate and binary translation, is to use nested page tables (NPTs ). Each guest operating system maintains one or more page tables to translate from virtual to physical memory. The VMM maintains NPTs to represent the guests pagetable

maintains NPTs to represent the guests pagetable state, just as it creates a VCPU to represent the guests CPU state. The VMM knows when the guest tries to change its page table, and it makes the equivalent change in the NPT.W h e n the guest is on the CPU,t h e VMM puts the pointer to the appropriate NPT into the appropriate CPU register to make that table the active page table. If the guest needs to modify the page table (for example, fullling a page fault), then that operation must be

a page fault), then that operation must be intercepted by the VMM and appropriate changes made to the nested and system page tables. Unfortunately, the use of NPTs can cause TLBmisses to increase, and many other complexities need to be addressed to achieve reasonable performance. Although it might seem that the binary translation method creates large amounts of overhead, it performed well enough to launch a new industry aimed at virtualizing Intel x86based systems. VMware tested the performance

x86based systems. VMware tested the performance impact of binary translation by booting one such system, Windows XP,a n d immediately shutting it down while monitoring the elapsed time and the number of translations produced by the binary translation method. The result was 950,000 translations, taking 3 microseconds each, for a total increase of 3 seconds (about 5) over native execution of Windows XP.T oa c h i e v e that result, developers used many performance improvements that we do not

used many performance improvements that we do not discuss here. For more information, consult the bibliographical notes at the end of this chapter. 16.4.3 Hardware Assistance Without some level of hardware support, virtualization would be impossible. The more hardware support available within a system, the more featurerich and stable the virtual machines can be and the better they can perform. In the Intel x86 CPU family, Intel added new virtualization support in successive generations (the VTx

support in successive generations (the VTx instructions) beginning in 2005. Now, binary translation is no longer needed. In fact, all major generalpurpose CPUsa r ep r o v i d i n ge x t e n d e da m o u n t s of hardware support for virtualization. For example, AMD virtualization tech nology (AMD V)has appeared in several AMD processors starting in 2006. It denes two new modes of operationhost and guestthus moving from a dualmode to a multimode processor. The VMM can enable host mode, dene the

processor. The VMM can enable host mode, dene the characteristics of each guest virtual machine, and then switch the system to guest mode, passing control of the system to a guest operating system that is running in the virtual machine. In guest mode, the virtualized operating system thinks it is running on native hardware and sees whatever devices are included in the hosts denition of the guest. If the guest tries to access a16.5 Types of Virtual Machines and Their Implementations 721

of Virtual Machines and Their Implementations 721 virtualized resource, then control is passed to the VMM to manage that inter action. The functionality in Intel VTx is similar, providing root and nonroot modes, equivalent to host and guest modes. Both provide guest VCPU state data structures to load and save guest CPU state automatically during guest context switches. In addition, virtual machine control structures (VMCS s)are provided to manage guest and host state, as well as the various

guest and host state, as well as the various guest execution controls, exit controls, and information about why guests exit back to the host. In the latter case, for example, a nested pagetable violation caused by an attempt to access unavailable memory can result in the guests exit. AMD and Intel have also addressed memory management in the virtual environment. With AMD sRVIand Intels EPTmemory management enhance ments, VMM s no longer need to implement software NPTs .I ne s s e n c e ,t h e s

implement software NPTs .I ne s s e n c e ,t h e s e CPUsi m p l e m e n tn e s t e dp a g et a b l e si nh a r d w a r et oa l l o wt h e VMM to fully control paging while the CPUs accelerate the translation from virtual to physical addresses. The NPTs add a new layer, one representing the guests view of logicaltophysical address translation. The CPU pagetable walking function includes this new layer as necessary, walking through the guest table to the VMM table to nd the physical address

table to the VMM table to nd the physical address desired. A TLB miss results in a per formance penalty, because more tables must be traversed (the guest and host page tables) to complete the lookup. Figure 16.4 shows the extra translation work performed by the hardware to translate from a guest virtual address to a nal physical address. IOis another area improved by hardware assistance. Consider that the standard directmemoryaccess ( DMA )c o n t r o l l e ra c c e p t sat a r g e tm e m o r y

t r o l l e ra c c e p t sat a r g e tm e m o r y address and a source IOdevice and transfers data between the two without operatingsystem action. Without hardware assistance, a guest might try to set up a DMA transfer that affects the memory of the VMM or other guests. In CPUs that provide hardwareassisted DMA (such as Intel CPUsw i t h VTd), even DMA has a level of indirection. First, the VMM sets up protection domains to tell the CPU which physical memory belongs to each guest. Next, it

physical memory belongs to each guest. Next, it assigns the IOdevices to the protection domains, allowing them direct access to those memory regions and only those regions. The hardware then transforms the address in a DMA request issued by an IOdevice to the host physical memory address associated with the IO.I nt h i sm a n n e r DMA transfers are passed through between a guest and a device without VMM interference. Similarly, interrupts must be delivered to the appropriate guest and must not

be delivered to the appropriate guest and must not be visible to other guests. By providing an interrupt remapping feature, CPUsw i t hv i r t u a l i z a t i o nh a r d w a r ea s s i s t a n c ea u t o m a t i c a l l yd e l i v e ra n interrupt destined for a guest to a core that is currently running a thread of that guest. That way, the guest receives interrupts without the VMM s needing to intercede in their delivery. Without interrupt remapping, malicious guests can generate interrupts

malicious guests can generate interrupts that can be used to gain control of the host system. (See the bibliographical notes at the end of this chapter for more details.) 16.5 Types of Virtual Machines and Their Implementations Weve now looked at some of the techniques used to implement virtualization. Next, we consider the major types of virtual machines, their implementation, their functionality, and how they use the building blocks just described to722 Chapter 16 Virtual Machines VMM Nested

to722 Chapter 16 Virtual Machines VMM Nested Page Table Data StructurePML4E PDPTE PDE PTE Phy Addr Host Physical AddressOffset Table Directory Directory Ptr PML4Guest Virtual Address Kernel Paging Data Structures Guest Physical AddressGuest 1 2 3 45 11 22 33 4 5 4 Figure 16.4 Nested page tables. create a virtual environment. Of course, the hardware on which the virtual machines are running can cause great variation in implementation methods. Here, we discuss the implementations in general, with

we discuss the implementations in general, with the understanding that VMM st a k ea d v a n t a g eo fh a r d w a r ea s s i s t a n c ew h e r ei ti sa v a i l a b l e . 16.5.1 The Virtual Machine Life Cycle Lets begin with the virtual machine life cycle. Whatever the hypervisor type, at the time a virtual machine is created, its creator gives the VMM certain parameters. These parameters usually include the number of CPUs, amount of memory, networking details, and storage details that the VMM

details, and storage details that the VMM will take into account when creating the guest. For example, a user might want to create a new guest with two virtual CPUs, 4 GBof memory, 10 GBof disk space, one network interface that gets its IPaddress via DHCP ,a n da c c e s st ot h e DVD drive. The VMM then creates the virtual machine with those parameters. In the case of a type 0 hypervisor, the resources are usually dedicated. In this situation, if there are not two virtual CPUsa v a i l a b l ea

there are not two virtual CPUsa v a i l a b l ea n du n a l l o c a t e d ,t h ec r e a t i o nr e q u e s t16.5 Types of Virtual Machines and Their Implementations 723 in our example will fail. For other hypervisor types, the resources are dedicated or virtualized, depending on the type. Certainly, an IPaddress cannot be shared, but the virtual CPUsa r eu s u a l l ym u l t i p l e x e do nt h ep h y s i c a l CPUsa s d i s c u s s e d in Section 16.6.1. Similarly, memory management usually

16.6.1. Similarly, memory management usually involves allocating more memory to guests than actually exists in physical memory. This is more complicated and is described in Section 16.6.2. Finally, when the virtual machine is no longer needed, it can be deleted. When this happens, the VMM rst frees up any used disk space and then removes the conguration associated with the virtual machine, essentially forgetting the virtual machine. These steps are quite simple compared with building, conguring,

quite simple compared with building, conguring, running, and removing physical machines. Creating a virtual machine from an existing one can be as easy as clicking the clone button and providing a new name and IPaddress. This ease of creation can lead to virtual machine sprawl ,w h i c h occurs when there are so many virtual machines on a system that their use, history, and state become confusing and difcult to track. 16.5.2 Type 0 Hypervisor Type 0 hypervisors have existed for many years under

0 hypervisors have existed for many years under many names, including partitions and domains .T h e ya r eah a r d w a r ef e a t u r e ,a n dt h a tb r i n g si t s own positives and negatives. Operating systems need do nothing special to take advantage of their features. The VMM itself is encoded in the rmware and loaded at boot time. In turn, it loads the guest images to run in each partition. The feature set of a type 0 hypervisor tends to be smaller than those of the other types because it

smaller than those of the other types because it is implemented in hardware. For example, a system might be split into four virtual systems, each with dedicated CPUs, memory, and IOdevices. Each guest believes that it has dedicated hardware because it does, simplifying many implementation details. IOpresents some difculty, because it is not easy to dedicate IOdevices to guests if there are not enough. What if a system has two Ethernet ports and more than two guests, for example? Either all

and more than two guests, for example? Either all guests must get their own IO devices, or the system must provided IOdevice sharing. In these cases, the hypervisor manages shared access or grants all devices to a control partition . In the control partition, a guest operating system provides services (such as networking) via daemons to other guests, and the hypervisor routes IO requests appropriately. Some type 0 hypervisors are even more sophisticated and can move physical CPUsa n dm e m o r

and can move physical CPUsa n dm e m o r yb e t w e e nr u n n i n gg u e s t s .I nt h e s e cases, the guests are paravirtualized, aware of the virtualization and assisting in its execution. For example, a guest must watch for signals from the hardware orVMM that a hardware change has occurred, probe its hardware devices to detect the change, and add or subtract CPUso rm e m o r yf r o mi t sa v a i l a b l e resources. Because type 0 virtualization is very close to raw hardware execution, it

is very close to raw hardware execution, it should be considered separately from the other methods discussed here. At y p e0h y p e r v i s o rc a nr u nm u l t i p l eg u e s to p e r a t i n gs y s t e m s( o n ei ne a c h hardware partition). All of those guests, because they are running on raw hardware, can in turn be VMM s. Essentially, the guest operating systems in at y p e0h y p e r v i s o ra r en a t i v eo p e r a t i n gs y s t e m sw i t has u b s e to fh a r d w a r e made

t e m sw i t has u b s e to fh a r d w a r e made available to them. Because of that, each can have its own guest operating724 Chapter 16 Virtual MachinesGuest 1 Guest 2 CPUs memoryCPUs memory Hypervisor (in firmware) IOCPUs memoryCPUs memoryGuest 3 Guest 4Guest Guest Guest Guest Guest Figure 16.5 Type 0 hypervisor. systems (Figure 16.5). Other types of hypervisors usually cannot provide this virtualizationwithinvirtualization functionality. 16.5.3 Type 1 Hypervisor Type 1 hypervisors are

16.5.3 Type 1 Hypervisor Type 1 hypervisors are commonly found in company data centers and are in a sense becoming the datacenter operating system. They are specialpurpose operating systems that run natively on the hardware, but rather than providing system calls and other interfaces for running programs, they create, run, and manage guest operating systems. In addition to running on standard hardware, they can run on type 0 hypervisors, but not on other type 1 hypervisors. Whatever the

but not on other type 1 hypervisors. Whatever the platform, guests generally do not know they are running on anything but the native hardware. Type 1 hypervisors run in kernel mode, taking advantage of hardware protection. Where the host CPU allows, they use multiple modes to give guest operating systems their own control and improved performance. They imple ment device drivers for the hardware they run on, because no other component could do so. Because they are operating systems, they must

so. Because they are operating systems, they must also provide CPU scheduling, memory management, IOmanagement, protection, and even security. Frequently, they provide APIs, but those APIss u p p o r ta p p l i c a t i o n si n guests or external applications that supply features like backups, monitoring, and security. Many type 1 hypervisors are closedsource commercial offerings, such as VMware ESX while some are open source or hybrids of open and closed source, such as Citrix XenServer and its

closed source, such as Citrix XenServer and its open Xen counterpart. By using type 1 hypervisors, datacenter managers can control and manage the operating systems and applications in new and sophisticated ways. An important benet is the ability to consolidate more operating systems and applications onto fewer systems. For example, rather than having ten systems running at 10 percent utilization each, a data center might have one server manage the entire load. If utilization increases, guests

the entire load. If utilization increases, guests and their applications can be moved to lessloaded systems live, without interruption of service. Using snapshots and cloning, the system can save the states of guests and duplicate those statesa much easier task than restoring from backups or installing manually or via scripts and tools. The price of this increased manageability16.5 Types of Virtual Machines and Their Implementations 725 is the cost of the VMM (if it is a commercial product), the

of the VMM (if it is a commercial product), the need to learn new management tools and methods, and the increased complexity. Another type of type 1 hypervisor includes various generalpurpose operating systems with VMM functionality. In this instance, an operating system such as RedHat Enterprise Linux, Windows, or Oracle Solaris performs its normal duties as well as providing a VMM allowing other operating systems to run as guests. Because of their extra duties, these hypervisors typically

of their extra duties, these hypervisors typically provide fewer virtualization features than other type 1 hypervisors. In many ways, they treat a guest operating system as just another process, albeit with special handling provided when the guest tries to execute special instructions. 16.5.4 Type 2 Hypervisor Type 2 hypervisors are less interesting to us as operatingsystem explorers, because there is very little operatingsystem involvement in these application level virtual machine managers.

these application level virtual machine managers. This type of VMM is simply another process run and managed by the host, and even the host does not know virtualization is happening within the VMM . Type 2 hypervisors have limits not associated with some of the other types. For example, a user needs administrative privileges to access many of the hardware assistance features of modern CPUs. If the VMM is being run by a standard user without additional privileges, the VMM cannot take advantage of

privileges, the VMM cannot take advantage of these features. Due to this limitation, as well as the extra overhead of running ag e n e r a l  p u r p o s eo p e r a t i n gs y s t e ma sw e l la sg u e s to p e r a t i n gs y s t e m s ,t y p e2 hypervisors tend to have poorer overall performance than type 0 or 1. As is often the case, the limitations of type 2 hypervisors also provide some benets. They run on a variety of generalpurpose operating systems, and running them requires no changes to

systems, and running them requires no changes to t he host operating system. A student can use a type 2 hypervisor, for example, to test a nonnative operating system without replacing the native operating system. In fact, on an Apple laptop, as t u d e n tc o u l dh a v ev e r s i o n so fW i n d o w s, Linux, Unix, and less common operating systems all available for learning and experimentation. 16.5.5 Paravirtualization As weve seen, paravirtualization takes a different tack than the other

takes a different tack than the other types of virtualization. Rather than try to trick a guest operating system into believing it has a system to itself, paravirtualization presents the guest with a system that is similar but not identical to the guests preferred system. The guest must be modied to run on the paravirtualized virtual hardware. The gain for this extra work is more efcient use of resources and a smaller virtualization layer. The Xen VMM ,w h i c hi st h el e a d e ri np a r a v i

VMM ,w h i c hi st h el e a d e ri np a r a v i r t u a l i z a t i o n ,h a si m p l e m e n t e d several techniques to optimize the performance of guests as well as of the host system. For example, as we have seen, some VMM sp r e s e n tv i r t u a ld e v i c e st o guests that appear to be real devices. Instead of taking that approach, the Xen VMM presents clean and simple device ab stractions that allow efcient IO,a s well as good communication between the guest and the VMM about device

between the guest and the VMM about device IO. For each device used by each guest, there is a circular buffer shared by the guest and the VMM via shared memory. Read and write data are placed in this buffer, as shown in Figure 16.6.726 Chapter 16 Virtual Machines Request Producer Shared pointer updated by guest OSRequest Consumer Private pointer in Xen Response Producer Shared pointer updated by XenResponse Consumer Private pointer in guest OS Request queue  Descriptors queued by the VM but not

queue  Descriptors queued by the VM but not yet accepted by Xen Outstanding descriptors  Descriptor slots awaiting a response from Xen Response queue  Descriptors returned by Xen in response to serviced requests Unused descriptorsFigure 16.6 Xen IO via shared circular buffer. For memory management, Xen does not implement nested page tables. Rather, each guest has its own set of page tables, set to readonly. Xen requires the guest to use a specic mechanism, a hypercall from the guest to the

mechanism, a hypercall from the guest to the hypervisor VMM ,w h e nap a g e  t a b l ec h a n g ei sn e e d e d .T h i sm e a n st h a tt h e guest operating systems kernel code must be changed from the default code to these Xenspecic methods. To optimize performance, Xen allows the guest to queue up multiple pagetable changes asyn chronously via hypercalls and then check to ensure that the change sa r ec o m p l e t eb e f o r ec o n t i n u i n go p e r a t i o n . Xen allowed virtualization

n go p e r a t i o n . Xen allowed virtualization of x86 CPUsw i t h o u tt h eu s eo fb i n a r yt r a n s l a  tion, instead requiring modications in the guest operating systems like the one described above. Over time, Xen has taken advantage of hardware features supporting virtualization. As a result, it no longer requires modied guests and essentially does not need the paravirtualization method. Paravirtualization is still used in other solutions, however, such as type 0 hypervisors. 16.5.6

however, such as type 0 hypervisors. 16.5.6 ProgrammingEnvironment Virtualization Another kind of virtualization, based on a different execution model, is the virtualization of programming environments. Here, a programming language is designed to run within a custombuilt virtualized environment. For example, Oracles Java has many features that depend on its running in the Java virtual machine (JVM),i n c l u d i n gs p e c i  cm e t h o d sf o rs e c u r i t ya n dm e m o r y management. If we

rs e c u r i t ya n dm e m o r y management. If we dene virtualization as including only duplication of hardware, this is not really virtualization at all. But we need not limit ourselves to that denition. Instead, we can dene a virtual environment, based on APIs, that provides as e to ff e a t u r e st h a tw ew a n tt oh a v ea v a i l a b l ef o rap a r t i c u l a rl a n g u a g e and programs written in that language. Java programs run within the JVM16.5 Types of Virtual Machines and Their

the JVM16.5 Types of Virtual Machines and Their Implementations 727 environment, and the JVM is compiled to be a native program on systems on which it runs. This arrangement means that Java programs are written once and then can run on any system (including all of the major operating systems) on which a JVM is available. The same can be said for interpreted languages , which run inside programs that read each instruction and interpret it into native operations. 16.5.7 Emulation Virtualization is

operations. 16.5.7 Emulation Virtualization is probably the most common method for running applications designed for one operating system on a different operating system, but on the same CPU.T h i sm e t h o dw o r k sr e l a t i v e l ye f  c iently because the applications were compiled for the same instruction set as the target system uses. But what if an application or operating system needs to run on a different CPU?H e r e ,i ti sn e c e s s a r yt ot r a n s l a t ea l lo ft h es o u r c

s a r yt ot r a n s l a t ea l lo ft h es o u r c e CPUs instructions so that they are turned into the equivalent instructions of the target CPU.S u c ha n environment is no longer virtualized but rather is fully emulated. Emulation is useful when the host system has one system architecture and the guest system was compiled for a different architecture. For example, suppose a company has replaced its outdated computer system with a new system but would like to continue to run certain important

would like to continue to run certain important programs that were compiled for the old system. The programs could be run in an emulator that translates each of the outdated systems instructions into the native instruction set of the new system. Emulation can increase the life of programs and allow us to explore old architectures without having an actual old machine. As may be expected, the major challenge of emulation is performance. Instructionset emulation can run an order of magnitude slower

emulation can run an order of magnitude slower than native instructions, because it may take ten instructions on the new system to read, parse, and simulate an instruction from the old system. Thus, unless the new machine is ten times faster than the old, the program running on the new machine will run more slowly than it did on its native hardware. Another challenge for emulator writers is that it is difcult to create a correct emulator because, in essence, this task involves writing an entire

in essence, this task involves writing an entire CPU in software. In spite of these challenges, emulation is very popular, particularly in gaming circles. Many popular video games were written for platforms that are no longer in production. Users who want to run those games frequently can nd an emulator of such a platform and then run the game unmodied within the emulator. Modern systems are so much faster than old game consoles that even the Apple iPhone has game emulators and games available

iPhone has game emulators and games available to run within them. 16.5.8 Application Containment The goal of virtualization in some instances is to provide a method to segregate applications, manage their performance and resource use, and create an easy way to start, stop, move, and manage them. In such cases, perhaps fulledged virtualization is not needed. If the applications are all compiled for the same operating system, then we do not need complete virtualization to provide these features.

complete virtualization to provide these features. We can instead use application containment.728 Chapter 16 Virtual Machines virtual platform device managementzone 1 global zone Solaris kernel network addresseszone 2 zone managementuser programs system programs CPU resources memory resourcesuser programs system programs network addresses device access CPU resources memory resourcesuser programs system programs network addresses device access CPU resources memory resources device device Figure

resources memory resources device device Figure 16.7 Solaris 10 with two zones. Consider one example of application containment. Starting with version 10, Oracle Solaris has included containers ,o rzones ,t h a tc r e a t eav i r t u a ll a y e r between the operating system and the applications. In this system, only one kernel is installed, and the hardware is not virtualized. Rather, the operating system and its devices are virtualized, providing processes within a zone with the impression

processes within a zone with the impression that they are the only processes on the system. One or more containers can be created, and each can have its own applications, network stacks, network address and ports, user accounts, and so on. CPU and memory resources can be divided among the zones and the systemwide processes. Each zone in fact can run its own scheduler to optimize the performance of its applications on the allotted resources. Figure 16.7 shows a Solaris 10 system with two

Figure 16.7 shows a Solaris 10 system with two containers and the standard global user space. 16.6 Virtualization and OperatingSystem Components Thus far, we have explored the building blocks of virtualization and the various types of virtualization. In this section, we take a deeper dive into the operating system aspects of virtualization, including how the VMM provides core operatingsystem functions like scheduling, IO,a n dm e m o r ym a n a g e m e n t . Here, we answer questions such as

a g e m e n t . Here, we answer questions such as these: How do VMM ss c h e d u l e CPU use when guest operating systems believe they have dedicated CPUs? How can memory management work when many guests require large amounts of memory?16.6 Virtualization and OperatingSystem Components 729 16.6.1 CPU Scheduling As y s t e mw i t hv i r t u a l i z a t i o n ,e v e nas i n g l e  CPU system, frequently acts like a multiprocessor system. The virtualization software presents one or more virtual

software presents one or more virtual CPUst oe a c ho ft h ev i r t u a lm a c h i n e sr u n n i n go nt h es y s t e ma n dt h e n schedules the use of the physical CPUsa m o n gt h ev i r t u a lm a c h i n e s . The signicant variations among virtualization technologies make it dif cult to summarize the effect of virtualization on scheduling. First, lets consider the general case of VMM scheduling. The VMM has a number of physical CPUs available and a number of threads to run on those CPUs.

and a number of threads to run on those CPUs. The threads can be VMM threads or guest threads. Guests are congured with a certain number of virtual CPUsa tc r e a t i o nt i m e ,a n dt h a tn u m b e rc a nb ea d j u s t e dt h r o u g h o u tt h e life of the VM.W h e nt h e r ea r ee n o u g h CPUst oa l l o c a t et h er e q u e s t e dn u m b e rt o each guest, the VMM can treat the CPUsa sd e d i c a t e da n ds c h e d u l eo n l yag i v e n guests threads on that guests CPUs. In this

v e n guests threads on that guests CPUs. In this situation, the guests act much like native operating systems running on native CPUs. Of course, in other situations, there may not be enough CPUst og o around. The VMM itself needs some CPU cycles for guest management and IO management and can steal cycles from the guests by scheduling its threads across all of the system CPUs, but the impact of this action is relatively minor. More difcult is the case of overcommitment ,i nw h i c ht h eg u e s

case of overcommitment ,i nw h i c ht h eg u e s t s are congured for more CPUst h a ne x i s ti nt h es y s t e m .H e r e ,a VMM can use standard scheduling algorithms to make progress on each thread but can also add a fairness aspect to those algorithms. For example, if there are six hardware CPUsa n d1 2g u e s t  a l l o c a t e d CPUs, the VMM could allocate CPU resources proportionally, giving each guest half of the CPU resources it believes it has. The VMM can still present all 12

believes it has. The VMM can still present all 12 virtual CPUst ot h eg u e s t s ,b u ti n mapping them onto physical CPUs, the VMM can use its scheduler to share them appropriately. Even given a scheduler that provides fairness, any guest operatingsystem scheduling algorithm that assumes a certain amount of progress in a given amount of time will be negatively affected by virtualization. Consider a time sharing operating system that tries to allot 100 milliseconds to each time slice to give

allot 100 milliseconds to each time slice to give users a reasonable response time. Within a virtual machine, this operating system is at the mercy of the virtualization system as to what CPU resources it actually receives. A given 100millisecond time slice may take much more than 100 milliseconds of virtual CPU time. Depending on how busy the system is, the time slice may take a second or more, resulting in very poor response times for users logged into that virtual machine. The effect on a

logged into that virtual machine. The effect on a realtime operating system can be even more serious. The net effect of such scheduling layering is that individual virtualized operating systems receive only a portion of the available CPU cycles, even though they believe they are receiving all of the cycles and indeed that they are scheduling all of those cycles. Commonly, the timeofday clocks in virtual machines are incorrect because timers take longer to trigger than they would on dedicated

longer to trigger than they would on dedicated CPUs. Virtualization can thus undo the good schedulingalgorithm efforts of the operating systems within virtual machines. To correct for this, a VMM will have an application available for each type of operating system that system administrators install into the guests. This730 Chapter 16 Virtual Machines application corrects clock drift and can have other functions such as virtual device management. 16.6.2 Memory Management Efcient memory use in

16.6.2 Memory Management Efcient memory use in generalpurpose operating systems is one of the major keys to performance. In virtualized environments, there are more users of memory (the guests and their applications, as well as the VMM ), leading to more pressure on memory use. Further adding to this pressure is that VMM s typically overcommit memory, so that the total memory with which guests are congured exceeds the amount of memory that physically exists in the system. The extra need for

exists in the system. The extra need for efcient memory use is not lost on the implementers of VMM s, who take great measures to ensure the optimal use of memory. For example, VMware ESXuses at least three methods of memory manage ment. Before memory optimization can occur, the VMM must establish how much real memory each guest should use. To do that, the VMM rst evaluates the maximum memory size of each guest as dictated when it is congured. Generalpurpose operating systems do not expect the

Generalpurpose operating systems do not expect the amount of memory in the system to change, so VMM sm u s tm a i n t a i nt h ei l l u s i o nt h a tt h eg u e s t has that amount of memory. Next, the VMM computes a target real memory allocation for each guest based on the congured memory for that guest and other factors, such as overcommitment and system load. It then uses the three lowlevel mechanisms below to reclaim memory from the guests. The overall effect is to enable guests to behave

The overall effect is to enable guests to behave and perform as if they had the full amount of memory requested although in reality they have less. 1.Recall that a guest believes it controls memory allocation via its page table management, whereas in reality the VMM maintains a nested page table that retranslates the guest page table to the real page table. The VMM can use this extra level of indirection to optimize the guests use of memory without the guests knowledge or help. One approach is

the guests knowledge or help. One approach is to provide double paging, in which the VMM has its own pagereplacement algorithms and pages to backingstore pages that the guest believes are in physical memory. Of course, the VMM has knows less about the guests memory access patterns than the guest does, so its paging is less efcient, creating performance problems. VMM sd ou s et h i sm e t h o dw h e no t h e r methods are not available or are not providing enough free memory. However, it is not

providing enough free memory. However, it is not the preferred approach. 2.Ac o m m o ns o l u t i o ni sf o rt h e VMM to install in each guest a pseudo device driver or kernel module that it controls. (A pseudodevice driver uses devicedriver interfaces, appearing to t he kernel to be a device driver, but does not actually control a device. Rather, it is an easy way to add kernelmode code without directly modifying the kernel.) This balloon memory manager communicates with the VMM and is told

manager communicates with the VMM and is told to allocate or deallocate memory. If told to allocate, it allocates memory and tells the operating system to pin the allocated pages into physical memory. Recall that pinning locks a page into physical memory so that it cannot be moved or paged out. The guest sees memory pressure becauses of these pinned pages, essentially decreasing the amount of physical memory it has available to use. The guest then may free up other physical memory16.6

guest then may free up other physical memory16.6 Virtualization and OperatingSystem Components 731 to be sure it has a sufcient pool of free memory. Meanwhile, the VMM , knowing that the pages pinned by the balloon process will never be used, removes those physical pages from the guest and allocates them to another guest. At the same time, the guest is using its own memory management and paging algorithms to manage the available memory, which is the most efcient option. If memory pressure within

the most efcient option. If memory pressure within the entire system decreases, the VMM will tell the balloon process within the guest to unpin and free some or all of the memory, allowing the guest more pages for its use. 3.Another common method for reducing memory pressure is for the VMM to determine if the same page has been loaded more than once. If this is the case, to the VMM reduces the number of copies of the page to one and maps the other users of the page to that one copy. VMware, for

users of the page to that one copy. VMware, for example, randomly samples guest memory and creates a hash for each page sampled. That hash value is a thumbprint of the page. The hash of every page examined is compared with other hashes already stored in a hash table. If there is a match, the pages are compared byte by byte to see if they really are identical. If they are, one page is freed, and its logical address is mapped to the others physical address. This technique might seem at rst to be

address. This technique might seem at rst to be ineffective, but consider that guests run operating systems. If multiple guests run the same operating system, then only one copy of the active operatingsystem pages need be in memory. Similarly, multiple guests could be running the same set of applications, again a likely source of memory sharing. 16.6.3 IO In the area of IO,h y p e r v i s o r sh a v es o m el e e w a ya n dc a nb el e s sc o n c e r n e d with exactly representing the underlying

e r n e d with exactly representing the underlying hardware to their guests. Because of all the variation in IOdevices, operating systems are used to dealing with varying and exible IOmechanisms. For example, operating systems have ad e v i c e  d r i v e rm e c h a n i s mt h a tp r o vides a uniform interface to the operating system whatever the IOdevice. Devicedriver interfac es are designed to allow thirdparty hardware manufacturers to provide device drivers connecting their devices to the

device drivers connecting their devices to the operating system. Usually, device drivers can be dynamically loaded and unloaded. Virtualization takes advantage of such builtin exibility by providing specic virtualized devices to guest operating systems. As described in Section 16.5, VMM sv a r yg r e a t l yi nh o wt h e yp r o v i d e IOto their guests. IOdevices may be dedicated to guests, for example, or the VMM may have device drivers onto which it maps guest IO.T h e VMM may also provide

which it maps guest IO.T h e VMM may also provide idealized device drivers to guests, which allows easy provision and management of guest IO.I nt h i sc a s e ,t h eg u e s ts e e sa ne a s y  t o  c o n t r o ld e v i c e , but in reality that simple device driver communicates to the VMM which sends those requests to a more complicated real device through a more complex real device driver. IOin virtual environments is complicated and requires careful VMM design and implementation. Consider the

VMM design and implementation. Consider the case of a hypervisor and hardware combination that allows devices to be dedicated to a guest and allows the guest to access those devices directly. Of course, a device dedicated to one guest is not available to any other guests, but this direct access can still be useful in some circumstances.732 Chapter 16 Virtual Machines The reason to allow direct access is to improve IOperformance. The less the hypervisor has to do to enable IOfor its guests, the

has to do to enable IOfor its guests, the faster the IOcan occur. With Type 0 hypervisors that provide direct device access, guests can often run at the same speed as native operating systems. When type 0 hypervisors instead provide shared devices, performance can suffer by comparison. With direct device access in type 1 and 2 hypervisors, performance can be similar to that of native operating systems if certain hardware support is present. The hardware needs to provide DMA passthrough with

The hardware needs to provide DMA passthrough with facilities like VTd, as well as direct interrupt delivery to specic guests. Given how frequently interrupts occur, it should be no surprise that the guests on hardware without these features have worse performance than if they were running natively. In addition to direct access, VMM sp r o v i d es h a r e da c c e s st od e v i c e s . Consider a disk drive to which multiple guests have access. The VMM must provide protection while sharing the

The VMM must provide protection while sharing the device, assuring that a guest can access only the blocks specied in the guests conguration. In such instances, the VMM must be part of every IO, checking it for correctness as well as routing the data to and from the appropriate devices and guests. In the area of networking, VMM sa l s oh a v ew o r kt od o .G e n e r a l  p u r p o s e operating systems typically have one Internet protocol ( IP)a d d r e s s ,a l t h o u g h they sometimes have

d d r e s s ,a l t h o u g h they sometimes have more than onefor example, to connect to a management network, backup network, and production network. With virtualization, each guest needs at least one IPaddress, because that is the guests main mode of communication. Therefore, a server running a VMM may have dozens of addresses, and the VMM acts as a virtual switch to route the network packets to the addressed guest. The guests can be directly connected to the network by an IPaddress that is

connected to the network by an IPaddress that is seen by the broader network (this is known as bridging ). Alternatively, the VMM can provide a network address translation (NAT)address. The NAT address is local to the server on which the guest is running, and the VMM provides routing between the broader network and the guest. The VMM also provides rewalling, moderating connections between guests within the system and between guests and external systems. 16.6.4 Storage Management An important

systems. 16.6.4 Storage Management An important question in determining how virtualization works is this: If multiple operating systems have been installed, what and where is the boot disk? Clearly, virtualized environments need to approach the area of storage management differently from native operating systems. Even the standard multiboot method of slicing the root disk into partitions, installing a boot manager in one partition, and installing each other operating system in another partition

each other operating system in another partition is not sufcient, because partitioning has limits that would prevent it from working for tens or hundreds of virtual machines. Once again, the solution to this problem depends on the type of hypervisor. Type 0 hypervisors do tend to allow root disk partitioning, partly because these systems tend to run fewer guests than other systems. Alternatively, they may have a disk manager as part of the control partition, and that disk manager provides disk

partition, and that disk manager provides disk space (including boot disks) to the other partitions.16.6 Virtualization and OperatingSystem Components 733 Type 1 hypervisors store the guest root disk (and conguration informa tion) in one or more les within the le systems provided by the VMM . Type 2 hypervisors store the same information within the host operating systems le systems. In essence, a disk image ,c o n t a i n i n ga l lo ft h ec o n t e n t so ft h er o o td i s k of the guest, is

n t e n t so ft h er o o td i s k of the guest, is contained within one le in the VMM .A s i d ef r o mt h ep o t e n t i a l performance problems that causes, it is a clever solution, because it simplies copying and moving guests. If the administrator wants a duplicate of the guest (for testing, for example), she simply copies the associated disk image of the guest and tells the VMM about the new copy. Booting that new VMbrings up an identical guest. Moving a virtual machine from one system to

guest. Moving a virtual machine from one system to another that runs the same VMM is as simple as halting the guest, copying the image to the other system, and starting the guest there. Guests sometimes need more disk space than is available in their root disk image. For example, a nonvirtualized database server might use several le systems spread across many disks to store various parts of the database. Virtualizing such a database usually involves creating several les and having the VMM

involves creating several les and having the VMM present those to the guest as disks. The guest then executes as usual, with the VMM translating the disk IOrequests coming from the guest into le IOcommands to the correct les. Frequently, VMM sp r o v i d eam e c h a n i s mt oc a p t u r eap h y s i c a ls y s t e ma s it is currently congured and convert it to a guest that the VMM can manage and run. Based on the discussion above, it should be clear that this physical tovirtual (PtoV

should be clear that this physical tovirtual (PtoV )conversion reads the disk blocks of the physical systems disks and stores them within les on the VMM s system or on shared storage that the VMM can access. Perhaps not as obvious is the need for a virtualto physical (VtoP )procedure for converting a guest to a physical system. This step is sometimes needed for debugging: a problem could be caused by the VMM or associated components, and the administrator could attempt to solve the problem by

could attempt to solve the problem by removing virtualization from the problem variables. VtoP conversion can take the les containing all of the guest data and generate disk blocks on a systems disk, recreating the guest as a native operating system and applications. Once the testing is concluded, the native system can be reused for other purposes when the virtual machine returns to service, or the virtual machine can be deleted and the native system can continue to run. 16.6.5 Live Migration

system can continue to run. 16.6.5 Live Migration One feature not found in generalpurpose operating systems but found in type 0a n dt y p e1h y p e r v i s o r si st h el i v em i g r a t i o no far u n n i n gg u e s tf r o mo n e system to another. We mentioned this capability earlier. Here, we explore the details of how live migration works and why VMM sh a v ear e l a t i v e l ye a s yt i m e implementing it while generalpurpose operating systems, in spite of some research attempts, do not.

in spite of some research attempts, do not. First, consider how live migration works. A running guest on one system is copied to another system running the same VMM .T h ec o p yo c c u r sw i t hs o little interruption of service that users logged in to the guest, and network connections to the guest, continue without noticeable impact. This rather astonishing ability is very powerful in resource management and hardware administration. After all, compare it with the steps necessary without

all, compare it with the steps necessary without virtu alization: warning users, shutting down the processes, possibly moving the734 Chapter 16 Virtual Machines binaries, and restarting the processes on the new system, with users only then able to use the services again. With live migration, an overloaded system can have its load decreased live with no discernible disruption. Similarly, a system needing hardware or system changes (for example, a rmware upgrade, hardware addition or removal, or

a rmware upgrade, hardware addition or removal, or hardware repair) can have guests migrated off, the work done, and guests migrated back without noticeable impact on users or remote connections. Live migration is made possible because of the welldened interfaces between guests and VMM s and the limited state the VMM maintains for the guest. The VMM migrates a guest via the following steps: 1.The source VMM establishes a connection with the target VMM and conrms that it is allowed to send a

target VMM and conrms that it is allowed to send a guest. 2.The target creates a new guest by creating a new VCPU ,n e wn e s t e dp a g e table, and other state storage. 3.The source sends all readonly memory pages to the target. 4.The source sends all readwrite pages to the target, marking them as clean. 5.The source repeats step 4, as during that step some pages were probably modied by the guest and are now dirty. These pages need to be sent again and marked again as clean. 6.When the cycle

again and marked again as clean. 6.When the cycle of steps 4 and 5 becomes very short, the source VMM freezes the guest, sends the VCPU s nal state, sends other state details, sends the nal dirty pages, and tells the target to start running the guest. Once the target acknowledges that the guest is running, the source terminates the guest. This sequence is shown in Figure 16.8. We conclude this discussion with a few interesting details and limita tions concerning live migration. First, for

limita tions concerning live migration. First, for network connections to continue uninterrupted, the network infrastructure needs to understand that a MAC Guest Target running5  Send Dirty Pages (repeatedly)4  Send RW Pages3  Send RO Pages1  Establish 0  Running Guest SourceVMM Source 7  Terminate Guest Source VMM Target2  Create Guest Target 6  Running Guest Target Figure 16.8 Live migration of a guest between two servers.16.7 Examples 735 addressthe hardware networking addresscan move between

hardware networking addresscan move between systems. Before virtualization, this did not happen, as the MAC address was tied to physical hardware. With virtualization, the MAC must be movable for exist ing networking connections to continue without resetting. Modern network switches understand this and route trafc wherever the MAC address is, even accommodating a move. Al i m i t a t i o no fl i v em i g r a t i o ni st h a tn od i s ks t a t ei st r a n s f e r r e d .O n er e a s o n live

t ei st r a n s f e r r e d .O n er e a s o n live migration is possible is that most of the guests state is maintained within the guestfor example, open le tables, systemcall state, kernel state, and so on. Because disk IOis so much slower than memory access, and used disk space is usually much larger than used memory, disks associated with the guest cannot be moved as part of a live migration. Rather, the disk must be remote to the guest, accessed over the network. In that case, disk access

over the network. In that case, disk access state is maintained within the guest, and network connections are all that matter to the VMM .T h e network connections are maintained during the migration, so remote disk access continues. Typically, NFS,CIFS,o r iSCSI is used to store virtual machine images and any other storage a guest needs access to. Those networkbased storage accesses simply continue when the network connections are continued once the guest has been migrated. Live migration

once the guest has been migrated. Live migration enables entirely new ways of managing data centers. For example, virtualization management tools can monitor all the VMM si n an environment and automatically balance resource use by moving guests between the VMM s. They can also optimize the use of electricity and cooling by migrating all guests off selected servers if other servers can handle the load and powering down the selected servers entir ely. If the load increases, these tools can power

ely. If the load increases, these tools can power up the servers and migrate guests back to them. 16.7 Examples Despite the advantages of virtual machines, t hey received little attention for an u m b e ro fy e a r sa f t e rt h e yw e r e r s td e v e l o p e d .T o d a y ,h o w e v e r ,v i r t u a l machines are coming into fashion as a means of solving system compatibility problems. In this section, we explore two popular contemporary virtual machines: the VMware Workstation and the Java

machines: the VMware Workstation and the Java virtual machine. As you will see, these virtual machines can typically run on top of operating systems of any of the design types discussed in earlier chapters. Thus, operatingsystem design methodssimple layers, microkernels, modules, and virtual machines are not mutually exclusive. 16.7.1 VMware VMware Workstation is a popular commercial application that abstracts Intel X86 and compatible hardware into isolated virtual machines. VMware Workstation

into isolated virtual machines. VMware Workstation is a prime example of a Type 2 hypervisor. It runs as an application on a host operating system such as Windows or Linux and allows this host system to run several different guest operating systems concurrently as independent virtual machines. The architecture of such a system is shown in Figure 16.9. In this scenario, Linux is running as the host operating system, and FreeBSD,W i n d o w s NT,a n d736 Chapter 16 Virtual Machines virtualization

n d736 Chapter 16 Virtual Machines virtualization layer host operating system (Linux) CPU memoryhardware IO devicesapplication application application application guest operating system (free BSD) virtual CPU virtual memory virtual devicesguest operating system (Windows NT) virtual CPU virtual memory virtual devicesguest operating system (Windows XP) virtual CPU virtual memory virtual devices Figure 16.9 VMware Workstation architecture. Windows XPare running as guest operating systems. At the

XPare running as guest operating systems. At the heart of VMware is the virtualization layer, which abstracts the physical hardware into isolated virtual machines running as guest operating systems. Each virtual machine has its own virtual CPU, memory, disk drives, network interfaces, and so forth. The physical disk that the guest owns and manages is really just a le within the le system of the host operating system. To create an identical guest, we can simply copy the le. Copying the le to

we can simply copy the le. Copying the le to another location protects the guest against a disaster at the original site. Moving the le to another location moves the guest system. These scenarios show how virtualization can improve the efciency of system administration as well as system resource use. 16.7.2 The Java Virtual Machine Java is a popular objectoriented programming language introduced by Sun Microsystems in 1995. In addition to a language specication and a large APIlibrary, Java

language specication and a large APIlibrary, Java provides a specication for a Java virtual machine, or JVM. Java therefore is an example of programmingenvironment virtualization, as discussed in Section 16.5.6. Java objects are specied with the class construct; a Java program consists of one or more classes. For each Java class, the compiler produces an architectureneutral bytecode output ( .class ) l et h a tw i l lr u no na n y implementation of the JVM. The JVM is a specication for an

of the JVM. The JVM is a specication for an abstract computer. It consists of a class loader and a Java interpreter that executes the architectureneutral bytecodes, as diagrammed in Figure 16.10. The class loader loads the compiled .class les from both the Java program and the Java APIfor execution by the Java interpreter. After a class is loaded, the verier checks that the .class le is valid Java bytecode and that it does not overow or underow the stack. It also16.8 Summary 737 host system

the stack. It also16.8 Summary 737 host system (Windows, Linux, etc.)class loader Java interpreterJava program .class filesJava API .class files Figure 16.10 The Java virtual machine. ensures that the bytecode does not perform pointer arithmetic, which could provide illegal memory access. If the class passes verication, it is run by the Java interpreter. The JVM also automatically manages memory by performing garbage collection thepracticeofreclaimingmemoryfromobjectsnolonger in use and

in use and returning it to the system. Much research focuses on garbage collection algorithms for increasing the performance of Java programs in the virtual machine. The JVM may be implemented in software on top of a host operating system, such as Windows, Linux, or Mac OS X ,o ra sp a r to faW e bb r o w s e r . Alternatively, the JVM may be implemented in hardware on a chip specically designed to run Java programs. If the JVM is implemented in software, the Java interpreter interprets the

in software, the Java interpreter interprets the bytecode operations one at a time. A faster software technique is to use a justintime (JIT)compiler. Here, the rst time a Java method is invoked, the bytecodes for th em e t h o da r et u r n e di n t on a t i v e machine language for the host system. These operations are then cached so that subsequent invocations of a method are performed using the native machine instructions, and the bytecode operation sn e e dn o tb ei n t e r p r e t e da l lo

sn e e dn o tb ei n t e r p r e t e da l lo v e ra g a i n . Running the JVM in hardware is potentially even faster. Here, a special Java chip executes the Java bytecode operation sa sn a t i v ec o d e ,t h u sb y p a s s i n gt h e need for either a software interpreter or a justintime compiler. 16.8 Summary Virtualization is a method of providing a guest with a duplicate of a systems underlying hardware. Multiple guests can run on a given system, each believing it is the native operating

system, each believing it is the native operating system in full control of the system. Virtualization started as a method to allow IBM to segregate users and provide them with their own execution environments on IBM mainframes. Since then, with improvements in system and CPU performance and through innovative software techniques, virtualization has become a common feature in data centers and even on personal computers. Because of the popularity of virtualization, CPU designers have added

of virtualization, CPU designers have added features to support virtualization. This snowball effect is likely to continue, with virtualization and its hardware support increasing over time. Type 0 virtualization is implemented in the hardware and requires modi cations to the operating system to ensure proper operation. These modications738 Chapter 16 Virtual Machines offer an example of paravirtualization, in which the operating system is not blind to virtualization but instead has features

blind to virtualization but instead has features added and algorithms changed to improve virtualizations features and performance. In Type 1 virtualization, ah o s tv i r t u a lm a c h i n em o n i t o r( VMM )p r o v i d e st h ee n v i r o n m e n ta n df e a t u r e s needed to create, run, and destroy guest virtual machines. Each guest includes all of the software typically associated with a full native system, including the operating system, device drivers, applications, user accounts, and

device drivers, applications, user accounts, and so on. Type 2 hypervisors are simply applications that run on other operating systems, which do not know that virtualization is taking place. These hypervi sors do not enjoy hardware or host support so must perform all virtualization activities in the context of a process. Other facilities that are similar to virtualization but do not meet the full denition of replicating hardware exactly are also common. Programming environment virtualization is

common. Programming environment virtualization is part of the design of a programming language. The language species a containing application in which programs run, and this application provides services to the programs. Emulation is used when a host system has one architecture and the guest was compiled for a different architecture. Every instruction the guest wants to execute must be translated from its instruction set to that of the native hardware. Although this method involves some perform

Although this method involves some perform penalty, it is balanced by the usefulness of being able to run old programs on newer, incompatible hardware or run games designed for old consoles on modern hardware. Implementing virtualization is challenging, especially when hardware support is minimal. Some hardware support must exist for virtualization, but the more features provided by the system, the easier virtualization is to implement and the better the performance of the guests. VMM st a k ea

the performance of the guests. VMM st a k ea d v a n t a g e of whatever hardware support is available when optimizing CPU scheduling, memory management, and IO modules to provide guests with optimum resource use while protecting the VMM from the guests and the guests from one another. Exercises 16.1 Describe the three types of traditional virtualization. 16.2 Describe the four virtualizationlike execution environments and why they are not truevirtualization. 16.3 Describe four benets of

truevirtualization. 16.3 Describe four benets of virtualization. 16.4 Why can VMM sn o ti m p l e m e n tt r a p  a n d  e m u l a t e  b a s e dv i r t u a l i z a t i o n on some CPUs? Lacking the ability to trapandemulate, what method can a VMM use to implement virtualization? 16.5 What hardware assistance for virtualization can be provided by modern CPUs? 16.6 Why is live migration possible in virtual environments but much less possible for a native operating system?Bibliography 739

for a native operating system?Bibliography 739 Bibliographical Notes The original IBM VM system was described in [Meyer and Seawright (1970)]. [Popek and Goldberg (1974)] established the characteristics that help dene VMM s. Methods of implementing virtual machines are discussed in [Agesen et al. (2010)]. Virtualization has been an active research area for many years. Disco was one of the rst attempts to use virtualization to enforce logical isolation and provide scalability on multicore systems

and provide scalability on multicore systems ([Bugnion et al. (1997)]). Based on that and and other work, QuestV used virtualization to create an entire distributed operating system within a multicore system ([Li et al. (2011)]). Intel x86 hardware virtualization support is described in [Neiger et al. (2006)]. AMD hardware virtualization support is described in a white paper (http:developer.amd.comassetsNPTWP1201nalTM.pdf ). KVM is described in [Kivity et al. (2007)]. Xen is described in [Barham

et al. (2007)]. Xen is described in [Barham et al. (2003)]. Oracle Solaris containers are similar to BSD jails, as described in [Poulhenning Kamp (2000)]. [Agesen et al. (2010)] discuss the performance of binary translation. Memory management in VMware is described in [Waldspurger (2002)]. The problem of IOoverhead in virtualized environments has a proposed solution in [Gordon et al. (2012)]. Some protection challenges and attacks in virtual environments are discussed in [Wojtczuk and Ruthkowska

are discussed in [Wojtczuk and Ruthkowska (2011)]. Live process migration research occurred in the 1980s and was rst dis cussed in [Powell and Miller (1983)]. Problems identied in that research left migration in a functionally limited state, as described in [Milojicic et al. (2000)]. VMware realized that virtualization could allow functional live migration and described prototype work in [Chandra et al. (2002)]. VMware shipped the vMotion live migration feature as part of VMware vCenter, as

migration feature as part of VMware vCenter, as described in VMware VirtualCenter Users Manual Version 1.0 (http:www.vmware.compdfVirtualCenter Users Manual.pdf ). The details of the implementation of a similar feature in the Xen VMM are found in [Clark et al. (2005)]. Research showing that, without interrupt remapping, malicious guests can generate interrupts that can be used to gain control of the host system is discussed in [Wojtczuk and Ruthkowska (2011)]. Bibliography [Agesen et al. (2010)]

(2011)]. Bibliography [Agesen et al. (2010)] O. Agesen, A. Garthwaite, J. Sheldon, and P . Subrah manyam, The Evolution of an x86 Virtual Machine Monitor ,Proceedings of the ACM Symposium on Operating Systems Principles (2010), pages 318. [Barham et al. (2003)] P. B a rh a m , B . D r a g o v i c , K . F r a s e r, S . H a n d , T. H a r r i s , A. Ho, R. Neugebauer, I. Pratt, and A. Wareld, Xen and the Art of Virtu alization ,Proceedings of the ACM Symposium on Operating Systems Principles

the ACM Symposium on Operating Systems Principles (2003), pages 164177.740 Chapter 16 Virtual Machines [Bugnion et al. (1997)] E. Bugnion, S. Devine, and M. Rosenblum, Disco: Run ning Commodity Operating Systems on Scalable Multiprocessors ,Proceedings of the ACM Symposium on Operating Systems Principles (1997), pages 143156. [Chandra et al. (2002)] R. Chandra, B. Pfaff, J. Chow, M. Lam, and M. Rosen blum, Optimizing the Migration of Virtual Computers (2002), pages 377390. [Clark et al. (2005)]

(2002), pages 377390. [Clark et al. (2005)] C. Clark, K. Fraser, S. Hand, J. G. Hansen, E. Jul, C. Limpach, I. Pratt, and A. Wareld, Live Migration of Virtual Machines ,Proceedings of the 2nd Conference on Symposium on Networked Systems Design  Implementation (2005), pages 273286. [Gordon et al. (2012)] A. Gordon, N. A. N. HarEl, M. BenYehuda, A. Landau, and A. S. andDan Tsafrir, ELI: Baremetal Performance for IOVirtualization , Proceedings of the International Conference on Architectural

of the International Conference on Architectural Support for Programming Languages and Operating Systems (2012), pages 411422. [Kivity et al. (2007)] A. Kivity, Y. Kamay, D. Laor, U. Lublin, and A. Liguori, kvm: the Linux Virtual Machine Monitor ,Proceedings of the Linux Symposium (2007). [Li et al. (2011)] Y. Li, M. Danish, and R. West, QuestV: A Virtualized Mul tikernel for HighCondence Systems , Technical report, Boston University (2011). [Meyer and Seawright (1970)] R. A. Meyer and L. H.

[Meyer and Seawright (1970)] R. A. Meyer and L. H. Seawright, AV i r t u a l Machine TimeSharing System ,IBM Systems Journal ,V o l u m e9 ,N u m b e r3 (1970), pages 199218. [Milojicic et al. (2000)] D. S. Milojicic, F. Douglis, Y. Paindaveine, R. Wheeler, and S. Zhou, Process Migration ,ACM Computing Surveys ,V o l u m e3 2 ,N u m b e r 3 (2000), pages 241299. [Neiger et al. (2006)] G. Neiger, A. Santoni, F. Leung, D. Rodgers, and R. Uhlig, Intel Virtualization Technology: Hardware Support for

Virtualization Technology: Hardware Support for Efcient Orocessor Virtualization ,Intel Technology Journal ,V o l u m e1 0 ,( 2 0 0 6 ) . [Popek and Goldberg (1974)] G. J. Popek and R. P . Goldberg, Formal Require ments for Virtualizable Third Generation Architectures ,Communications of the ACM ,V o l u m e1 7 ,N u m b e r7( 1 9 7 4 ) ,p a g e s4 1 2  4 2 1 . [Poulhenning Kamp (2000)] R. N. M. W. Poulhenning Kamp, Jails: Conning the Omnipotent Root ,Proceedings of the 2nd International System

Root ,Proceedings of the 2nd International System Administration and Networking Conferenc (2000). [Powell and Miller (1983)] M. Powell and B. Miller, Process Migration in DEMOSMP ,Proceedings of the ACM Symposium on Operating Systems Principles (1983). [Waldspurger (2002)] C. Waldspurger, Memory Resource Management in VMware ESX Server ,Operating Systems Review ,V o l u m e3 6 ,N u m b e r4( 2 0 0 2 ) , pages 181194. [Wojtczuk and Ruthkowska (2011)] R. Wojtczuk and J. Ruthkowska, Follow ing the

R. Wojtczuk and J. Ruthkowska, Follow ing the White Rabbit: Software Attacks Against Intel VTd Technology ,The Invisible Things Labs blog (2011).17CHAPTER Distributed Systems Ad i s t r i b u t e ds y s t e mi sac o l l e c t i o no fp r o c e s s o r st h a td on o ts h a r em e m o r yo r ac l o c k .I n s t e a d ,e a c hn o d eh a si t so w nl o c a lm e m o r y .T h en o d e sc o m m u n i c a t e with one another through various networks, such as highspeed buses and the Internet. In this

such as highspeed buses and the Internet. In this chapter, we discuss the general structure of distributed systems and the networks that interconnect them. We also contrast the main differences in operatingsystem design between these systems and centralized systems. CHAPTER OBJECTIVES To provide a highlevel overview of distributed systems and the networks that interconnect them. To describe the general structure of distributed operating systems. To explain general communication structure and

To explain general communication structure and communication protocols. To discuss issues concerning the design of distributed systems. 17.1 Advantages of Distributed Systems Adistributed system is a collection of loosely coupled nodes interconnected by a communication network. From the point of view of a specic node in ad i s t r i b u t e ds y s t e m ,t h er e s to ft h en o d e sa n dt h e i rr e s p e c t i v er e s o u r c e sa r e remote, whereas its own resources are local. The nodes in

whereas its own resources are local. The nodes in a distributed system may vary in size and function. They may include small microprocessors, personal computers, and large generalpurpose computer systems. These processors are referred to by a number of names, such asprocessors, sites, machines, and hosts, depending on the context in which they are mentioned. We mainly use siteto indicate the location of a machine and node to refer to a specic system at a site. Generally, one node at one site,

system at a site. Generally, one node at one site, the server, has a resource that another node at another site, the client (or user), would like to use. A general structure of a distributed system is shown in Figure 17.1. There are four major reasons for building distributed systems: resource sharing, computation speedup, reliability, and communication. In this section, we briey discuss each of them. 741742 Chapter 17 Distributed Systemssite C resources site Bclientcommunicationsite A server

C resources site Bclientcommunicationsite A server network Figure 17.1 A distributed system. 17.1.1 Resource Sharing If a number of different sites (with different capabilities) are connected to one another, then a user at one site may be able to use the resources available at another. For example, a user at site A may be using a laser printer located at site B. Meanwhile, a user at B may access a le that resides at A. In general, resource sharing in a distributed system provides mechanisms for

in a distributed system provides mechanisms for sharing les at remote sites, processing information in a distributed database, printing les at remote sites, using remote specialized hardware devices (such as a supercomputer), and performing other operations. 17.1.2 Computation Speedup If a particular computation can be partitioned into subcomputations that can run concurrently, then a distributed system allows us to distribute the subcomputations among the various sites. The subcomputations can

among the various sites. The subcomputations can be run concurrently and thus provide computation speedup .I na d d i t i o n ,i fa particular site is currently overloaded with jobs, some of them can be moved to other, lightly loaded sites. This movement of jobs is called load sharing or job migration . Automated load sharing, in which the distributed operating system automatically moves jobs, is not yet common in commercial systems. 17.1.3 Reliability If one site fails in a distributed system,

If one site fails in a distributed system, the remaining sites can continue operating, giving the system better reliability. If the system is composed of multiple large autonomous installations (that is, generalpurpose computers), the failure of one of them should not affect the rest. If, however, the system is composed of small machines, each of which is responsible for some crucial system function (such as the web server or the le system), then a single failure may halt the operation of the

a single failure may halt the operation of the whole system. In general, with enough17.2 Types of Networkbased Operating Systems 743 redundancy (in both hardware and data), the system can continue operation, even if some of its sites have failed. The failure of a site must be detected by the system, and appropriate action may be needed to recover from the failure. The system must no longer use the services of that site. In addition, if the function of the failed site can be taken over by another

of the failed site can be taken over by another site, the system must ensure that the transfer of function occurs correctly. Finally, when the failed site recovers or is repaired, mechanisms must be available to integrate it back into the system smoothly. 17.1.4 Communication When several sites are connected to one another by a communication network, users at the various sites have the opportunity to exchange information. At al o wl e v e l , messages are passed between systems, much as messages

are passed between systems, much as messages are passed between processes in the singlecomputer message system discussed in Section 3.4. Given message passing, all the higherlevel functionality found in standalone systems can be expanded to encompass the distributed system. Such functions include le transfer, login, mail, and remote procedure calls (RPCs). The advantage of a distributed system is that these functions can be carried out over great distances. Two people at geographically distant

distances. Two people at geographically distant sites can collaborate on a project, for example. By transferring the les of the project, logging in to each others remote systems to run programs, and exchanging mail to coordinate the work, users minimize the limitations inherent in long distance work. We wrote this book by collaborating in such a manner. The advantages of distributed systems have resulted in an industrywide trend toward downsizing .M a n yc o m p a n i e sa r er e p l a c i n gt

.M a n yc o m p a n i e sa r er e p l a c i n gt h e i rm a i n f r a m e s with networks of workstations or personal computers. Companies get a bigger bang for the buck (that is, better functionality for the cost), more exibility in locating resources and expanding facilities, better user interfaces, and easier maintenance. 17.2 Types of Networkbased Operating Systems In this section, we describe the two general categories of networkoriented operating systems: network operating systems and

operating systems: network operating systems and distributed operating systems. Network operating systems are simpler to implement but generally more difcult for users to access and utilize than are distributed operating systems, which provide more features. 17.2.1 Network Operating Systems Anetwork operating system provides an environment in which users, who are aware of the multiplicity of machines, can access remote resources by either logging in to the appropriate remote machine or

logging in to the appropriate remote machine or transferring data from the remote machine to their own machines. Currently, all generalpurpose operating systems, and even embedded operating systems such as Android and i OS,a r en e t w o r ko p e r a t i n gs y s t e m s .744 Chapter 17 Distributed Systems 17.2.1.1 Remote Login An important function of a network operating system is to allow users to log in remotely. The Internet provides the ssh facility for this purpose. To illustrate, lets

ssh facility for this purpose. To illustrate, lets suppose that a user at Westminster College wishes to compute oncs.yale.edu, ac o m p u t e rt h a ti sl o c a t e da tY a l eU n i v e r s i t y .T od os o ,t h e user must have a valid account on that machine. To log in remotely, the user issues the command ssh cs.yale.edu This command results in the formation of an encrypted socket connection between the local machine at Westminster College and the cs.yale.edu  computer. After this connection

the cs.yale.edu  computer. After this connection has been established, the networking software creates a transparent, bidirectional link so that all characters entered by the user are sent to a process on cs.yale.edu and all the output from that process is sent back to the user. The process on the remote machine asks the user for a login name and a password. Once the correct information has been received, the process acts as a proxy for the user, who can compute on the remote machine just as any

who can compute on the remote machine just as any local user can. 17.2.1.2 Remote File Transfer Another major function of a network operating system is to provide a mechanism for remote le transfer from one machine to another. In such an environment, each computer maintains its own local le system. If a user at one site (say, cs.uvm.edu )w a n t st oa c c e s sa l el o c a t e do na n o t h e rc o m p u t e r (say, cs.yale.edu ), then the le must be copied explicitly from the computer at Yale to

be copied explicitly from the computer at Yale to the computer at the University of Vermont. The Internet provides a mechanism for such a transfer with the le transfer protocol ( FTP) program and the more private secure le transfer protocol ( SFTP ) program. Suppose that a user on cs.uvm.edu wants to copy a Java program Server.java that resides on cs.yale.edu. The user must rst invoke the sftp program by executing sftp cs.yale.edu The program then asks the user for a login name and a password.

asks the user for a login name and a password. Once the correct information has been received, the user must connect to the subdirectory where the le Server.java resides and then copy the le by executing get Server.java In this scheme, the le location is not transparent to the user; users must know exactly where each le is. Moreover, there is no real le sharing, because a user can only copy a l ef r o mo n es i t et oa n o t h e r .T h u s ,s e v e r a lc o p i e so ft h es a m e le may exist,

v e r a lc o p i e so ft h es a m e le may exist, resulting in a waste of space. In addition, if these copies are modied, the various copies will be inconsistent. Notice that, in our example, the user at the University of Vermont must have login permission on cs.yale.edu. FTPalso provides a way to allow a user17.2 Types of Networkbased Operating Systems 745 who does not have an account on the Yale computer to copy les remotely. This remote copying is accomplished through the anonymous FTPmethod,

is accomplished through the anonymous FTPmethod, which works as follows. The le to be copied (that is, Server.java )m u s tb ep l a c e d in a special subdirectory (say, ftp)w i t ht h ep r o t e c t i o ns e tt oa l l o wt h ep u b l i ct o read the le. A user who wishes to copy the le uses the ftpcommand. When the user is asked for the login name, the user supplies the name anonymous  and an arbitrary password. Once anonymous login is accomplished, the system must ensure that this partially

the system must ensure that this partially authorized user does not access inappropriate les. Generally, the user is allowed to access only those les that are in the directory tree of user anonymous. Any les placed here are accessible to any anonymous users, subject to the usual leprotection scheme used on that machine. Anonymous users, however, cannot access les outside of this directory tree. Implementation of the FTPmechanism is similar to ssh implementation. Ad a e m o no nt h er e m o t es

implementation. Ad a e m o no nt h er e m o t es i t ew a t c h e sf o rr e q u e s t st oc o n n e c tt ot h es y s t e m  s FTP port. Login authentication is accomplished, and the user is allowed to execute transfer commands remotely. Unlike the sshdaemon, which executes any command for the user, the FTPdaemon responds only to a predened set of lerelated commands. These include the following: getT ransfer a le from the remote machine to the local machine. putTransfer from the local machine to

machine. putTransfer from the local machine to the remote machine. lsordirList les in the current directory on the remote machine. cdChange the current directory on the remote machine. There are also various commands to change transfer modes (for binary or ASCII les) and to determine connection status. An important point about ssh and FTP is that they require the user to change paradigms. FTP requires the user to know a command set entirely different from the normal operatingsystem commands.

from the normal operatingsystem commands. With ssh,t h eu s e r must know appropriate commands on the remote system. For instance, a user on a Windows machine who connects remotely to a UNIX machine must switch toUNIX commands for the duration of the ssh session. (In networking, a session is a complete round of communication, frequently beginning with a login to authenticate and ending with a logoff to terminate the communication.) Obviously, users would nd it more convenient not to be required

would nd it more convenient not to be required to use ad i f f e r e n ts e to fc o m m a n d s .D i s t r i b u t e do p e r a t i n gs y s t e m sa r ed e s i g n e dt o address this problem. 17.2.2 Distributed Operating Systems In a distributed operating system, users access remote resources in the same way they access local resources. Data and process migration from one site to another is under the control of the distributed operating system. 17.2.2.1 Data Migration Suppose a user on site A

17.2.2.1 Data Migration Suppose a user on site A wants to access data (such as a le) that reside at site B. The system can transfer the data by one of two basic methods. One approach746 Chapter 17 Distributed Systems todata migration is to transfer the entire le to site A. From that point on, all access to the le is local. When the user no longer needs access to the le, a copy of the le (if it has been modied) is sent back to site B. Even if only a modest change has been made to a large le, all

a modest change has been made to a large le, all the data must be transferred. This mechanism can be thought of as an automated FTPsystem. This approach was used in the Andrew le system, but it was found to be too inefcient. The other approach is to transfer to site A only those portions of the le that are actually necessary for the immediate task. If another portion is required later, another transfer will take place. When the user no longer wants to access the le, any part of it that has been

to access the le, any part of it that has been modied must be sent back to site B. (Note the similarity to demand paging.) The Sun Microsystems network le system ( NFS)p r o t o c o lu s e st h i sm e t h o d( S e c t i o n1 2 . 8 ) ,a sd on e w e rv e r s i o n s of Andrew. The Microsoft SMB protocol (also known as Common Internet File System, or CIFS)a l s oa l l o w s l es h a r i n go v e ran e t w o r k . SMB is described in Section 19.6.2.1. Clearly, if only a small part of a large le is

Clearly, if only a small part of a large le is being accessed, the latter approach is preferable. If signicant portions of the le are being accessed, however, it is more efcient to copy the entire le. Whichever method is used, data migration includes more than the mere transfer of data from one site to another. The system must also perform various data translations if the two sites involved are not directly compatible (for instance, if they use different charactercode representations or

use different charactercode representations or represent integers with a different number or order of bits). 17.2.2.2 Computation Migration In some circumstances, we may want to transfer the computation, rather than the data, across the system; this process is called computation migration .F o r example, consider a job that needs to access various large les that reside at different sites, to obtain a summary of those les. It would be more efcient to access the les at the sites where they reside

to access the les at the sites where they reside and return the desired results to the site that initiated the computation. Generally, if the time to transfer the data is longer than the time to execute the remote command, the remote command should be used. Such a computation can be carried out in different ways. Suppose that process P wants to access a le at site A. Access to the le is carried out at site Aa n dc o u l db ei n i t i a t e db ya n RPC.A n RPC uses network protocols to execute ar

n RPC.A n RPC uses network protocols to execute ar o u t i n eo nar e m o t es y s t e m( S e c t i o n3 . 6 . 2 ) .P r o c e s sPi n v o k e sap r e d e  n e d procedure at site A. The procedure executes appropriately and then returns the results to P . Alternatively, process P can send a message to site A. The operating system at site A then creates a new process Q whose function is to carry out the designated task. When process Q completes its execution, it sends the needed result back to P

execution, it sends the needed result back to P via the message system. In this scheme, process P may execute concurrently with process Q. In fact, it may have several processes running concurrently on several sites. Either method could be used to access several les residing at various sites. One RPC might result in the invocation of another RPC or even in the transfer of messages to another site. Similarly, process Q could, during the course of its execution, send a message to another site,

of its execution, send a message to another site, which in turn would create another process. This process might either sen dam e s s a g eb a c kt oQo rr e p e a tt h ec y c l e .17.3 Network Structure 747 17.2.2.3 Process Migration A logical extension of computation migration is process migration .W h e na process is submitted for execution, it is not always executed at the site at which it is initiated. The entire process, or parts of it, may be executed at different sites. This scheme may be

be executed at different sites. This scheme may be used for several reasons: Load balancing .T h ep r o c e s s e s( o rs u b p r o c e s s e s )m a yb ed i s t r i b u t e da c r o s s the network to even the workload. Computation speedup .I fas i n g l ep r o c e s sc a nb ed i v i d e di n t oan u m b e r of subprocesses that can run concurrently on different sites, then the total process turnaround time can be reduced. Hardware preference .T h ep r o c e s sm a yh a v ec h a r a c t e r i s

h ep r o c e s sm a yh a v ec h a r a c t e r i s t i c st h a tm a k ei t more suitable for execution on some specialized processor (such as matrix inversion on an array processor) rather than on a microprocessor. Software preference .T h ep r o c e s sm a yr e q u i r es o f t w a r et h a ti sa v a i l a b l e at only a particular site, and either the software cannot be moved, or it is less expensive to move the process. Data access .J u s ta si nc o m p u t a t i o nm i g r a t i o n ,i ft h

si nc o m p u t a t i o nm i g r a t i o n ,i ft h ed a t ab e i n gu s e di nt h e computation are numerous, it may be more efcient to have a process run remotely than to transfer all the data. We use two complementary techniques to move processes in a computer network. In the rst, the system can attempt to hide the fact that the process has migrated from the client. The client then need not code her program explicitly to accomplish the migration. This method is usually employed for achieving

This method is usually employed for achieving load balancing and computation speedup among homogeneous systems, as they do not need user input to help them execute programs remotely. The other approach is to allow (or require) the user to specify explicitly how the process should migrate. This method is usually employed when the process must be moved to satisfy a hardware or software preference. You have probably realized that the World Wide Web has many aspects of ad i s t r i b u t e dc o m p

has many aspects of ad i s t r i b u t e dc o m p u t i n ge n v i r o n m e n t .C e r t a i n l yi tp r o v i d e sd a t am i g r a t i o n (between a web server and a web client). It also provides computation migration. For instance, a web client could trigger a database operation on a web server. Finally, with Java, Javascript, and similar languages, it provides a form of process migration: Java applets and Javascript scripts are sent from the server to the client, where they are executed. A

server to the client, where they are executed. A network operating system provides most of these features, but a distributed operating system makes them seamless and easily accessible. The result is a powerful and easytouse facility one of the reasons for the huge growth of the W orld W ide W eb. 17.3 Network Structure There are basically two types of networks: localarea networks (LAN )and widearea networks (WAN ). The main difference between the two is the way in which they are geographically

two is the way in which they are geographically distributed. Localarea networks are composed748 Chapter 17 Distributed Systems of hosts distributed over small areas (such as a single building or a number of adjacent buildings), whereas widearea networks are composed of systems distributed over a large area (such as the United States). These differences imply major variations in the speed and reliability of the communications networks, and they are reected in the distributed operatingsystem

are reected in the distributed operatingsystem design. 17.3.1 LocalArea Networks Localarea networks emerged in the early 1970s as a substitute for large mainframe computer systems. For many enterprises, it is more economical to have a number of small computers, each with its own selfcontained applications, than to have a single large system. Because each small computer is likely to need a full complement of peripheral devices (such as disks and printers), and because some form of data sharing is

and because some form of data sharing is likely to occur in as i n g l ee n t e r p r i s e ,i tw a san a t u r a ls t e pt oc o n n e c tt h e s es m a l ls y s t e m si n t oa network. LAN s, as mentioned, are usually designed to cover a small geographical area, and they are generally used in an ofce environment. All the sites in such systems are close to one another, so the communication links tend to have ah i g h e rs p e e da n dl o w e re r r o rr a t et h an do their counterparts in

e re r r o rr a t et h an do their counterparts in widearea networks. The most common links in a localarea network are twistedpair and ber optic cabling. The most common conguration is the star network. In a star network, the nodes connect to one or more switches, and the switches connect to each other, enabling any two nodes to communicate. Communication speeds range from 1 megabit per second for networks such as AppleTalk, infrared, and the Bluetooth local radio network to 40 gigabits per

Bluetooth local radio network to 40 gigabits per second for the fastest Ethernet. Ten megabits per second is the speed of 10BaseT Ethernet .100BaseT Ethernet and 1000BaseT Ethernet provide throughputs of 100 megabits and 1g i g a b i tp e rs e c o n do v e rt w i s t e d  p a i rc o p p e rc a b l e .T h eu s eo fo p t i c a l  ber cabling is growing; it provides higher communication rates over longer distances than are possible with copper. A typical LAN may consist of a number of different

A typical LAN may consist of a number of different computers (from mainframes to laptops or other mobile devices), various shared peripheral devices (such as laser printers and storage arrays), and one or more routers (specialized network communication processors) that provide access to other networks (Figure 17.2). Ethernet is commonly used to construct LAN s. An Ethernet network has no central c ontroller, because it is a multiaccess bus, so new hosts can be added easily to the network. The

new hosts can be added easily to the network. The Ethernet protocol is dened by the IEEE 802.3 standard. The wireless spectrum is increasingly used for designing localarea net works. Wireless (or WiFi )t e c h n o l o g ya l l o w su st oc o n s t r u c tan e t w o r ku s i n g only a wireless router to transmit signals between hosts. Each host has a wireless transmitter and receiver that it uses to participate in the network. A disadvantage of wireless networks conc erns their speed. Whereas

wireless networks conc erns their speed. Whereas Ether net systems often run at 1 gigabit per second, WiFi networks typically run considerably slower. There are several IEEE standards for wireless networks. The 802.11g standard can theoretically run at 54 megabits per second, but in practice, data rates are often less than half that. The recent 802.11n standard provides theoretically much higher data rates. In actual practice, though, these17.3 Network Structure 749 LAN Switch FirewallRouter WAN

Structure 749 LAN Switch FirewallRouter WAN Link LAN WAN Figure 17.2 Localarea network. networks have typical data rates of around 75 megabits per second. Data rates of wireless networks are heavily inuenced by the distance between the wireless router and the host, as well as interference in the wireless spectrum. On the positive side, wireless networks often have a physical advantage over wired Ethernet networks because they re quire no cabling to connect communicating hosts. As a result,

to connect communicating hosts. As a result, wireless networks are popular in homes and businesses, as well as public areas such as libraries, Internet cafes, sports arenas, and even buses and airplanes. 17.3.2 WideArea Networks Widearea networks emerged in the late 1960s, mainly as an academic research project to provide efcient communication among sites, allowing hardware and software to be shared conveniently and economically by a wide community of users. The rst WAN to be designed and

community of users. The rst WAN to be designed and developed was the Arpanet .B e g u n in 1968, the Arpanet has grown from a foursite experimental network to a worldwide network of networks, the Internet, comprising millions of computer systems. Because the sites in a WAN are physically distributed over a large geographi cal area, the communication links are, by default, relatively slow and unreliable. Typical links are telephone lines, leased ( dedicated data) lines, optical cable, microwave

( dedicated data) lines, optical cable, microwave links, radio waves, and satellite channels. These communication links are controlled by special communication processors (Figure 17.3), com monly known as gateway routers or simply routers ,t h a ta r er e s p o n s i b l ef o r dening the interface through which the sites communicate over the network, as well as for transferring information among the various sites. For example, the Internet WAN enables hosts at geographically separated sites to

enables hosts at geographically separated sites to communicate with one another. The host computers typically differ from one another in speed, CPU type, operating system, and so on. Hosts are generally on LAN s, which are, in turn, connected to the Internet via regional networks. The regional networks, such as NSFnet in the northeast United States, are interlinked with routers (Section 17.4.2) to form the worldwide750 Chapter 17 Distributed Systems communication processorcommunication subsystem

communication processorcommunication subsystem HH HH Huser processesuser processes network hostnetwork host host operating systemhost operating system CPCP CP CP Figure 17.3 Communication processors in a widearea network. network. Connections between networks sometimes use a telephonesystem service called T1, which provides a transfer rate of 1.544 megabits per second over a leased line. For sites requiring faster Internet access, T1s are collected into multipleT1 units that work in parallel to

into multipleT1 units that work in parallel to provide more throughput. For instance, a T3 is composed of 28 T1 connections and has a transfer rate of 45 megabits per second. Connections such as OC12 are common and provide 622 megabits per second. Residences can connect to the Internet by either telephone, cable, or specialized Intern et service providers that install routers to connect the residences to central services. Of course, there are other WAN s besides the Internet. A company might

other WAN s besides the Internet. A company might create its own private WAN for increased security, performance, or reliability. As mentioned, WAN sa r eg e n e r a l l ys l o w e rt h a n LAN s, although backbone WAN connections that link major cities may have transfer rates of over 40 gigabits per second. Frequently, WAN sa n d LAN si n t e r c o n n e c t ,a n di ti sd i f  c u l t to tell where one ends and the other starts. Consider the cellular phone data network. Cell phones are used for

phone data network. Cell phones are used for both voice and data communications. Cell phones in a given area connect via radio waves to a cell tower that contains receivers and transmitters. This part of the network is similar to a LAN except that the cell phones do not communicate with each other (unless two people talking or exchanging data happen to be conne cted to the same tower). Rather, the towers are connected to other t owers and to hubs that connect the tower communications to land

hubs that connect the tower communications to land lines or other communication mediums and route the packets toward their destinations. This part of the network is more WAN like. Once the appropriate tower receives the packets, it uses its transmitters to send them to the correct recipient.17.4 Communication Structure 751 17.4 Communication Structure Now that we have discussed the physical aspects of networking, we turn to the internal workings. The designer of a communication network must

The designer of a communication network must address ve basic issues: Naming and name resolution .H o wd ot w op r o c e s s e sl o c a t ee a c ho t h e rt o communicate? Routing strategies .H o wa r em e s s a g e ss e n tt h r o u g ht h en e t w o r k ? Packet strategies .A r ep a c k e t ss e n ti n d i v i d u a l l yo ra sas e q u e n c e ? Connection strategies .H o wd ot w op r o c e s s e ss e n das e q u e n c eo fm e s  sages? In the following sections, we elaborate on each of these

following sections, we elaborate on each of these issues. 17.4.1 Naming and Name Resolution The rst issue in network communication involves the naming of the systems in the network. For a process at site A to exchange information with a process at site B, each must be able to specify the other. Within a computer system, each process has a process identier, and messages may be addressed with the process identier. Because networked systems share no memory, however, a host within the system

share no memory, however, a host within the system initially has no knowledge about the processes on other hosts. To solve this problem, processes on remote systems are generally identied by the pair host name, identier ,w h e r e host name is a name unique within the network and identier is a process identier or other unique number within that host. A host name is usually an alphanumeric identier, rather than a number, to make it easier for users to specify. For instance, site A might have

users to specify. For instance, site A might have hosts named homer, marge, bart, and lisa. Bart is certainly easier to remember than is 12814831100. Names are convenient for humans to use, but computers prefer numbers for speed and simplicity. For this reason, there must be a mechanism to resolve the host name into a hostid that describes the destination system to the networking hardware. This mechanism is similar to the nametoaddress binding that occurs during program compilation, linking,

that occurs during program compilation, linking, loading, and execution (Chapter 8). In the case of host names, two possibilities exist. First, every host may have a data le containing the names and addresses of all the other hosts reachable on the network (similar to binding at compile time). The problem with this model is that adding or removing a host from the network requires updating the data les on all the hosts. The alternative is to distribute the information among systems on the

to distribute the information among systems on the network. The network must then use a protocol to distribute and retrieve the information. This scheme is like executiontime binding. The rst method was the one originally used on the Internet. As the Internet grew, however, it became untenab le. The second method, the domainname system (DNS ),i st h eo n en o wi nu s e . DNS species the naming structure of the hosts, as well as nametoaddress resolution. Hosts on the Internet are logically

resolution. Hosts on the Internet are logically addressed with multipart names known as IPaddresses. The parts of an IPaddress progress from the most752 Chapter 17 Distributed Systems specic to the most general, with periods separating the elds. For instance, bob.cs.brown.edu refers to host bobin the Department of Computer Science at Brown University within the toplevel domain edu.( O t h e rt o p  l e v e ld o m a i n s include comfor commercial sites and orgfor organizations, as well as a

sites and orgfor organizations, as well as a domain for each country connected to the network, for systems specied by country rather than organization type.) Generally, the system resolves addresses by examining the hostname components in r everse order. Each component has a name server simply a process on a systemthat accepts a name and returns the address of the name server responsible for that name. As the nal step, the name server for the host in question is contacted, and a hostid is

the host in question is contacted, and a hostid is returned. For example, a request made by a process on system A to communicate with bob.cs.brown.edu would result in the following steps: 1.The system library or the kernel on system A issues a request to the name server for the edudomain, asking for the address of the name server for brown.edu . The name server for the edudomain must be at a known address, so that it can be queried. 2.The edu name server returns the address of the host on which

server returns the address of the host on which the brown.edu name server resides. 3.System A then queries the name server at this address and asks about cs.brown.edu. 4.An address is returned. Now, nally, a request to that address for bob.cs.brown.edu returns an Internet address hostid for that host (for example, 128.148.31.100 ). This protocol may seem inefcient, but individual hosts cache the IPaddresses they have already resolved to speed the p rocess. (Of course, the contents of these

the p rocess. (Of course, the contents of these caches must be refreshed over time in case the name server is moved or its address changes.) In fact, the protocol is so important that it has been optimized many times and has had many safeguards added. Consider what would happen if the primary edu name server crashed. It is possible that noedu hosts would be able to have their addresses resolved, making them all unreachable! The solution is to use secondary, backup name servers that duplicate the

secondary, backup name servers that duplicate the contents of the primary servers. Before the domainname service was introduced, all hosts on the Internet needed to have copies of a le that contained the names and addresses of each host on the network. All changes to this le had to be registered at one site (host SRINIC ), and periodically all hosts had to copy the updated le from SRINIC to be able to contact new systems or nd hosts whose addresses had changed. Under the domainname service, ea

had changed. Under the domainname service, ea ch nameserver site is responsible for updating the host information for that domain. For instance, any host changes at Brown University are the responsibility of the name server for brown.edu and need not be reported anywhere else. DNS lookups will automatically retrieve the updated information because they will contact brown.edu directly. Domains may contain autonomous subdomains to further distribute the responsibility for hostname and hostid

the responsibility for hostname and hostid changes. Java provides the necessary APIto design a program that maps IPnames to IPaddresses. The program shown in Figure 17.4 is passed an IPname (such as17.4 Communication Structure 753  U s a g e :j a v aD N S L o o k U p I Pn a m e  i . e .j a v aD N S L o o k U pw w w . w i l e y . c o m  public class DNSLookUp  public static void main(String[] args)  InetAddress hostAddress; try  hostAddress  InetAddress.getByName(args[0]);

try  hostAddress  InetAddress.getByName(args[0]); System.out.println(hostAddress.getHostAddress());  catch (UnknownHostException uhe)  System.err.println(Unknown host:   args[0]);    Figure 17.4 Java program illustrating a DNS lookup. bob.cs.brown.edu )o nt h ec o m m a n dl i n ea n de i t h e ro u t p u t st h e IPaddress of the host or returns a message indicating that the host name could not be resolved. AnInetAddress is a Java class representing an IPname or address. The static method

an IPname or address. The static method getByName() belonging to the InetAddress class is passed a string representation of an IPname, and it returns the corresponding InetAddress . The program then invokes the getHostAddress() method, which internally uses DNS to look up the IPaddress of the designated host. Generally, the operating system is responsible for accepting from its processes a message destined for host name, identier and for transferring that message to the appropriate host. The

that message to the appropriate host. The kernel on the destination host is then responsible for transferring the message to the process named by the identier. This exchange is by no means trivial; it is described in Section 17.4.4. 17.4.2 Routing Strategies When a process at site A wants to communicate with a process at site B, how is the message sent? If there is only one physical path from A to B, the message must be sent through that path. However, if there are multiple physical paths from A

if there are multiple physical paths from A to B, then several routing options exist. Each site has a routing table indicating the alternative paths that can be used to send a message to other sites. The table may include information about the speed and cost of the various communication paths, and it may be updated as necessary, either manually or via programs that exchange routing information. The three most common routing schemes are xed routing ,virtual routing ,a n d dynamic routing . Fixed

,virtual routing ,a n d dynamic routing . Fixed routing .Ap a t hf r o mAt oBi ss p e c i  e di na d v a n c ea n dd o e sn o t change unless a hardware failure disables it. Usually, the shortest path is chosen, so that communication costs are minimized. Virtual routing .Ap a t hf r o mAt oBi s x e df o rt h ed u r a t i o no fo n es e s s i o n . Different sessions involving messages from A to B may use different paths.754 Chapter 17 Distributed Systems A session could be as short as a le

Systems A session could be as short as a le transfer or as long as a remotelogin period. Dynamic routing .T h ep a t hu s e dt os e n dam e s s a g ef r o ms i t eAt os i t e Bi sc h o s e no n l yw h e nt h em e s s a g ei ss e n t .B e c a u s et h ed e c i s i o ni sm a d e dynamically, separate messages may be assigned different paths. Site A will make a decision to send the message to site C. C, in turn, will decide to send it to site D, and so on. Eventually, a site will deliver the

D, and so on. Eventually, a site will deliver the message to B. Usually, a site sends a message to another site on whatever link is the least used at that particular time. There are tradeoffs among these th ree schemes. Fixed routing cannot adapt to link failures or load changes. In other words, if a path has been established between A and B, the messages must be sent along this path, even if the path is down or is used more heavily than another possible path. We can partially remedy this

possible path. We can partially remedy this problem by using virtual routing and can avoid it completely by using dynamic routing. Fixed routing and virtual routing ensure that messages from A to B will be delivered in the order in which they were sent. In dynamic routing, messages may arrive out of order. We can remedy this problem by appending a sequence number to each message. Dynamic routing is the most complicated to set up and run; however, it is the best way to manage routin gi nc o m p l

it is the best way to manage routin gi nc o m p l i c a t e de n v i r o n m e n t s . UNIX provides both xed routing for use on hosts within simple networks and dynamic routing for complicated network environments. It is also possible to mix the two. Within a site, the hosts may just need to know how to reach the system that connects the local network to other networks (such as companywide networks or the Internet). Such a node is known as a gateway .E a c hi n d i v i d u a lh o s th a s as t

.E a c hi n d i v i d u a lh o s th a s as t a t i cr o u t et ot h eg a t e w a y ,b u tt h eg a t e w a yi t s e l fu s e sd y n a m i cr o u t i n gt o reach any host on the rest of the network. Ar o u t e ri st h ec o m m u n i c a t i o n sp r o c e s s o rw i t h i nt h ec o m p u t e rn e t w o r k responsible for routing messages. A router can be a host computer with routing software or a specialpurpose device. Either way, a router must have at least two network connections, or else it

have at least two network connections, or else it would have nowhere to route messages. Ar o u t e rd e c i d e sw h e t h e ra n yg i v e nm e s s a g en e e d st ob ep a s s e df r o mt h e network on which it is received to any other network connected to the router. It makes this determination by examining the destination Internet address of the message. The router checks its tab les to determine the location of the destination host, or at least of the network to which it will send the

at least of the network to which it will send the message toward the destination host. In the case of static routing, this table is changed only by manual update (a new le is loaded onto the router). With dynamic routing, a routing protocol is used between routers to inform them of network changes and to allow them to update their routing tables automatically. Gateways and routers have typically been dedicated hardware devices that run code out of rmware. More recently, routing has been managed

of rmware. More recently, routing has been managed by software that directs multiple network devices more intelligently than a single router could. The software is devicein dependent, enablin gn e t w o r kd e v i c e s from multiple vendors to cooperate more easily. For example, the OpenFlow standard allows developers to introduce new networking efciencies and features by decoupling datarouting decisions from the underlying networking devices.17.4 Communication Structure 755 17.4.3 Packet

Communication Structure 755 17.4.3 Packet Strategies Messages generally vary in length. To simplify the system design, we com monly implement communication with xedlength messages called packets , frames ,o r datagrams .Ac o m m u n i c a t i o ni m p l e m e n t e di no n ep a c k e tc a nb e sent to its destination in a connectionless message .Ac o n n e c t i o n l e s sm e s s a g e can be unreliable, in which case the sender has no guarantee that, and cannot tell whether, the packet reached

that, and cannot tell whether, the packet reached its d estination. Alternatively, the packet can be reliable. Usually, in this case, an ackn owledgement packet is returned from the destination indicating that the original packet arrived. (Of course, the return packet could be lost along the way.) If a message is too long to t within one packet, or if the packets need to ow back and forth between the two communicators, a connection is established to allow the reliable exchange of multiple

to allow the reliable exchange of multiple packets. 17.4.4 Connection Strategies Once messages are able to reach their destinations, processes can institute communications sessions to exchange information. Pairs of processes that want to communicate over the network can be connected in a number of ways. The three most common schemes are circuit switching ,message switching ,a n d packet switching . Circuit switching .I ft w op r o c e s s e sw a n tt oc o m m u n i c a t e ,ap e r m a n e n t

a n tt oc o m m u n i c a t e ,ap e r m a n e n t physical link is established between them. This link is allocated for the duration of the communication session, and no other process can use that link during this period (even if the two processes are not actively communicating for a while). This scheme is similar to that used in the telephone system. Once a communication line has been opened between two parties (that is, party A calls party B), no one else can use this circuit until the

B), no one else can use this circuit until the communication is terminated explicitly (for example, when the parties hang up). Message switching . If two processes want to communicate, a temporary link is established for the duration of one message transfer. Physical links are allocated dynamically among correspondents as needed and are allocated for only short periods. Each message is a block of data with system informationsuch as the source, the destination, and error correction codes (

the destination, and error correction codes ( ECC)that allows the communication network to deliver the message to the destination correctly. This scheme is similar to the postofce mailing system. Each letter is a message that contains both the destination address and source (return) address. Many messages (from different users) can be shipped over the same link. Packet switching .O n el o g i c a lm e s s a g em a yh a v et ob ed i v i d e di n t oa number of packets. Each packet may be sent to

t oa number of packets. Each packet may be sent to its destination separately, and each therefore must include a source and a destination address with its data. Furthermore, the various packets may take different paths through the network. The packets must be reassembled into messages as they arrive. Note that it is not harmful for data to be broken into packets, possibly routed separately, and reassembled at the destination. Breaking756 Chapter 17 Distributed Systems up an audio signal (say, a

17 Distributed Systems up an audio signal (say, a telephone communication), in contrast, could cause great confusion if it was not done carefully. There are obvious tradeoffs among these schemes. Circuit switching requires substantial setup time and may waste network bandwidth, but it incurs less overhead for shipping each message. Conversely, message and packet switching require less setup time but incur more overhead per message. Also, in packet switching, each message must be divided into

switching, each message must be divided into packets and later reassembled. Packet switching is the method most commonly used on data networks because it makes the best use of network bandwidth. 17.5 Communication Protocols When we are designing a communication network, we must deal with the inherent complexity of coordinating asynchronous operations communicating in a potentially slow and errorprone environment. In addition, the systems on the network must agree on a protocol or a set of

the network must agree on a protocol or a set of protocols for determining host names, locating hosts on the network, establishing connections, and so on. We can simplify the design problem (and related implementation) by partitioning the problem into multiple layers. Each layer on one system communicates with the equivalent layer on other systems. Typically, each layer has its own protocols, and communication takes place between peer layers using a specic protocol. The protocols may be

using a specic protocol. The protocols may be implemented in hardware or software. For instance, Figure 17.5 shows the logical communications between two computers, with the three lowestlevel layers implemented in hardware. The International Standards Organization created the OSI model for describing the various layers of networking. While these layers are not imple mented in practice, they are useful for understanding how networking logically works, and we describe them below: real systems

works, and we describe them below: real systems environmentOSI environmentnetwork environmentdata networkcomputer A application layer presentation layer session layer trans port layer network layer link layer physical layerAPcomputer B AL (7) PL (6) S L (5) TL (4) NL (3) LL (2) PL (1)AP Figure 17.5 Two computers communicating via the OSI network model.17.5 Communication Protocols 757 1.Layer 1: Physical layer .T h ep h y s i c a ll a y e ri sr e s p o n s i b l ef o rh a n d l i n g both the

e s p o n s i b l ef o rh a n d l i n g both the mechanical and the electrical details of the physical transmission of a bit stream. At the physical layer, the communicating systems must agree on the electrical representation of a binary 0 and 1, so that when data are sent as a stream of electrical signals, the receiver is able to interpret the data properly as binary data. This layer is implemented in the hardware of the networking device. It is responsible for delivering bits. 2.Layer 2:

It is responsible for delivering bits. 2.Layer 2: Datalink layer . The datalink layer is responsible for handling frames, or xedlength parts of packets, including any error detection and recovery that occurs in the p hysical layer. It sends frames between physical addresses. 3.Layer 3: Network layer .T h en e t w o r kl a y e ri sr e s p o n s i b l ef o rb r e a k i n g messages into packets, providing connections between logical addresses, and routing packets in the communication network,

and routing packets in the communication network, including handling the addresses of outgoing packets, decoding the addresses of incoming packets, and maintaining routing information for proper response to changing load levels. Routers work at this layer. 4.Layer 4: Transport layer . The transport layer is responsible for transfer of messages between nodes, including partitioning messages into packets, maintaining packet order, and controlling ow to avoid congestion. 5.Layer 5: Session layer .T

to avoid congestion. 5.Layer 5: Session layer .T h es e s s i o nl a y e ri sr e s p o n s i b l ef o ri m p l e m e n t i n g sessions, or processtoprocess communication protocols. 6.Layer 6: Presentation layer .T h ep r e s e n t a t i o nl a y e ri sr e s p o n s i b l ef o r resolving the differences in formats among the various sites in the network, including character conversions and half duplexfull duplex modes (character echoing). 7.Layer 7: Application layer .T h ea p p l i c a t i o nl

7: Application layer .T h ea p p l i c a t i o nl a y e ri sr e s p o n s i b l ef o ri n t e r  acting directly with users. This layer deals with le transfer, remotelogin protocols, and electronic mail, as well as with schemas for distributed databases. Figure 17.6 summarizes the OSI protocol stack a set of cooperating protocolsshowing the physical ow of data. As mentioned, logically each layer of a protocol stack communicates with the equivalent layer on other systems. But physically, a

layer on other systems. But physically, a message starts at or above the application layer and is passed through each lower level in turn. Each layer may modify the message and include messageheader data for the equivalent layer on the receiving side. Ultimately, the message reaches the datanetwork layer and is transferred as one or more packets (Figure 17.7). The datalink layer of the target system receives these data, and the message is moved up through the protocol stack. It is analyzed,

up through the protocol stack. It is analyzed, modied, and stripped of headers as it progresses. It nally reaches the application layer for use by the receiving process. The OSI model formalizes some of the earlier work done in network protocols but was developed in the late 1970s and is currently not in widespread use. Perhaps the most widely adopted protocol stack is the TCPIP model, which has been adopted by virtually all Internet sites. The TCPIP protocol stack has fewer layers than the

The TCPIP protocol stack has fewer layers than the OSImodel. Theoretically, because it combines several758 Chapter 17 Distributed Systems datacommunication networkendtoend message transfer (connection management, error control, fragmentation, flow control) physical connection to network termination equipmenttransport layer network routing, addressing, call setup and clearingtransfersyntax negotiation datarepresentation transformations networkindependent messageinterchange servicepresentation

messageinterchange servicepresentation layerfile transfer, access, and management; document and message interchange; job transfer and manipulation syntaxindependent message interchange serviceenduser application processdistributed information services application layer dialog and synchronization control for application entitiessession layer network layer link layer physical layerdatalink control (framing, data transparency, error control) mechanical and electrical networkinterface connections

and electrical networkinterface connections Figure 17.6 The OSI protocol stack. functions in each layer, it is more difcult to implement but more efcient than OSInetworking. The relationship between the OSIand TCPIP models is shown in Figure 17.8. The TCPIP application layer identies several protocols in widespread use in the Internet, including HTTP ,FTP,T e l n e t , ssh,DNS,a n d SMTP .T h et r a n s p o r t layer identies the unreliable, connectionless user datagram protocol (UDP ) and the

user datagram protocol (UDP ) and the reliable, connectionoriented transmission control protocol (TCP). The Internet protocol ( IP)i sr e s p o n s i b l ef o rr o u t i n g IPdatagrams through the Internet. The TCPIP model does not formally identify a link or physical layer, allowing TCPIP trafc to run across any physical network. In Section 17.6, we consider the TCPIP model running over an Ethernet network. Security should be a concern in the design and implementation of any modern

in the design and implementation of any modern communication protocol. Both strong authentication and encryption are needed for secure communication. S trong authentication ensures that the sender and receiver of a communication are who or what they are17.5 Communication Protocols 759 datalinklayer headernetworklayer header transportlayer header sessionlayer header presentation layer application layer message datalinklayer trailer Figure 17.7 An OSI network message. supposed to be. Encryption

An OSI network message. supposed to be. Encryption protects the contents of the communication from eavesdropping. Weak authentication and cleartext communication are still very common, however, for a variety of reasons. When most of the common protocols were designed, security was frequently less important than performance, simplicity, and efciency. This legacy is still showing itself today, data linktransportapplicationHTTP, DNS, Telnet SMTP, FTP not defined not definednot definedOSI TCPIP not

not defined not definednot definedOSI TCPIP not defined IPTCPUDP physicalnetworksessionpresentation Figure 17.8 The OSI and TCPIP protocol stacks.760 Chapter 17 Distributed Systems as adding security to existing infrastructure is proving to be difcult and complex. Strong authentication requires a multistep handshake protocol or authen tication devices, adding complexity to a protocol. Modern CPUsc a ne f  c i e n t l y perform encryption, frequently including cryptographic acceleration instruc

including cryptographic acceleration instruc tions, so system performance is not compromised. Longdistance communica tion can be made secure by authenticating the endpoints and encrypting the stream of packets in a virtual private network, as discussed in Section 15.4.2. LAN communication remains unencrypted at most sites, but protocols such asNFS Version 4, which includes strong native authentication and encryption, should help improve even LAN security. 17.6 An Example: TCPIP We now return to

security. 17.6 An Example: TCPIP We now return to the nameresolution issue raised in Section 17.4.1 and examine its operation with respect to the TCPIP protocol stack on the Internet. Then we consider the processing n eeded to transfer a packet between hosts on different Ethernet netwo rks. We base our description on the IPV4p r o t o c o l s , which are the type most commonly used today. In a TCPIP network, every host has a name and an associated IPaddress (or hostid). Both of these strings

IPaddress (or hostid). Both of these strings must be unique; and so that the name space can be managed, they are segmented. The name is hierarchical (as explained in Section 17.4.1), describing the host name and then the organization with which the host is associated. The hostid is split into a network number and a host number. The proportion of the split varies, depending on the size of the network. Once the Internet administrators assign a network number, the site with that number is free to

number, the site with that number is free to assign hostids. The sending system checks its routing tables to locate a router to send the frame on its way. The routers use the network part of the hostid to transfer the packet from its source network to the destination network. The destination system then receives the packet. The packet may be a complete message, or it may just be a component of a message, with more packets needed before the message can be reassembled and passed to the TCPUDP

can be reassembled and passed to the TCPUDP layer for transmission to the destination process. Within a network, how does a packet move from sender (host or router) to receiver? Every Ethernet device has au n i q u eb y t en u m b e r ,c a l l e dt h e medium access control (MAC )address ,a s s i g n e dt oi tf o ra d d r e s s i n g .T w od e v i c e so na LAN communicate with each other only with this number. If a system needs to send data to another system, the networking software generates

another system, the networking software generates an address resolution protocol (ARP)packet containing the IPaddress of the destination system. This packet is broadcast to all other systems on that Ethernet network. Ab r o a d c a s tu s e sas p e c i a ln e t w o r ka d d r e s s( u s u a l l y ,t h em a x i m u m address) to signal that all hosts should receive and process the packet. The broadcast is not resent by gateways, so only systems on the local network receive it. Only the system

on the local network receive it. Only the system whose IPaddress matches the IPaddress of the ARP request responds and sends back its MAC address to the system that initiated the query. For efciency, the host caches the IPMAC address pair in an internal table. The cache entries are aged, so that an entry is eventually removed from the cache if an access to that system is not required within a given time. In17.6 An Example: TCPIP 761 preamblestart of packet start of frame delimiter destination

of packet start of frame delimiter destination address source address length of data section pad (optional) frame checksumbytes 7 1 2 or 6 2 or 6 2 each byte pattern 10101010 pattern 10101011 Ethernet address or broadcast Ethernet address length in bytes message data message must be  63 bytes long for error detection01500 046 4data Figure 17.9 An Ethernet packet. this way, hosts that are removed from a network are eventually forgotten. For added performance, ARP entries for heavily used hosts

performance, ARP entries for heavily used hosts may be pinned in the ARP cache. Once an Ethernet device has announced its hostid and address, commu nication can begin. A process may specify the name of a host with which to communicate. Networking software takes that name and determines the IP address of the target, using a DNS lookup. The message is passed from the application layer, through the software layers, and to the hardware layer. At the hardware layer, the packet (or packets) has the

hardware layer, the packet (or packets) has the Ethernet address at its start; at r a i l e ri n d i c a t e st h ee n do ft h ep a c k e ta n dc o n t a i n sa checksum for detection of packet damage (Figure 17.9). The packet is placed on the network by the Ethernet device. The data section of the packet may contain some or all of the data of the original message, but it may also contain some of the upperlevel headers that compose the message. In other words, all parts of the original message

In other words, all parts of the original message must be sent from source to destination, and all headers above the 802.3 layer (datalink layer) are included as data in the Ethernet packets. If the destination is on the same local network as the source, the system can look in its ARP cache, nd the Ethernet address of the host, and place the packet on the wire. The destination Ethernet device then sees its address in the packet and reads in the packet, passing it up the protocol stack. If the

packet, passing it up the protocol stack. If the destination system is on a network different from that of the source, the source system nds an appropriate router on its network and sends the packet there. Routers then pass the packet along the WAN until it reaches its destination network. The router that conne cts the destination network checks itsARP cache, nds the Ethernet numbe ro ft h ed e s t i n a t i o n ,a n ds e n d st h e packet to that host. Through all of these transfers, the

to that host. Through all of these transfers, the datalinklayer header may change as the Ethernet address of the next router in the chain is used, but the other headers of the packet remain the same until the packet is received and processed by the protocol stack and nally passed to the receiving process by the kernel.762 Chapter 17 Distributed Systems 17.7 Robustness Ad i s t r i b u t e ds y s t e mm a ys u f f e rf r o mv a r i o u st y p e so fh a r d w a r ef a i l u r e .T h e failure of a

fh a r d w a r ef a i l u r e .T h e failure of a link, the failure of a site, and the loss of a message are the most common types. To ensure that the system is robust, we must detect any of these failures, recongure the system so that computation can continue, and recover when a site or a link is repaired. 17.7.1 Failure Detection In an environment with no shared memory, we are generally unable to differentiate among link failure, site failure, and message loss. We can usually detect only that

and message loss. We can usually detect only that one of these failures has occurred. Once a failure has been detected, appropriate action must be taken. W hat action is appropriate depends on the particular application. To detect link and site failure, we use a heartbeat procedure. Suppose that sites A and B have a direct physical lin kb e t w e e nt h e m .A t x e di n t e r v a l s ,t h e sites send each other an Iamup message. If site A does not receive this message within a predetermined

not receive this message within a predetermined time period, it can assume that site B has failed, that the link between A and B has failed, or that the message from B has been lost. At this point, site A has two choices. It can wait for another time period to receive an Iamup message from B, or it can send an Areyouup? message to B. If time goes by and site A still has not received an Iamup message, or if site Ah a ss e n ta n Areyouup? message and has not received a reply, the procedure can be

and has not received a reply, the procedure can be repeated. Again, the only conclusion that site A can draw safely is that some type of failure has occurred. Site A can try to differentiate between link failure and site failure by sending anAreyouup? message to B by another route (if one exists). If and when B receives this message, it immediately replies positively. This positive reply tells A that B is up and that the failure is in the direct link between them. Since we do not know in advance

link between them. Since we do not know in advance how long it will take the message to travel from A to B and back, we must use a timeout scheme .A tt h et i m eAs e n d st h e Areyouup? message, it species a time interval during which it is willing to wait for the reply from B. If A receives the reply message within that time interval, then it can safely conclude that B is up. If not, however (that is, if a timeout occurs), then A may conclude only that one or more of the following situations

only that one or more of the following situations has occurred: Site B is down. The direct link (if one exists) from A to B is down. The alternative path from A to B is down. The message has been lost. Site A cannot, however, determine which of these events has occurred. 17.7.2 Reconguration Suppose that site A has discovered, through the mechanism just described, that a failure has occurred. It must then initiate a procedure that will allow the system to recongure and to continue its normal

the system to recongure and to continue its normal mode of operation.17.7 Robustness 763 If a direct link from A to B has failed, this information must be broadcast to every site in the system, so that the various routing tables can be updated accordingly. If the system believes that a site has failed (because that site can be reached no longer), then all sites in the system must be notied, so that they will no longer attempt to use the services of the failed site. The failure of a site that

of the failed site. The failure of a site that serves as a central coordinator for some activity (such as deadlock detection) requires the election of a new coordinator. Similarly, if the failed site is part of a logical ring, then a new logical ring must be constructed. Note that, if the site has not failed (that is, if it is up but cannot be reached), then we may have the undesirable situation in which two sites serve as the coordinator. When the network is partitioned, the two coordinators

the network is partitioned, the two coordinators (each for its own partition) may initiate conicting actions. For example, if the coordinators are responsible for implementing mutual exclusion, we may have a situation in which two processes are executing simultaneously in their critical sections. 17.7.3 Recovery from Failure When a failed link or site is repaired, it must be integrated into the system gracefully and smoothly. Suppose that a link between A and B has failed. When it is repaired,

between A and B has failed. When it is repaired, both A and B must be notied. We can accomplish this notication by continuously repeating the heartbeat procedure described in Section 17.7.1. Suppose that site B has failed. When it recovers, it must notify all other sites that it is up again. Site B then may have to receive information from the other sites to update its local tables. For example, it may need routing table information, a list of sites that are down, undelivered messages, a

of sites that are down, undelivered messages, a transaction log of unexecuted transactions, and mail. If the site has not failed but simply could not be reached, then it still needs this information. 17.7.4 Fault Tolerance Ad i s t r i b u t e ds y s t e mm u s tt o l e r a t eac e r t a i nl e v e lo ff a i l u r ea n dc o n t i n u et o function normally when faced with various types of failures. Making a facility fault tolerant starts at the protocol level, as described above, but continues

protocol level, as described above, but continues through all aspects of the system. We use the term fault tolerance in a broad sense. Communication faults, certain machine failures, storagedevice crashes, and decays of storage media should all be tolerated to some extent. A fault tolerant system should continue to function, perhaps in a degraded form, when faced with such failures. The degradation can affect performance, functionality, or both. It should be proportional, however, to the

both. It should be proportional, however, to the failures that caused it. A system that grinds to a halt when only one of its components fails is certainly not fault tolerant. Unfortunately, fault tolerance can be difcult and expensive to implement. At the network layer, multiple redundant communication paths and network devices such as switches and routers are needed to avoid a communication failure. A storage failure can cause loss of the operating system, applications, or data. Storage units

system, applications, or data. Storage units can include redundant hardware components that764 Chapter 17 Distributed Systems automatically take over from each other in case of failure. In addition, RAID systems can ensure continued access to the data even in the event of one or more disk failures (Section 10.7). A system failure without redundancy can cause an application or an entire facility to stop operation. The most simple system failure involves a system running only stateless

failure involves a system running only stateless applications. These applications can be restarted without compromising the operation; so as long as the applications can run on more than one computer (node), operation can continue. Such a facility is commonly known as a compute cluster because it centers on computation. In contrast, datacentric systems involve running applications that access and modify shared data. As a result, datacentric computing facilities are more difcult to make fault

facilities are more difcult to make fault tolerant. They require failuremonitoring software and special infrastructure. For instance, highavailability clusters include two or more computers and a set of shared disks. Any given application can be stored on the computers or on the shared disk, but the data must be stored on the shared disk. The running applications node has exclusive access to the applications data on disk. The application is monitored by the cluster software, and if it fails it

by the cluster software, and if it fails it is automatically restarted. If it cannot be restarted, or if the entire computer fails, the nodes exclusive access to the applications data is terminated and is granted to another node in the cluster. The application is restarted on that new node. The application loses whatever state information was in the failed systems memory but can continue based on whatever state it last wrote to the shared disk. From a users point of view, a service was

disk. From a users point of view, a service was interrupted and then restarted, possibly with some data missing. Specic applications may improve on this functionality by implementing lock management along with clustering. With lock management, the applica tion can run on multiple nodes and can use the same data on shared disks concurrently. Clustered databases frequently implement this functionality. If an o d ef a i l s ,t r a n s a c t i o n sc a nc o n t i n u eo no t h e rn o d e s ,a n du s

a nc o n t i n u eo no t h e rn o d e s ,a n du s e r sn o t i c en o interruption of service, as long as the client is able to automatically locate the other nodes in the cluster. Any noncommitted transactions on the failed node are lost, but again, client applications can be designed to retry noncommitted transactions if they detect a failure of their database node. 17.8 Design Issues Making the multiplicity of processors and storage devices transparent to the users has been a key challenge to

to the users has been a key challenge to many designers. Ideally, a distributed system should look to its users like a conventional, centralized system. The user interface of a transparent distributed system should not distinguish between local and remote resources. That is, users should be able to access remote resources as though these resources were local, and the distributed system should be responsible for locating the resources and for arranging for the appropriate interaction. Another

arranging for the appropriate interaction. Another aspect of transparency is user mobility. It would be convenient to allow users to log into any machine in the system rather than forcing them to use as p e c i  cm a c h i n e .At r a n s p a r e n td i s t r i b u t e ds y s t e mf a c i l i t a t e su s e rm o b i l i t y by bringing over the users environment (for example, home directory) to wherever he logs in. Protocols like LDAP provide an authentication system for17.9 Distributed File

an authentication system for17.9 Distributed File Systems 765 local, remote, and mobile users. Once the authentication is complete, facilities like desktop virtualization allow users to see their desktop sessions at remote facilities. Still another issue is scalability the capability of a system to adapt to increased service load. Systems have bounded resources and can become completely saturated under increased load. For example, with respect to a le system, saturation occurs either when a

to a le system, saturation occurs either when a servers CPU runs at a high utilization rate or when disks IO requests overwhelm the IO subsystem. Scalability is a relative property, but it can be measured accurately. A scalable system reacts more gracefully to increased load than does a nonscalable one. First, its performance degrades more moderately; an ds e c o n d ,i t sr e s o u r c e sr e a c ha saturated state later. Even perfect design cannot accommodate an evergrowing load. Adding new

cannot accommodate an evergrowing load. Adding new resources might solve the problem, but it might generate additional indirect load on other resources (for example, adding machines to ad i s t r i b u t e ds y s t e mc a nc l o gt h en e t w o r ka n di n c r e a s es e r v i c el o a d s ) .E v e n worse, expanding the system can call for expensive design modications. A scalable system should have the potential to grow without these problems. In ad i s t r i b u t e ds y s t e m ,t h ea b i l

In ad i s t r i b u t e ds y s t e m ,t h ea b i l i t yt os c a l eu pg r a c e f u l l yi so fs p e c i a li m p o r t a n c e , since expanding the network by addi ng new machines or interconnecting two networks is commonplace. In short, a scalable design should withstand high service load, accommodate growth of the user community, and allow simple integration of added resources. Scalability is related to fault tolerance, discussed earlier. A heavily loaded component can become paralyzed and

heavily loaded component can become paralyzed and behave like a faulty component. In addition, shifting the load from a faulty component to that components backup can saturate the latter. Generally, having spare resources is essential for ensuring reliability as well as for handling peak loads gracefully. Thus, the multiple resources in a distributed system represent an inherent advantage, giving the system a greater potential for fault tolerance and scalability. However, inappropriate design

and scalability. However, inappropriate design can obscure this potential. Faulttolerance and scalability considerations call for a design demonstrating distribution of control and data. Facilities like the Hadoop distributed le system were created with this problem in mind. Hadoop is based on Googles MapReduce and Google File System projects that created a facility to track every web page on the Internet. Hadoop is an opensource programming framework that supports the processing of large data

that supports the processing of large data sets in distributed computing environments. Traditional systems with traditional databases cannot scale to the capacity and performance needed by big data projects (at least not at reasonable prices). Examples of big data projects include mining Twitter for information pertinent to a company and sifting nancial data to look for trends in stock pricing. With Hadoop and its related tools, thousands of systems can work together to manage a distributed

systems can work together to manage a distributed database of petabytes of information. 17.9 Distributed File Systems Although the World Wide Web is the predominant distributed system in use today, it is not the only one. Another important and popular use of distributed computing is the distributed le system ,o r DFS.I nt h i ss e c t i o n ,w ed i s c u s s766 Chapter 17 Distributed Systems distributed le systems. In doing so, we use two running examplesOpen AFS, an opensource distributed le

examplesOpen AFS, an opensource distributed le system, and NFS,t h em o s tc o m m o n UNIX based DFS.NFS has several versions, and here we refer to NFS Version 3 unless otherwise noted. To explain the structure of a DFS,w en e e dt od e  n et h et e r m s service, server, and client in the DFS context. A service is a software entity running on one or more machines and providing a particular type of function to clients. Aserver is the service software running on a single machine. A client is ap

running on a single machine. A client is ap r o c e s st h a tc a ni n v o k eas e r v i c eu s i n gas e to fo p e r a t i o n st h a tf o r mi t s client interface . Sometimes a lowerlevel interface is dened for the actual crossmachine interaction; it is the intermachine interface . Using this terminology, we say that a le system provides le services to clients. A client interface for a le service is formed by a set of primitive le operations, such as create a le, delete a le, read from a le,

such as create a le, delete a le, read from a le, and write to a le. The primary hardware component that a le server controls is a set of local secondarystorage devices (usually, magnetic disks) on which les are stored and from which they are retrieved according to the clients requests. ADFS is a le system whose clients, servers, and storage devices are dispersed among the machines of a distributed system. Accordingly, service activity has to be carried out across the network. Instead of a

to be carried out across the network. Instead of a single centralized data repository, the system frequently has multiple and independent storage devices. As you will see, the concrete conguration and implementation of a DFS may vary from system to system. In some congurations, servers run on dedicated machines. In others, a machine can be both a server and a client. A DFS can be implemented as part of a distributed operating system or, alternatively, by a software layer whose task is to manage

by a software layer whose task is to manage the communication between conventional operating systems and le systems. The distinctive features of a DFS are the multiplicity and autonomy of clients and servers in the system. Ideally, though, a DFS should appear to its clients to be a conventional, centralized le system. That is, the client interface of a DFS should not distinguish between local and remote les. It is up to the DFSto locate the les and to arrange for the transport of the data. A

and to arrange for the transport of the data. A transparent DFSlike the transparent distributed systems mentioned earlierfacilitates user mobility by bringing a users environment (that is, home directory) to wherever the user logs in. The most important performance measure of a DFS is the amount of time needed to satisfy service requests. In conventional systems, this time consists of diskaccess time and a small amount of CPUprocessing time. In a DFS,h o w e v e r , ar e m o t ea c c e s sh a st

DFS,h o w e v e r , ar e m o t ea c c e s sh a st h ea d d i t i o n a lo v e r h e a da s s o c i a t e dw i t ht h ed i s t r i b u t e d structure. This overhead includes the time to deliver the request to a server, as well as the time to get the response across the network back to the client. For each direction, in addition to the transfer of the information, there is the CPU overhead of running the communication protocol software. The performance of a DFScan be viewed as another dimension

of a DFScan be viewed as another dimension of the DFSs transparency. That is, the performance of an ideal DFSwould be comparable to that of a conventional le system. The fact that a DFSmanages a set of dispersed storage devices is the DFSs key distinguishing feature. The overall storage space managed by a DFS is composed of different and remotely located smaller storage spaces. Usually, these constituent storage spaces correspond to sets of les. A component unit17.9 Distributed File Systems 767

A component unit17.9 Distributed File Systems 767 is the smallest set of les that can be stored on a single machine, independently from other units. All les belonging to the same component unit must reside in the same location. 17.9.1 Naming and Transparency Naming is a mapping between logical and physical objects. For instance, users deal with logical data objects represented by le names, whereas the system manipulates physical blocks of data stored on disk tracks. Usually, a user refers to a

stored on disk tracks. Usually, a user refers to a le by a textual name. The latter is mapped to a lowerlevel numerical identier that in turn is mapped to disk blocks. This multilevel mapping provides users with an abstraction of a le that hides the details of how and where on the disk the le is stored. In a transparent DFS,an e wd i m e n s i o ni sa d d e dt ot h ea b s t r a c t i o n :t h a to f hiding where in the network the le is located. In a conventional le system, the range of the

In a conventional le system, the range of the naming mapping is an address within a disk. In a DFS,t h i sr a n g e is expanded to include the specic machine on whose disk the le is stored. Going one step further with the concept of treating les as abstractions leads to the possibility of le replication .G i v e na l en a m e ,t h em a p p i n gr e t u r n sa set of the locations of this les replicas. In this abstraction, both the existence of multiple copies and their locations are hidden.

of multiple copies and their locations are hidden. 17.9.1.1 Naming Structures We need to differentiate two related n otions regarding name mappings in a DFS: 1. Location transparency .T h en a m eo fa l ed o e sn o tr e v e a la n yh i n to ft h e les physical storage location. 2. Location independence . The name of a le does not need to be changed when the les physical storage location changes. Both denitions relate to the level of na ming discussed previously, since les have different names at

previously, since les have different names at different levels (that is, userlevel textual names and systemlevel numerical identiers). A locationindependent naming scheme is ad y n a m i cm a p p i n g ,s i n c ei tc a nm a pt h es a m e l en a m et od i f f e r e n tl o c a t i o n s at two different times. Therefore, loca tion independence is a stronger property than is location transparency. In practice, most of the current DFSs provide a static, locationtransparent mapping for userlevel

static, locationtransparent mapping for userlevel names. Some support le migration thatis,changing the location of a le automatically, providing location independence. Open AFS supports location independence and le mobility, for example. The Hadoop distributed le system (HDFS )a special le system written for the Hadoop frameworkis a more recent creation. It includes le migration but does so without following POSIX standards, providing more exibility in imple mentation and interface. HDFS keeps

in imple mentation and interface. HDFS keeps track of the location of data but hides this information from clients. This dynamic location transparency allows the underlying mechanism to selftune. In another example, Amazons 3c l o u d storage facility provides blocks of storage on demand via APIs, placing the storage where it sees t and moving the data as necessary to meet performance, reliability, and capacity requirements.768 Chapter 17 Distributed Systems Af e wa s p e c t sc a nf u r t h e

Systems Af e wa s p e c t sc a nf u r t h e rd i f f e r e n t i a t el o c a t i o ni n d e p e n d e n c ea n ds t a t i c location transparency: Divorce of data from location, as exh ibited by location independence, provides a better abstraction for les. A le name should denote the les most signicant attributes, which are its contents rather than its location. Locationindependent les can be viewed as logical data containers that are not attached to a specic storage location. If only static

to a specic storage location. If only static location transparency is supported, the le name still denotes a specic, although hidden, set of physical disk blocks. Static location transparency provides users with a convenient way to share data. Users can share remote les by simply naming the les in a locationtransparent manner, as though the les were local. Dropbox and other cloudbased storage solutions work this way. Location independence promotes sharing the storage space itself, as well as the

sharing the storage space itself, as well as the data objects. When les can be mobilized, the overall, systemwide storage space looks like as i n g l ev i r t u a lr e s o u r c e .Ap o s s i b l eb e n e  ti st h ea b i l i t yt ob a l a n c et h e utilization of storage across the system. Location independence separates the naming hierarchy from the storage devices hierarchy and from the intercomputer structure. By contrast, if static location transparency is used (although names are

location transparency is used (although names are transparent), we can easily expose the correspondence between component units and machines. The machines are congured in a pattern similar to the naming structure. This conguration may restrict the architecture of the system unnecessarily and conict with other considerations. A server in charge of a root directory is an example of a structure that is dictated by the naming hierarchy and contradicts decentralization guidelines. Once the separation

decentralization guidelines. Once the separation of name and location has been completed, clients can access les residing on remote server systems. In fact, these clients may bediskless and rely on servers to provide all les, including the operating system kernel. Special protocols are needed for the boot sequence, however. Consider the problem of getting t he kernel to a diskless workstation. The diskless workstation has no kernel, so it cannot use the DFS code to retrieve the kernel. Instead,

use the DFS code to retrieve the kernel. Instead, a special boot protocol, stored in readonly memory ( ROM ) on the client, is invoked. It enables n etworking and retrieves only one special le (the kernel or boot code) from a xed location. Once the kernel is copied over the network and loaded, its DFSmakes all the other operatingsystem les available. The advantages of diskless clients are many, including lower cost (because the client machines require no disks) and greater convenience (when an

require no disks) and greater convenience (when an operatingsystem upgrade occurs, only the server needs to be modied). The disadvantages are the added complexity of the boot protocols and the performance loss resulting from the use of a network rather than a local disk. 17.9.1.2 Naming Schemes There are three main approaches to naming schemes in a DFS.I nt h es i m p l e s t approach, a le is identied by some combination of its host name and local name, which guarantees a unique systemwide

local name, which guarantees a unique systemwide name. In Ibis, for instance, a17.9 Distributed File Systems 769 le is identied uniquely by the name host:localname, where localname is a UNIX like path. The Internet URL system also uses this approach. This naming scheme is neither location transparent nor location independent. The DFS is structured as a collection of isolated component units, each of which is an entire conventional le system. Component units remain isolated, although means are

units remain isolated, although means are provided to refer to remote les. We do not consider this scheme any further here. The second approach was popularized by Suns network le system, NFS.NFS is found in many systems, including UNIX and Linux distributions. NFS provides a means to attach remote directories to local directories, thus giving the appearance of a coherent directory tree. Early NFS versions allowed only previously mounted remote directories to be accessed transparently. The advent

to be accessed transparently. The advent of the automount feature allowed mounts to be done on demand based on a table of mount points and lestructure names. Components are integrated to support transparent sharing, but this integration is limited and is not uniform, because each machine may attach different remote directories to its tree. The resulting structure is versatile. We can achieve total integration of the component le systems by using the third approach. Here, a single global name

the third approach. Here, a single global name structure spans all the les in the system. Ideally, the composed lesystem structure is the same as the structure of a conventional le system. In practice, however, the many special les (for example, UNIX device les and machinespecic binary directories) make this goal difcult to attain. To evaluate naming structures, we look at their administrative complexity. The most complex and most difcultto maintain structure is the NFS structure. Because any

structure is the NFS structure. Because any remote directory can be attached anywhere onto the local directory tree, the resulting hierarchy can be highly unstructured. If a server be comes unavailable, some arbitrary set of directories on different machines becomes unavailable. In addition, a separate accreditation mechanism controls which machine is allowed to attach which directory to its tree. Thus, a user might be able to access a remote directory tree on one client but be denied access on

tree on one client but be denied access on another client. 17.9.1.3 Implementation Techniques Implementation of transparent naming requires a provision for the mapping of a le name to the associated location. To keep this mapping manageable, we must aggregate sets of les into component units and provide the mapping on a componentunit basis rather than on a singlele basis. This aggregation serves administrative purposes as well. UNIX like systems use the hierarchical directory tree to provide

use the hierarchical directory tree to provide nametolocation mapping and to aggregate les recursively into directories. To enhance the availability of the crucial mapping information, we can use replication, local caching, or both. As we noted, location independence means that the mapping changes over time. Hence, replicating the mapping makes a simple yet consistent update of this information impossible. To overcome this obstacle, we can introduce lowlevel, locationindependent le identiers .(

lowlevel, locationindependent le identiers .( O p e n AFSuses this approach.) Textual le names are mapped to lowerlevel le identiers that indi cate to which component unit the le belongs. These identiers are still location independent. They can be replicated and cached freely without being in validated by migration of component770 Chapter 17 Distributed Systems units. The inevitable price is the need for a second level of mapping, which maps component units to locations and needs a simple yet

units to locations and needs a simple yet consistent update mechanism. Implementing UNIX like directory trees using these lowlevel, locationindependent identiers makes the whole hierarchy invariant under componentunit migration. The only aspect that does change is the component unit location mapping. Ac o m m o nw a yt oi m p l e m e n tl o w  l e v e li d e n t i  e r si st ou s es t r u c t u r e d names. These names are bit strings that usually have two parts. The rst part identies the

usually have two parts. The rst part identies the component unit to which the le belongs; the second part identies the particular le within the unit. Variants with more parts are possible. The invariant of structured names, however, is that individual parts of the name are unique at all times only within the context of the rest of the parts. We can obtain uniqueness at all times by taking care not to reuse a name that is still in use, by adding sufciently more bits (this method is used in Open

sufciently more bits (this method is used in Open AFS), or by using a timestamp as one part of the name (as was done in Apollo Domain). Another way to view this process is that we are taking a locationtransparent system, such as Ibis, and adding another level of abstraction to produce a locationindependent naming scheme. 17.9.2 Remote File Access Next, lets consider a user who requests access to a remote le. The server storing the le has been located by the naming scheme, and now the actual data

by the naming scheme, and now the actual data transfer must take place. One way to achieve this transfer is through a remoteservice mechanism , whereby requests for accesses are delivered to the server, the server machine performs the accesses, and their results are forwarded back to the user. One of the most common ways of implementing remote service is the RPC paradigm, which we discussed in Chapter 3. A direct analogy exists between diskaccess methods in conventional le systems and the

methods in conventional le systems and the remoteservice method in a DFS: using the remoteservice method is analogous to performing a disk access for each access request. To ensure reasonable performance of a remoteservice mechanism, we can use a form of caching. In conventional le systems, the rationale for caching is to reduce disk IO(thereby increasing performance), whereas in DFSs, the goal is to reduce both network trafc and disk IO.I nt h ef o l l o w i n gd i s c u s s i o n ,w e describe

o l l o w i n gd i s c u s s i o n ,w e describe the implementation of caching in a DFSand contrast it with the basic remoteservice paradigm. 17.9.2.1 Basic Caching Scheme The concept of caching is simple. If the data needed to satisfy the access request are not already cached, then a copy of those data is brought from the server to the client system. Accesses are performed on the cached copy. The idea is to retain recently accessed disk blocks in the cache, so that repeated accesses to the same

the cache, so that repeated accesses to the same information can be handled locally, without additional network trafc. A replacement policy (for example, the leastrecentlyused algorithm) keeps the cache size bounded. No direct correspondence exists between accesses and trafc to the s erver. Files are still identied with one master copy residing at the server machine, but copies (or parts) of the le are scattered in different caches. When a cached copy is modied, the changes17.9 Distributed File

copy is modied, the changes17.9 Distributed File Systems 771 need to be reected on the master copy to preserve the relevant consistency semantics. The problem of keeping the ca ched copies consistent with the master le is the cacheconsistency problem , which we discuss in Section 17.9.2.4. DFS caching could just as easily be called network virtual memory .I ta c t ss i m i l a r l y to demandpaged virtual memory, except that the backing store usually is a remote server rather than a local disk.

is a remote server rather than a local disk. NFSallows the swap space to be mounted remotely, so it actually can implement virtual memory over a network, though with a resulting performance penalty. The granularity of the cached data in a DFS can vary from blocks of a le to an entire le. Usually, more data are cached than are needed to satisfy a single access, so that many accesses can be served by the cached data. This procedure is much like disk readahead (Section 12.6.2). Open AFScaches les

readahead (Section 12.6.2). Open AFScaches les in large chunks (64 KB). The other systems discussed here support caching of individual blocks driven by client demand. Increasing the caching unit increases the hit ratio, but it also increases the miss penalty, because each miss requires more data to be transferred. It increases the potential for consistency problems as well. Selecting the unit of caching involves considering parameters such as the network transfer unit and the RPC protocol

as the network transfer unit and the RPC protocol service unit (if an RPC protocol is used). The network transfer un it (for Ethernet, a packet) is about 1.5KB,s ol a r g e ru n i t so fc a c h e dd a t an e e dt ob ed i s a s s e m b l e df o rd e l i v e r ya n d reassembled on reception. Block size and total cache size are obviously of importance for block caching schemes. In UNIX like systems, common block sizes are 4 KBand 8 KB. For large caches (over 1 MB), large block sizes (over 8 KB)

caches (over 1 MB), large block sizes (over 8 KB) are benecial. For smaller caches, large block sizes are less benecial because they result in fewer blocks in the cache and a lower hit ratio. 17.9.2.2 Cache Location Where should the cached data be storedon disk or in main memory? Disk caches have one clear advantage over main memory caches: they are reliable. Modications to cached data are lost in a crash if the cache is kept in volatile memory. Moreover, if the cached data are kept on disk,

Moreover, if the cached data are kept on disk, they are still there during recovery, and there is no need to fetch them again. Mainmemory caches have several advantages of their own, however: Mainmemory caches permit workstations to be diskless. Data can be accessed more quickly from a cache in main memory than from one on a disk. Technology is moving toward larger and less expensive memory. The resulting performance speedup is predicted to outweigh the advantages of disk caches. The server

outweigh the advantages of disk caches. The server caches (used to speed up disk IO)w i l lb ei nm a i nm e m o r y regardless of where user caches are located; if we use mainmemory caches on the user machine, too, we can build a single caching mechanism for use by both servers and users. Many remoteaccess implementations can be thought of as hybrids of caching and remote service. In NFS, for instance, the implementation is based on772 Chapter 17 Distributed Systems remote service but is

17 Distributed Systems remote service but is augmented with client and serverside memory caching for performance. Similarly, Sprites implementation is based on caching, but under certain circumstances, a remoteservice method is adopted. Thus, to evaluate the two methods, we must evaluate the degree to which either method is emphasized. The NFS protocol and most implementations do not provide disk caching. 17.9.2.3 CacheUpdate Policy The policy used to write modied data blocks back to the servers

to write modied data blocks back to the servers master copy has a critical effect on the systems performance and reliability. The simplest policy is to write data through to disk as soon as they are placed in any cache. The advantage of a writethrough policy is reliability: little information is lost when a client system crashes. However, this policy requires each write access to wait until the information is sent to the server, so it causes poor write performance. Caching with writethr ough is

write performance. Caching with writethr ough is equivalent to using remote service for write accesses and exploiting caching only for read accesses. An alternative is the delayedwrite policy ,a l s ok n o w na s writeback caching ,w h e r ew ed e l a yu p d a t e st ot h em a s t e rc o p y .M o d i  c a t i o n sa r ew r i t t e n to the cache and then are written through to the server at a later time. This policy has two advantages over writethrough. First, because writes are made to the

First, because writes are made to the cache, write accesses complete much more quickly. Second, data may be overwritten before they are written back, in which case only the last update needs to be written at all. Unfortun ately, delayedwrite schemes introduce reliability problems, since unwritten data are lost whenever a user machine crashes. Variations of the delayedwrite policy differ in when modied data blocks are ushed to the server. One alternative is to ush a block when it is about to be

is to ush a block when it is about to be ejected from the clients cache. This option can result in good performance, but some blocks can reside in the clients cache a long time before they are written back to the server. A compromise between this alternative and the writethrough policy is to scan the cache at regular intervals and to ush blocks that have been modied since the most recent scan, just as UNIX scans its local cache. Sprite uses this policy with a 30second interval. NFS uses the

this policy with a 30second interval. NFS uses the policy for le data, but once a write is issued to the server during a cache ush, the write must reach the servers disk before it is considered complete. NFS treats metadata (directory data and leattribute data) differently. Any metadata changes are issued synchronously to the server. Thus, lestructure loss and directorystructure corruption are avoided when a client or the server crashes. Yet another variation on delayed write is to write data

variation on delayed write is to write data back to the server when the le is closed. This writeonclose policy is used in Open AFS.I nt h e case of les that are open for short periods or are modied rarely, this policy does not signicantly reduce network trafc. In addition, the writeonclose policy requires the closing process to delay while the le is written through, which reduces the performance advantages of delayed writes. For les that are open for long periods and are modied frequently,

open for long periods and are modied frequently, however, the performance advantages of this policy over delayed write with more frequent ushing are apparent.17.10 Summary 773 17.9.2.4 Consistency Ac l i e n tm a c h i n ei ss o m e t i m e sf a c e dw i t ht h ep r o b l e mo fd e c i d i n gw h e t h e ra locally cached copy of data is consistent with the master copy (and hence can be used). If the client machine determines that its cached data are out of date, it must cache an uptodate copy

are out of date, it must cache an uptodate copy of the data before allowing further accesses. There are two approaches to verifying the validity of cached data: 1.Clientinitiated approach .T h ec l i e n ti n i t i a t e sav a l i d i t yc h e c k ,i nw h i c hi t contacts the server and checks whether the local data are consistent with the master copy. The frequency of the validity checking is the crux of this approach and determines the resulting consistency semantics. It can range from a

consistency semantics. It can range from a check before every access to a check only on rst access to a l e( o n l eo p e n ,b a s i c a l l y ) .E v e r ya c c e s sc o u p l e dw i t hav a l i d i t yc h e c k is delayed, compared with an access served immediately by the cache. Alternatively, checks can be initiated at xed time intervals. Depending on its frequency, the validity check can load both the network and the server. 2.Serverinitiated approach .T h es e r v e rr e c o r d s ,f o re a

approach .T h es e r v e rr e c o r d s ,f o re a c hc l i e n t ,t h e l e s (or parts of les) that it caches. When the server detects a potential inconsistency, it must react. A potential for inconsistency occurs when two different clients in conicting modes cache a le. If UNIX semantics (Section 11.5.3) is implemented, we can resolve the potential inconsistency by having the server play an active role. The server must be notied whenever a le is opened, and the intended mode (read or write)

is opened, and the intended mode (read or write) must be indicated for every open. The s erver can then act when it detects that a l eh a sb e e no p e n e ds i m u l t a n e o u s l yi nc o n  i c t i n gm o d e sb yd i s a b l i n g caching for that particular le. Actually, disabling caching results in switching to a remoteservice mode of operation. Distributed le systems are in common use today, providing le sharing within LAN s and across WAN sa sw e l l .T h ec o m p l e x i t yo fi m p l e

sa sw e l l .T h ec o m p l e x i t yo fi m p l e m e n t i n gs u c h as y s t e ms h o u l dn o tb eu n d e r e s t i m a t e d ,e s p e c i a l l yc o n s i d e r i n gt h a ti tm u s tb e operatingsystem independent for widespread adoption and must provide availability and good performance in the presence of long distances and sometimesfrail networking. 17.10 Summary Ad i s t r i b u t e ds y s t e mi sac o l l e c t i o no fp r o c e s s o r st h a td on o ts h a r em e m o r yo r ac l o c

r st h a td on o ts h a r em e m o r yo r ac l o c k .I n s t e a d ,e a c hp r o c e s s o rh a si t so w nl o c a lm e m o r y ,a n dt h ep r o c e s s o r s communicate with one another through various communication lines, such as highspeed buses and the Internet. The processors in a distributed system vary in size and function. They may include small microprocessors, personal computers, and large generalpurpose computer systems. The processors in the system are connected through a

processors in the system are connected through a communication network. Ad i s t r i b u t e ds y s t e mp r o v i d e st h eu s e rw i t ha c c e s st oa l ls y s t e mr e s o u r c e s . Access to a shared resource can be provided by data migration, computation774 Chapter 17 Distributed Systems migration, or process migration. The access can be specied by the user or implicitly supplied by the operating system and applications. Communications within a distributed system may occur via circuit

within a distributed system may occur via circuit switch ing, message switching, or packet switching. Packet switching is the method most commonly used on data networks. Through these methods, messages can be exchanged by nodes in the system. Protocol stacks, as specied by network layering models, add information to a message to ensure that it reaches its destination. A naming system (such asDNS)m u s tb eu s e dt ot r a n s l a t ef r o mah o s tn a m et oan e t w o r ka d d r e s s ,a n d

o s tn a m et oan e t w o r ka d d r e s s ,a n d another protocol (such as ARP) may be needed to translate the network number to a network device address (an Ethernet address, for instance). If systems are located on separate networks, routers are needed to pass packets from source network to destination network. There are many challenges to overcome for a distributed system to work correctly. Issues include naming of nodes and processes in the system, fault tolerance, error recovery, and

the system, fault tolerance, error recovery, and scalability. ADFS is a leservice system whose clients, servers, and storage devices are dispersed among the sites of a distributed system. Accordingly, service activity has to be carried out across the network; instead of a single centralized data repository, there are multiple independent storage devices. Ideally, a DFS should look to its clients like a conventional, centralized le system. The multiplicity and dispersion of its servers and

The multiplicity and dispersion of its servers and storage devices should be transparent. A transparent DFSfacilitates client mobility by bringing the clients environment to the site where the client logs in. There are several approaches to naming schemes in a DFS.I nt h es i m p l e s t approach, les are named by some combination of their host name and local name, which guarantees a unique systemwide name. Another approach, popularized by NFS,p r o v i d e sam e a n st oa t t a c hr e m o t ed

r o v i d e sam e a n st oa t t a c hr e m o t ed i r e c t o r i e st ol o c a l directories, thus giving the appearance of a coherent directory tree. Requests to access a remote le are usually handled by two complementary methods. With remote service, requests for accesses are delivered to the server. The server machine performs the accesses, and the results are forwarded back to the client. With caching, if the data needed to satisfy the access request are not already cached, then a copy of

request are not already cached, then a copy of the data is brought from the server to the client. Accesses are performed on the cached copy. The problem of keeping the cached copies consistent with the master le is the cacheconsistency problem. Practice Exercises 17.1 Why would it be a bad idea for gateways to pass broadcast packets between networks? What would be the advantages of doing so? 17.2 Discuss the advantages and disadvantages of caching name transla tions for computers located in

name transla tions for computers located in remote domains. 17.3 What are the advantages and disadvantages of using circuit switching? For what kinds of applications is circuit switching a viable strategy? 17.4 What are two formidable problems that designers must solve to implement a network system that has the quality of transparency?Exercises 775 17.5 Process migration within a heterogeneous network is usually impos sible, given the differences in a rchitectures and operating systems. Describe

in a rchitectures and operating systems. Describe a method for process migration across different architectures running: a. The same operating system b. Different operating systems 17.6 To build a robust distributed system, you must know what kinds of failures can occur. a. List three possible types of failure in a distributed system. b. Specify which of the entries in your list also are applicable to a centralized system. 17.7 Is it always crucial to know that the message you have sent has

crucial to know that the message you have sent has arrived at its destination safely? If your answer is yes,explain why. If your answer is no,give appropriate examples. 17.8 Ad i s t r i b u t e ds y s t e mh a st w os i t e s ,Aa n dB .C o n s i d e rw h e t h e rs i t eA can distinguish among the following: a. B goes down. b. The link between A and B goes down. c. B is extremely overloaded, and its response time is 100 times longer than normal. What implications does your answer have for

What implications does your answer have for recovery in distributed systems? Exercises 17.9 What is the difference between computation migration and process migration? Which is easier to implement, and why? 17.10 Even though the OSImodel of networking species seven layers of functionality, most computer systems use fewer layers to implement a network. Why do they use fewer layers? What problems could the use of fewer layers cause? 17.11 Explain why doubling the speed of the systems on an

why doubling the speed of the systems on an Ethernet segment may result in decreased network performance. What changes could help solve this problem? 17.12 What are the advantages of using dedicated hardware devices for routers and gateways? What are the disadvantages of using these devices compared with using generalpurpose computers? 17.13 In what ways is using a name server better than using static host tables? What problems or complications are associated with name servers? What methods

are associated with name servers? What methods could you use to decrease the amount of trafc name servers generate to satisfy translation requests?776 Chapter 17 Distributed Systems 17.14 Name servers are organized in a hierarchical manner. What is the purpose of using a hierarchical organization? 17.15 The lower layers of the OSInetwork model provide datagram service, with no delivery guarantees for messages. A transportlayer protocol such as TCP is used to provide reliability. Discuss the

as TCP is used to provide reliability. Discuss the advantages and disadvantages of supporting reliable message delivery at the lowest possible layer. 17.16 How does using a dynamic routing strategy affect application behav ior? For what type of applications is it benecial to use virtual routing instead of dynamic routing? 17.17 Run the program shown in Figure 17.4 and determine the IPaddresses of the following host names: www.wiley.com www.cs.yale.edu www.apple.com www.westminstercollege.edu

www.apple.com www.westminstercollege.edu www.ietf.org 17.18 The original HTTP protocol used TCPIP as the underlying network protocol. For each page, graphic, or applet, a separate TCP session was constructed, used, and torn down. Because of the overhead of building and destroying TCPIP connections, performance problems resulted from this implementation method. Would using UDP rather than TCP be a good alternative? What other changes could you make to improve HTTP performance? 17.19 What are the

to improve HTTP performance? 17.19 What are the advantages and the disadvantages of making the com puter network transparent to the user? 17.20 What are the benets of a DFS compared with a le system in a centralized system? 17.21 Which of the example DFSsd i s c u s s e di nt h i sc h a p t e rw o u l dh a n d l ea large, multiclient database application most efciently? Explain your answer. 17.22 Discuss whether Open AFSand NFS provide the following: (a) location transparency and (b) location

(a) location transparency and (b) location independence. 17.23 Under what circumstances would a client prefer a location transparent DFS?U n d e rw h a tc i r c u m s t a n c e sw o u l ds h ep r e f e ra locationindependent DFS?D i s c u s st h er e a s o n sf o rt h e s ep r e f e r e n c e s . 17.24 What aspects of a distributed system would you select for a system running on a totally reliable network? 17.25 Consider Open AFS,w h i c hi sas t a t e f u ld i s t r i b u t e d l es y s t e m

t a t e f u ld i s t r i b u t e d l es y s t e m .W h a t actions need to be performed to recover from a server crash in order to preserve the consistency guaranteed by the system?Bibliography 777 17.26 Compare and contrast the techniques of caching disk blocks locally, on ac l i e n ts y s t e m ,a n dr e m o t e l y ,o nas e r v e r . 17.27 Open AFSis designed to support a large number of clients. Discuss three techniques used to make Open AFSas c a l a b l es y s t e m . 17.28 What are the

c a l a b l es y s t e m . 17.28 What are the benets of mapping objects into virtual memory, as Apollo Domain does? What are the drawbacks? 17.29 Describe some of the fundamental differences between Open AFS and NFS (see Chapter 12). Bibliographical Notes [Tanenbaum (2010)] and [Kurose and Ross (2013)] provide general overviews of computer networks. The Internet and its protocols are described in [Comer (1999)] and [Comer (2000)]. Coverage of TCPIP can be found in [Fall and Stevens (2011)] and

can be found in [Fall and Stevens (2011)] and [Stevens (1995)]. UNIX network programming is described thoroughly in [Steven et al. ()] and [Stevens (1998)]. Load balancing and load sharing are discussed by [HarcholBalter and Downey (1997)] and [Vee and Hsu (2000)]. [Harish and Owens (1999)] describe loadbalancing DNS servers. Suns network le system ( NFS)i sd e s c r i b e db y[ C a l l a g h a n( 2 0 0 0 ) ]a n d [Sandberg et al. (1985)]. The Open AFS system is discussed by [Morris et al.

The Open AFS system is discussed by [Morris et al. (1986)], [Howard et al. (1988)], and [Satyanarayanan (1990)]. Information about Open AFS is available from http:www.openafs.org .T h eA n d r e w l es y s t e m is discussed in [Howard et al. (1988)]. The Google MapReduce method is described in http:research.google.comarchivemapreduce.html . Bibliography [Callaghan (2000)] B. Callaghan, NFS Illustrated ,A d d i s o n  W e s l e y( 2 0 0 0 ) . [Comer (1999)] D. Comer, Internetworking with TCPIP,

(1999)] D. Comer, Internetworking with TCPIP, Volume II, Third Edition, Prentice Hall (1999). [Comer (2000)] D. Comer, Internetworking with TCPIP, Volume I, Fourth Edition, Prentice Hall (2000). [Fall and Stevens (2011)] K. Fall and R. Stevens, TCPIP Illustrated, Volume 1: The Protocols, Second Edition, John Wiley and Sons (2011). [HarcholBalter and Downey (1997)] M. HarcholBalter and A. B. Downey, Exploiting Process Lifetime Distributions for Dynamic Load Balancing ,ACM Transactions on Computer

Load Balancing ,ACM Transactions on Computer Systems ,V o l u m e1 5 ,N u m b e r3( 1 9 9 7 ) ,p a g e s2 5 3  2 8 5 . [Harish and Owens (1999)] V. C . H a r i s h a n d B. O w e n s , Dynamic Load Balanc ing DNS ,Linux Journal ,V o l u m e1 9 9 9 ,N u m b e r6 4( 1 9 9 9 ) . [Howard et al. (1988)] J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, and R. N. Sidebotham, Scale and Performance in a778 Chapter 17 Distributed Systems Distributed File System ,ACM Transactions

Systems Distributed File System ,ACM Transactions on Computer Systems ,V o l u m e6 , Number 1 (1988), pages 5581. [Kurose and Ross (2013)] J. Kurose and K. Ross, Computer NetworkingA Top Down Approach, Sixth Edition, AddisonWesley (2013). [Morris et al. (1986)] J. H. Morris, M. Satyanarayanan, M. H. Conner, J. H. Howard, D. S. H. Rosenthal, and F. D. Smith, Andrew: A Distributed Personal Computing Environment ,Communications of the ACM ,V o l u m e2 9 ,N u m b e r3 (1986), pages 184201.

o l u m e2 9 ,N u m b e r3 (1986), pages 184201. [Sandberg et al. (1985)] R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon, Design and Implementation of the Sun Network Filesystem ,Proceed ings of the Summer USENIX Conference (1985), pages 119130. [Satyanarayanan (1990)] M. Satyanarayanan, Scalable, Secure and Highly Available Distributed File Access ,Computer ,V o l u m e2 3 ,N u m b e r5( 1 9 9 0 ) ,p a g e s 921. [Steven et al. ()] R. Steven, B. Fenner, and A. Rudoff, Unix Network

R. Steven, B. Fenner, and A. Rudoff, Unix Network Programming, Volume 1: The Sockets Networking API, Third Edition) ,p u b l i s h e rw i l e y ,y e a r2 0 0 3 . [Stevens (1995)] R. Stevens, TCPIP Illustrated, Volume 2: The Implementation , AddisonWesley (1995). [Stevens (1998)] W. R. Stevens, UNIX Network ProgrammingVolume II ,P r e n t i c e Hall (1998). [Tanenbaum (2010)] A. S. Tanenbaum, Computer Networks, Fifth Edition, Pren tice Hall (2010). [Vee and Hsu (2000)] V. Ve e a n d W. H s u ,

[Vee and Hsu (2000)] V. Ve e a n d W. H s u , LocalityPreserving LoadBalancing Mechanisms for Synchronous Simulations on SharedMemory Multiproces sors,Proceedings of the Fourteenth Workshop on Parallel and Distributed Simulation (2000), pages 131138.Part Seven Case Studies In the nal part of the book, we integrate the concepts described earlier by examining real operating systems. We cover two such systems in detailLinux and Windows 7. We chose Linux for several reasons: it is popular, it is

Linux for several reasons: it is popular, it is freely available, and it represents a fullfeatured UNIX system. This gives a student of operating systems an opportunity to readand modify real operatingsystem source code. We also cover Windows 7 in detail. This recent operating system from Microsoft is gaining popularity not only in the standalonemachine market but also in the workgroupserver market. We chose Windows 7 because it provides an opportunity to study a modern operating system that has

to study a modern operating system that has ad e s i g na n di m p l e m e n t a t i o nd r a s t i c a l l yd i f f e r e n tf r o mt h o s eo f UNIX . In addition, we briey discuss other highly inuential operating sys tems. Finally, we provide online coverage of two more systems: FreeBSD and Mach. The FreeBSD system is another UNIX system. However, whereas Linux combines features from several UNIX systems, FreeBSD is based on the BSD model. FreeBSD source code, like Linux source code, is

FreeBSD source code, like Linux source code, is freely available. Mach is a modern operating system that provides compatibility with BSD UNIX .18CHAPTER The Linux System Updated by Robert Love This chapter presents an indepth examination of the Linux operating system. By examining a complete, real system, we can see how the concepts we have discussed relate both to one another and to practice. Linux is a variant of UNIX that has gained popularity over the last several decades, powering devices

over the last several decades, powering devices as small as mobile phones and as large as room lling supercomputers. In this chapter, we look at the history and development of Linux and cover the user and programmer interfaces that Linux presents interfaces that owe a great deal to the UNIX tradition. We also discuss the design and implementation of these interfaces. Linux is a rapidly evolving operating system. This chapter describes developments through the Linux 3.2 kernel, which was released

through the Linux 3.2 kernel, which was released in 2012. CHAPTER OBJECTIVES To explore the history of the UNIX operating system from which Linux is derived and the principles upon which Linuxs design is based. To examine the Linux process model and illustrate how Linux schedules processes and provides interprocess communication. To look at memory management in Linux. To explore how Linux implements le systems and manages IOdevices. 18.1 Linux History Linux looks and feels much like any other

History Linux looks and feels much like any other UNIX system; indeed, UNIX compatibility has been a major design goal of the Linux project. However, Linux is much younger than most UNIX systems. Its development began in 1991, when a Finnish university student, Linus Torvalds, began developing as m a l lb u ts e l f  c o n t a i n e dk e r n e lf o rt h e8 0 3 8 6p r o c e s s o r ,t h e r s tt r u e3 2  b i t processor in Intels range of PCcompatible CPUs. 781782 Chapter 18 The Linux System

CPUs. 781782 Chapter 18 The Linux System Early in its development, the Linux source code was made available free both at no cost and with minimal distributional restrictionson the Internet. As a result, Linuxs history has been one of collaboration by many developers from all around the world, correspondi ng almost exclusively over the Internet. From an initial kernel that partially implemented a small subset of the UNIX system services, the Linux system has grown to include all of the

the Linux system has grown to include all of the functionality expected of a modern UNIX system. In its early days, Linux development revolved largely around the central operatingsystem kernelthe core, privileged executive that manages all system resources and interacts directly with the computer hardware. We need much more than this kernel, of course, to produce a full operating system. We thus need to make a distinction between the Linux kernel and ac o m p l e t eL i n u xs y s t e m .T h e

and ac o m p l e t eL i n u xs y s t e m .T h e Linux kernel is an original piece of software developed from scratch by the Linux community. The Linux system ,a sw e know it today, includes a multitude of components, some written from scratch, others borrowed from other developmen tp r o j e c t s ,a n ds t i l lo t h e r sc r e a t e di n collaboration with other teams. The basic Linux system is a standard environment for applications and user programming, but it does not enforce any standard

programming, but it does not enforce any standard means of managing the available functionality as a whole. As Linux has matured, a need has arisen for another layer of functionality on top of the Linux system. This need has been met by various Linux distributions. A Linux distribution includes all the standard components of the Linux system, plus a set of administrative tools to simplify the initial installation and subsequent upgrading of Linux and to manage installation and removal of other

and to manage installation and removal of other packages on the system. A modern distribution also typically includes tools for management of le systems, creation and management of user accounts, administration of networks, Web browsers, word processors, and so on. 18.1.1 The Linux Kernel The rst Linux kernel released to the public was version 0.01, dated May 14, 1991. It had no networking, ran only on 80386compatible Intel processors and PChardware, and had extremely limited devicedriver

PChardware, and had extremely limited devicedriver support. The virtual memory subsystem was also fairly basic and included no support for memorymapped les; however, even this early incarnation supported shared pages with copyonwrite and protected address spaces. The only le system supported was the Minix le system, as the rst Linux kernels were crossdeveloped on a Minix platform. The next milestone, Linux 1.0, was released on March 14, 1994. This release culminated three years of rapid

1994. This release culminated three years of rapid development of the Linux kernel. Perhaps the single biggest new feature was networking: 1.0 included support for UNIX s standard TCPIP networking protocols, as well as a BSDcompatible socket interface for networking programming. Devicedriver support was added for running IPover Ethernet or (via the PPPorSLIP protocols) over serial lines or modems. The 1.0 kernel also included a new, much enhanced le system without the limitations of the original

le system without the limitations of the original Minix le system, and it supported a range of SCSI controllers for highperformance disk access. The developers extended the vir tual memory subsystem to support paging to swap les and memory mapping18.1 Linux History 783 of arbitrary les (but only readonly memory mapping was implemented in 1.0). Ar a n g eo fe x t r ah a r d w a r es u p p o r tw a si n c l u d e di nt h i sr e l e a s e .A l t h o u g h still restricted to the Intel PCplatform,

o u g h still restricted to the Intel PCplatform, hardware support had grown to include oppydisk and CDROM devices, as well as sound cards, a range of mice, and international keyboards. Floatingpoint emulation was provided in the kernel for 80386 users who had no 80387 math coprocessor. System V UNIX style interprocess communication (IPC),i n c l u d i n gs h a r e dm e m o r y ,s e m a p h o r e s , and message queues, was implemented. At this point, development started on the 1.1 kernel

this point, development started on the 1.1 kernel stream, but numerous bugx patches were released subsequently for 1.0. A pattern was adopted as the standard numbering convention for Linux kernels. Kernels with an odd minorversion number, such as 1.1 or 2.5, are development kernels ;e v e n  numbered minorversion numbers are stable production kernels .U p d a t e s for the stable kernels are intended o nly as remedial versions, whereas the development kernels may include newer and relatively

kernels may include newer and relatively untested functionality. As we will see, this pattern remained in effect until version 3. In March 1995, the 1.2 kernel was released. This release did not offer nearly the same improvement in function ality as the 1.0 release, but it did support a much wider variety of hardware, including the new PCIhardware bus architecture. Developers added another PCspecic featuresupport for the 80386 CPUs virtual 8086 modeto allow emulation of the DOS operating system

modeto allow emulation of the DOS operating system for PCcomputers. They also updated the IPimplementation with support for accounting and rewalling. Simple support for dynamically loadable and unloadable kernel modules was supplied as well. The 1.2 kernel was the nal PConly Linux kernel. The source distribution for Linux 1.2 included partially implemented support for SPARC ,A l p h a ,a n d MIPS CPU s, but full integration of these other architectures did not begin until after the 1.2 stable

did not begin until after the 1.2 stable kernel was released. The Linux 1.2 release concentrated on wider hardware support and more complete implementations of existing functionality. Much new functionality was under development at the time, but integration of the new code into the main kernel source code was deferred until after the stable 1.2 kernel was released. As a result, the 1.3 development stream saw a great deal of new functionality added to the kernel. This work was released in June

to the kernel. This work was released in June 1996 as Linux version 2.0. This release was given a major versionnumber increment because of two major new capabilities: support for multiple architectures, including a 64bit native Alpha port, and symmetric multiprocessing ( SMP)s u p p o r t .A d d i t i o n a l l y ,t h em e m o r y  management code was substantially improved to provide a unied cache for lesystem data independent of the caching of block devices. As a result of this change, the

of block devices. As a result of this change, the kernel offered greatly increased lesystem and virtual memory performance. For the rst time, lesystem caching was extended to networked le systems, and writable memorymapped regions were also supported. Other major improvements inc luded the addition of internal kernel threads, a mechanism exposing dependencies between loadable modules, support for the automatic loading of modules on demand, lesystem quotas, and POSIX compatible realtime

lesystem quotas, and POSIX compatible realtime processscheduling classes.784 Chapter 18 The Linux System Improvements continued with the release of Linux 2.2 in 1999. A port to Ultra SPARC systems was added. Networking was enhanced with more exible rewalling, improved routing and trafc management, and support for TCP large window and selective acknowledgement. Acorn, Apple, and NTdisks could now be read, and NFS was enhanced with a new kernelmode NFS daemon. Signal handling, interrupts, and some

NFS daemon. Signal handling, interrupts, and some IO were locked at a ner level than before to improve symmetric multiprocessor ( SMP)p e r f o r m a n c e . Advances in the 2.4 and 2.6 releases of the kernel included increased support for SMP systems, journaling le systems, and enhancements to the memorymanagement and block IO systems. The process scheduler was modied in version 2.6, providing an efcient O(1) scheduling algorithm. In addition, the 2.6 kernel was preemptive, allowing a process

the 2.6 kernel was preemptive, allowing a process to be preempted even while running in kernel mode. Linux kernel version 3.0 was released in July 2011. The major version bump from 2 to 3 occurred to commemorate the twentieth anniversary of Linux. New features include improved virtualization support, a new page writeback facility, improvements to the memorymanagement system, and yet another new process schedulerthe Completely Fair Scheduler ( CFS). We focus on this newest kernel in the remainder

We focus on this newest kernel in the remainder of this chapter. 18.1.2 The Linux System As we noted earlier, the Linux kernel forms the core of the Linux project, but other components make up a complete Linux operating system. Whereas the Linux kernel is composed entirely of code written from scratch specically for the Linux project, much of the supporting software that makes up the Linux system is not exclusive to Linux but is common to a number of UNIX like operating systems. In particular,

of UNIX like operating systems. In particular, Linux uses many tools developed as part of Berkeleys BSD operating system, MITs X Window System, and the Free Software Foundations GNU project. This sharing of tools has worked in both directions. The main system libraries of Linux were originated by the GNU project, but the Linux community greatly improved the libraries by addressing omissions, inefciencies, and bugs. Other components, such as the GNU Cc o m p i l e r (gcc),w e r ea l r e a d yo f

GNU Cc o m p i l e r (gcc),w e r ea l r e a d yo f sufciently high quality to be used directly in Linux. The network administra tion tools under Linux were derived from code rst developed for 4.3 BSD,b u t more recent BSDderivatives, such as FreeBSD, have borrowed code from Linux in return. Examples of this sharing include the Intel oatingpointemulation math library and the PCsoundhardware device drivers. The Linux system as a whole is maintained by a loose network of developers collaborating

by a loose network of developers collaborating over the Intern et, with small groups or individuals having responsibility for maintaining the integrity of specic components. As m a l ln u m b e ro fp u b l i cI n t e r n e t l e  t r a n s f e r  p r o t o c o l( FTP)a r c h i v es i t e s act as de facto standard repositories for these components. The File System Hierarchy Standard document is also maintained by the Linux community as a means of ensuring compatibility across the various system

ensuring compatibility across the various system components. This standard species the overall layout of a standard Linux le system; it determines under which directory names conguration les, libraries, system binaries, and runtime data les should be stored.18.1 Linux History 785 18.1.3 Linux Distributions In theory, anybody can install a Linux system by fetching the latest revisions of the necessary system components from the FTPsites and compiling them. In Linuxs early days, this is precisely

them. In Linuxs early days, this is precisely what a Linux user had to do. As Linux has matured, however, various individuals and groups have attempted to make this job less painful by providing standard, precompiled sets of packages for easy installation. These collections, or distributions, include much more than just the basic Linux system. They typically include extra systeminstallation and management utilities, as well as precompiled and readytoinstall packages of many of the common UNIX

readytoinstall packages of many of the common UNIX tools, such as news servers, web browsers, textprocessing and editing tools, and even games. The rst distributions managed these packages by simply providing am e a n so fu n p a c k i n ga l lt h e l e si n t ot h ea p p r o p r i a t ep l a c e s .O n eo f the important contributions of modern d istributions, however, is advanced package management. Todays Linux distributions include a packagetracking database that allows packages to be

database that allows packages to be installed, upgraded, or removed painlessly. The SLSdistribution, dating back to the early days of Linux, was the rst collection of Linux packages that was recognizable as a complete distribution. Although it could be installed as a single entity, SLS lacked the package management tools now expected of Linux distributions. The Slackware distribution represented a great improvement in overall quality, even though it also had poor package management. In fact, it

it also had poor package management. In fact, it is still one of the most widely installed distributions in the Linux community. Since Slackwares release, many commercial and noncommercial Linux distributions have become available. Red Hat andDebian are particularly pop ular distributions; the rst comes from a commercial Linux support company and the second from the freesoftware Linux community. Other commercially supported versions of Linux include distributions from Canonical and SuSE , and

distributions from Canonical and SuSE , and others too numerous to list here. There are too many Linux distributions in circulation for us to list all of them here. The variety of distributions does not prevent Linux distributions from b eing compatible, however. The RPM package le format is used, or at least understood, by the majority of distributions, and commercial applications distributed in this format can be installed and run on any distribution that can accept RPM les. 18.1.4 Linux

distribution that can accept RPM les. 18.1.4 Linux Licensing The Linux kernel is distribu ted under version 2.0 of the GNU General Public License ( GPL), the terms of which are set out by the Free Software Foundation. Linux is not publicdomain software. Public domain implies that the authors have waived copyright rights in the software, but copyright rights in Linux code are still held by the codes various authors. Linux is freesoftware, however, in the sense that people can copy it, modify it,

in the sense that people can copy it, modify it, use it in any manner they want, and give away (or sell) their own copies. The main implication of Linuxs licensing terms is that nobody using Linux, or creating a derivative of Linux (a legitimate exercise), can distribute the derivative without including the source code. Software released under the GPL cannot be redistributed as a binaryonly product. If you release software that includes any components covered by the GPL,t h e n ,u n d e rt h e

covered by the GPL,t h e n ,u n d e rt h e GPL,y o um u s t786 Chapter 18 The Linux System make source code available alongside any binary distributions. (This restriction does not prohibit makingor even sellingbinary software distributions, as long as anybody who receives binaries is also given the opportunity to get the originating source code for a reasonable distribution charge.) 18.2 Design Principles In its overall design, Linux resemb les other traditional, nonmicrokernel UNIX

resemb les other traditional, nonmicrokernel UNIX implementations. It is a multiuser, preemptively multitasking system with a full set of UNIX compatible tools. Linuxs le system adheres to traditional UNIX semantics, and the standard UNIX networking model is fully implemented. The internal details of Linuxs design have been inuenced heavily by the history of this operating systems development. Although Linux runs on a wide variety of platforms, it was originally developed exclusively on

it was originally developed exclusively on PCarchitecture. A great deal of that early devel opment was carried out by individual enthusiasts rather than by wellfunded development or research facilities, so from the start Linux attempted to squeeze as much functionality as possible from limited resources. Today, Linux can run happily on a multiprocessor machine with many gigabytes of main memory and many terabytes of disk space, but it is still capable of operating usefully in under 16 MBofRAM .

of operating usefully in under 16 MBofRAM . AsPCsb e c a m em o r ep o w e r f u la n da sm e m o r ya n dh a r dd i s k sb e c a m e cheaper, the original, minimalist Linux kernels grew to implement more UNIX functionality. Speed and efciency are still important design goals, but much recent and current work on Linux has concentrated on a third major design goal: standardization. One of the prices paid for the diversity of UNIX implementations currently available is that source code written for

available is that source code written for one may not necessarily compile or run correctly on another. Even when the same system calls are present on two different UNIX systems, they do not necessarily behave in exactly the same way. The POSIX standards comprise a set of specications for different aspects of operatingsystem behavior. There are POSIX documents for common operatingsystem functionality and for extensions such as process threads and realtime operations. Linux is designed to comply

realtime operations. Linux is designed to comply with the relevant POSIX documents, and at least two Linux distributions have achieved ofcial POSIX certication. Because it gives standard interfaces to both the programmer and the user, Linux presents few surprises to anybody familiar with UNIX .W ed on o td e t a i l these interfaces here. The section s on the programmer interface (Section A.3) and user interface (Section A.4) of BSDapply equally well to Linux. By default, however, the Linux pro

well to Linux. By default, however, the Linux pro gramming interface adheres to SVR4 UNIX semantics, rather than to BSDbehavior. A separate set of libraries is available to implement BSD semantics in places where the two behaviors differ signicantly. Many other standards exist in the UNIX world, but full certication of Linux with respect to these standards is sometimes slowed because certication is often available only for a fee, an dt h ee x p e n s ei n v o l v e di nc e r t i f y i n ga n

p e n s ei n v o l v e di nc e r t i f y i n ga n operating systems compliance with most standards is substantial. However, supporting a wide base of applications is important for any operating system, so implementation of standards is a major goal for Linux development, even if the implementation is not formally certied. In addition to the basic POSIX18.2 Design Principles 787 standard, Linux currently supports the POSIX threading extensionsPthreads and a subset of the POSIX extensions for

and a subset of the POSIX extensions for realtime process control. 18.2.1 Components of a Linux System The Linux system is composed of three main bodies of code, in line with most traditional UNIX implementations: 1.Kernel .T h ek e r n e li sr e s p o n s i b l ef o rm a i n t a i n i n ga l lt h ei m p o r t a n t abstractions of the operating system, including such things as virtual memory and processes. 2.System libraries . The system libraries dene a standard set of functions through which

dene a standard set of functions through which applications can interact with the kernel. These functions implement much of the operatingsystem functionality that does not need the full privileges of kernel code. The most important system library is theCl i b r a r y ,k n o w na s libc .I na d d i t i o nt op r o v i d i n gt h es t a n d a r dC library, libc implements the user mode side of the Linux system call interface, as well as other critical systemlevel interfaces. 3.System utilities .T

systemlevel interfaces. 3.System utilities .T h es y s t e mu t i l i t i e sa r ep r o g r a m st h a tp e r f o r mi n d i  vidual, specialized management tasks. Some system utilities are invoked just once to initialize and congure some aspect of the system. Others known as daemons inUNIX terminologyrun permanently, handling such tasks as responding to incoming network connections, accepting logon requests from terminals, and updating log les. Figure 18.1 illustrates the various components

Figure 18.1 illustrates the various components that make up a full Linux system. The most important distinction here is between the kernel and everything else. All the kernel code executes in the processors privileged mode with full access to all the physical resources of the computer. Linux refers to this privileged mode as kernel mode . Under Linux, no user code is built into the kernel. Any operatingsystemsupport code that does not need to run in kernel mode is placed into the system

to run in kernel mode is placed into the system libraries and runs in user mode . Unlike kernel mode, user mode has access only to a controlled subset of the systems resources. system shared libraries Linux kernel loadable kernel modulessystem management programsuser processesuser utility programscompilers Figure 18.1 Components of the Linux system.788 Chapter 18 The Linux System Although various modern operating systems have adopted a message passing architecture for their kernel internals,

passing architecture for their kernel internals, Linux retains UNIX s historical model: the kernel is created as a single, monolithic binary. The main reason is performance. Because all kernel code and data structures are kept in a single address space, no context switches are necessary when a process calls an operatingsystem function or when a hardware interrupt is delivered. More over, the kernel can pass data and make requests between various subsystems using relatively cheap C function

subsystems using relatively cheap C function invocation and not more complicated inter process communication ( IPC). This single address space contains not only the core scheduling and virtual memory code but all kernel code, including all device drivers, le systems, and networking code. Even though all the kernel components sh are this same melting pot, there is still room for modularity. In the same way that user applications can load shared libraries at run time to pull in a needed piece of

libraries at run time to pull in a needed piece of code, so the Linux kernel can load (and unload) modules dynamically at run time. The kernel does not need to know in advance which modules may be loadedthey are truly independent loadable components. The Linux kernel forms the core of the Linux operating system. It provides all the functionality necessary to run processes, and it provides system services to give arbitrated and protected access to hardware resources. The kernel implements all the

hardware resources. The kernel implements all the features required to qualify as an operating system. On its own, however, the operating system provided by the Linux kernel is not ac o m p l e t e UNIX system. It lacks much of the functionality and behavior of UNIX ,a n dt h ef e a t u r e st h a ti td o e sp r o v i d ea r en o tn e c e s s a r i l yi nt h ef o r m a t in which a UNIX application expects them to appear. The operatingsystem interface visible to running applications is not

interface visible to running applications is not maintained directly by the kernel. Rather, applications make calls to the system libraries, which in turn call the operatingsystem services as necessary. The system libraries provide many types of functionality. At the simplest level, they allow applications to make system calls to the Linux kernel. Making as y s t e mc a l li n v o l v e st r a n s f e r r i n gc o n t r o lf r o mu n p r i v i l e g e du s e rm o d et o privileged kernel mode;

l e g e du s e rm o d et o privileged kernel mode; the details of this transfer vary from architecture to architecture. The libraries take care of collecting the systemcall arguments and, if necessary, arranging those arguments in the special form necessary to make the system call. The libraries may also provide more complex versions of the basic system calls. For example, the C languages buffered lehandling functions are all implemented in the system libraries, providing more advanced control

system libraries, providing more advanced control of le IOthan the basic kernel system calls. The libraries also provide routines that do not correspond to system calls at all, such as sorting algorithms, mathematical functions, and stringmanipulation routines. All the functions necessary to support the running of UNIX orPOSIX applications are implemented in the system libraries. The Linux system includes a wide variety of usermode programsboth system utilities and user utilities. The system

system utilities and user utilities. The system utilities include all the programs necessary to initialize and then administer the system, such as those to set up networking interfaces and to add and remove users from the system. User utilities are also necessary to the basic operation of the system but do not require elevated privileges t or u n .T h e yi n c l u d es i m p l e l e  m a n a g e m e n t utilities such as those to copy les, create directories, and edit text les. One18.3 Kernel

directories, and edit text les. One18.3 Kernel Modules 789 of the most important user utilities is the shell ,t h es t a n d a r dc o m m a n d  l i n e interface on UNIX systems. Linux supports many shells; the most common is thebourneAgain shell (bash ). 18.3 Kernel Modules The Linux kernel has the ability to load and unload arbitrary sections of kernel code on demand. These loadable kernel modules run in privileged kernel mode and as a consequence have full access to all the hardware

a consequence have full access to all the hardware capabilities of the machine on which they run. In theory, there is no restriction on what a kernel module is allowed to do. Among other things, a kernel module can implement ad e v i c ed r i v e r ,a l es y s t e m ,o ran e t w o r k i n gp r o t o c o l . Kernel modules are convenient for several reasons. Linuxs source code is free, so anybody wanting to write kernel code is able to compile a modied kernel and to reboot into that new

a modied kernel and to reboot into that new functionality. However, recompiling, relinking, and reloading the entire kernel is a cumbersome cycle to undertake when you are developing a new driver. If you use kernel modules, you do not have to make a new kernel to test a new driverthe driver can be compiled on its own and loaded into the already running kernel. Of course, once a new driver is written, it can be distributed as a module so that other users can benet from it without having to

other users can benet from it without having to rebuild their kernels. This latter point has another implication. Because it is covered by the GPL license, the Linux kernel canno tb er e l e a s e dw i t hp r o p r i e t a r yc o m p o n e n t s added to it unless those new compone nts are also released under the GPL and the source code for them is made available on demand. The kernels module interface allows third parties to write and distribute, on their own terms, device drivers or le systems

on their own terms, device drivers or le systems that could not be distributed under the GPL. Kernel modules allow a Linux system to be set up with a standard minimal kernel, without any extra device drivers built in. Any device drivers that the user needs can be either loaded explicitly by the system at startup or loaded automatically by the system on demand and unloaded when not in use. For example, a mouse driver can be loaded when a USB mouse is plugged into the system and unloaded when the

is plugged into the system and unloaded when the mouse is unplugged. The module support under Linux has four components: 1.The modulemanagement system allows modules to be loaded into memory and to communicate with the rest of the kernel. 2.The module loader and unloader ,w h i c ha r eu s e r  m o d eu t i l i t i e s ,w o r k with the modulemanagement system to load a module into memory. 3.The driverregistration system allows modules to tell the rest of the kernel that a new driver has become

rest of the kernel that a new driver has become available. 4.Aconictresolution mechanism allows different device drivers to reserve hardware resources and to protect those resources from accidental use by another driver. 18.3.1 Module Management Loading a module requires more than just loading its binary contents into kernel memory. The system must also make sure that any references the790 Chapter 18 The Linux System module makes to kernel symbols or entry points are updated to point to the

or entry points are updated to point to the correct locations in the kernels address space. Linux deals with this reference updating by splitting the job of module loading into two separate sections: the management of sections of module code i nk e r n e lm e m o r ya n dt h eh a n d l i n g of symbols that modules are allowed to reference. Linux maintains an internal symbol table in the kernel. This symbol table does not contain the full set of symbols dened in the kernel during the latters

of symbols dened in the kernel during the latters compilation; rather, a symbol must be explicitly exported. The set of exported symbols constitutes a welldened interface by which a module can interact with the kernel. Although exporting symbols from a kernel function requires an explicit request by the programmer, no special effort is needed to import those symbols into a module. A module writer just uses the standard external linking of the Cl a n g u a g e .A n ye x t e r n a ls y m b o l sr

a n g u a g e .A n ye x t e r n a ls y m b o l sr e f e r e n c e db yt h em o d u l eb u tn o td e c l a r e d by it are simply marked as unresolved in the nal module binary produced by the compiler. When a module is to be loaded into the kernel, a system utility rst scans the module for these unresolved references. All symbols that still need to be resolved are looked up in the kern els symbol table, and the correct addresses of those symbols in the currently running kernel are substituted

in the currently running kernel are substituted into the modules code. Only then is the m odule passed to the kernel for loading. If the system utility cannot resolve all references in the module by looking them up in the kernels symbol table, then the module is rejected. The loading of the module is performed in two stages. First, the module loader utility asks the kernel to reserve a continuous area of virtual kernel memory for the module. The kernel returns the address of the memory

The kernel returns the address of the memory allocated, and the loader utility can use this address to relocate the modules machine code to the correct loading address. A second system call then passes the module, plus any symbol table that the new module wants to export, to the kernel. The module itself is now copied verbatim into the previously allocated space, and the kernels symbol table is updated with the new symbols for possible use by other modules not yet loaded. The nal

use by other modules not yet loaded. The nal modulemanagement component is the module requester. The kernel denes a communication interface to which a modulemanagement program can connect. With this connection established, the kernel will inform the management process whenever a process requests a device driver, le system, or network service that is not currently loaded and will give the manager the opportunity to load that service. The original service request will complete once the module is

service request will complete once the module is loaded. The manager process regularly queries the kernel to see whether a dynamically loaded module is still in use and unloads that module when it is no longer actively needed. 18.3.2 Driver Registration Once a module is loaded, it remains no more than an isolated region of memory until it lets the rest of the kernel know what new functionality it provides. The kernel maintains dynamic tables of all known drivers and provides a set of routines to

known drivers and provides a set of routines to allow drivers to be added to or removed from these tables at any time. The kernel makes sure that it calls a modules startup routine when that module is loaded and calls the modules cleanup routine before18.3 Kernel Modules 791 that module is unloaded. These routines are responsible for registering the modules functionality. Am o d u l em a yr e g i s t e rm a n yt y p e so ff u n c t i o n a l i t y ;i ti sn o tl i m i t e d to only one type. For

y ;i ti sn o tl i m i t e d to only one type. For example, a device driver might want to register two separate mechanisms for accessing the device. Registration tables include, among others, the following items: Device drivers .T h e s ed r i v e r si n c l u d ec h a r a c t e r devices (such as printers, terminals, and mice), block devices (including all disk drives), and network interface devices. File systems .T h e l es y s t e mm a yb ea n y t h i n gt h a ti m p l e m e n t sL i n u x  s

y t h i n gt h a ti m p l e m e n t sL i n u x  s virtual le system calling routines. It might implement a format for storing les on a disk, but it might equally well be a network le system, such as NFS,o rav i r t u a l l es y s t e mw h o s ec o n t e n t sa r eg e n e r a t e do nd e m a n d ,s u c h as Linuxs proc le system. Network protocols .Am o d u l em a yi m p l e m e n ta ne n t i r en e t w o r k i n g protocol, such as TCP or simply a new set of packetltering rules for an e t w o r

a new set of packetltering rules for an e t w o r k r e w a l l . Binary format .T h i sf o r m a ts p e c i  e saw a yo fr e c o g n i z i n g ,l o a d i n g ,a n d executing a new type of executable le. In addition, a module can register a new set of entries in the sysctl andproc tables, to allow that module to be congured dynamically (Section 18.7.4). 18.3.3 Conict Resolution Commercial UNIX implementations are usually sold to run on a vendors own hardware. One advantage of a singlesupplier

own hardware. One advantage of a singlesupplier solution is that the software vendor has a good idea about what hardware congurations are possible. PChardware, however, comes in a vast number of congurations, with large numbers of possible drivers for devices such as network cards and video display adapters. The problem of managing the hardware conguration becomes more severe when modular device drivers are supported, since the currently active set of devic es becomes dynamically variable. Linux

of devic es becomes dynamically variable. Linux provides a central conictresolution mechanism to help arbitrate access to certain hardware resources. Its aims are as follows: To prevent modules from clashing over access to hardware resources To prevent autoprobes devicedriver probes that autodetect device congurationfrom interfering with existing device drivers To resolve conicts among multiple drivers trying to access the same hardwareas, for example, when both the parallel printer driver and

example, when both the parallel printer driver and the parallel line IP(PLIP)n e t w o r kd r i v e rt r yt ot a l kt ot h ep a r a l l e lp o r t To these ends, the kernel maintains lists of allocated hardware resources. The PChas a limited number of possible IOports (addresses in its hardware IOaddress space), interrupt lines, and DMA channels. When any device driver wants to access such a resource, it is expected to reserve the resource with792 Chapter 18 The Linux System the kernel database

Chapter 18 The Linux System the kernel database rst. This requirement incidentally allows the system administrator to determine exactly which r esources have been allocated by which driver at any given point. Am o d u l ei se x p e c t e dt ou s et h i sm e c h a n i s mt or e s e r v ei na d v a n c ea n y hardware resources that it expects to use. If the reservation is rejected because the resource is not present or is already in use, then it is up to the module to decide how to proceed. It

is up to the module to decide how to proceed. It may fail in its initialization attempt and request that it be unloaded if it cannot continue, or it may carry on, using alternative hardware resources. 18.4 Process Management Ap r o c e s si st h eb a s i cc o n t e x ti nw h i c ha l lu s e r  r e q u e s t e da c t i v i t yi ss e r v i c e d within the operating system. To be compatible with other UNIX systems, Linux must use a process model similar to those of other versions of UNIX .L i n u

to those of other versions of UNIX .L i n u x operates differently from UNIX in a few key places, however. In this section, we review the traditional UNIX process model (Section A.3.2) and introduce Linuxs threading model. 18.4.1 The fork() and exec() Process Model The basic principle of UNIX process management is to separate into two steps two operations that are usually combined into one: the creation of a new process and the running of a new program. A new process is created by the fork()

program. A new process is created by the fork() system call, and a new program is run after a call to exec() .T h e s ea r e two distinctly separate functions. We can create a new process with fork() without running a new programthe new subprocess simply continues to execute exactly the same program, at exactly the same point, that the rst (parent) process was running. In the same way, running a new program does not require that a new process be created rst. Any process may call exec() at any

be created rst. Any process may call exec() at any time. A new binary object is loaded into the processs address space and the new executable starts executing in the context of the existing process. This model has the advantage of great simplicity. It is not necessary to specify every detail of the environment of a new program in the system call that runs that program. The new program simply runs in its existing environment. If a parent process wishes to modify the environment in which a new

wishes to modify the environment in which a new program is to be run, it can fork and then, still running the original executable in a child process, make any system calls it requires to modify that child process before nally executing the new program. Under UNIX ,t h e n ,ap r o c e s se n c o m p a s s e sa l lt h ei n f o r m a t i o nt h a tt h e operating system must maintain to track the context of a single execution of a single program. Under Linux, we can break down this context into a

Under Linux, we can break down this context into a number of specic sections. Broadly, process properties fall into three groups: the process identity, environment, and context. 18.4.1.1 Process Identity Ap r o c e s si d e n t i t yc o n s i s t sm a i n l yo ft h ef o l l o w i n gi t e m s : Process ID(PID).E a c hp r o c e s sh a sau n i q u ei d e n t i  e r .T h e PIDis used to specify the process to the operating system when an application makes a18.4 Process Management 793 system call to

makes a18.4 Process Management 793 system call to signal, modify, or wait for the process. Additional identiers associate the process with a process group (typically, a tree of processes fork ed by a single user command) and login session. Credentials .E a c hp r o c e s sm u s th a v ea na s s o c i a t e du s e r IDand one or more group IDs( u s e rg r o u p sa r ed i s c u s s e di nS e c t i o n1 1 . 6 . 2 )t h a td e t e r m i n et h e rights of a process to access system resources and les.

of a process to access system resources and les. Personality .P r o c e s sp e r s o n a l i t i e sa r en o tt r a d i t i o n a l l yf o u n do n UNIX systems, but under Linux each process has an associated personality identier that can slightly modify the semantics of certain system calls. Personalities are primarily used by emulation libraries to request that system calls be compatible with certain varieties of UNIX . Namespace .E a c hp r o c e s si sa s s o c i a t e dw i t has p e c i  cv

c e s si sa s s o c i a t e dw i t has p e c i  cv i e wo ft h e l e  system hierarchy, called its namespace .M o s tp r o c e s s e ss h a r eac o m m o n namespace and thus operate on a shared lesystem hierarchy. Processes and their children can, however, have different namespaces, each with a unique lesystem hierarchytheir own root directory and set of mounted le systems. Most of these identiers are under the limited control of the process itself. The process group and session identiers can

The process group and session identiers can be changed if the process wants to start a new group or session. Its credentials can be changed, subject to appropriate security checks. However, the primary PID of a process is unchangeable and uniquely identies that process until termination. 18.4.1.2 Process Environment Ap r o c e s s  se n v i r o n m e n ti si n h e r i t e df r o mi t sp a r e n ta n di sc o m p o s e do ft w o nullterminated vectors: the argument vector and the environment

vectors: the argument vector and the environment vector. The argument vector simply lists the commandline arguments used to invoke the running program; it conventionally starts with the name of the program itself. Theenvironment vector is a list of NAMEVALUE pairs that associates named environment variables with arbitrary textual values. The environment is not held in kernel memory but is stored in the processs own usermode address space as the rst datum at the top of the processs stack. The

rst datum at the top of the processs stack. The argument and environment vectors are not altered when a new process is created. The new child process will i nherit the environment of its parent. However, a completely new environment is set up when a new program is invoked. On calling exec() , a process must supply the environment for the new program. The kernel passes these e nvironment variables to the next program, replacing the processs current en vironment. The kernel otherwise leaves the

en vironment. The kernel otherwise leaves the environment and command line vectors alonetheir interpretation is left entirely to the usermode libraries and applications. The passing of environment variables from one process to the next and the inheriting of these variables by the ch ildren of a process provide exible ways to pass information to components of the usermode system software. Various important environment variables have conventional meanings to related parts of the system software.

meanings to related parts of the system software. For example, the TERM variable is set up to name the type of terminal connected to a users login session. Many programs use this794 Chapter 18 The Linux System variable to determine how to perform operations on the users display, such as moving the cursor and scrolling a region of text. Programs with multilingual support use the LANG variable to determine the language in which to display system messages for programs that include multilingual

messages for programs that include multilingual support. The environmentvariable mechanism customtailors the operating system on a perprocess basis. Users can choose their own languages or select their own editors independently of one another. 18.4.1.3 Process Context The process identity and environment properties are usually set up when a process is created and not changed until that process exits. A process may choose to change some aspects of its identity if it needs to do so, or it may

of its identity if it needs to do so, or it may alter its environment. In contrast, process context is the state of the running program at any one time; it changes constantly. Process context includes the following parts: Scheduling context .T h em o s ti m p o r t a n tp a r to ft h ep r o c e s sc o n t e x ti si t s scheduling contextthe information that the scheduler needs to suspend and restart the process. This information includes saved copies of all the processs registers. Floatingpoint

of all the processs registers. Floatingpoint registers are stored separately and are restored only when needed. Thus, processes that do not use oatingpoint arithmetic do not incur the overhe ad of saving that state. The scheduling context also includes information about scheduling priority and about any outstanding signals waiting to be delivered to the process. A key part of the scheduling context is the processs kernel stack, a separate area of kernel memory reserved for use by kernelmode

of kernel memory reserved for use by kernelmode code. Both system calls and interrupts that occur while the process is executing will use this stack. Accounting .T h ek e r n e lm a i n t a i n sa c c o u n t i n gi n f o r m a t i o na b o u tt h e resources currently being consumed by each process and the total resources consumed by the process in its entire lifetime so far. File table . The le table is an array of pointers to kernel le structures representing open les. When making le IOsystem

representing open les. When making le IOsystem calls, processes refer to les by an integer, known as a le descriptor (fd),t h a tt h ek e r n e lu s e s to index into this table. Filesystem context .W h e r e a st h e l et a b l el i s t st h ee x i s t i n go p e n l e s ,t h e lesystem context applies to requests to open new les. The lesystem context includes the processs root directory, current working directory, and namespace. Signalhandler table .UNIX systems can deliver asynchronous

table .UNIX systems can deliver asynchronous signals to ap r o c e s si nr e s p o n s et ov a r i o u se x t e r n al events. The signalhandler table denes the action to take in response to a specic signal. Valid actions include ignoring the signal, terminating the process, and invoking a routine in the processs address space. Virtual memory context .T h ev i r t u a lm e m o r yc o n t e x td e s c r i b e st h ef u l l contents of a processs private address space; we discuss it in Section

private address space; we discuss it in Section 18.6.18.5 Scheduling 795 18.4.2 Processes and Threads Linux provides the fork() system call, which duplicates a process without loading a new executable image. Linux also provides the ability to create threads via the clone() system call. Linux does not distinguish between processes and threads, however. I nf a c t ,L i n u xg e n e r a l l yu s e st h et e r m task rather than process orthread when referring to a ow of control within a program.

referring to a ow of control within a program. The clone() system call behaves identically to fork() , except that it accepts as arguments a set of ags that dictate what resources are shared between the parent and child (whereas a process created with fork() shares no resources with its parent). The ags include:flag meaning CLONEFS CLONEVM CLONESIGHAND CLONEFILESFilesystem information is shared. The same memory space is shared. Signal handlers are shared. The set of open files is shared.Thus, if

shared. The set of open files is shared.Thus, if clone() is passed the ags CLONE FS,CLONE VM,CLONE SIGHAND , and CLONE FILES ,t h ep a r e n ta n dc h i l dt a s k sw i l ls h a r et h es a m e l e  s y s t e m information (such as the current working directory), the same memory space, the same signal handlers, and the same set of open les. Using clone() in this fashion is equivalent to creating a thread in other systems, since the parent task shares most of its resources with its child task. If

most of its resources with its child task. If none of these ags is set when clone() is invoked, however, the associated resources are not shared, resulting in functionality similar to that of the fork() system call. The lack of distinction between processes and threads is possible because Linux does not hold a processs entire context within the main process data structure. Rather, it holds the context within independent subcontexts. Thus, ap r o c e s s  s l e  s y s t e mc o n t e x t , l e  d

c e s s  s l e  s y s t e mc o n t e x t , l e  d e s c r i p t o rt a b l e ,s i g n a l  h a n d l e rt a b l e ,a n d virtual memory context are held in separate data structures. The process data structure simply contains pointers to these other structures, so any number of processes can easily share a subcontext by pointing to the same subcontext and incrementing a reference count. The arguments to the clone() system call tell it which subcontexts to copy and which to share. The new process

to copy and which to share. The new process is always given a new identity and a new scheduling contextthese are the e ssentials of a Linux process. According to the arguments passed, however, the kernel may either create new subcontext data structures initialized so as to be copies of the parents or set up the new process to use the same subcontext data structures being used by the parent. The fork() system call is nothing more than a special case of clone() that copies all subcontexts, sharing

of clone() that copies all subcontexts, sharing none. 18.5 Scheduling Scheduling is the job of allocating CPU time to different tasks within an operat ing system. Linux, like all UNIX systems, supports preemptive multitasking . In such a system, the process scheduler decides which process runs and when.796 Chapter 18 The Linux System Making these decisions in a way that balances fairness and performance across many different workloads is one of the more complicated challenges in modern operating

more complicated challenges in modern operating systems. Normally, we think of scheduling as the running and interrupting of user processes, but another aspect of scheduling is also important to Linux: the running of the various kernel tasks. Kernel tasks encompass both tasks that are requested by a running process and tasks that execute internally on behalf of the kernel itself, such as tasks spawned by Linuxs IOsubsystem. 18.5.1 Process Scheduling Linux has two separate processscheduling

Linux has two separate processscheduling algorithms. One is a timesharing algorithm for fair, preemptive scheduling among multiple processes. The other is designed for realtime tasks, where absolute priorities are more important than fairness. The scheduling algorithm used for routine timesharing tasks received am a j o ro v e r h a u lw i t hv e r s i o n2 . 6o ft h ek e r n e l .E a r l i e rv e r s i o n sr a na variation of the traditional UNIX scheduling algorithm. This algorithm does not

UNIX scheduling algorithm. This algorithm does not provide adequate support for SMP systems, does not scale well as the number of tasks on the system grows, and does not maintain fairness among interactive tasks, particularly on systems such as desktops and mobile devices. The process scheduler was rst overhauled with version 2.5 of the kernel. Version 2.5 implemented a scheduling algorithm that selects which task to run in constant timeknown as O(1)regardless of the number of tasks or

as O(1)regardless of the number of tasks or processors in the system. The new scheduler also provided increased support for SMP,i n c l u d i n gp r o c e s s o ra f  n i t ya n dl o a db a l a n c i n g .T h e s e changes, while improving scalability, did not improve interactive performance or fairnessand, in fact, made these problems worse under certain workloads. Consequently, the process scheduler wa so v e r h a u l e das e c o n dt i m e ,w i t hL i n u x kernel version 2.6. This version

,w i t hL i n u x kernel version 2.6. This version ushered in the Completely Fair Scheduler (CFS ). The Linux scheduler is a preemptive, prioritybased algorithm with two separate priority ranges: a realtime range from 0 to 99 and a nice value ranging from 20 to 19. Smaller nice values indicate higher priorities. Thus, by increasing the nice value, you are decreasing your priority and being nice to the rest of the system. CFSis a signicant departure from the traditional UNIX process scheduler. In

from the traditional UNIX process scheduler. In the latter, the core variables in the scheduling algorithm are priority and time slice. The time slice is the length of timethe slice of the processor that a process is afforded. Traditional UNIX systems give processes a xed time slice, perhaps with a boost or penalty for high or lowpriority processes, respectively. A process may run for the length of its time slice, and higher priority processes run before lowerpriority processes. It is a simple

run before lowerpriority processes. It is a simple algorithm that many non UNIX systems employ. Such simplicity worked well for early timesharing systems but has proved incapable of delivering good interactive performance and fairness on todays modern desktops and mobile devices. CFS introduced a new scheduling algorithm called fair scheduling that eliminates time slices in the traditional sense. Instead of time slices, all processes are allotted a proportion of the processors time. CFS

allotted a proportion of the processors time. CFS calculates how long a process should run as a function of the total number of runnable processes.18.5 Scheduling 797 To start, CFS says that if there are Nrunnable processes, then each should be afforded 1 Nof the processors time. CFS then adjusts this allotment by weighting each processs allotment by its nice value. Processes with the default nice value have a weight of 1their priority is unchanged. Processes with a smaller nice value (higher

Processes with a smaller nice value (higher priority) receive a high er weight, while processes with a larger nice value (lower priority) receive a lower weight. CFSthen runs each process for a time slice proportional to the processs weight divided by the total weight of all runnable processes. To calculate the actual length of time a process runs, CFS relies on a congurable variable called target latency , which is the interval of time during which every runnable task should run at least once.

every runnable task should run at least once. For example, assume that the target latency is 10 milliseconds. Further assume that we have two runnable processes of the same priority. Each of these processes has the same weight and therefore receives the same proportion of the processors time. In this case, with a target latency of 10 milliseconds, the rst process runs for 5m i l l i s e c o n d s ,t h e nt h eo t h e rp r o c e s sr u n sf o r5m i l l i s e c o n d s ,t h e nt h e r s t process

i l l i s e c o n d s ,t h e nt h e r s t process runs for 5 milliseconds again, and so forth. If we have 10 runnable processes, then CFSwill run each for a millisecond before repeating. But what if we had, say, 1 ,000 processes? Each process would run for 1 microsecond if we followed the procedure just described. Due to switching costs, scheduling processes for such short lengths of time is inefcient. CFS consequently relies on a second congurable variable, the minimum granularity , which is a

variable, the minimum granularity , which is a minimum length of time any process is allotted the processor. All processes, regardless of the target latency, will run for at least the minimum granularity. In this manner, CFSensures that switching costs do not grow unacceptably large when the number of runnable processes grows too large. In doing so, it violates its attempts at fairness. In the usual case, however, the number of runnable processes remains reasonable, and both fairness and

remains reasonable, and both fairness and switching costs are maximized. With the switch to fair scheduling, CFSbehaves differently from traditional UNIX process schedulers in several ways. Most notably, as we have seen, CFS eliminates the concept of a static time s lice. Instead, each process receives ap r o p o r t i o no ft h ep r o c e s s o r  st i m e .H o wl o n gt h a ta l l o t m e n ti sd e p e n d so n how many other processes are runna ble. This approach solves several problems in

ble. This approach solves several problems in mapping priorities to time slices inherent in preemptive, prioritybased scheduling algorithms. It is possible, of co urse, to solve these problems in other ways without abandoning the classic UNIX scheduler. CFS,h o w e v e r ,s o l v e st h e problems with a simple algorithm that performs well on interactive workloads such as mobile devices without compromising throughput performance on the largest of servers. 18.5.2 RealTime Scheduling Linuxs

of servers. 18.5.2 RealTime Scheduling Linuxs realtime scheduling algorithm is signicantly simpler than the fair scheduling employed for standard timesharing processes. Linux implements the two realtime scheduling classes required by POSIX .1b: rstcome, rst served ( FCFS )a n dr o u n d  r o b i n( S e c t i o n6 . 3 . 1a n dS e c t i o n6 . 3 . 4 ,r e s p e c t i v e l y ) .I n both cases, each process has a priority in addition to its scheduling class. The scheduler always runs the process

class. The scheduler always runs the process with the highest priority. Among processes of equal priority, it runs the process that has been waiting longest. The only798 Chapter 18 The Linux System difference between FCFS and roundrobin scheduling is that FCFS processes continue to run until they either exit or b lock, whereas a roundrobin process will be preempted after a while and will be moved to the end of the scheduling queue, so roundrobin processes of equal priority will automatically

processes of equal priority will automatically timeshare among themselves. Linuxs realtime scheduling is softrather than hardreal time. The scheduler offers strict guarantees about the relative priorities of realtime processes, but the kernel does not offer any guarantees about how quickly a realtime process will be scheduled once that process becomes runnable. In contrast, a hard realtime system can guarantee a minimum latency between when a process becomes runnable and when it actually runs.

becomes runnable and when it actually runs. 18.5.3 Kernel Synchronization The way the kernel schedules its ow no p e r a t i o n si sf u n d a m e n t a l l yd i f f e r e n t from the way it schedules processes. A request for kernelmode execution can occur in two ways. A running program may request an operatingsystem service, either explicitly via a system call or implicitlyfor example, when a page fault occurs. Alternatively, a device controller may deliver a hardware interrupt that causes the

may deliver a hardware interrupt that causes the CPU to start executing a kerneldened handler for that interrupt. The problem for the kernel is that all these tasks may try to access the same internal data structures. If one kernel task is in the middle of accessing some data structure when an interrupt servic er o u t i n ee x e c u t e s ,t h e nt h a ts e r v i c e routine cannot access or modify the same data without risking data corruption. This fact relates to the idea of critical

This fact relates to the idea of critical sectionsportions of code that access shared data and thus must not be allowed to execute concurrently. As a result, kernel synchronization involves much more than just process scheduling. A framework is required that allows kernel tasks to run without violating the integrity of shared data. Prior to version 2.6, Linux was a nonpreemptive kernel, meaning that a process running in kernel mode could not be preemptedeven if a higher priority process became

preemptedeven if a higher priority process became available to run. With version 2.6, the Linux kernel became fully preemptive. Now, a task can be preempted when it is running in the kernel. The Linux kernel provides spinlocks an ds e m a p h o r e s( a sw e l la sr e a d e r  writer versions of these two locks) for locking in the kernel. On SMP machines, the fundamental locking mechanism is a spinlock, and the kernel is designed so that spinlocks are held for only short durations. On

spinlocks are held for only short durations. On singleprocessor machines, spinlocks are not appropriate for use and are replaced by enabling and disabling kernel preemption. That is, rather than holding a spinlock, the task disables kernel preemption. When the task would otherwise release the spinlock, it enables kernel preemption. This pattern is summarized below:single processor multiple processors Acquire spin lock. Release spin lock.Disable kernel preemption. Enable kernel preemption.18.5

kernel preemption. Enable kernel preemption.18.5 Scheduling 799 Linux uses an interesting approach t od i s a b l ea n de n a b l ek e r n e lp r e  emption. It provides two simple kernel interfaces preempt disable() and preempt enable() .I na d d i t i o n ,t h ek e r n e li sn o tp r e e m p t i b l ei fak e r n e l  m o d e task is holding a spinlock. To enforce this rule, each task in the system has athreadinfo structure that includes the eld preempt count ,w h i c hi sa counter indicating

preempt count ,w h i c hi sa counter indicating the number of locks being held by the task. The counter is incremented when a lock is acquired and decremented when a lock is released. If the value of preempt count for the task currently running is greater than zero, it is not safe to preempt the kernel, as this task currently holds a lock. If the count is zero, the kernel can safely be interrupted, assuming there are no outstanding calls to preempt disable() . Spinlocksalong with the enabling

disable() . Spinlocksalong with the enabling and disabling of kernel preemption are used in the kernel only when the lock is held for short durations. When a lock must be held for longer periods, semaphores are used. The second protection technique used by Linux applies to critical sections that occur in interrupt service routines. The basic tool is the processors interruptcontrol hardware. By disabling i nterrupts (or using spinlocks) during ac r i t i c a ls e c t i o n ,t h ek e r n e lg u a

r i t i c a ls e c t i o n ,t h ek e r n e lg u a r a n t e e st h a t it can proceed without the risk of concurrent access to shared data structures. However, there is a penalty for disabling interrupts. On most hardware architectures, interrupt enable and disable instructions are not cheap. More importantly, as long as interrupts remain disabled, all IOis suspended, and any device waiting for servicing will have to wait until interrupts are reenabled; thus, performance degrades. To address

reenabled; thus, performance degrades. To address this pr oblem, the Linux kernel uses a synchronization architecture that allows long critical sections to run for their entire duration without having interrupts disabled. This ability is especially useful in the networking code. An interr upt in a network device driver can signal the arrival of an entire network pac ket, which may result in a great deal of code being executed to disassemble, route, and forward that packet within the interrupt

and forward that packet within the interrupt service routine. Linux implements this architecture b ys e p a r a t i n gi n t e r r u p ts e r v i c er o u t i n e s into two sections: the top half and the bottom half. The top half is the standard interrupt service routine that runs with recursive interrupts disabled. Interrupts of the same number (or line) are disabled, but other interrupts may run. The bottom half of a service routine is run, with all interrupts enabled, by a miniature

run, with all interrupts enabled, by a miniature scheduler that ensure st h a tb o t t o mh a l v e sn e v e ri n t e r r u p t themselves. The bottomhalf scheduler is invoked automatically whenever an interrupt service routine exits. This separation means that the kernel can complete any complex processing that has to be done in response to an interrupt without worrying about being interrupted itself. If another interrupt occurs while a bottom half is executing, then that interrupt can request

half is executing, then that interrupt can request that the same bottom half execute, but the execution will be deferred until the one currently running completes. Each execution of the bottom half can be interrupted by a top half but can never be interrupted by a similar bottom half. The tophalfbottomhalf architecture is completed by a mechanism for disabling selected bottom halves while executing normal, foreground kernel code. The kernel can code critical sections easily using this system.

code critical sections easily using this system. Interrupt handlers can code their critical sections as bottom halves; and when the foreground kernel wants to enter a critical section, it can disable any relevant800 Chapter 18 The Linux Systemtophalf interrupt handlers bottomhalf interrupt handlers kernelsystem service routines (preemptible) usermode programs (preemptible) increasing priority Figure 18.2 Interrupt protection levels. bottom halves to prevent any other critical sections from

halves to prevent any other critical sections from interrupting it. At the end of the critical section, the kernel can reenable the bottom halves and run any bottomhalf tasks that have been queued by tophalf interrupt service routines during the critical section. Figure 18.2 summarizes the various levels of interrupt protection within the kernel. Each level may be interrupted by code running at a higher level but will never be interrupted by code running at the same or a lower level. Except for

running at the same or a lower level. Except for usermode code, user processes can always be preempted by another process when a timesharing scheduling interrupt occurs. 18.5.4 Symmetric Multiprocessing The Linux 2.0 kernel was the rst stable Linux kernel to support symmetric multiprocessor (SMP )hardware, allowing separate processes to execute in parallel on separate processors. The original implementation of SMP imposed the restriction that only one processor at a time could be executing

only one processor at a time could be executing kernel code. In version 2.2 of the kernel, a sin gle kernel spinlock (sometimes termed BKL forbig kernel lock )w a sc r e a t e dt oa l l o wm u l t i p l ep r o c e s s e s( r u n n i n go n different processors) to be active in th ek e r n e lc o n c u r r e n t l y .H o w e v e r ,t h e BKL provided a very coarse level of locking granularity, resulting in poor scalability to machines with many processors and processes. Later releases of the

processors and processes. Later releases of the kernel made the SMP implementation more scalable by splitting this single kernel spinlock into multiple locks, each of which protects only a small subset of the kernels data structures. Such spinlocks are described in Section 18.5.3. The 3.0 kernel provides additional SMP enhancements, including everner locking, processor afnity, and loadbalancing algorithms. 18.6 Memory Management Memory management under Linux has two components. The rst deals

under Linux has two components. The rst deals with allocating and freeing physical memorypages, groups of pages, and small blocks of RAM .T h es e c o n dh a n d l e sv i r t u a lm e m o r y ,w h i c hi sm e m o r y  m a p p e d into the address space of running processes. In this section, we describe these two components and then examine the mechanisms by which the loadable components of a new program are brought into a processs virtual memory in response to an exec() system call.18.6 Memory

in response to an exec() system call.18.6 Memory Management 801 18.6.1 Management of Physical Memory Due to specic hardware constraints, Linux separates physical memory into four different zones ,o rr e g i o n s : ZONE DMA ZONE DMA32 ZONE NORMAL ZONE HIGHMEM These zones are architecture specic. For example, on the Intel x8632 architec ture, certain ISA(industry standard architecture) devices can only access the lower 16 MBof physical memory using DMA .O nt h e s es y s t e m s ,t h e r s t1 6

DMA .O nt h e s es y s t e m s ,t h e r s t1 6 MBof physical memory comprise ZONE DMA.O no t h e rs y s t e m s ,c e r t a i nd e v i c e s can only access the rst 4 GBof physical memory, despite supporting 64 bit addresses. On such systems, the rst 4 GBof physical memory comprise ZONE DMA32 .ZONE HIGHMEM (for high memory ) refers to physical memory that is not mapped into the kernel address space. For example, on the 32bit Intel architecture (where 232provides a 4 GBaddress space), the kernel

232provides a 4 GBaddress space), the kernel is mapped into the rst 896 MBof the address space; the remaining memory is referred to as high memory and is allocated from ZONE HIGHMEM .F i n a l l y , ZONE NORMAL comprises everything elsethe normal, regularly mapped pages. Whether an architecture has a given zone depends on its constraints. A modern, 64bit architecture such as Intel x8664 has a small 16 MBZONE DMA(for legacy devices) and all the rest of its memory in ZONE NORMAL ,w i t hn o high

rest of its memory in ZONE NORMAL ,w i t hn o high memory . The relationship of zones and physical addresses on the Intel x8632 architecture is shown in Figure 18.3. The kernel maintains a list of free pages for each zone. When a request for physical memory arrives, the kernel satises the request using the appropriate zone. The primary physicalmemory manager in the Linux kernel is the page allocator .E a c hz o n eh a si t so w na l l o c a t o r ,w h i c hi sr e s p o n s i b l ef o ra l l o c

r ,w h i c hi sr e s p o n s i b l ef o ra l l o c a t i n g and freeing all physical pages for the zone and is capable of allocating ranges of physically contiguous pages on request. The allocator uses a buddy system (Section 9.8.1) to keep track of available physical pages. In this scheme, adjacent units of allocatable memory ar e paired together (hence its name). Each allocatable memory region has an adjacen tp a r t n e r( o rb u d d y ) .W h e n e v e rt w o allocated partner regions are

h e n e v e rt w o allocated partner regions are freed up, they are combined to form a larger regiona buddy heap . That larger region also has a partner, with which it can combine to form a still larger free region. Conversely, if a small memory requestzone physical memory  16 MB 16 .. 896 MB  896 MBZONEDMA ZONENORMAL ZONEHIGHMEMFigure 18.3 Relationship of zones and physical addresses in Intel x8632.802 Chapter 18 The Linux System cannot be satised by allocation of an existing small free region,

by allocation of an existing small free region, then a larger free region will be subdivided into two partners to satisfy the request. Separate linked lists are used to record the free memory regions of each allowable size. Under Linux, the smallest size allocatable under this mechanism is a single physical page. Figure 18.4 shows an example of buddyheap allocation. A 4 KB region is being allocated, but the smallest available region is 16 KB. The region is broken up recursively until a piece of

region is broken up recursively until a piece of the desired size is available. Ultimately, all memory allocations in the Linux kernel are made either statically, by drivers that reserve a contiguous area of memory during system boot time, or dynamically, by the page alloc ator. However, kernel functions do not have to use the basic allocator to reserve memory. Several specialized memorymanagement subsystems use the underlying page allocator to man age their own pools of memory. The most

to man age their own pools of memory. The most important are the virtual memory system, described in Section 18.6.2; the kmalloc() variablelength allocator; the slab allocator, used for allocating memory for kernel data structures; and the page cache, used for caching pages belonging to les. Many components of the Linux operating system need to allocate entire pages on request, but often smaller blocks of memory are required. The kernel provides an additional allocator for arbitrarysized

an additional allocator for arbitrarysized requests, where the size of ar e q u e s ti sn o tk n o w ni na d v a n c ea n dm a yb eo n l yaf e wb y t e s .A n a l o g o u s to the C languages malloc() function, this kmalloc() service allocates entire physical pages on demand but then splits them into smaller pieces. The kernel maintains lists of pages in use by the kmalloc() service. Allocating memory involves determining the appropriate list and either taking the rst free piece available on the

either taking the rst free piece available on the list or allocating a new page and splitting it up. Memory regions claimed by the kmalloc() system are allocated permanently until they are freed explicitly with a corresponding call to kfree() ;t h e kmalloc() system cannot reallocate or reclaim these regions in response to memory shortages. Another strategy adopted by Linux for allocating kernel memory is known as slab allocation. A slab is used for allocating memory for kernel data structures

for allocating memory for kernel data structures and is made up of one or more physically contiguous pages. A cache consists of one or more slabs. There is a single cache for each unique kernel data structurefor example, a cache for the data structure representing process descriptors, a cache for le objects, a cache for inodes, and so forth. 16KB8KB 8KB8KB 4KB 4KB Figure 18.4 Splitting of memory in the buddy system.18.6 Memory Management 803 3KB objects 7KB objectskernel objects caches slabs

3KB objects 7KB objectskernel objects caches slabs physically contiguous pages Figure 18.5 Slab allocator in Linux. Each cache is populated with objects that are instantiations of the kernel data structure the cache represents. For example, the cache representing inodes stores instances of inode structures, and the cache representing process descriptors stores instances of process descriptor structures. The relationship among slabs, caches, and objects is shown in Figure 18.5. The gure shows two

is shown in Figure 18.5. The gure shows two kernel objects 3 KBin size and three objects 7 KBin size. These objects are stored in the respective caches for 3 KBand 7 KBobjects. The slaballocation algorithm uses caches to store kernel objects. When a cache is created, a number of objects are allocated to the cache. The number of objects in the cache depends on th es i z eo ft h ea s s o c i a t e ds l a b .F o re x a m p l e , a1 2  KBslab (made up of three contiguous 4 KBpages) could store six 2

of three contiguous 4 KBpages) could store six 2 KB objects. Initially, all the objects in the cache are marked as free .W h e nan e w object for a kernel data structure is needed, the allocator can assign any free object from the cache to satisfy the request. The object assigned from the cache is marked as used . Lets consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor. In Linux systems, ap r o c e s sd e s c r i p t o

In Linux systems, ap r o c e s sd e s c r i p t o ri so ft h et y p e struct task struct ,w h i c hr e q u i r e s approximately 1.7 KBof memory. When the Linux kernel creates a new task, it requests the necessary memory for the struct task struct object from its cache. The cache will fulll the request using a struct task struct object that has already been allocated in a slab and is marked as free. In Linux, a slab may be in one of three possible states: 1.Full.A l lo b j e c t si nt h es l a

states: 1.Full.A l lo b j e c t si nt h es l a ba r em a r k e da su s e d . 2.Empty .A l lo b j e c t si nt h es l a ba r em a r k e da sf r e e . 3.Partial .T h es l a bc o n s i s t so fb o t hu s e da n df r e eo b j e c t s . The slab allocator rst attempts to satisfy the request with a free object in a partial slab. If none exist, a free object is assigned from an empty slab. If no empty slabs are available, a new slab is allocated from contiguous physical804 Chapter 18 The Linux System

contiguous physical804 Chapter 18 The Linux System pages and assigned to a cache; memory for the object is allocated from this slab. Two other main subsystems in Linux do their own management of physical pages: the page cache and the virtual memory system. These systems are closely related to each other. The page cache is the kernels main cache for les and is the main mechanism through which IOto block devices (Section 18.8.1) is performed. File systems of all types, including the native Linux

systems of all types, including the native Linux diskbased le systems and the NFS networked le system, perform their IO through the page cache. The page cache stores entire pages of le contents and is not limited to block devices. It can also cache networked data. The virtual memory system manages the contents of each processs virtual address space. These two systems interact closely with each other because reading a page of data into the page cache requires mapping pages in the page cache using

requires mapping pages in the page cache using the virtual memory system. In the following section, we look at the virtual memory system in greater detail. 18.6.2 Virtual Memory The Linux virtual memory system is responsible for maintaining the address space accessible to each process. It creates pages of virtual memory on demand and manages loading those pages from disk and swapping them back out to disk as required. Under Linux, the virtual memory manager maintains two separate views of a

memory manager maintains two separate views of a processs address space: as a set of separate regions and as as e to fp a g e s . The rst view of an address space is the logical view, describing instructions that the virtual memory system has received concerning the layout of the address space. In this view, the address space consists of a set of nonoverlapping regions, each region representing a continuous, pagealigned subset of the address space. Each region is described internally by a single

Each region is described internally by a single vm area struct structure that denes the properties of the region, including the processs read, write, and execute permissions in the region as well as information about any les associated with the region. The regions for each address space are linked into a balanced binary tree to allow fast lookup of the region corresponding to any virtual address. The kernel also maintains a second, physical view of each address space. This view is stored in the

of each address space. This view is stored in the hardware page tables for the process. The page table entries identify the exact current location of each page of virtual memory, whether it is on disk or in physical memory. The physical view is managed by a set of routines, which are invoked from t he kernels softwareinterrupt handlers whenever a process tries to access a page that is not currently present in the page tables. Each vm area struct in the addressspace description contains a eld

in the addressspace description contains a eld pointing to a table of functions that i mplement the key pagemanagement functionality for any given virtual memory region. All requests to read or write an unavailable page are eventually dispatched to the appropriate handler in the function table for the vm area struct ,s ot h a tt h ec e n t r a lm e m o r y  management routines do not have to know the details of managing each possible type of memory region.18.6 Memory Management 805 18.6.2.1

memory region.18.6 Memory Management 805 18.6.2.1 Virtual Memory Regions Linux implements several types of virtual memory regions. One property that characterizes virtual memory is the backing store for the region, which describes where the pages for the region come from. Most memory regions are backed either by a le or by nothing. A region backed by nothing is the simplest type of virtual memory region. Such a region represents demandzero memory :w h e nap r o c e s st r i e st or e a dap a g

:w h e nap r o c e s st r i e st or e a dap a g ei ns u c har e g i o n ,i ti ss i m p l yg i v e n back a page of memory lled with zeros. A region backed by a le acts as a viewport onto a section of that le. Whenever the process tries to access a page within that region, the page table is lled with the address of a page within the kernels page cache corresponding to the appropriate offset in the le. The same page of physical memory is used by both the page cache and the processs page tables, so

the page cache and the processs page tables, so any changes made to the le by the le system are immediately visible to any processes that have mapped that le into their address space. Any number of processes can map the same region of the same le, and they will all end up using the same page of physical memory for the purpose. Av i r t u a lm e m o r yr e g i o ni sa l s od e  n e db yi t sr e a c t i o nt ow r i t e s .T h e mapping of a region into the processs address space can be either

into the processs address space can be either private or shared .I fap r o c e s sw r i t e st oap r i v a t e l ym a p p e dr e g i o n ,t h e nt h ep a g e rd e t e c t s that a copyonwrite is necessary to keep the changes local to the process. In contrast, writes to a shared region result in updating of the object mapped into that region, so that the change will be visible immediately to any other process that is mapping that object. 18.6.2.2 Lifetime of a Virtual Address Space The kernel

Lifetime of a Virtual Address Space The kernel creates a new virtual address space in two situations: when a process runs a new program with the exec() system call and when a new process is created by the fork() system call. The rst case is easy. When a new program is executed, the process is given a new, completely empty virtual address space. It is up to the routines for loading the program to populate the address space with virtual memory regions. The second case, creating a new process with

The second case, creating a new process with fork() ,i n v o l v e sc r e a t i n g ac o m p l e t ec o p yo ft h ee x i s t i n gp r o c esss virtual address space. The kernel copies the parent processs vm area struct descriptors, then creates a new set of page tables for the child. The parents page tables are copied directly into the childs, and the reference count o fe a c hp a g ec o v e r e di si n c r e m e n t e d .T h u s , after the fork, the parent and child share the same physical

fork, the parent and child share the same physical pages of memory in their address spaces. As p e c i a lc a s eo c c u r sw h e nt h ec o p y i n go p e r a t i o nr e a c h e sav i r t u a lm e m o r y region that is mapped privately. Any pages to which the parent process has written within such a region are private, and subsequent changes to these pages by either the parent or the child must no t update the page in the other processs address space. When the pagetable entries for such regions

space. When the pagetable entries for such regions are copied, they are set to be read only and are marked for copyonwrite. As long as neither process modies these pages, the two processes share the same page of physical memory. However, if either process tries to modify a copyonwrite page, the reference count on the page is checked. If the page is still shared, then the806 Chapter 18 The Linux System process copies the pages contents to a brandnew page of physical memory and uses its copy

brandnew page of physical memory and uses its copy instead. This mechanism ensures that private data pages are shared between processes whenever possib le and copies are made only when absolutely necessary. 18.6.2.3 Swapping and Paging An important task for a virtual memory system is to relocate pages of memory from physical memory out to disk when that memory is needed. Early UNIX systems performed this relocation by swapping out the contents of entire processes at once, but modern versions of

entire processes at once, but modern versions of UNIX rely more on pagingthe movement of individual pages of virtual memory between physical memory and disk. Linux does not implement wholeprocess swapping; it uses the newer paging mechanism exclusively. The paging system can be divided into two sections. First, the policy algorithm decides which pages to write out to disk and when to write them. Second, the paging mechanism carries out the transfer and pages data back into physical memory when

and pages data back into physical memory when they are needed again. Linuxs pageout policy uses a modied version of the standard clock (or secondchance) algorithm described in Section 9.4.5.2. Under Linux, a multiple pass clock is used, and every page has an agethat is adjusted on each pass of the clock. The age is more precisely a measure of the pages youthfulness, or how much activity the page has seen recently. Frequently accessed pages will attain a higher age value, but the age of

will attain a higher age value, but the age of infrequently accessed pages will drop toward zero with each pass. This age valuing allows the pager to select pages to page out based on a least frequently used ( LFU)p o l i c y . The paging mechanism supports pagin gb o t ht od e d i c a t e ds w a pd e v i c e s and partitions and to normal les, although swapping to a le is signicantly slower due to the extra overhead incurred by the le system. Blocks are allocated from the swap devices according

are allocated from the swap devices according to a bitmap of used blocks, which is maintained in physical memory at all times. The allocator uses a nextt algorithm to try to write out pages to continuous runs of disk blocks for improved performance. The allocator records the fact that a page has been paged out to disk by using a feature of the page tables on modern processors: the pagetable entrys pagenotpresent bit is set, allowing the rest of the page table entry to be lled with an index

of the page table entry to be lled with an index identifying where the page has been written. 18.6.2.4 Kernel Virtual Memory Linux reserves for its own internal use a constant, architecturedependent region of the virtual address space of every process. The pagetable entries that map to these kernel pages are marked as protected, so that the pages are not visible or modiable when the processor is running in user mode. This kernel virtual memory area contains two regions. The rst is a static area

contains two regions. The rst is a static area that contains pagetable references to every available physical page of memory in the system, so that a simple translation from physical to virtual addresses occurs when kernel code is run. The core of the kernel, along with all pages allocated by the normal page allocator, resides in this region.18.6 Memory Management 807 The remainder of the kernels reserved section of address space is not reserved for any specic purpose. Pagetab le entries in this

for any specic purpose. Pagetab le entries in this address range can be modied by the kernel to point to any other areas of memory. The kernel provides a pair of facilities that allow kernel code to use this virtual memory. The vmalloc() function allocates an arbitrary number of physical pages of memory that may not be physically contiguous into a single region of virtually contiguous kernel memory. The vremap() function maps a sequence of virtual addresses to point to an area of memory used by

addresses to point to an area of memory used by a device driver for memorymapped IO. 18.6.3 Execution and Loading of User Programs The Linux kernels execution of user programs is triggered by a call to theexec() system call. This exec() call commands the kernel to run a new program within the current process, completely overwriting the current execution context with the initial context of the new program. The rst job of this system service is to verify that the calling process has permission

to verify that the calling process has permission rights to the le being executed. Once that matter has been checked, the kernel invokes al o a d e rr o u t i n et os t a r tr u n n i n gt h ep r o g r a m .T h el o a d e rd o e sn o tn e c e s s a r i l y load the contents of the program le into physical memory, but it does at least set up the mapping of the program into virtual memory. There is no single routine in Linux for loading a new program. Instead, Linux maintains a table of possible

Instead, Linux maintains a table of possible loader functions, and it gives each such function the opportunity to try loading the given le when an exec() system call is made. The initial reason for this loader table was that, between the releases of the 1.0 and 1.2 kernels, the standard format for Linuxs binary les was changed. Older Linux kernels understood the a.out format for binary lesa relatively simple format common on older UNIX systems. Newer Linux systems use the more modern ELF format,

Linux systems use the more modern ELF format, now supported by most current UNIX implementations. ELFhas a number of advantages over a.out , including exibility and extendability. New sections can be added to an ELF binary (for example, to add extra debugging information) without causing the loader routines to become confused. By allowing registration of multiple loader routines, Linux can easily support the ELFanda.out binary formats in as i n g l er u n n i n gs y s t e m . In Section 18.6.3.1

l er u n n i n gs y s t e m . In Section 18.6.3.1 and Section 18.6.3.2, we concentrate exclusively on the loading and running of ELFformat binaries. The procedure for loading a.out binaries is simpler but similar in operation. 18.6.3.1 Mapping of Programs into Memory Under Linux, the binary loader does not load a binary le into physical memory. Rather, the pages of the binary le are mapped into regions of virtual memory. Only when the program tries to access a given page will a page fault result

to access a given page will a page fault result in the loading of that page into physical memory using demand paging. It is the responsibility of the kernels binary loader to set up the initial memory mapping. An ELFformat binary le consists of a header followed by several pagealigned sections. The ELFloader works by reading the header and mapping the sections of the le into separate regions of virtual memory. Figure 18.6 shows the typical layout of memory regions set up by the ELF loader. In a

of memory regions set up by the ELF loader. In a reserved region at one end of the address space sits the kernel, in808 Chapter 18 The Linux Systemkernel virtual memory memory invisible to usermode code stack memorymapped region memorymapped region memorymapped region runtime data uninitialized data initialized data program textthe brk pointer forbidden region Figure 18.6 Memory layout for ELF programs. its own privileged region of virtual memory inaccessible to normal usermode programs. The

inaccessible to normal usermode programs. The rest of virtual memory is available to applications, which can use the kernels memorymapping functions to create regions that map a portion of a le or that are available for application data. The loaders job is to set up the initial memory mapping to allow the execution of the program to start. The regions that need to be initialized include the stack and the programs text and data regions. The stack is created at the top of the usermode virtual

is created at the top of the usermode virtual memory; it grows downward toward lowernumbered addresses. It includes copies of the arguments and environment v ariables given to the program in the exec() system call. The other regions are created near the bottom end of virtual memory. The sections of the binary le that contain program text or readonly data are mapped into memory as a writeprotected region. Writable initialized data are mapped next; then any uninitialized data are mapped in as a

then any uninitialized data are mapped in as a private demandzero region. Directly beyond these xedsized regions is a variablesized region that programs can expand as needed to hold data allocated at run time. Each process has a pointer, brk,t h a tp o i n t st ot h ec u r r e n te x t e n to ft h i sd a t ar e g i o n , and processes can extend or contract their brkregion with a single system call sbrk() . Once these mappings have been set up, the loader initializes the processs programcounter

the loader initializes the processs programcounter register with the starting point recorded in the ELFheader, and the process can be scheduled. 18.6.3.2 Static and Dynamic Linking Once the program has been loaded and has started running, all the necessary contents of the binary le have b een loaded into the processs virtual address18.7 File Systems 809 space. However, most programs also need to run functions from the system libraries, and these library functions must also be loaded. In the

library functions must also be loaded. In the simplest case, the necessary library functions are embedded directly in the programs executable binary le. Such a program is statically linked to its libraries, and statically linked executables can commence running as soon as they are loaded. The main disadvantage of static linking is that every program generated must contain copies of exactly the same common system library functions. It is much more efcient, in terms of both physical memory and

more efcient, in terms of both physical memory and diskspace usage, to load the system libraries into memory only once. Dynamic linking allows that to happen. Linux implements dynamic linking in user mode through a special linker library. Every dynamically linked program contains a small, statically linked function that is called when the program starts. This static function just maps the link library into memory and runs the code that the function contains. The link library determines the

function contains. The link library determines the dynamic libraries required by the program and the names of the variables and functions needed from those libraries by reading the information contained in sections of the ELFbinary. It then maps the libraries into the middle of virtual memory and resolves the references to the symbols contained in those libraries. It does not matter exactly where in memory these shared libraries are mapped: they are compiled into positionindependent code (PIC),w

are compiled into positionindependent code (PIC),w h i c hc a nr u na ta n ya d d r e s si nm e m o r y . 18.7 File Systems Linux retains UNIX s standard lesystem model. In UNIX , a le does not have to be an object stored on disk or fetched over a network from a remote le server. Rather, UNIX les can be anything capable of handling the input or output of a stream of data. Device drivers can appear as les, and interprocess communication channels or network connections also look like les to the

or network connections also look like les to the user. The Linux kernel handles all these type so f l e sb yh i d i n gt h ei m p l e m e n  tation details of any single le type behind a layer of software, the virtual le system ( VFS). Here, we rst cover the virtual le system and then discuss the standard Linux le systemext3. 18.7.1 The Virtual File System The Linux VFS is designed around objectoriented principles. It has two components: a set of denitions that specify what lesystem objects are

denitions that specify what lesystem objects are allowed to look like and a layer of software to manipulate the objects. The VFSdenes four main object types: Aninode object represents an individual le. Ale object represents an open le. Asuperblock object represents an entire le system. Adentry object represents an individual directory entry.810 Chapter 18 The Linux System For each of these four object types, the VFS denes a set of operations. Every object of one of these types contain sap o i n

object of one of these types contain sap o i n t e rt oaf u n c t i o nt a b l e .T h e function table lists the addresses of the actual functions that implement the dened operations for that object. For example, an abbreviated APIfor some of the le objects operations includes: int open(. . .) O p e na l e . ssize tr e a d ( ... ) R e a df r o ma l e . ssize tw r i t e ( ... ) W r i t et oa l e . int mmap(. . .) M e m o r y  m a pa l e . The complete denition of the le object is specied in the

denition of the le object is specied in the struct file operations ,w h i c hi sl o c a t e di nt h e l e usrincludelinuxfs.h . An implementation of the le object (for a specic le type) is required to implement each function specied in the denition of the le object. The VFSsoftware layer can perform an operation on one of the lesystem objects by calling the appropriate function from the objects function table, without having to know in advance exactly what kind of object it is dealing with. The

what kind of object it is dealing with. The VFSdoes not know, or care, whether an inode represents a networked le, a disk le, a network socket, or a directory le. The appropriate function for that les read() operation will always be at the same place in its function table, and the VFSsoftware layer will call that function without caring how the data are actually read. The inode and le objects are the me chanisms used to access les. An inode object is a data structure containing pointers to the

is a data structure containing pointers to the disk blocks that contain the actual le contents, and a le object represents a point of access to the data in an open le. A process cannot access an inodes contents without rst obtaining a le object pointing to the inode. The le object keeps track of where in the le the process is currently reading or writing, to keep track of sequential le IO. It also remembers the permissions (for example, read or write) requested when the le was opened and tracks

write) requested when the le was opened and tracks the processs activity if necessary to perform adaptive readahead, fetching le data into memory before the process requests the data, to improve performance. File objects typically belong to a single process, but inode objects do not. There is one le object for every instance of an open le, but always only a single inode object. Even when a le is no longer in use by any process, its inode object may still be cached by the VFSto improve

object may still be cached by the VFSto improve performance if the le is used again in the near future. All cached le data are linked onto a list in the les inode object. The inode also maintains standard information about each le, such as the owner, size, and time most recently modied. Directory les are dealt with slightly differently from other les. The UNIX programming interface denes a number of operations on directories, such as creating, deleting, and renaming a le in a directory. The

deleting, and renaming a le in a directory. The system calls for these directory operations do not require that the user open the les concerned, unlike the case for reading or writing data. The VFS therefore denes these directory operations in the inode object, rather than in the le object. The superblock object represents a c onnected set of les that form a selfcontained le system. The operatingsystem kernel maintains a single18.7 File Systems 811 superblock object for each disk device mounted

811 superblock object for each disk device mounted as a le system and for each networked le system currently connected. The main responsibility of the superblock object is to provide access to inodes. The VFS identies every inode by a unique lesysteminode number pair, and it nds the inode corresponding to a particular inode number by asking the superblock object to return the inode with that number. Finally, a dentry object represents a directory entry, which may include the name of a directory

entry, which may include the name of a directory in the path name of a le (such as usr )o rt h ea c t u a l l e (such as stdio.h ). For example, the le usrincludestdio.h contains the directory entries (1) ,( 2 ) usr, (3) include ,a n d( 4 ) stdio.h . Each of these values is represented by a separate dentry object. As an example of how dentry objects are used, consider the situ ation in which a process wishes to open the le with the pathname usrincludestdio.h using an editor. Because Linux treats

using an editor. Because Linux treats directory names as les, translating this path requires rst obtaining the inode for the root .T h eo p e r a t i n gs y s t e mm u s tt h e nr e a dt h r o u g ht h i s l et oo b t a i nt h ei n o d e for the le include .I tm u s tc o n t i n u et h i sp r o c e s su n t i li to b t a i n st h ei n o d ef o r the le stdio.h .B e c a u s ep a t h  n a m et r a n s l a t i o nc a nb eat i m e  c o n s u m i n gt a s k , Linux maintains a cache of dentry

m i n gt a s k , Linux maintains a cache of dentry objects, which is consulted during pathname translation. Obtaining the inode from the dentry cache is considerably faster than having to read the ondisk le. 18.7.2 The Linux ext3 File System The standard ondisk le system used by Linux is called ext3,f o rh i s t o r i c a l reasons. Linux was originally programmed with a Minixcompatible le system, to ease exchanging data with the Minix development system, but that le system was severely

system, but that le system was severely restricted by 14character lename limits and a maximum lesystem size of 64 MB.T h eM i n i x l es y s t e mw a ss u p e r s e d e db y a new le system, which was christened the extended le system (extfs ).A later redesign to improve performance and scalability and to add a few missing features led to the second extended le system (ext2).F u r t h e rd e v e l o p m e n t added journaling capabilities, and the system was renamed the third extended le system

system was renamed the third extended le system (ext3).L i n u xk e r n e ld e v e l o p e r sa r ew o r k i n go na u g m e n t i n ge x t 3 with modern lesystem features such as extents. This new le system is called thefourth extended le system (ext4).T h er e s to ft h i ss e c t i o nd i s c u s s e se x t 3 , however, since it remains the mostdeployed Linux le system. Most of the discussion applies equally to ext4. Linuxs ext3 has much in common with the BSD Fast File System ( FFS) (Section

with the BSD Fast File System ( FFS) (Section A.7.7). It uses a similar mechanism for locating the data blocks belonging to a specic le, storing datablock pointers in indirect blocks throughout the le system with up to three levels of indirection. As in FFS, directory les are stored on disk just like normal les, although their contents are interpreted differently. Each block i nad i r e c t o r y l ec o n s i s t so fal i n k e dl i s t of entries. In turn, each entry contains the length of the

In turn, each entry contains the length of the entry, the name of a le, and the inode number of the inode to which that entry refers. The main differences between ext3 and FFSlie in their diskallocation policies. In FFS,t h ed i s ki sa l l o c a t e dt o l e si nb l o c k so f8 KB.T h e s eb l o c k s are subdivided into fragments of 1 KBfor storage of small les or partially lled blocks at the ends of les. In contrast, ext3 does not use fragments at all812 Chapter 18 The Linux System but

at all812 Chapter 18 The Linux System but performs all its allocations in smaller units. The default block size on ext3 varies as a function of the total size of the le system. Supported block sizes are 1, 2, 4, and 8 KB. To maintain high performance, the operating system must try to perform IO operations in large chunks whenever possible by clustering physically adjacent IOrequests. Clustering reduces the perrequest overhead incurred by device drivers, disks, and diskcontroller hardware. A

drivers, disks, and diskcontroller hardware. A blocksized IOrequest size is too small to maintain good performance, so ext3 uses allocation policies designed to place logically adjacent blocks of a le into physically adjacent blocks on disk, so that it can submit an IOrequest for several disk blocks as a single operation. The ext3 allocation policy works as follows: As in FFS,a ne x t 3 l es y s t e mi s partitioned into multiple segments. In ext3, these are called block groups .FFS uses the

ext3, these are called block groups .FFS uses the similar concept of cylinder groups ,w h e r ee a c hg r o u pc o r r e s p o n d st o as i n g l ec y l i n d e ro fap h y s i c a ld i s k .( N o t et h a tm o d e r nd i s k  d r i v et e c h n o l o g y packs sectors onto the disk at different densities, and thus with different cylinder sizes, depending on how far the disk head is from the center of the disk. Therefore, xedsized cylinder groups do not necessarily correspond to the disks

groups do not necessarily correspond to the disks geometry.) When allocating a le, ext3 must rst select the block group for that le. For data blocks, it attempts to allocate the le to the block group to which the les inode has been allocated. For inode allocations, it selects the block group in which the les parent directory re sides for nondirectory les. Directory les are not kept together but rather are dispersed throughout the available block groups. These policies are designed not only to

groups. These policies are designed not only to keep related information within the same block group but also to spread out the disk load among the disks block groups to reduce the fragmentation of any one area of the disk. Within a block group, ext3 tries to keep allocations physically contiguous if possible, reducing fragmentation if it can. It maintains a bitmap of all free blocks in a block group. When allocating the rst blocks for a new le, it starts searching for a free block from the

le, it starts searching for a free block from the beginning of the block group. When extending a le, it continues the search from the block most recently allocated to the le. The search is performed in two stages. First, ext3 searches for an entire free byte in the bitmap; if it fails to nd one, it looks for any free bit. The search for free bytes aims to allocate disk space in chunks of at least eight blocks where possible. Once a free block has been identied, the search is extended backward

has been identied, the search is extended backward until an allocated block is encountered. When a free byte is found in the bitmap, this backward extension prevents ext3 from leaving a hole between the most recently allocated block in the previous nonzero byte and the zero byte found. Once the next block to be allocated has been found by either bit or byte search, ext3 extends the allocation forward for up to eight blocks and preallocates these extra blocks to the le. This preallocation helps

extra blocks to the le. This preallocation helps to reduce fragmentation during interleaved writes to separate  les and also reduces the CPU cost of disk allocation by allocating multiple blocks simultaneously. The preallocated blocks are returned to the freespace bitmap when the le is closed. Figure 18.7 illustrates the allocation policies. Each row represents a sequence of set and unset bits in an allocation bitmap, indicating used and free blocks on disk. In the rst case, if we can nd any

blocks on disk. In the rst case, if we can nd any free blocks sufciently near the start of the search, then we allocate them no matter how fragmented18.7 File Systems 813allocating scattered free blocks allocating continuous free blocks block in use bit boundaryblock selected by allocator free block byte boundary bitmap search Figure 18.7 ext3 blockallocation policies. they may be. The fragmentation is partially compensated for by the fact that the blocks are close together and can probably all

the blocks are close together and can probably all be read without any disk seeks. Furthermore, allocating them all to one le is better in the long run than allocating isolated blocks to separate les once large free areas become scarce on disk. In the second case, we have not immediately found a free block close by, so we search forward for an entire free byte in the bitmap. If we allocated that byte as a whole, we would end up creating a fragmented area of free space between it and the

a fragmented area of free space between it and the allocation preceding it. Thus, before allocating, we back up to make this allocation ush with the allocation preceding it, and then we allocate forward to satisfy the default allocation of eight blocks. 18.7.3 Journaling The ext3 le system supports a popular feature called journaling ,w h e r e b y modications to the le system are written sequentially to a journal. A set of operations that performs a specic task is a transaction .O n c eat r a n

a specic task is a transaction .O n c eat r a n s a c t i o n is written to the journal, it is considered to be committed. Meanwhile, the journal entries relating to the transaction are replayed across the actual le system structures. As the changes are made, a pointer is updated to indicate which actions have completed and which are still incomplete. When an entire committed transaction is completed, it is removed from the journal. The journal, which is actually a circular buffer, may be in a

which is actually a circular buffer, may be in a separate section of the le system, or it may even be on a separate disk spindle. It is more efcient, but more complex, to have it under separate r eadwrite heads, thereby decreasing head contention and seek times. If the system crashes, some transactions may remain in the journal. Those transactions were never completed to the le system even though they were committed by the operating system, so they must be completed once the system814 Chapter 18

must be completed once the system814 Chapter 18 The Linux System recovers. The transactions can be executed from the pointer until the work is complete, and the lesystem structures remain consistent. The only problem occurs when a transaction has been abortedthat is, it was not committed before the system crashed. Any changes from those transactions that were applied to the le system must be undone, again preserving the consistency of the le system. This recovery is all that is needed after a

This recovery is all that is needed after a crash, eliminating all problems with consistency checking. Journaling le systems may perform some operations faster than non journaling systems, as updates proceed much faster when they are applied to the inmemory journal rather than directly to the ondisk data structures. The reason for this improvement is found in the performance advantage of sequential IOover random IO.C o s t l ys y n c h r o n o u sr a n d o mw r i t e st ot h e l e system are

o u sr a n d o mw r i t e st ot h e l e system are turned into much less costly synchronous sequential writes to the le systems journal. Those changes, in turn, are replayed asynchronously via random writes to the appropriate structures. The overall result is a signicant gain in performance of lesystem metadataoriented operations, such as le creation and deletion. Due to this performance improvement, ext3 can be congured to journal only metadata and not le data. 18.7.4 The Linux Process File

and not le data. 18.7.4 The Linux Process File System The exibility of the Linux VFSenables us to implement a le system that does not store data persistently at all but rather provides an interface to some other functionality. The Linux process le system ,k n o w na st h e proc le system, is an example of a le system whose contents are not actually stored anywhere but are computed on demand according to user le IOrequests. Aproc le system is not unique to Linux. SVR4 UNIX introduced a proc le

unique to Linux. SVR4 UNIX introduced a proc le system as an efcient interface to the kernels process debugging support. Each subdirectory of the le system corresponded not to a directory on any disk but rather to an active process on the current system. A listing of the le system reveals one directory per process, with the directory name being the ASCII decimal representation of the processs unique process identier ( PID). Linux implements such a proc le system but extends it greatly by adding

a proc le system but extends it greatly by adding a number of extra directories and text les under the le systems root directory. These new entries correspond to various statistics about the kernel and the associated loaded drivers. The proc le system provides a way for programs to access this information as plain text les; the standard UNIX user environment provides powerful tools to process such les. For example, in the past, the traditional UNIX pscommand for listing the states of all running

pscommand for listing the states of all running processes has been implemented as a privileged process that reads the process state directly from the kernels virtual memory. Under Linux, this command is implemented as an entirely unprivileged program that simply parses and formats the information from proc . The proc le system must implement two things: a directory structure and the le contents within. Because a UNIX le system is dened as a set of le and directory inodes identied by their inode

of le and directory inodes identied by their inode numbers, the proc le system must dene a unique and persistent inode n umber for each directory and the associated les. Once such a mapping exists, the le system can use this inode number to identify just what operation is required when a user tries to read from a particular le inode or to perform a lookup in a particular directory18.8 Input and Output 815 inode. When data are read from one of these les, the proc le system will collect the

of these les, the proc le system will collect the appropriate information, format it into textual form, and place it into the requesting processs read buffer. The mapping from inode number to information type splits the inode number into two elds. In Linux, a PIDis 16 bits in size, but an inode number is 32 bits. The top 16 bits of the inode number are interpreted as a PID,a n dt h e remaining bits dene what type of information is being requested about that process. APID of zero is not valid, so

about that process. APID of zero is not valid, so a zero PID eld in the inode number is taken to mean that this inode contains globalrather than processspecic information. Separate global les exist in proc to report information such as the kernel version, free memory, performan ce statistics, and drivers currently running. Not all the inode numbers in this ran ge are reserved. The kernel can allocate new proc inode mappings dynamically, maintaining a bitmap of allocated inode numbers. It also

a bitmap of allocated inode numbers. It also maintains a tree data structure of registered global proc lesystem entries. Each entry contains the les inode number, le name, and access permissions, along with the special functions used to generate the les contents. Drivers can register and der egister entries in this tree at any time, and a special section of the treeappearing under the procsys directory is reserved for kernel variables. Files under this tree are managed by a set of common

under this tree are managed by a set of common handlers that allow both r eading and writing of these variables, so a system administrator can tune the value of kernel parameters simply by writing out the new desired values in ASCII decimal to the appropriate le. To allow efcient access to these variables from within applications, the procsys subtree is made available through a special system call, sysctl() , that reads and writes the same variables in binary, rather than in text, without the

in binary, rather than in text, without the overhead of the le system. sysctl() is not an extra facility; it simply reads theproc dynamic entry tree to identify the variables to which the application is referring. 18.8 Input and Output To the user, the IOsystem in Linux looks much like that in any UNIX system. That is, to the extent possible, all d evice drivers appear as normal les. Users can open an access channel to a device in the same way they open any other ledevices can appear as objects

open any other ledevices can appear as objects within the le system. The system administrator can create special les within a le system that contain references to a specic device driver, and a user opening such a le will be able to read from and write to the device referenced. By using the normal leprotection system, which determines who can access which le, the administrator can set access permissions for each device. Linux splits all devices into three classes: block devices, character

into three classes: block devices, character devices, and network devices. Figure 18.8 illustrates the overall structure of the device driver system. Block devices include all devices that allow random access to completely independent, xedsized blocks of data, including hard disks and oppy disks, CDROM sa n dB l u  r a yd i s c s ,a n d a s hm e m o r y .B l o c kd e v i c e sa r et y p i c a l l y816 Chapter 18 The Linux System used to store le systems, but direct access to a block device is

le systems, but direct access to a block device is also allowed so that programs can create and repair the le system that the device contains. Applications can also access these block devices directly if they wish. For example, a database application may prefer to perform its own netuned layout of data onto a disk rather than using the generalpurpose le system. Character devices include most other devices, such as mice and keyboards. The fundamental difference between block and character devices

difference between block and character devices is random accessblock devices are accesse dr a n d o m l y ,w h i l ec h a r a c t e rd e v i c e sa r e accessed serially. For example, seeking to a certain position in a le might be supported for a DVD but makes no sense for a pointing device such as a mouse. Network devices are dealt with differently from block and character devices. Users cannot directly trans fer data to network devices. Instead, they must communicate indirectly by opening a

they must communicate indirectly by opening a connection to the kernels networking subsystem. We discuss the interface to network devices separately in Section 18.10. 18.8.1 Block Devices Block devices provide the main interface to all disk devices in a system. Performance is particularly important for disks, and the blockdevice system must provide functionality to ensure that disk access is as fast as possible. This functionality is achieved th rough the scheduling of IOoperations. In the

th rough the scheduling of IOoperations. In the context of block devices, a block represents the unit with which the kernel performs IO.W h e nab l o c ki sr e a di n t om e m o r y ,i ti ss t o r e di nab u f f e r . The request manager is the layer of software that manages the reading and writing of buffer contents to and from a blockdevice driver. A separate list of requests is kept for each blockdevice driver. Traditionally, these requests have been scheduled according to a

these requests have been scheduled according to a unidirectionalelevator (CSCAN )a l g o r i t h mt h a te x p l o i t st h eo r d e ri nw h i c hr e q u e s t sa r ei n s e r t e di n and removed from the lists. The request lists are maintained in sorted order of increasing startingsector number. Whe nar e q u e s ti sa c c e p t e df o rp r o c e s s i n g by a blockdevice driver, it is not removed from the list. It is removed only after the IOis complete, at which point the driver continues

IOis complete, at which point the driver continues with the next request in the list, even if new requests have been inserted in the list before the activefile systemblock device filecharacter device file protocol driverline disciplineTTY driverIO scheduler SCSI manager SCSI device driverblock device drivercharacter device drivernetwork socket network device driveruser application Figure 18.8 Devicedriver block structure.18.8 Input and Output 817 request. As new IOrequests are made, the request

request. As new IOrequests are made, the request manager attempts to merge requests in the lists. Linux kernel version 2.6 introduced a new IO scheduling algorithm. Although a simple elevator algorithm remains available, the default IO scheduler is now the Completely Fair Queueing (CFQ )scheduler. The CFQ IO scheduler is fundamentally different from elevatorbased algorithms. Instead of sorting requests into a list, CFQ maintains a set of listsby default, one for each process. Requests

of listsby default, one for each process. Requests originating from a process go in that processs list. For example, if two processes are issuing IO requests, CFQ will maintain two separate lists of requests, one for each process. The lists are maintained according to the CSCAN algorithm. CFQ services the lists differently as well. Where a traditional CSCAN algorithm is indifferent to a specic process, CFQ services each processs list roundrobin. It pulls a congurable number of requests (by

It pulls a congurable number of requests (by default, four) from each list before moving on to the next. This method results in fairness at the process leveleach process receives an equal fraction of the disks bandwidth. The result is benecial with interactive workloads where IO latency is important. In practice, however, CFQ performs well with most workloads. 18.8.2 Character Devices Ac h a r a c t e r  d e v i c ed r i v e rc a nb ea l m o s t any device driver that does not offer random

o s t any device driver that does not offer random access to xed blocks of data. Any characterdevice drivers registered to the Linux kernel must also register a set of functions that implement the le IOoperations that the driver can handle. The kernel performs almost no preprocessing of a le read or write request to a character device. It simply passes the request to the device in question and lets the device deal with the request. The main exception to this rule is the special subset of

exception to this rule is the special subset of characterdevice drivers that implement terminal devices. The kernel maintains a standard interface to these drivers by means of a set of tty struct structures. Each of these structures provides buffering and ow control on the data stream from the terminal device and feeds th ose data to a line discipline. Aline discipline is an interpreter for the information from the terminal device. The most common line discipline is the ttydiscipline, which

common line discipline is the ttydiscipline, which glues the terminals data stream onto the standard input and output streams of a users running processes, allowing those processes to communicate directly with the users terminal. This job is complicated by the fact that several such processes may be running simultaneously, and the ttyline discipline is responsible for attaching and detaching the terminals input and output from the various processes connected to it as those processes are

processes connected to it as those processes are suspended or awakened by the user. Other line disciplines also are implemented that have nothing to do with IO to a user process. The PPP and SLIP networking protocols are ways of encoding a networking connection over a terminal device such as a serial line. These protocols are implemented un der Linux as drivers that at one end appear to the terminal system as line disciplines and at the other end appear to the networking system as networkdevice

appear to the networking system as networkdevice drivers. After one of these line disciplines has been enabled on a terminal device, any data appearing on that terminal will be routed directly to the appropriate networkdevice driver.818 Chapter 18 The Linux System 18.9 Interprocess Communication Linux provides a rich environment fo rp r o c e s s e st oc o m m u n i c a t ew i t he a c h other. Communication may be just a matter of letting another process know that some event has occurred, or it

process know that some event has occurred, or it may involve transferring data from one process to another. 18.9.1 Synchronization and Signals The standard Linux mechanism for informing a process that an event has occurred is the signal .S i g n a l sc a nb es e n tf r o ma n yp r o c e s st oa n yo t h e r process, with restrictions on signals sent to processes owned by another user. However, a limited number of signals are available, and they cannot carry information. Only the fact that a

cannot carry information. Only the fact that a signal has occurred is available to a process. Signals are not generated only by proc esses. The kernel also generates signals internally. For example, it can send a sign al to a server process when data arrive on a network channel, to a parent process when a child terminates, or to aw a i t i n gp r o c e s sw h e nat i m e re x p i r e s . Internally, the Linux kernel does not use signals to communicate with processes running in kernel mode. If a

with processes running in kernel mode. If a kernelmode process is expecting an event to occur, it will not use signals to receive notication of that event. Rather, communication about incoming a synchronous events within the kernel takes place through the use of scheduling states and wait queue structures. These mechanisms allow kernelmode p rocesses to inform one another about relevant events, and they also allow events to be generated by device drivers or by the networking system. Whenever a

drivers or by the networking system. Whenever a process wants to wait for some event to complete, it places itself on a wait queue associated with that event and tells the scheduler that it is no longer eligib le for execution. Once the event has completed, every process on the wait queue will be awoken. This procedure allows multiple processes to wait for a single event. For example, if several processes are trying to read a le from a disk, then they will all be awakened once the data have been

they will all be awakened once the data have been read into memory successfully. Although signals have always been the main mechanism for commu nicating asynchronous events among p rocesses, Linux also implements the semaphore mechanism of System V UNIX .Ap r o c e s sc a nw a i to nas e m a p h o r e as easily as it can wait for a signal, but semaphores have two advantages: large numbers of semaphores can be shared among multiple independent pro cesses, and operations on multiple semaphores can

cesses, and operations on multiple semaphores can be performed atomically. Internally, the standard Linux wait queue mechanism synchronizes processes that are communicating with semaphores. 18.9.2 Passing of Data among Processes Linux offers several mechanisms for passing data among processes. The stan dard UNIX pipe mechanism allows a child process to inherit a communication channel from its parent; data written to one end of the pipe can be read at the other. Under Linux, pipes appear as just

at the other. Under Linux, pipes appear as just another type of inode to virtual le system software, and each pipe has a pair of wait queues to synchronize the reader and writer. UNIX also denes a set of networking facilities that can send streams of data to both local and remote processes. Networking is covered in Section 18.10.18.10 Network Structure 819 Another process communications method, shared memory, offers an extremely fast way to communicate large or small amounts of data. Any data

large or small amounts of data. Any data written by one process to a shared memory region can be read immediately by any other process that has mapped that region into its address space. The main disadvantage of shared memory is that, on its own, it offers no synchronization. Ap r o c e s sc a nn e i t h e ra s kt h eo p e r a t i n gs y s t e mw h e t h e rap i e c eo fs h a r e d memory has been written to nor suspend execution until such a write occurs. Shared memory becomes particularly

a write occurs. Shared memory becomes particularly powerful when used in conjunction with another interprocesscommunication mechanism that provides the missing synchronization. As h a r e d  m e m o r yr e g i o ni nL i n u xi sap e r s i s t e n to b j e c tt h a tc a nb ec r e a t e d or deleted by processes. Such an object is treated as though it were a small, independent address space. The Linux paging algorithms can elect to page sharedmemory pages out to disk, just as they can page out a

pages out to disk, just as they can page out a processs data pages. The sharedmemory object acts as a backing store for sharedmemory regions, just as a le can act as a backing store for a memorymapped memory region. When a le is mapped into a virtual address space region, then any page faults that occur cause the appropriate page of the le to be mapped into virtual memory. Similarly, sharedmemory mappings direct page faults to map in pages from a persistent sharedmemory object. Also just as for

a persistent sharedmemory object. Also just as for les, shared memory objects remember their contents even if no processes are currently mapping them into virtual memory. 18.10 Network Structure Networking is a key area of functionality for Linux. Not only does Linux support the standard Internet protocols used for most UNIX toUNIX com munications, but it also implements a number of protocols native to other, non UNIX operating systems. In particular, since Linux was originally imple mented

since Linux was originally imple mented primarily on PCs, rather than on large workstations or on serverclass systems, it supports many of the protocols typically used on PCnetworks, such as AppleTalk and IPX. Internally, networking in the Linux kernel is implemented by three layers of software: 1.The socket interface 2.Protocol drivers 3.Networkdevice drivers User applications perform all networking requests through the socket interface. This interface is designed to look like the 4.3 BSD

interface is designed to look like the 4.3 BSD socket layer, so that any programs designed to make use of Berkeley sockets will run on Linux without any sourcecode changes. This inte rface is described in Section A.9.1. The BSD socket interface is sufciently general to represent network addresses for a wide range of networking protocols. This single interface is used in Linux to access not just those protocols implemented on standard BSDsystems but all the protocols supported by the system.820

but all the protocols supported by the system.820 Chapter 18 The Linux System The next layer of software is the protocol stack, which is similar in organization to BSDs own framework. Whenever an yn e t w o r k i n gd a t aa r r i v ea t this layer, either from an applications soc ket or from a networkdevice driver, the data are expected to have been tagged with an identier specifying which network protocol they contain. Protocols can communicate with one another if they desire; for example,

with one another if they desire; for example, within the Intern et protocol set, separate protocols manage routing, error reporting, and reliable retransmission of lost data. The protocol layer may rewrite packets, create new packets, split or reassemble packets into fragments, or simply discard incoming data. Ulti mately, once the protocol layer has nished processing a set of packets, it passes them on, either upward to the socket interface if the data are destined for a local connection or

if the data are destined for a local connection or downward to a device driver if the data need to be transmitted remotely. The protocol layer decides to which socket or device it will send the packet. All communication between the layers of the networking stack is per formed by passing single skbuff (socket buffer) structures. Each of these structures contains a set of pointers into a single continuous area of memory, representing a buffer inside which network packets can be constructed. The

which network packets can be constructed. The valid data in a skbuff do not need to start at the beginning of the skbuff s buffer, and they do not need to run to the end. The networking code can add data to or trim data from either end of the packet, as long as the result still ts into the skbuff .T h i sc a p a c i t yi se s p e c i a l l yi m p o r t a n to nm o d e r n microprocessors, where improvements in CPU speed have far outstripped the performance of main memory. The skbuff architecture

of main memory. The skbuff architecture allows exibility in manipulating packet headers and checksums while avoiding any unnecessary data copying. The most important set of protocols in the Linux networking system is the TCPIP protocol suite. This suite comprises a number of separate protocols. The IPprotocol implements routing between diffe rent hosts anywhere on the network. On top of the routing protocol are the UDP,TCP,a n d ICMP protocols. The UDP protocol carries arbitrary individual

The UDP protocol carries arbitrary individual datagrams between hosts. The TCP protocol implements reliable connection sb e t w e e nh o s t sw i t hg u a r a n t e e d inorder delivery of packets and automatic retransmission of lost data. The ICMP protocol carries various error and status messages between hosts. Each packet ( skbuff )a r r i v i n ga tt h en e t w o r k i n gs t a c k  sp r o t o c o ls o f t w a r e is expected to be already tagged with an internal identier indicating the

tagged with an internal identier indicating the protocol to which the packet is relevan t. Different networkingdevice drivers encode the protocol type in different ways; thus, the protocol for incoming data must be identied in the device driver. The device driver uses a hash table of known networkingprotocol identiers to look up the appropriate protocol and passes the packet to that protocol. New protocols can be added to the hash table as kernelloadable modules. Incoming IPpackets are delivered

modules. Incoming IPpackets are delivered to the IPdriver. The job of this layer is to perform routing. After deciding where the packet is to be sent, the IP driver forwards the packet to the appr opriate internal protocol driver to be delivered locally or injects it back into a selected networkdevicedriver queue to be forwarded to another host. It performs the routing decision using two tables: the persistent forwarding information base ( FIB)a n dac a c h eo fr e c e n t routing decisions. The

n dac a c h eo fr e c e n t routing decisions. The FIBholds routingconguration information and can specify routes based either on a speci cd e s t i n a t i o na d d r e s so ro naw i l d c a r d18.11 Security 821 representing multiple destinations. The FIBis organized as a set of hash tables indexed by destination address; the tables representing the most specic routes are always searched rst. Successful lookups from this table are added to the routecaching table, which caches r outes only by

routecaching table, which caches r outes only by specic destination. No wildcards are stored in the cache, so lookups can be made quickly. An entry in the route cache expires after a xed period with no hits. At various stages, the IPsoftware passes packets to a separate section of code for rewall management selective ltering of packets according to arbitrary criteria, usually for security purposes. The rewall manager maintains a number of separate rewall chains and allows a skbuff to be matched

rewall chains and allows a skbuff to be matched against any chain. Chains are reserved for separate purposes: one is used for forwarded packets, one for packets being input to this host, and one for data generated at this host. Each chain is held as an ordered list of rules, where a rule species one of a number of possible rewalldecision functions plus some arbitrary data for matching purposes. Two other functions performed by the IPdriver are disassembly and reassembly of large packets. If an

disassembly and reassembly of large packets. If an outgoing packet is too large to be queued to ad e v i c e ,i ti ss i m p l ys p l i tu pi n t os m a l l e r fragments ,w h i c ha r ea l lq u e u e dt o the driver. At the receiving host, these fragments must be reassembled. The IP driver maintains an ipfrag object for each fragment awaiting reassembly and anipqfor each datagram being assembled. Incoming fragments are matched against each known ipq.I fam a t c hi sf o u n d ,t h ef r a g m e n

ipq.I fam a t c hi sf o u n d ,t h ef r a g m e n ti sa d d e dt oi t ; otherwise, a new ipq is created. Once the nal fragment has arrived for a ipq,ac o m p l e t e l yn e w skbuff is constructed to hold the new packet, and this packet is passed back into the IPdriver. Packets identied by the IPas destined for this host are passed on to one of the other protocol drivers. The UDP and TCP protocols share a means of associating packets with source and destination sockets: each connected pair of

and destination sockets: each connected pair of sockets is uniquely identied by its source and destination addresses and by the source and destination port numbers. The socket lists are linked to hash tables keyed on these four address and port values for socket lookup on incoming packets. The TCPprotocol has to deal with unreliable connections, so it maintains ordered lists of unacknowledged outgoing packets to retransmit after a timeout and of incoming outoforder packets to be presented to the

incoming outoforder packets to be presented to the socket when the missing data have arrived. 18.11 Security Linuxs security model is closely related to typical UNIX security mechanisms. The security concerns can be classied in two groups: 1.Authentication .M a k i n gs u r et h a tn o b o d yc a na c c e s st h es y s t e mw i t h o u t rst proving that she has entry rights 2.Access control .P r o v i d i n gam e c h a n i s mf o rc h e c k i n gw h e t h e rau s e rh a s the right to access a

gw h e t h e rau s e rh a s the right to access a certain object and preventing access to objects as required822 Chapter 18 The Linux System 18.11.1 Authentication Authentication in UNIX has typically been performed through the use of a publicly readable password le. A users password is combined with a random saltvalue, and the result is encoded with a oneway transformation function and stored in the password le. The use of the oneway function means that the original password cannot be deduced

means that the original password cannot be deduced from the password le except by trial and error. When a user presents a password to the system, the password is recombined with the salt value stored in the password le and passed through the same oneway transformation. If the result matches the contents of the password le, then the password is accepted. Historically, UNIX implementations of this mechanism have had several drawbacks. Passwords were often limited to eight characters, and the

were often limited to eight characters, and the number of possible salt values was so low that an attacker could easily combine a dictionary of commonly used passwords with every possible salt value and have a good chance of matching one or more passwords in the password le, gaining unauthorized access to any accounts compromised as a result. Extensions to the password mechanism have been introduced that keep the encrypted password secret in a le that is not publicly readable, that allow longer

that is not publicly readable, that allow longer passwords, or that use more secure methods of encoding the password. Other authentication mechanisms have been introduced that limit the periods during which a user is permitted to connect to the system. Also, mechanisms exist to distribute authentication information to all the related systems in a network. An e ws e c u r i t ym e c h a n i s mh a sb e e nd e v e l o p e db y UNIX vendors to address authentication problems. The pluggable

to address authentication problems. The pluggable authentication modules (PAM )system is based on a shared library that can be used by any system component that needs to authenticate users. An implementation of this system is available under Linux. PAM allows authentication modules to be loaded on demand as specied in a systemwide conguration le. If a new authentication mechanism is added at a later date, it can be added to the conguration le, and all system components will immediately be able

and all system components will immediately be able to take advantage of it. PAM modules can specify authentication methods, account restrictions, session setup functions, and passwordchanging functions (so that, when users change their passwords, all the necessary authentication mechanisms can be updated at once). 18.11.2 Access Control Access control under UNIX systems, including Linux, is performed through the use of unique numeric identiers. A user identier ( UID)i d e n t i  e sas i n g l eu

user identier ( UID)i d e n t i  e sas i n g l eu s e r or a single set of access rights. A group identier ( GID)i sa ne x t r ai d e n t i  e r that can be used to identify rights belonging to more than one user. Access control is applied to various objects in the system. Every le available in the system is protected by the standard accesscontrol mecha nism. In addition, other shared objects, such as sharedmemory sections and semaphores, employ the same access system. Every object in a UNIX

the same access system. Every object in a UNIX system under user and group access control has a single UIDand a single GIDassociated with it. User processes also have a single UID,b u tt h e ym a yh a v em o r et h a no n e GID. If a processs UIDmatches the UID of an object, then the process has user rights orowner rights to that object.18.11 Security 823 If the UIDsd on o tm a t c hb u ta n y GID of the process matches the objects GID, then group rights are conferred; otherwise, the process has

rights are conferred; otherwise, the process has world rights to the object. Linux performs access control by assigning objects a protection mask that species which access modesread, write, or executeare to be granted to processes with owner, group, or world access. Thus, the owner of an object might have full read, write, and execute access to a le; other users in a certain group might be given read access but denied write access; and everybody else might be given no access at all. The only

else might be given no access at all. The only exception is the privileged root UID.Ap r o c e s sw i t ht h i ss p e c i a l UID is granted automatic access to any object in the system, bypassing normal access checks. Such processes are also granted permission to perform privileged operations, such as reading any physical memory or opening reserved network sockets. This mechan ism allows the kernel to prevent normal users from accessing these resources: most of the kernels key internal

these resources: most of the kernels key internal resources are implicitly owned by the root UID. Linux implements the standard UNIX setuid mechanism described in Section A.3.2. This mechanism allows a program to run with privileges different from those of the user running the program. For example, the lpr program (which submits a job to a print queue) has access to the systems print queues even if the user running that program does not. The UNIX implementation of setuid distinguishes between a

implementation of setuid distinguishes between a processs real and effective UID.T h er e a l UIDis that of the user running the program; the effective UIDis that of the les owner. Under Linux, this mechanism is augmented in two ways. First, Linux implements the POSIX specications saved userid mechanism, which allows a process to drop and reacquire its effective UIDrepeatedly. For security reasons, a program may want to perform most of its operations in a safe mode, waiving the privileges

operations in a safe mode, waiving the privileges granted by its setuid status; but it may wish to perform selected operations with all its privileges. Standard UNIX implementations achieve this capacity only by swapping the real and effective UIDs. When this is done, the previous effective UID is remembered, but the programs real UID does not always correspond to the UIDof the user running the program. Saved UIDsa l l o wap r o c e s st os e ti t se f f e c t i v e UID to its real UID and then

t se f f e c t i v e UID to its real UID and then return to the previous value of its effective UIDwithout having to modify the real UIDat any time. The second enhancement provided by Linux is the addition of a process characteristic that grants just a subset of the rights of the effective UID.T h e fsuid and fsgid process properties are used when access rights are granted to les. The appropriate property is set every time the effective UID orGID is set. However, the fsuid and fsgid can be set

is set. However, the fsuid and fsgid can be set independently of the effective ids, allowing a process to access les on b ehalf of another user without taking on the identity of that other user in any other way. Specically, server processes can use this mechanism to serve les t oac e r t a i nu s e rw i t h o u tb e c o m i n gv u l n e r a b l e to being killed or suspended by that user. Finally, Linux provides a mechanism for exible passing of rights from one program to anothera mechanism that

rights from one program to anothera mechanism that has become common in modern versions of UNIX . When a local network socket has been set up between any two processes on the system, either of those processes may send to the other process a le descriptor for one of its open les; the other process receives a824 Chapter 18 The Linux System duplicate le descriptor for the same le. This mechanism allows a client to pass access to a single le selectively to s ome server process without granting that

to s ome server process without granting that process any other privileges. For example, it is no longer necessary for a print server to be able to read all the les of a user who submits a new print job. The print client can simply pass the server le descriptors for any les to be printed, denying the server access to any of the users other les. 18.12 Summary Linux is a modern, free operating system based on UNIX standards. It has been designed to run efciently and reliably on common PChardware;

run efciently and reliably on common PChardware; it also runs on av a r i e t yo fo t h e rp l a t f o r m s ,s u c ha sm o b i l ep h o n e s .I tp r o v i d e sap r o g r a m m i n g interface and user interface compatible with standard UNIX systems and can run a large number of UNIX applications, including an increasing number of commercially supported applications. Linux has not evolved in a vacuum. A complete Linux system includes many components that were developed in dependently of Linux.

that were developed in dependently of Linux. The core Linux operatingsystem kernel is entirely original, but it allows much existing free UNIX software to run, resulting in an entire UNIX compatible operating system free from proprietary code. The Linux kernel is implemented as a traditional monolithic kernel for performance reasons, but it is modular enough in design to allow most drivers to be dynamically loaded and unloaded at run time. Linux is a multiuser system, providing protection

Linux is a multiuser system, providing protection between processes and running multiple processes according to a timesharing scheduler. Newly created processes can share selective parts of their execution environment with their parent processes, allowing multithreaded programming. Interpro cess communication is supported by both System V mechanismsmessage queues, semaphores, and shared memoryand BSDs socket interface. Multi ple networking protocols can be accessed simultaneously through the

can be accessed simultaneously through the socket interface. The memorymanagement system uses page sharing and copyonwrite to minimize the duplication of data shared by different processes. Pages are loaded on demand when they are rst referenced and are paged back out to backing store according to an LFU algorithm if physical memory needs to be reclaimed. To the user, the le system appears as a hierarchical directory tree that obeys UNIX semantics. Internally, Linux uses an abstraction layer to

Internally, Linux uses an abstraction layer to manage multiple le systems. Deviceoriented, networked, and virtual le systems are supported. Deviceoriented le systems access disk storage through a page cache that is unied with the virtual memory system. Practice Exercises 18.1 Dynamically loadable kernel modules give exibility when drivers are added to a system, but do they have disadvantages too? Under what circumstances would a kernel be compiled into a single binary le, and when would it be

into a single binary le, and when would it be better to keep it split into modules? Explain your answer.Exercises 825 18.2 Multithreading is a commonly used programming technique. Describe three different ways to implement threads, and compare these three methods with the Linux clone() mechanism. When might using each alternative mechanism be better or worse than using clones? 18.3 The Linux kernel does not allow paging out of kernel memory. What effect does this restriction have on the kernels

effect does this restriction have on the kernels design? What are two advantages and two disadvantages of this design decision? 18.4 Discuss three advantages of dyna mic (shared) linkage of libraries compared with static linkage. Describe two cases in which static linkage is preferable. 18.5 Compare the use of networking sockets with the use of shared memory as a mechanism for communicating data between processes on a single computer. What are the advantages of each method? When might each be

the advantages of each method? When might each be preferred? 18.6 At one time, UNIX systems used disklayout optimizations based on the rotation position of disk data, but modern implementations, including Linux, simply optimize for sequential data access. Why do they do so? Of what hardware characteristics does sequential access take advantage? Why is rotational optimization no longer so useful? Exercises 18.7 What are the advantages and disadvantages of writing an operating system in a

disadvantages of writing an operating system in a highlevel language, such as C? 18.8 In what circumstances is the systemcall sequence fork() exec() most appropriate? When is vfork() preferable? 18.9 What socket type should be used to implement an intercomputer letransfer program? What type should be used for a program that periodically tests to see whether another computer is up on the network? Explain your answer. 18.10 Linux runs on a variety of hardware platforms. What steps must Linux

of hardware platforms. What steps must Linux developers take to ensure that the system is portable to different processors and memorymanagement architectures and to minimize the amount of architec turespecic kernel code? 18.11 What are the advantages and disadvantages of making only some of the symbols dened inside a kernel accessible to a loadable kernel module? 18.12 What are the primary goals of the conictresolution mechanism used by the Linux kernel for loading kernel modules? 18.13 Discuss

kernel for loading kernel modules? 18.13 Discuss how the clone() operation supported by Linux is used to support both processes and threads. 18.14 Would you classify Linux threads as userlevel threads or as kernellevel threads? Support your answer with the appropriate arguments. 18.15 What extra costs are incurred in the creation and scheduling of a process, compared with the cost of a cloned thread?826 Chapter 18 The Linux System 18.16 How does Linuxs Completely Fair Scheduler ( CFS)p r o v i d

Linuxs Completely Fair Scheduler ( CFS)p r o v i d ei m p r o v e d fairness over a traditional UNIX process scheduler? When is the fairness guaranteed? 18.17 What are the two congurable variables of the Completely Fair Sched uler ( CFS)? What are the pros and cons of setting each of them to very small and very large values? 18.18 The Linux scheduler implements softrealtime scheduling. What features necessary for certain realtime programming tasks are missing? How might they be added to the

tasks are missing? How might they be added to the kernel? What are the costs (downsides) of such features? 18.19 Under what circumstances would a user process request an operation that results in the allocation of a demandzero memory region? 18.20 What scenarios would cause a page of memory to be mapped into a user programs address space with the copyonwrite attribute enabled? 18.21 In Linux, shared libraries perform many operations central to the operating system. What is the advantage of

to the operating system. What is the advantage of keeping this functionality out of the kernel? Are there any drawbacks? Explain your answer. 18.22 What are the benets of a journaling le system such as Linuxs ext3? What are the costs? Why does ext3 provide the option to journal only metadata? 18.23 The directory structure of a Linux operating system could include les corresponding to several different le systems, including the Linux proc le system. How might the need to support different

le system. How might the need to support different lesystem types affect the structure of the Linux kernel? 18.24 In what ways does the Linux setuid feature differ from the setuid feature SVR4 ? 18.25 The Linux source code is freely and widely available over the Inter net and from CDROM vendors. What are three implications of this availability for the security of the Linux system? Bibliographical Notes The Linux system is a product of the Internet; as a result, much of the available

the Internet; as a result, much of the available documentation on Linux is available in some form on the Internet. The following key sites reference most of the useful information available: The Linux CrossReference Page ( LXR)(http:lxr.linux.no )m a i n t a i n sc u r r e n t listings of the Linux kernel, browsable via the Web and fully cross referenced. The Kernel Hackers Guide provides a helpful overview of the Linux kernel components and internals and is located at

kernel components and internals and is located at http:tldp.orgLDPtlktlk.html .Bibliography 827 The Linux Weekly News ( LWN)(http:lwn.net )p r o v i d e sw e e k l yL i n u x  related news, including a very well researched subsection on Linux kernel news. Many mailing lists devoted to Linux are also available. The most important are maintained by a mailinglist manager that can be reached at the email address majordomovger.rutgers.edu .S e n de  m a i lt ot h i sa d d r e s sw i t ht h e single

m a i lt ot h i sa d d r e s sw i t ht h e single line help in the mails body for information on how to access the list server and to subscribe to any lists. Finally, the Linux system itself can be obtained over the Internet. Complete Linux distributions are available from the home sites of the companies concerned, and the Linux community also maintains archives of current system components at several places on the Internet. The most important is ftp:ftp.kernel.orgpublinux . In addition to

is ftp:ftp.kernel.orgpublinux . In addition to investigating Internet resources, you can read about the internals of the Linux kernel in [Mauerer (2008)] and [Love (2010)]. Bibliography [Love (2010)] R. Love, Linux Kernel Development, Third Edition, Developers Library (2010). [Mauerer (2008)] W. Mauerer, Professional Linux Kernel Architecture ,J o h nW i l e y and Sons (2008).19CHAPTER Windows 7 Updated by Dave Probert The Microsoft Windows 7 operating system is a 3264bit preemptive mul

7 operating system is a 3264bit preemptive mul titasking client operating system for microprocessors implementing the Intel IA32 and AMD 64 instruction set architectures ( ISAs). Microsofts corresponding server operating system, Windows Server 2008 R2, is based on the same code as Windows 7 but supports only the 64bit AMD 64 and IA64 (Itanium) ISAs. Windows 7 is the latest in a series of Microsoft operating systems based on its NTcode, which replaced the earlier systems based on Windows 9598. In

the earlier systems based on Windows 9598. In this chapter, we discuss the key goals of Windows 7, the layered architecture of the system that has made it so easy to use, the le system, the networking features, and the programming interface. CHAPTER OBJECTIVES To explore the principles underlying Windows 7s design and the specic components of the system. To provide a detailed discussion of the Windows 7 le system. To illustrate the networking protocols supported in Windows 7. To describe the

protocols supported in Windows 7. To describe the interface available in Windows 7 to system and application programmers. To describe the important algorithms implemented with Windows 7. 19.1 History In the mid1980s, Microsoft and IBMcooperated to develop the OS2 operating system ,w h i c hw a sw r i t t e ni na s s e m b l yl a n g u a g ef o rs i n g l e  p r o c e s s o rI n t e l 80286 systems. In 1988, Microsoft decided to end the joint effort with IBM and develop its own new technology

effort with IBM and develop its own new technology (orNT)p o r t a b l eo p e r a t i n gs y s t e mt o support both the OS2 and POSIX applicationprogramming interfaces ( APIs). In 829830 Chapter 19 Windows 7 October 1988, Dave Cutler, the architect of the DEC VAXVMS operating system, was hired and given the charter of building Microsofts new operating system. Originally, the team planned to use the OS2 API asNTs native environment, but during development, NTwas changed to use a new 32bit

development, NTwas changed to use a new 32bit Windows API (called Win 32), based on the popular 16bit APIused in Windows 3.0. The rst versions of NTwere Windows NT3.1 and Windows NT3.1 Advanced Server. (At that time, 16bit Windows was at Version 3.1.) Windows NTVersion 4.0 adopted the Windows 95 user interface and incorporated Internet webserver and webbrowser software. In addition, use rinterface routines and all graphics code were moved into the kernel to improve performance, with the side

the kernel to improve performance, with the side effect of decreased system reliability. Although previous versions of NThad been ported to other microprocessor architectures, the Windows 2000 version, released in February 2000, supported only Intel (and compatible) processors due to marketplace factors. Windows 2000 incorporated signicant changes. It added Active Directory (an X.500based directory service), better networking and laptop support, support for plugandplay devices, a distributed le

support for plugandplay devices, a distributed le system, and support for more processors and more memory. In October 2001, Windows XPwas released as both an update to the Windows 2000 desktop operating system and a replacement for Windows 9598. In 2002, the server edition of Windows XPbecame available (called Windows .Net Server). Windows XPupdated the graphical user interface (GUI) with a visual design that took advantage of more recent hardware advances and many new easeofuse features .N u m

advances and many new easeofuse features .N u m e r o u sf e a t u r e sw e r ea d d e d to automatically repair problems in applications and the operating system itself. As a result of these changes, Windows XPprovided better networking and device experience (including zeroco nguration wireless, instant messaging, streaming media, and digital photographyvideo), dramatic performance improvements for both the desktop and large multiprocessors, and better reliability and security than earlier

and better reliability and security than earlier Windows operating systems. The longawaited update to Windows XP,c a l l e dW i n d o w sV i s t a ,w a s released in November 2006, but it was not well received. Although Win dows Vista included many improvements that later showed up in Windows 7, these improvements were overshadow ed by Windows Vistas perceived sluggishness and compatibility problems. Microsoft responded to criticisms of Windows Vista by improving its engineering processes and

Vista by improving its engineering processes and working more closely with the makers of Windows hardware and applications. The result was Windows 7 ,w h i c hw a sr e l e a s e di nO c t o b e r2 0 0 9 ,a l o n gw i t hc o r r e s p o n d i n g server editions of Windows. Among the signicant engineering changes is the increased use of execution tracing rather than counters or proling to analyze system behavior. Tracing runs constantly in the system, watching hundreds of scenarios execute. When

watching hundreds of scenarios execute. When one of these scenarios fails, or when it succeeds but does not perform well, the traces can be analyzed to determine the cause. Windows 7 uses a clientserver architecture (like Mach) to implement two operatingsystem personalities, Win 32and POSIX ,w i t hu s e r  l e v e lp r o c e s s e s called subsystems. (At one time, Windows also supported an OS2 subsystem, but it was removed in Windows XPdue to the demise of OS2 .) The subsystem architecture

to the demise of OS2 .) The subsystem architecture allows enhancements to be made to one operatingsystem person ality without affecting the application compatibility of the other. Although the POSIX subsystem continues to be available for Windows 7, the Win 32 API has become very popular, and the POSIX API sa r eu s e db yo n l yaf e ws i t e s .T h e subsystem approach continues to be interesting to study from an operating19.2 Design Principles 831 system perspective, but machinevirtualization

831 system perspective, but machinevirtualization technologies are now becoming the dominant way of running multiple operating systems on a single machine. Windows 7 is a multiuser operating system, supporting simultaneous access through distributed services or through multiple instances of the GUI via the Windows terminal services. The server editions of Windows 7 support simultaneous terminal server sessions from Windows desktop systems. The desktop editions of terminal server multiplex the

desktop editions of terminal server multiplex the keyboard, mouse, and monitor between virtual terminal sessions for each loggedon user. This feature, called fast user switching ,a l l o w su s e r st op r e e m p te a c ho t h e ra tt h ec o n s o l eo f aPCwithout having to log off and log on. We noted earlier that some GUI implementation moved into kernel mode in Windows NT4.0. It started to move into user mode again with Windows Vista, which included the desktop window manager (DWM )as a

included the desktop window manager (DWM )as a usermode process. DWM implements the desktop compositing of Windows, providing the Windows Aero interface look on top of the Windows DirectX graphic software. DirectX continues to run in the kernel, as does the code implementing Windows previous windowing and graphics models (Win32k and GDI). Windows 7 made substantial changes to the DWM ,s i g n i  c a n t l yr e d u c i n gi t s memory footprint and improving its performance. Windows XPwas the rst

improving its performance. Windows XPwas the rst version of Windows to ship a 64bit version (for the IA64 in 2001 and the AMD 64 in 2005). Internally, the native NTle system (NTFS )a n dm a n yo ft h eW i n 32 API sh a v ea l w a y su s e d6 4  b i ti n t e g e r sw h e r e appropriateso the major extension to 64bit in Windows XPwas support for large virtual addresses. However, 64bit editions of Windows also support much larger physical memories. By the time Windows 7 shipped, the AMD 64ISA had

By the time Windows 7 shipped, the AMD 64ISA had become available on almost all CPUsf r o mb o t hI n t e la n dA M D .I na d d i t i o n , by that time, physical memories on client systems frequently exceeded the 4GBlimit of the IA32. As a result, the 64bit version of Windows 7 is now commonly installed on larger client systems. Because the AMD 64 architecture supports highdelity IA32 compatibility at the level of individual processes, 32 and 64bit applications can be freely mixed in a single

64bit applications can be freely mixed in a single system. In the rest of our description of Windows 7, we will not distinguish between the client editions of Windows 7 and the corresponding server editions. They are based on the same core components and run the same binary les for the kernel and most drivers. Similarly, although Microsoft ships a variety of different editions of each release to address different market price points, few of the differences between editions are reected in the

differences between editions are reected in the core of the system. In this chapter, we focus primarily on the core components of Windows 7. 19.2 Design Principles Microsofts design goals for Windows included security, reliability, Windows and POSIX application compatibility, high performance, extensibility, porta bility, and international support. Some additional goals, energy efciency and dynamic device support, have recently been added to this list. Next, we discuss each of these goals and

list. Next, we discuss each of these goals and how it is achieved in Windows 7. 19.2.1 Security Windows 7 security goals required more than just adherence to the design standards that had enabled Windows NT4.0 to receive a C2 security classica832 Chapter 19 Windows 7 tion from the U.S. government (A C2 classication signies a moderate level of protection from defective software and malicious attacks. Classications were dened by the Department of Defe nse Trusted Computer System Evaluation

of Defe nse Trusted Computer System Evaluation Criteria, also known as the Orange Book ,a sd e s c r i b e di nS e c t i o n1 5 . 8 . )E x t e n  sive code review and testing were combined with sophisticated automatic analysis tools to identify and investigate potential defects that might represent security vulnerabilities. Windows bases security on discretionary access controls. System objects, including les, registry settings, and kernel objects, are protected by access control lists

objects, are protected by access control lists (ACLs)(see Section 11.6.2). ACLsa r ev u l n e r a b l et ou s e ra n d programmer errors, however, as well as to the most common attacks on consumer systems, in which the user is tricked into running code, often while browsing the Web. Windows 7 includes a mechanism called integrity levels that acts as a rudimentary capability system for controlling access. Objects and processes are marked as having low, medium, or high integrity. Windows does not

low, medium, or high integrity. Windows does not allow a process to modify an object with a higher integrity level, no matter what the setting of the ACL. Other security measures include addressspace layout randomization (ASLR ),n o n e x e c u t a b l es t a c k sa n dh e a p s ,a n de n c r y p t i o na n d digital signature facilities. ASLR thwarts many forms of attack by preventing small amounts of injected code from jumping easily to code that is already loaded in a process as part of

that is already loaded in a process as part of normal operation. This safeguard makes it likely that a system under attack will fail or crash rather than let the attacking code take control. Recent chips from both Intel and AMD are based on the AMD 64 architecture, which allows memory pages to be marked so that they cannot contain executable instruction code. Window st r i e st om a r ks t a c k sa n dm e m o r yh e a p s so that they cannot be used to execut ec o d e ,t h u sp r e v e n t i n

used to execut ec o d e ,t h u sp r e v e n t i n ga t t a c k si nw h i c h a program bug allows a buffer to overow and then is tricked into executing the contents of the buffer. This technique cannot be applied to all programs, because some rely on modifying data and executing it. A column labeled data execution prevention in the Windows task manager shows which processes are marked to prevent these attacks. Windows uses encryption as part of common protocols, such as those used to communicate

protocols, such as those used to communicate securely with websites. Encryption is also used to protect user les stored on disk from prying eyes. Windows 7 allows users to easily encrypt virtually a whole disk, as well as removable storage devices such as USB ash drives, with a feature called BitLocker. If a computer with an encrypted disk is stolen, the thieves will need very sophisticated technology (such as an electron microscope) to gain access to any of the computers les. Windows uses

access to any of the computers les. Windows uses digital signatures to sign operating system binaries so it can verify that the les were produced by Microsoft or another known company. In some editions of Windows, a code integrity module is activated at boot to ensure that all the loaded modules in the kernel have valid signatures, assuring that they have not been tampered with by an offline attack. 19.2.2 Reliability Windows matured greatly as an operating system in its rst ten years, leading

an operating system in its rst ten years, leading to Windows 2000. At the same time, its reliability increased due to such factors as maturity in the source code, extensive stress testing of the system, improved CPU architectures, and automatic detection of many serious errors in drivers19.2 Design Principles 833 from both Microsoft and third parties. Windows has subsequently extended the tools for achieving reliability to include automatic analysis of source code for errors, tests that include

of source code for errors, tests that include providing invalid or unexpected input parameters (known as fuzzing to detect validation failures, and an application version of the driver verier that applies dynamic checking for an extensive set of common usermode programming errors. Other improvements in reliability have resulted from moving more code out of the kernel and into usermode services. Windows provides extensive s upport for writing drivers in user mode. System facilities that were once

in user mode. System facilities that were once in the kernel and are now in user mode include the Desktop Window Manager and much of the software stack for audio. One of the most signicant improvements in the Windows experience came from adding memory diagnostics as an option at boot time. This addition is especially valuable because so few consumer PCsh a v ee r r o r  correcting memory. When bad RAM starts to drop bits here and there, the result is frustratingly erratic behavior in the system.

is frustratingly erratic behavior in the system. The availability of memory diagnostics has greatly reduced th es t r e s sl e v e l so fu s e r sw i t hb a d RAM . Windows 7 introduced a faulttolerant memory heap. The heap learns from application crashes and automatically inserts mitigations into future execution of an application that has crashed. This makes the application more reliable even if it contains common bugs such as using memory after freeing it or accessing past the end of the

after freeing it or accessing past the end of the allocation. Achieving high reliability in Windows is particularly challenging because almost one billion computers run Windows. Even reliability problems that affect only a small percentage of users still impact tremendous numbers of human beings. The complexity of the Windows ecosystem also adds to the challenges. Millions of instances of applications, drivers, and other software are being constantly downloaded and run on Windows systems. Of

downloaded and run on Windows systems. Of course, there is also a constant stream of malware attacks. As Windows itself has become harder to attack directly, exploits increasingly target popular applications. To cope with these challenges, Microsoft is increasingly relying on com munications from customer machines to collect large amounts of data from the ecosystem. Machines can be sampled to see how they are performing, what software they are running, and what problems they are encountering.

running, and what problems they are encountering. Customers can send data to Microsoft when systems or software crashes or hangs. This constant stream of data from customer machines is collected very carefully, with the users consent and without invading privacy. The result is that Microsoft is building an everimprovin gp i c t u r eo fw h a ti sh a p p e n i n gi nt h e Windows ecosystem that allows continuous improvements through software updates, as well as providing data to guide future

updates, as well as providing data to guide future releases of Windows. 19.2.3 Windows and POSIX Application Compatibility As mentioned, Windows XPwas both an update of Windows 2000 and ar e p l a c e m e n tf o rW i n d o w s9 5  9 8 .W i n d o w s2 0 0 0f o c u s e dp r i m a r i l yo n compatibility for business applications. The requirements for Windows XP included a much higher compatibility with the consumer applications that ran on Windows 9598. Application compatibility is difcult to

9598. Application compatibility is difcult to achieve because many applications check for a particular version of Windows, may depend to some extent on the quirks of the implementation of APIs, may have latent application bugs that were masked in the previous system, and so834 Chapter 19 Windows 7 forth. Applications may also have been compiled for a different instruction set. Windows 7 implements several strategies to run applications despite incompatibilities. Like Windows XP,W i n d o w s7h a

Like Windows XP,W i n d o w s7h a sac o m p a t i b i l i t yl a y e rt h a ts i t sb e t w e e n applications and the Win 32 API s. This layer makes Windows 7 look (almost) bugforbug compatible with previous versions of Windows. Windows 7, like earlier NTreleases, maintains support for running many 16bit applications using a thunking ,o rc o n v e r s i o n ,l a y e rt h a tt r a n s l a t e s1 6  b i t API calls into equivalent 32bit calls. Similarly, the 64bit version of Windows 7 provides a

the 64bit version of Windows 7 provides a thunking layer that translates 32bit APIcalls into native 64bit calls. The Windows subsystem model allows multiple operatingsystem person alities to be supported. As noted earlier, although the API most commonly used with Windows is the Win 32 API , some editions of Windows 7 support a POSIX subsystem. POSIX is a standard specication for UNIX that allows most available UNIX compatible software to compile and run without modication. As a nal compatibility

and run without modication. As a nal compatibility measure, several editions of Windows 7 provide av i r t u a lm a c h i n et h a tr u n sW i n d o w s XPinside Windows 7. This allows applications to get bugforbug compatibility with Windows XP. 19.2.4 High Performance Windows was designed to provide high performance on desktop systems (which are largely constrained by IOperformance), server systems (where the CPU is often the bottleneck), and large multithreaded and multiprocessor environments

multithreaded and multiprocessor environments (where locking performance and cacheline management are keys to scalability). To satisfy performance requirements, NTused a variety of techniques, such as asynchronous IO,o p t i m i z e dp r o t o c o l sf o rn e t w o r k s , kernelbased graphics rendering, and sophisticated caching of lesystem data. The memorymanagement and synchronization algorithms were designed with an awareness of the performance considerations related to cache lines and

considerations related to cache lines and multiprocessors. Windows NTwas designed for symmetrical multiprocessing ( SMP); on am u l t i p r o c e s s o rc o m p u t e r ,s e v e r a lt h r e a d sc a nr u na tt h es a m et i m e ,e v e n in the kernel. On each CPU,W i n d o w s NTuses prioritybased preemptive scheduling of threads. Except while executing in the kernel dispatcher or at interrupt level, threads in any process running in Windows can be preempted by higherpriority threads. Thus, the

be preempted by higherpriority threads. Thus, the system responds quickly (see Chapter 6). The subsystems that constitute Windows NTcommunicate with one another efciently through a local procedure call (LPC)facility that provides highperformance message passing. When a thread requests a synchronous service from another process through an LPC,t h es e r v i c i n gt h r e a di sm a r k e d ready ,a n di t sp r i o r i t yi st e m p o r a r i l yb o o s t e dt oa v o i dt h es c h e d u l i n gd e

o o s t e dt oa v o i dt h es c h e d u l i n gd e l a y s that would occur if it had to wait for threads already in the queue. Windows XPfurther improved performance by reducing the codepath length in critical functions, using better algorithms and perprocessor data structures, using memory coloring for nonuniform memory access (NUMA ) machines, and implementing more scalable locking protocols, such as queued spinlocks. The new locking protocols helped reduce system bus cycles and included

helped reduce system bus cycles and included lockfree lists and queues, atomic readmodifywrite operations (like interlocked increment), and other advanced synchronization techniques.19.2 Design Principles 835 By the time Windows 7 was developed, several major changes had come to computing. Clientserver computing had increased in importance, so an advanced local procedure call ( ALPC )f a c i l i t yw a si n t r o d u c e dt op r o v i d e higher performance and more reliability than LPC.T h en u

performance and more reliability than LPC.T h en u m b e ro f CPUs and the amount of physical memory available in the largest multiprocessors had increased substantially, so quite a lot of effort was put into improving operatingsystem scalability. The implementation of SMP in Windows NTused bitmasks to represent collections of processors and to identify, for example, which set of processors a particular thread could be scheduled on. These bitmasks were dened as tting within a single word of

were dened as tting within a single word of memory, limiting the number of processors supported within a system to 64. Windows 7 added the concept of processor groups to represent arbitrary numbers of CPUs, thus accommodating more CPU cores. The number of CPU cores within single systems has continued to increase not only because of more cores but also because of cores that support more than one logical thread of execution at a time. All these additional CPUsc r e a t e dag r e a td e a lo fc o n

CPUsc r e a t e dag r e a td e a lo fc o n t e n t i o nf o rt h el o c k s used for scheduling CPUsa n dm e m o r y .W i n d o w s7b r o k et h e s el o c k sa p a r t .F o r example, before Windows 7, a single lock was used by the Windows scheduler to synchronize access to the queues co ntaining threads waiting for events. In Windows 7, each object has its own lock, allowing the queues to be accessed concurrently. Also, many execution paths in the scheduler were rewritten to be lockfree. This

the scheduler were rewritten to be lockfree. This change resulted in good scalability performance for Windows even on systems with 256 hardware threads. Other changes are due to the increasing importance of support for parallel computing. For years, the computer industry has been dominated by Moores Law, leading to higher densities of tran sistors that manifest themselves as faster clock rates for each CPU. Moores Law continues to hold true, but limits have been reached that prevent CPU clock

limits have been reached that prevent CPU clock rates from increasing further. Instead, transistors are being used to build more and more CPUsi n t oe a c hc h i p .N e w programming models for achieving parallel execution, such as Microsofts Concurrency RunTime (ConcRT) and Intels Threading Building Blocks ( TBB), are being used to express parallelism in C programs. Where Moores Law has governed computing for forty years, it now seems that Amdahls Law, which governs parallel computing, will

Law, which governs parallel computing, will rule the future. To support taskbased parallelism, Windows 7 provides a new form of usermode scheduling (UMS ).UMS allows programs to be decomposed into tasks, and the tasks are then scheduled on the available CPUsb yas c h e d u l e r that operates in user mode rather than in the kernel. The advent of multiple CPUso nt h es m a l l e s tc o m p u t e r si so n l yp a r to f the shift taking place to parallel computing. Graphics processing units (

to parallel computing. Graphics processing units ( GPUs) accelerate the computational algorithms needed for graphics by using SIMD architectures to execute a single instruction for multiple data at the same time. This has given rise to the use of GPUs for general computing, not just graphics. Operatingsystem support for software like Open CLand CUDA is allowing programs to take advantage of the GPUs. Windows supports use of GPUs through software in its Direct Xgraphics support. This software,

in its Direct Xgraphics support. This software, called DirectCompute, allows programs to specify computational kernels using the same HLSL (highlevel shader language) pr ogramming model used to program the SIMD hardware for graphics shaders .T h ec o m p u t a t i o n a lk e r n e l sr u nv e r y836 Chapter 19 Windows 7 quickly on the GPU and return their results to the main computation running on the CPU. 19.2.5 Extensibility Extensibility refers to the capacity of an operating system to keep

to the capacity of an operating system to keep up with advances in computing technology. To facilitate change over time, the devel opers implemented Windows using a layered architecture. The Windows executive runs in kernel mode and provides the basic system services and abstractions that support shared use of the system. On top of the executive, several server subsystems operate in user mode. Among them are environ mental subsystems that emulate different operating systems. Thus, programs

different operating systems. Thus, programs written for the Win 32 API sa n d POSIX all run on Windows in the appropriate environment. Because of the modular stru cture, additional environmental sub systems can be added without affecting the executive. In addition, Windows uses loadable drivers in the IO system, so new le systems, new kinds of IO devices, and new kinds of networking can be added while the system is running. Windows uses a clientserver model like the Mach operating system and

model like the Mach operating system and supports distributed processing by remote procedure calls (RPCs) as dened by the Open Software Foundation. 19.2.6 Portability An operating system is portable if it can be moved from one CPU architecture to another with relatively few changes. Windows was designed to be portable. Like the UNIX operating system, Windows is written primarily in C and C. The architecturespecic source c ode is relatively small, and there is very little use of assembly code.

and there is very little use of assembly code. Porting Windows to a new architecture mostly affects the Windows kernel, since the usermode code in Windows is almost exclusively written to be architecture in dependent. To port Windows, the kernels architecturespecic code must be ported, and sometimes conditional compilation is needed in other parts of the kernel because of changes in major data structures, such as the pagetable format. The entire Windows system must then be recompiled for the new

Windows system must then be recompiled for the new CPU instruction set. Operating systems are sensitive not only to CPU architecture but also to CPU support chips and hardware boot programs. The CPU and support chips are collectively known as a chipset .T h e s ec h i p s e t sa n dt h ea s s o c i a t e db o o tc o d e determine how interrupts are delivered, desc ribe the physical characteristics of each system, and provide interfaces to deeper aspects of the CPU architecture, such as error

aspects of the CPU architecture, such as error recovery and power ma nagement. It would be burdensome to have to port Windows to each type of support chip as well as to each CPU architecture. Instead, Windows isolates most of the chipsetdependent code in ad y n a m i cl i n kl i b r a r y( DLL), called the hardwareabstraction layer (HAL ),t h a t is loaded with the kernel. The Windows kernel depends on the HAL interfaces rather than on the underlying chipset details. This allows the single set

chipset details. This allows the single set of kernel and driver binaries for a particular CPU to be used with different chipsets simply by loading a different version of the HAL . Over the years, Windows has been ported to a number of different CPU architectures: Intel IA32compatible 32bit CPUs,AMD 64compatible and IA64 64bit CPUs, the DEC Alpha, and the MIPS and Power PC CPU s. Most of these CPU architectures failed in the market. When Windows 7 shipped, only the19.2 Design Principles 837 IA32

7 shipped, only the19.2 Design Principles 837 IA32 and AMD 64 architectures were supported on client computers, along with AMD 64 and IA64 on servers. 19.2.7 International Support Windows was designed for international and multinational use. It provides support for different locales via the nationallanguagesupport (NLS)API. The NLS API provides specialized routines to format dates, time, and money in accordance with national customs. String comparisons are specialized to account for varying

comparisons are specialized to account for varying character sets. UNICODE is Windowss native character code. Windows supports ANSI characters by converting them to UNICODE characters before manipulating them (8bit to 16bit conversion). System text strings are kept in resource les that can be replaced to localize the system for different languages. Multiple locales can be used concurrently, which is important to multilingual individuals and businesses. 19.2.8 Energy Efciency Increasing energy

19.2.8 Energy Efciency Increasing energy efciency for c omputers causes batteries to last longer for laptops and netbooks, saves signicant operating costs for power and cooling of data centers, and contributes to gr een initiatives aimed at lowering energy consumption by businesses and consumers. For some time, Windows has implemented several strategies for decreasing energy use. The CPUsa r em o v e d to lower power statesfor example, by lowering clock frequencywhenever possible. In addition,

clock frequencywhenever possible. In addition, when a computer is not being actively used, Windows may put the entire computer into a lowpower state (sleep) or may even save all of memory to disk and shut the computer off (hibernation). When the user returns, the computer powers up and continues from its previous state, so the user does not need to reboot and restart applications. Windows 7 added some new strategies for saving energy. The longer a CPU can stay unused, the more energy can be

a CPU can stay unused, the more energy can be saved. Because computers are so much faster than human beings, a lot of energy can be saved just while humans are thinking. The problem is that too many programs are constantly polling to see what is happening in the system. A swarm of software timers are ring, keeping the CPU from staying idle long enough to save much energy. Windows 7e x t e n d s CPU idle time by skipping clock ticks, coalescing software timers into smaller numbers of events, and

timers into smaller numbers of events, and parking entire CPUsw h e ns y s t e m sa r en o t heavily loaded. 19.2.9 Dynamic Device Support Early in the history of the PCindustry, computer congurations were fairly static. Occasionally, new devices might be plugged into the serial, printer, or game ports on the back of a computer, but that was it. The next steps toward dynamic conguration of PCsw e r el a p t o pd o c k sa n d PCMIA cards. A PCcould suddenly be connected to or disconnected from a

suddenly be connected to or disconnected from a whole set of peripherals. In ac o n t e m p o r a r y PC, the situation has completely changed. PCsa r ed e s i g n e d to enable users to plug and unplug a huge host of peripherals all the time; external disks, thumb drives, cameras, and the like are constantly coming and going.838 Chapter 19 Windows 7 OS2 applications OS2 subsystemWin16 applicationsMSDOS applications Win18 VDM window manageruser mode file systemIO managerMSDOS VDM Win32

mode file systemIO managerMSDOS VDM Win32 subsystemPOSIX subsystemlogon process security subsystem authentication package security account manager databaseWin32 applicationsPOSIX applications graphic device driverskernelexecutive hardware abstraction layer hardwarecache manager device drivers network driversobject managersecurity reference monitorprocess managerplug and play managervirtual memory managerlocal procedure call facility Figure 19.1 Windows block diagram. Support for dynamic

19.1 Windows block diagram. Support for dynamic conguration of devices is continually evolving in Windows. The system can automatically recognize devices when they are plugged in and can nd, install, and load the appropriate drivers often without user intervention. Whe nd e v i c e sa r eu n p l u g g e d ,t h ed r i v e r s automatically unload, and system execution continues without disrupting other software. 19.3 System Components The architecture of Windows is a layered system of modules, as

of Windows is a layered system of modules, as shown in Figure 19.1. The main layers are the HAL , the kernel, and the executive, all of which run in kernel mode, and a collection of subsystems and services that run in user mode. The usermode subsystems fall into two categories: the environmental subsystems, which emulate different operating systems, and the protection subsystems ,w h i c hp r o v i d es e c u r i t yf u n c t i o n s .O n eo ft h ec h i e fa d v a n t a g e so f this type of

ft h ec h i e fa d v a n t a g e so f this type of architecture is that interactions between modules are kept simple. The remainder of this section describes these layers and subsystems. 19.3.1 HardwareAbstraction Layer The HAL is the layer of software that hides hardware chipset differences from upper levels of the operating system. The HAL exports a virtual hardware19.3 System Components 839 interface that is used by the kernel dispatcher, the executive, and the device drivers. Only a single

executive, and the device drivers. Only a single version of each device driver is required for each CPU architecture, no matter what support chips might be present. Device drivers map devices and access them dir ectly, but the chipsetspecic details of mapping memory, conguring IObuses, setting up DMA ,a n dc o p i n gw i t h motherboardspecic facilities are all provided by the HAL interfaces. 19.3.2 Kernel The kernel layer of Windows has four main responsibilities: thread scheduling, lowlevel

main responsibilities: thread scheduling, lowlevel processor synchronization, in terrupt and exception handling, and switching between user mode and kern el mode. The kernel is implemented in the C language, using assembly language only where absolutely necessary to interface with the lowest level of the hardware architecture. The kernel is organized accord ing to objectoriented design principles. An object type in Windows is a systemdened data type that has a set of attributes (data values) and

that has a set of attributes (data values) and a set of methods (for example, functions or operations). An object is an instance of an object type. The kernel performs its job by using a set of kernel objects whose attributes store the kernel data and whose methods perform the kernel activities. 19.3.2.1 Kernel Dispatcher The kernel dispatcher provides the foundation for the executive and the subsystems. Most of the dispatcher is never paged out of memory, and its exe cution is never preempted.

of memory, and its exe cution is never preempted. Its main responsibilities are thread scheduling and context switching, implementation of synchronization primitives, timer man agement, software interrupts (asynchron ous and deferred procedure calls), and exception dispatching. 19.3.2.2 Threads and Scheduling Like many other modern operating systems, Windows uses processes and threads for executable code. Each process has one or more threads, and each thread has its own scheduling state,

and each thread has its own scheduling state, including actual priority, processor afnity, and CPU usage information. There are six possible thread states: ready ,standby ,running ,waiting , transition ,a n d terminated .Ready indicates that the thread is waiting to run. The highestpriority ready thread is moved to the standby state, which means it is the next thread to run. In a multiprocessor system, each processor keeps one thread in a standby state. A thread is running when it is executing

state. A thread is running when it is executing on a processor. It runs until it is preempted by a higherpriority thread, until it terminates, until its allotted execution time (quantum) ends, or until it waits on a dispatcher object, such as an event signaling IOcompletion. A thread is in the waiting state when it is waiting for a dispatcher object to be signaled. At h r e a di si nt h e transition state while it waits for resources necessary for execution; for example, it may be waiting for

for execution; for example, it may be waiting for its kernel stack to be swapped in from disk. A thread enters the terminated state when it nishes execution. The dispatcher uses a 32level priority scheme to determine the order of thread execution. Priorities are divided into two classes: variable class and realtime class. The variable class contains threads having priorities from 1 to840 Chapter 19 Windows 7 15, and the realtime class contains threads with priorities ranging from 16 to 31. The

threads with priorities ranging from 16 to 31. The dispatcher uses a queue for each scheduling priority and traverses the set of queues from highest to lowest until it nds a thread that is ready to run. If a thread has a particular processor afnity but that processor is not available, the dispatcher skips past it and continues looking for a ready thread that is willing to run on the available processor. If no ready thread is found, the dispatcher executes a special thread called the idle thread

executes a special thread called the idle thread .P r i o r i t yc l a s s0 is reserved for the idle thread. When a threads time quantum runs out, the clock interrupt queues a quantumend deferred procedure call (DPC)to the processor. Queuing the DPC results in a software interrupt when the processor returns to normal interrupt priority. The software interrupt causes the dispatcher to reschedule the processor to execute the next a vailable thread at the preempted threads priority level. The

at the preempted threads priority level. The priority of the preempted thread may be modied before it is placed back on the dispatcher queues. If the preempted thread is in the variable priority class, its priority is lowered. The priority is never lowered below the base priority. Lowering the threads priority tends to limit the CPU consumption of computebound threads versus IObound threads. When a variablepriority thread is released from a wait operation, the dispatcher boosts the priority. The

operation, the dispatcher boosts the priority. The amount of the boost depends on the device for which the thread was waiting. For example, a thread waiting for keyboard IO would get a large priority increase, whereas a thread waiting for a disk operation would get a moderate one. This strategy tends to give good response times to interactive threads using a mouse and windows. It also enables IObound threads to keep the IO devices busy while permitting computebound threads to use spare CPU

permitting computebound threads to use spare CPU cycles in the background. In addition, the thread associated with the users active GUI window receives a priority boost to enhance its response time. Scheduling occurs when a thread enters the ready or wait state, when at h r e a dt e r m i n a t e s ,o rw h e na na p p l i c a t i o nc h a n g e sat h r e a d  sp r i o r i t yo r processor afnity. If a higherpriority th read becomes ready while a lower priority thread is running, the

while a lower priority thread is running, the lowerpriority thread is preempted. This preemption gives the higherpriority thread preferential access to the CPU. Windows is not a hard realtime operating system, however, because it does not guarantee that a realtime thread will start to execute within a particular time limit; threads are blocked indenitely while DPCsa n d interrupt service routines (ISRs)are running (as further discussed below). Traditionally, operatingsystem schedulers used

Traditionally, operatingsystem schedulers used sampling to measure CPU utilization by threads. The system timer would re periodically, and the timer interrupt handler would take note of what thread was currently scheduled and whether it was executing in user or kernel mode when the interrupt occurred. This sampling technique was necessary because either the CPU did not have ah i g h  r e s o l u t i o nc l o c ko rt h ec l o c kw a st o oe x p e n s i v eo ru n r e l i a b l et oa c c e s s

p e n s i v eo ru n r e l i a b l et oa c c e s s frequently. Although efcient, sampling was inaccurate and led to anomalies such as incorporating interrupt servicing time as thread time and dispatching threads that had run for only a fraction of the quantum. Starting with Windows Vista, CPU time in Windows has been tracked using the hardware timestamp counter (TSC)included in recent processors. Using the TSC results in more accurate accounting of CPU usage, and the scheduler will not preempt

of CPU usage, and the scheduler will not preempt threads before they have run for a full quantum.19.3 System Components 841 19.3.2.3 Implementation of Synchronization Primitives Key operatingsystem data structures are managed as objects using common facilities for allocation, reference counting, and security. Dispatcher objects control dispatching and synchronization in the system. Examples of these objects include the following: The event object is used to record an event occurrence and to

is used to record an event occurrence and to synchronize this occurrence with some action. Notication events signal all waiting threads, and synchronization events signal a single waiting thread. The mutant provides kernelmode or usermode mutual exclusion associ ated with the notion of ownership. The mutex ,a v a i l a b l eo n l yi nk e r n e lm o d e ,p r o v i d e sd e a d l o c k  f r e em u t u a l exclusion. The semaphore object acts as a counter or gate to control the number of threads

a counter or gate to control the number of threads that access a resource. The thread object is the entity that is scheduled by the kernel dispatcher. It is associated with a process object ,w h i c he n c a p s u l a t e sav i r t u a la d d r e s s space. The thread object is signaled when the thread exits, and the process object, when the process exits. The timer object is used to keep track of time and to signal timeouts when operations take too long and need to be interrupted or when a

take too long and need to be interrupted or when a periodic activity needs to be scheduled. Many of the dispatcher objects are accessed from user mode via an open operation that returns a handle. The usermode code polls or waits on handles to synchronize with other threads as well as with the operating system (see Section 19.7.1). 19.3.2.4 Software Interrupts: Asynchronous and Deferred Procedure Calls The dispatcher implements two types of software interrupts: asynchronous procedure calls

software interrupts: asynchronous procedure calls (APCs)and deferred procedure calls ( DPCs, mentioned earlier). An asynchronous procedure call breaks into an executing thread and calls ap r o c e d u r e . APCs are used to begin execution of new threads, suspend or resume existing threads, terminate threads or processes, deliver notication that an asynchronous IOhas completed, and extract the contents of the CPU registers from a running thread. APCsa r eq u e u e dt os p e c i  ct h r e a d sa

APCsa r eq u e u e dt os p e c i  ct h r e a d sa n da l l o w the system to execute both system and user code within a processs context. Usermode execution of an APC cannot occur at arbitrary times, but only when the thread is waiting in the kernel and marked alertable . DPCsare used to postpone interrupt processing. After handling all urgent deviceinterrupt processing, the ISRschedules the remaining processing by queuing a DPC.T h ea s s o c i a t e ds o f t w a r ei n t e r r u p tw i l ln o

a t e ds o f t w a r ei n t e r r u p tw i l ln o to c c u ru n t i lt h e CPU is next at a priority lower than the priority of all IOdevice interrupts but higher than the priority at which threads run. Thus, DPCsd on o tb l o c ko t h e rd e v i c e ISRs. In addition to deferring deviceinterrup tp r o c e s s i n g ,t h ed i s p a t c h e ru s e s842 Chapter 19 Windows 7 DPCst op r o c e s st i m e re x p i r a t i o n sa n dt op r e e m p tt h r e a de x e c u t i o na tt h ee n d of the

tt h r e a de x e c u t i o na tt h ee n d of the scheduling quantum. Execution of DPCsp r e v e n t st h r e a d sf r o mb e i n gs c h e d u l e do nt h ec u r r e n t processor and also keeps APCsf r o ms i g n a l i n gt h ec o m p l e t i o no f IO.T h i si s done so that completion of DPC routines does not take an extended amount of time. As an alternative, the dispatcher maintains a pool of worker threads. ISRsa n d DPCsm a yq u e u ew o r ki t e m st ot h ew o r k e rt h r e a d sw h e r

ki t e m st ot h ew o r k e rt h r e a d sw h e r et h e yw i l lb e executed using normal thread scheduling. DPC routines are restricted so that they cannot take page faults (be paged out of memory), call system services, or take any other action that might result in an attempt to wait for a dispatcher object to be signaled. Unlike APCs,DPC routines make no assumptions about what process context the processor is executing. 19.3.2.5 Exceptions and Interrupts The kernel dispatcher also provides

and Interrupts The kernel dispatcher also provides trap handling for exceptions and interrupts generated by hardware or software. Windows denes several architecture independent exceptions, including: Memoryaccess violation Integer overow Floatingpoint overow or underow Integer divide by zero Floatingpoint divide by zero Illegal instruction Data misalignment Privileged instruction Pageread error Access violation Paging le quota exceeded Debugger breakpoint Debugger single step The trap handlers

breakpoint Debugger single step The trap handlers deal with simple ex ceptions. Elaborate exception handling is performed by the kernels exception dispatcher. The exception dispatcher creates an exception record containing the reason for the exception and nds an exception handler to deal with it. When an exception occurs in kernel mode, the exception dispatcher simply calls a routine to locate the exception handler. If no handler is found, a fatal system error occurs, and the user is left with

system error occurs, and the user is left with the infamous blue screen of death that signies system failure. Exception handling is more complex for usermode processes, because an environmental subsystem (such as the POSIX system) sets up a debugger port and an exception port for every process it creates. (For details on ports,19.3 System Components 843 see Section 19.3.3.4.) If a debugger port is registered, the exception handler sends the exception to the port. If the debugger port is not

exception to the port. If the debugger port is not found or does not handle that exception, the dispatcher attempts to nd an appropriate exception handler. If no handler is found, the debugger is called again to catch the error for debugging. If no debugger is running, a message is sent to the processs exception port to give the environmental subsystem a chance to translate the exception. For example, the POSIX environment translates Windows exception messages into POSIX signals before sending

messages into POSIX signals before sending them to the thread that caused the exception. Finally, if nothing else works, the kernel simply terminates the process containing the thread that caused the exception. When Windows fails to handle an exception, it may construct a description of the error that occurred and request permission from the user to send the information back to Microsoft for further analysis. In some cases, Microsofts automated analysis may be able to recognize the error

analysis may be able to recognize the error immediately and suggest a xo rw o r k a r o u n d . The interrupt dispatcher in the kernel handles interrupts by calling either an interrupt service routine ( ISR)s u p p l i e db yad e v i c ed r i v e ro rak e r n e l traphandler routine. The interrupt is represented by an interrupt object that contains all the information needed to handle the interrupt. Using an interrupt object makes it easy to associate in terruptservice routines with an interrupt

in terruptservice routines with an interrupt without having to access the interrupt hardware directly. Different processor architectures have different types and numbers of inter rupts. For portability, the interrupt dispatcher maps the hardware interrupts into a standard set. The interrupts are prioritized and are serviced in priority order. There are 32 interrupt request levels ( IRQL s) in Windows. Eight are reserved for use by the kernel; the remaining 24 represent hardware interrupts via

the remaining 24 represent hardware interrupts via the HAL (although most IA32 systems use only 16). The Windows interrupts are dened in Figure 19.2. The kernel uses an interruptdispatch table to bind each interrupt level to a service routine. In a multiprocessor computer, Windows keeps a separate interruptdispatch table ( IDT)f o re a c hp r o c e s s o r ,a n de a c hp r o c e s s o r s IRQL can be set independently to mask out interru pts. All interrupts that occur at a level equal to or less

interrupts that occur at a level equal to or less than the IRQL of a processor are blocked until the IRQL is loweredinterrupt levels types of interrupts 31 30 29machine check or bus error power fail clock (used to keep track of time) profile traditional PC IRQ hardware interrupts dispatch and deferred procedure call (DPC) (kernel) asynchronous procedure call (APC) passive28 27 326 2 1 0interprocessor notification (request another processor to act; e.g., dispatch a process or update the TLB)

act; e.g., dispatch a process or update the TLB) Figure 19.2 Windows interruptrequest levels.844 Chapter 19 Windows 7 by a kernellevel thread or by an ISRreturning from interrupt processing. Windows takes advantage of this property and uses software interrupts to deliver APCsa n d DPCs, to perform system functions such as synchronizing threads with IOcompletion, to start thread execution, and to handle timers. 19.3.2.6 Switching between UserMode and KernelMode Threads What the programmer thinks

and KernelMode Threads What the programmer thinks of as a thread in traditional Windows is actually two threads: a usermode thread (UT)and a kernelmode thread (KT).E a c hh a s its own stack, register values, and execution context. A UTrequests a system service by executing an instruction that causes a trap to kernel mode. The kernel layer runs a trap handler that switches between the UTand the corresponding KT.W h e na KThas completed its kernel execution and is ready to switch back to the

execution and is ready to switch back to the corresponding UT, the kernel layer is called to make the switch to the UT, which continues its execution in user mode. Windows 7 modies the behavior o f the kernel layer to support user mode scheduling of the UTs. Usermode schedulers in Windows 7 support cooperative scheduling. A UTcan explicitly yield to another UTby calling the usermode scheduler; it is not necessary to enter the kernel. Usermode scheduling is explained in more detail in Section

scheduling is explained in more detail in Section 19.7.3.7. 19.3.3 Executive The Windows executive provides a set of services that all environmental subsystems use. The services are grouped as follows: object manager, virtual memory manager, process manager, advanced local procedure call facility, IO manager, cache manager, security refer ence monitor, plugandplay and power managers, registry, and booting. 19.3.3.1 Object Manager For managing kernelmode entities, Win dows uses a generic set of

entities, Win dows uses a generic set of interfaces that are manipulated by usermode programs. Windows calls these entities objects ,a n dt h ee x e c u t i v ec o m p o n e n tt h a tm a n i p u l a t e st h e mi st h e object manager .E x a m p l e so fo b j e c t sa r es e m a p h o r e s ,m u t e x e s ,e v e n t s ,p r o c e s s e s , and threads; all these are dispatcher objects .T h r e a d sc a nb l o c ki nt h ek e r n e l dispatcher waiting for any of these objects to be signaled. The

for any of these objects to be signaled. The process, thread, and virtual memory APIs use process and thread handles to identify the process or thread to be operated on. Other examples of objects include les, sections, ports, and various internal IOobjects. File objects are used to maintain the open state of les and devices. Sections are used to map les. Localcommunication endpoints are implemented as port objects. Usermode code accesses these objects using an opaque value called a handle ,w h i

using an opaque value called a handle ,w h i c hi sr e t u r n e db ym a n y APIs. Each process has a handle table containing entries that track the objects used by the process. The system process ,w h i c hc o n t a i n st h ek e r n e l ,h a si t so w nh a n d l et a b l e ,w h i c hi sp r o t e c t e d from user code. The handle tables in Windows are represented by a tree structure, which can expand from holding 1,024 handles to holding over 16 million. Kernelmode code can access an object by

million. Kernelmode code can access an object by using either a handle or a referenced pointer .19.3 System Components 845 Ap r o c e s sg e t sah a n d l eb yc r e a t i n ga no b j e c t ,b yo p e n i n ga ne x i s t i n g object, by receiving a duplicated hand le from another process, or by inheriting ah a n d l ef r o mt h ep a r e n tp r o c e s s .W h e nap r o c e s se x i t s ,a l li t so p e nh a n d l e s are implicitly closed. Since the object manager is the only entity that generates

object manager is the only entity that generates object handles, it is the natural place to check security. The object manager checks whether a process has the r ight to access an object when the process tries to open the object. The object manager also enforces quotas, such as the maximum amount of memory a process may use, by charging a process for the memory occupied by all its referenced objects and refusing to allocate more memory when the accumulated charges exceed the processs quota. The

accumulated charges exceed the processs quota. The object manager keeps track of two coun ts for each object: the number of handles for the object and the numb er of referenced pointers. The handle count is the number of handles that refe rt ot h eo b j e c ti nt h eh a n d l et a b l e s of all processes, including the system process that contains the kernel. The referenced pointer count is incr emented whenever a n ew pointer is needed by the kernel and decremented when the kernel is done with

and decremented when the kernel is done with the pointer. The purpose of these reference counts is to ensure that an object is not freed while it is still referenced by either a handle or an internal kernel pointer. The object manager maintains the Wind ows internal name space. In contrast to UNIX ,w h i c hr o o t st h es y s t e mn a m es p a c ei nt h e l es y s t e m , Windows uses an abstract name space and connects the le systems as devices. Whether a Windows object has a name is up to its

Whether a Windows object has a name is up to its creator. Processes and threads are created without names and re ferenced either by handle or through a separate numerical identier. Synch ronization events usually have names, so that they can be opened by un related processes. A name can be either permanent or temporary. A permanent name represents an entity, such as a disk drive, that remains even if no process is accessing it. A temporary name exists only while a process holds a handle to the

exists only while a process holds a handle to the object. The object manager supports directories and symbolic links in the name space. As an example, MSDOS drive letters are implemented using symbolic links; Global?? C:is a symbolic link to the device object Device HarddiskVolume2 ,r e p r e s e n t i n ga mounted lesystem volume in the Device directory. Each object, as mentioned earlier, is an instance of an object type .T h e object type species how instances are to be allocated, how the data

how instances are to be allocated, how the data elds are to be dened, and how the standard set of virtual functions used for all objects are to be implemented. The standard functions implement operations such as mapping names to objects, closing and deleting, and applying security checks. Functions that are specic to a particular type of object are implemented by system services designed to operate on that particular object type, not by the methods specied in the object type. The parse()

methods specied in the object type. The parse() function is the most interesting of the standard object functions. It allows the implementation of an object. The le systems, the registry conguration store, and GUI objects are the most notable users of parse functions to extend the Windows name space. Returning to our Windows naming example, device objects used to represent lesystem volumes provide a parse function. This allows a name like Global?? C:foobar.doc to be interpreted as the le

Global?? C:foobar.doc to be interpreted as the le foobar.doc on the volume represented by the device object HarddiskVolume2 .W ec a ni l l u s t r a t e how naming, parse functions, objects, and handles work together by looking at the steps to open the le in Windows:846 Chapter 19 Windows 7 1.An application requests that a le named C:foobar.doc be opened. 2.The object manager nds the device object HarddiskVolume2 ,l o o k su p the parse procedure IopParseDevice from the objects type, and invokes

IopParseDevice from the objects type, and invokes it with the les name relative to the root of the le system. 3.IopParseDevice() allocates a le object and passes it to the le system, which lls in the details of how to access C:foobar.doc on the volume. 4.When the le system returns, IopParseDevice() allocates an entry for the le object in the handle table for t he current process and returns the handle to the application. If the le cannot successfully be opened, IopParseDevice() deletes the le

be opened, IopParseDevice() deletes the le object it allocated and returns an error indication to the application. 19.3.3.2 Virtual Memory Manager The executive component that man ages the virtual address space, physical memory allocation, and paging is the virtual memory (VM)manager .T h e design of the VMmanager assumes that the underlying hardware supports virtualtophysical mapping, a paging mechanism, and transparent cache coherence on multiprocessor systems, as well as allowing multiple

systems, as well as allowing multiple pagetable entries to map to the same physical page frame. The VMmanager in Windows uses a pagebased management scheme with page sizes of 4 KBand 2 MBon AMD 64 and IA32compatible processors and 8 KBon the IA64. Pages of data allocated to a process that are not in physical memory are either stored in the paging les on disk or mapped directly to a regular le on a local or remote le system. A page can also be marked zerollondemand, which initializes the page

marked zerollondemand, which initializes the page with zeros before it is allocated, thus erasing the previous contents. On IA32 processors, each process has a 4 GBvirtual address space. The upper 2 GBare mostly identical for all processes and are used by Windows in kernel mode to access the operatingsystem code and data structures. For the AMD 64 architecture, Windows provides a 8 TBvirtual address space for user mode out of the 16 EBsupported by existing hardware for each process. Key areas of

existing hardware for each process. Key areas of the kernelmode region that are not identical for all processes are the selfmap, hyperspace, and session space. The hardware references a processs page table using physical pageframe numbers, and the page table selfmap makes the contents of the processs page table accessible using virtual addresses. Hyperspace maps the current processs workingset information into the kernelmode address space. Session space is used to share an instance of the Win32

space is used to share an instance of the Win32 and other sessionspeci cd r i v e r sa m o n ga l lt h ep r o c e s s e si n the same terminalserver ( TS)s e s s i o n .D i f f e r e n t TSsessions share different instances of these drivers, yet they are mapped at the same virtual addresses. The lower, usermode region of virtual address space is specic to each process and accessible by both user and kernelmode threads. The Windows VM manager uses a twostep process to allocate virtual memory. The

a twostep process to allocate virtual memory. The rst step reserves one or more pages of virtual addresses in the processs virtual address space. The second step commits the allocation by assigning virtual memory space (physical memory or space in the paging les). Windows limits the amount of virtual memory space a process consumes by enforcing a quota on committed memory. A process decommits memory that it19.3 System Components 847 is no longer using to free up virtual memory space for use by

using to free up virtual memory space for use by other processes. The APIsu s e dt or e s e r v ev i r t u a la d d r e s s e sa n dc o m m i tv i r t u a lm e m o r yt a k ea handle on a process object as a parameter. This allows one process to control the virtual memory of another. Environmental subsystems manage the memory of their client processes in this way. Windows implements shared memory by dening a section object .A f t e r getting a handle to a section object, a process maps the

a handle to a section object, a process maps the memory of the section to ar a n g eo fa d d r e s s e s ,c a l l e da view .Ap r o c e s sc a ne s t a b l i s hav i e wo ft h ee n t i r e section or only the portion it needs. Windows allows sections to be mapped not just into the current process but into any process for which the caller has a handle. Sections can be used in many ways. A section can be backed by disk space either in the systempaging le or in a regular le (a memorymapped le ). A

le or in a regular le (a memorymapped le ). A section can be based ,m e a n i n gt h a ti ta p p e a r sa tt h es a m ev i r t u a la d d r e s sf o ra l l processes attempting to access it. Sections can also represent physical memory, allowing a 32bit process to access more physical memory than can t in its virtual address space. Finally, the memory protection of pages in the section can be set to readonly, readwrite, readwriteexecute, executeonly, no access, or copyonwrite. Lets look more

no access, or copyonwrite. Lets look more closely at the last two of these protection settings: Anoaccess page raises an exception if accessed. The exception can be used, for example, to check whether a faulty program iterates beyond the end of an array or simply to detect that the program attempted to access virtual addresses that are not committed to memory. User and kernelmode stacks use noaccess pages as guard pages to detect stack overows. Another use is to look for heap buffer overruns.

Another use is to look for heap buffer overruns. Both the user mode memory allocator and the special kernel allocator used by the device verier can be congured to map each allocation onto the end of a page, followed by a noaccess page to detect programming errors that access beyond the end of an allocation. The copyonwrite mechanism enables the VMmanager to use physical memory more efciently. When two processes want independent copies of data from the same section object, the VMmanager places a

the same section object, the VMmanager places a single shared copy into virtual memory and activates the copyonwrite property for that region of memory. If one of the processes tries to modify data in a copyonwrite page, the VMmanager makes a private copy of the page for the process. The virtual address translation in Windows uses a multilevel page table. For IA32 and AMD 64 processors, each process has a page directory that contains 512pagedirectory entries (PDEs)8b y t e si ns i z e .E a c h

entries (PDEs)8b y t e si ns i z e .E a c h PDE points to a PTEtable that contains 512 pagetable entries (PTEs)8b y t e si ns i z e .E a c h PTEpoints to a4 KBpage frame in physical memory. For a variety of reasons, the hardware requires that the page directories or PTEtables at each level of a multilevel page table occupy a single page. Thus, the number of PDEso r PTEst h a t ti nap a g e determine how many virtual addresses are translated by that page. See Figure 19.3 for a diagram of this

that page. See Figure 19.3 for a diagram of this structure. The structure described so far can be used to represent only 1 GBof virtual address translation. For IA32, a second pagedirectory level is needed,848 Chapter 19 Windows 7 Page table entry 0Page table 0Page table entry 0 4 KB page4 KB page4 KB page4 KB pagePage table 511Page table entry 511Page table entry 511Page directory entry 0Page directory 0Page directory entry 0Page directory 3Page directory entry 511Page directory entry

3Page directory entry 511Page directory entry 511Pointer 0 Pointer 1 Pointer 2 Pointer 3Page directory pointer table Figure 19.3 Pagetable layout. containing only four entries, as shown in the diagram. On 64bit processors, more levels are needed. For AMD 64, Windows uses a total of four full levels. The total size of all pagetable pages needed to fully represent even a 32bit virtual address space for a process is 8 MB.T h e VMmanager allocates pages of PDEsa n d PTEs as needed and moves

pages of PDEsa n d PTEs as needed and moves pagetable pages to disk when not in use. The pagetable pages are faulted back into memory when referenced. We next consider how virtual addresses are translated into physical addresses on IA32compatible processors. A 2bit value can represent the values 0, 1, 2, 3. A 9bit value can represent values from 0 to 511; a 12bit value, values from 0 to 4,095. Thus, a 12bit value can select any byte within a 4KBpage of memory. A 9bit value can represent any of

of memory. A 9bit value can represent any of the 512 PDEso r PTEs in a page directory or PTEtable page. As shown in Figure 19.4, translating a virtual address pointer to a byte address in physical memory involves breaking the 32bit pointer into four values, starting from the most signicant bits: Two bits are used to index into the four PDEsa tt h et o pl e v e lo ft h ep a g e table. The selected PDE will contain the physical page number for each of the four pagedirectory pages that map 1 GBof

of the four pagedirectory pages that map 1 GBof the address space.PTR PTE index PDE index pa ge offset31 0 Figure 19.4 Virtualtophysical address translation on IA32.19.3 System Components 849 Nine bits are used to select another PDE,t h i st i m ef r o mas e c o n d  l e v e lp a g e directory. This PDE will contain the physical page numbers of up to 512 PTEtable pages. Nine bits are used to select one of 512 PTEsf r o mt h es e l e c t e d PTEtable page. The selected PTEwill contain the

d PTEtable page. The selected PTEwill contain the physical page number for the byte we are accessing. Twelve bits are used as the byte offset into the page. The physical address of the byte we are accessing is constructed by appending the lowest 12 bits of the virtual address to the end of the physical page number we found in the selected PTE. The number of bits in a physical address may be different from the number of bits in a virtual address. In the original IA32 architecture, the PTEand PDE

In the original IA32 architecture, the PTEand PDE were 32bit structures that had room for only 20 bits of physical page number, so the physical address size and the virtual address size were the same. Such systems could address only 4 GBof physical memory. Later, the IA32 was extended to the larger 64bit PTEsize used today, and the hardware supported 24bit physical addresses. These systems could support 64 GBand were used on server systems. Today, all Windows servers are based on either the AMD

all Windows servers are based on either the AMD 64 or the IA64 and support very, very large physical addressesmore than we can possibly use. (Of course, once upon a time 4 GBseemed optimistically large for physical memory.) To improve performance, the VMmanager maps the pagedirectory and PTEtable pages into the same contiguous region of virtual addresses in every process. This selfmap allows the VMmanager to use the same pointer to access the current PDE orPTEcorresponding to a particular

the current PDE orPTEcorresponding to a particular virtual address no matter what process is running. The selfmap for the IA32 takes a contiguous 8 MB region of kernel virtual address space; the AMD 64 selfmap occupies 512 GB. Although the selfmap occupies signicant address space, it does not require any additional virtual memory pages. It also allows the page tables pages to be automatically paged in and out of physical memory. In the creation of a selfmap, one of the PDEsi nt h et o p  l e v e

a selfmap, one of the PDEsi nt h et o p  l e v e lp a g ed i r e c t o r y refers to the pagedirectory page itself, forming a loop in the pagetable translations. The virtual pages are accessed if the loop is not taken, the PTEtable pages are accessed if the loop is taken once, the lowestlevel pagedirectory pages are accessed if the loop is taken twice, and so forth. The additional levels of page directories used for 64bit virtual memory are translated in the same way except that the virtual

translated in the same way except that the virtual address pointer is broken up into even more values. For the AMD 64, Windows uses four full levels, each of which maps 512 pages, or 999912  48 bits of virtual address. To avoid the overhead of translating every virtual address by looking up thePDE and PTE,p r o c e s s o r su s e translation lookaside buffer (TLB)hardware, which contains an associative memory cache for mapping virtual pages to PTEs. The TLB is part of the memorymanagement unit

PTEs. The TLB is part of the memorymanagement unit (MMU )within each processor. The MMU needs to walk (navigate the data structures of) the page table in memory only when a needed translation is missing from the TLB. The PDEsa n d PTEsc o n t a i nm o r et h a nj u s tp h y s i c a lp a g en u m b e r s .T h e y also have bits reserved for operatingsystem use and bits that control how the hardware uses memory, such as whether hardware caching should be used for850 Chapter 19 Windows 7 each page.

be used for850 Chapter 19 Windows 7 each page. In addition, the entries specify what kinds of access are allowed for both user and kernel modes. APDE can also be marked to say that it should function as a PTE rather than a PDE.O na IA32, the rst 11 bits of the virtual address pointer select a PDE in the rst two levels of translation. If the selected PDE is marked to act as a PTE,t h e nt h er e m a i n i n g2 1b i t so ft h ep o i n t e ra r eu s e da st h eo f f s e to f the byte. This results

s e da st h eo f f s e to f the byte. This results in a 2 MBsize for the page. Mixing and matching 4 KB and 2 MBpage sizes within the page table is easy for the operating system and can signicantly improve the performance of some programs by reducing how often the MMU needs to reload entries in the TLB,s i n c eo n e PDE mapping 2 MB replaces 512 PTEse a c hm a p p i n g4 KB. Managing physical memory so that 2 MBpages are available when needed is difcult, however, as they may continually be

is difcult, however, as they may continually be broken up into 4 KBpages, causing external fragmentation of memory. Also, the large pages can result in very signicant internal fragmentation .B e c a u s eo ft h e s ep r o b l e m s ,i ti s typically only Windows itself, along with large server applications, that use large pages to improve the performance of the TLB.T h e ya r eb e t t e rs u i t e dt od o so because operatingsystem and server applications start running when the system boots,

applications start running when the system boots, before memory has become fragmented. Windows manages physical memory by associating each physical page with one of seven states: free, zeroed, m odied, standby, bad, transition, or valid. Afreepage is a page that has no particular content. Azeroed page is a free page that has been zeroed out and is ready for immediate use to satisfy zeroondemand faults. Amodied page has been written by a process and must be sent to the disk before it is allocated

must be sent to the disk before it is allocated for another process. Astandby page is a copy of information already stored on disk. Standby pages may be pages that were not mod ied, modied pages that have already been written to the disk, or pages that were prefetched because they are expected to be used soon. Abad page is unusable because a hardware error has been detected. Atransition page is on its way in from disk to a page frame allocated in physical memory. Avalid page is part of the

in physical memory. Avalid page is part of the working set of one or more processes and is contained within these processes page tables. While valid pages are contained in processes page tables, pages in other states are kept in separate lists according to state type. The lists are constructed by linking the corresponding entries in the page frame number (PFN)database, which includes an entry for each physical memory page. The PFN entries also include information such as reference counts, locks,

information such as reference counts, locks, and NUMA information. Note that the PFN database represents pages of physical memory, whereas the PTEsr e p r e s e n tp a g e so fv i r t u a lm e m o r y . When the valid bit in a PTEis zero, hardware ignores all the other bits, and the VMmanager can dene them for its own use. Invalid pages can have a number of states represented by bits in the PTE.P a g e   l ep a g e st h a th a v en e v e r19.3 System Components 85163 V32 prot T PPage file31

System Components 85163 V32 prot T PPage file31 0Page file offset Figure 19.5 Pagele pagetable entry. The valid bit is zero. been faulted in are marked zeroon demand. Pages mapped through section objects encode a pointer to the appropriate section object. PTEs for pages that have been written to the page le contain enough information to locate the page on disk, and so forth. The structure of the pagele PTEis shown in Figure 19.5. The T, P , and V bits are all zero for this type of PTE.T h e

and V bits are all zero for this type of PTE.T h e PTEincludes 5 bits for page protection, 32 bits for pagele offset, and 4 bits to select the paging le. There are also 20 bits reserved for additional bookkeeping. Windows uses a perworkingset, leastrecentlyused ( LRU)r e p l a c e m e n t policy to take pages from processes as appropriate. When a process is started, it is assigned a default minimum working set size. The working set of each process is allowed to grow until the amount of remaining

is allowed to grow until the amount of remaining physical memory starts to run low, at which point the VMmanager starts to track the age of the pages in each working set. Eventually, when the available memory runs critically low, the VMmanager trims the working set to remove older pages. How old a page is depends not on how long it has been in memory but on when it was last referenced. This is determined by periodically making a pass through the working set of each proc ess and incrementing the

working set of each proc ess and incrementing the age for pages that have not been marked in the PTEas referenced since the last pass. When it becomes necessary to trim the working sets, the VMmanager uses heuristics to decide how much to trim from each process and then removes the oldest pages rst. Ap r o c e s sc a nh a v ei t sw o r k i n gs e tt r i m m e de v e nw h e np l e n t yo fm e m o r y is available, if it was given a hard limit on how much physical memory it could use. In Windows

how much physical memory it could use. In Windows 7, the VMmanager will also trim processes that are growing rapidly, even if memory is plentiful. This policy change signicantly improves the responsiveness of the system for other processes. Windows tracks working sets not only for usermode processes but also for the system process, which includes all the pageable data structures and code that run in kernel mode. Windows 7 created additional working sets for the system process and associated them

sets for the system process and associated them with particular categories of kernel memory; the le cache, kernel heap, and kernel code now have their own working sets. The distinct working sets allow the VMmanager to use different policies to trim the different categories of kernel memory.852 Chapter 19 Windows 7 The VMmanager does not fault in only the page immediately needed. Research shows that the memory referencing of a thread tends to have a locality property. That is, when a page is

have a locality property. That is, when a page is used, it is likely that adjacent pages will be referenced in the near future. (Think of iterating over an array or fetching sequential instructions that form the executable code for a thread.) Because of locality, when the VMmanager faults in a page, it also faults in a few adjacent pages. This prefetching tends to reduce the total number of page faults and allows reads to be clustered to improve IOperformance. In addition to managing committed

IOperformance. In addition to managing committed memory, the VMmanager manages each processs reserved memory, or virtual address space. Each process has an associated tree that describes the ranges of virtual addresses in use and what the uses are. This allows the VMmanager to fault in pagetable pages as needed. If the PTEfor a faulting address is uninitialized, the VMmanager searches for the address in the processs tree of virtual address descriptors (VA D s)and uses this information to ll in

(VA D s)and uses this information to ll in the PTEand retrieve the page. In some cases, a PTEtable page itself may not exist; such a page must be transparently allocated and initialized by the VMmanager. In other cases, the page may be shared as part of a section object, and the VAD will contain a pointer to that section object. The section object contains information on how to nd the shared virtual page so that the PTEcan be initialized to point at it directly. 19.3.3.3 Process Manager The

point at it directly. 19.3.3.3 Process Manager The Windows process manager provides services for creating, deleting, and using processes, threads, and jobs. It has no knowledge about parentchild relationships or process hierarchies; those renements are left to the particular environmental subsystem that owns the process. The process manager is also not involved in the scheduling of processes, other than setting the priorities and afnities in processes and threads when they are created. Thread

and threads when they are created. Thread scheduling takes place in the kernel dispatcher. Each process contains one or more threads. Processes themselves can be collected into larger units called job objects .T h eu s eo fj o bo b j e c t sa l l o w s limits to be placed on CPU usage, workingset size, and processor afnities that control multiple processes at once. Job objects are used to manage large datacenter machines. An example of process creation in the Win 32environment is as follows:

creation in the Win 32environment is as follows: 1.AW i n 32application calls CreateProcess() . 2.Am e s s a g ei ss e n tt ot h eW i n 32subsystem to notify it that the process is being created. 3.CreateProcess() in the original process then calls an APIin the process manager of the NTexecutive to actually create the process. 4.The process manager calls the object manager to create a process object and returns the object handle to Win 32. 5.Win 32calls the process manager again to create a

32calls the process manager again to create a thread for the process and returns handles to the new process and thread. The Windows APIsf o rm a n i p u l a t i n gv i r t u a lm e m o r ya n dt h r e a d sa n d for duplicating handles take a process handle, so subsystems can perform19.3 System Components 853 operations on behalf of a new process with out having to execute directly in the new processs context. Once a new process is created, the initial thread is created, and an asynchronous

the initial thread is created, and an asynchronous procedure call is delivered to the thread to prompt the start of execution at the usermode image loader. The loader is inntdll.dll ,w h i c hi sal i n kl i b r a r ya u t o m a t i c a l l ym a p p e di n t oe v e r yn e w l y created process. Windows also supports a UNIX fork() style of process creation in order to support the POSIX environmental subsystem. Although the Win 32 environment calls the process mana ger directly from the client

the process mana ger directly from the client process, POSIX uses the crossprocess nature of the Windows APIst oc r e a t et h en e wp r o c e s s from within the subsystem process. The process manager relies on the asynchronous procedure calls ( APCs) implemented by the kernel layer. APCsa r eu s e dt oi n i t i a t et h r e a de x e c u t i o n , suspend and resume threads, access thread registers, terminate threads and processes, and support debuggers. The debugger support in the process

debuggers. The debugger support in the process manager includes the APIst os u s p e n d and resume threads and to create threads that begin in suspended mode. There are also processmanager APIs that get and set a threads register context and access another processs virtual memory. Thre ads can be created in the current process; they can also be injected into another process. The debugger makes use of thread injection to execute code within a process being debugged. While running in the

a process being debugged. While running in the executive, a thread can temporarily attach to a different process. Thread attach is used by kernel worker threads that need to execute in the context of the process originating a work request. For example, the VMmanager might use thread attach when it needs access to a processs working set or page tables, and the IOmanager might use it in updating the status variable in a process for asynchronous IOoperations. The process manager also supports

IOoperations. The process manager also supports impersonation .E a c ht h r e a dh a sa n associated security token . When the login process authenticates a user, the security token is attached to the users process and inherited by its child processes. The token contains the security identity (SID)of the user, the SIDso f the groups the user belongs to, the privileges the user has, and the integrity level of the process. By default, all threads within a process share a common token, representing

a process share a common token, representing the user and the applic ation that started the process. However, a thread running in a process with a security token belonging to one user can set at h r e a d  s p e c i  ct o k e nb e l o n g i n gt oa n o t h e ru s e rt oi m p e r s o n a t et h a tu s e r . The impersonation facility is fundamental to the clientserver RPC model, where services must act on behalf of a variety of clients with different security IDs. The right to impersonate a user

security IDs. The right to impersonate a user is most often delivered as part of an RPC connection from a client process to a server process. Impersonation allows the server to access system services as if it were the client in order to access or create objects and les on behalf of the client. The server process must be trustworthy and must be carefully written to be robust against attacks. Otherwise, one client could take over a server process and then impersonate any user who made a subsequent

then impersonate any user who made a subsequent client request. 19.3.3.4 Facilities for ClientServer Computing The implementation of Windows uses a clientserver model throughout. The environmental subsystems are servers that implement particular operating system personalities. Many other services, such as user authentication, network854 Chapter 19 Windows 7 facilities, printer spooling, web services, network le systems, and plug andplay, are also implemented using this model. To reduce the

also implemented using this model. To reduce the memory footprint, multiple services are often collected into a few processes running thesvchost.exe program. Each service is loaded as a dynamiclink library (DLL), which implements the service by relying on the usermode threadpool facilities to share threads and wait for messages (see Section 19.3.3.3). The normal implementation paradigm for clientserver computing is to use RPCs to communicate requests. The Win 32 API supports a standard RPC

requests. The Win 32 API supports a standard RPC protocol, as described in Section 19.6.2.7. RPC uses multiple transports (for example, named pipes and TCPIP)a n dc a nb eu s e dt oi m p l e m e n t RPCsb e t w e e n systems. When an RPC always occurs between a client and server on the local system, the advanced local procedure call facility ( ALPC )c a nb eu s e da st h e transport. At the lowest level of the system, in the implementation of the environmental systems, and for services that must

environmental systems, and for services that must be available in the early stages of booting, RPC is not available. Instead, native Windows services use ALPC directly. ALPC is a messagepassing mechanism. The server process publishes a globally visible connectionport object. When a client wants services from as u b s y s t e mo rs e r v i c e ,i to p e n sah a n d l et ot h es e r v e r  sc o n n e c t i o n  p o r t object and sends a connection request to the port. The server creates a channel

request to the port. The server creates a channel and returns a handle to the client. The channel consists of a pair of private communication ports: one for clienttoserver messages and the other for servertoclient messages. Communication channels support a callback mechanism, so the client and server can accept requests when they would normally be expecting a reply. When an ALPC channel is created, one of three messagepassing techniques is chosen. 1.The rst technique is suitable for small to

1.The rst technique is suitable for small to medium messages (up to 63 KB). In this case, the ports message queue is used as intermediate storage, and the messages are copied from one process to the other. 2.The second technique is for lar ger messages. In this case, a shared memory section object is created for the channel. Messages sent through the ports message queue contain a pointer and size information referring to the section object. This avoids the need to copy large messages. The sender

avoids the need to copy large messages. The sender places data into the shared s ection, and the receiver views them directly. 3.The third technique uses APIst h a tr e a da n dw r i t ed i r e c t l yi n t oap r o c e s s  s address space. ALPC provides functions and synchronization so that a server can access the data in a client. This technique is normally used by RPC to achieve higher performance for specic scenarios. The Win 32window manager uses its own form of message passing, which is

uses its own form of message passing, which is independent of the executive ALPC facilities. When a client asks for a connection that uses windowmanager messaging, the server sets up three objects: (1) a dedicated server thread to handle requests, (2) a 64 KBshared section object, and (3) an eventpair object. An eventpair object is a synchronization object used by the Win 32subsystem to provide notication when the client thread19.3 System Components 855 has copied a message to the Win 32server,

855 has copied a message to the Win 32server, or vice versa. The section object is used to pass the messages, and the eventpair object provides synchronization. Windowmanager messaging has several advantages: The section object eliminates message copying, since it represents a region of shared memory. The eventpair object eliminates the over head of using the port object to pass messages containing pointers and lengths. The dedicated server thread eliminates th eo v e r h e a do fd e t e r m i n

eliminates th eo v e r h e a do fd e t e r m i n i n gw h i c h client thread is calling the server, since there is one server thread per client thread. The kernel gives scheduling prefere nce to these dedicated server threads to improve performance. 19.3.3.5 IO Manager The IOmanager is responsible for managing le systems, device drivers, and network drivers. It keeps track of which device drivers, lter drivers, and le systems are loaded, and it also manages buffers for IO requests. It works

it also manages buffers for IO requests. It works with the VMmanager to provide memorymapped le IOand controls the Windows cache manager, which handles caching for the entire IOsystem. The IOmanager is fundamentally asynchronous, providing synchronous IOby explicitly waiting for an IOoperation to complete. The IOmanager provides several models of asynchronous IOcompletion, including setting of events, updating of a status variable in the calling process, delivery of APCst oi n i t i a t i n g

process, delivery of APCst oi n i t i a t i n g threads, and use of IOcompletion ports, which allow a single thread to process IOcompletions from many other threads. Device drivers are arranged in a list for each device (called a driver or IOstack). A driver is represented in the system as a driver object .B e c a u s ea single driver can operate on multiple devices, the drivers are represented in the IOstack by a device object ,w h i c hc o n t a i n sal i n kt ot h ed r i v e ro b j e c t .

t a i n sal i n kt ot h ed r i v e ro b j e c t . The IOmanager converts the requests it rec eives into a standard form called anIOrequest packet (IRP).I tt h e nf o r w a r d st h e IRPto the rst driver in the targeted IOstack for processing. After a driver processes the IRP,i tc a l l st h e IOmanager either to forward the IRPto the next driver in the stack or, if all processing is nished, to complete the operation represented by the IRP. The IOrequest may be completed in a context different

IOrequest may be completed in a context different from the one in which it was made. For example, if a driver is performing its part of an IO operation and is forced to block for an extended time, it may queue the IRPto a worker thread to continue processing in the system context. In the original thread, the driver returns a status indicating that the IOrequest is pending so that the thread can continue executing in parallel with the IOoperation. An IRPmay also be processed in interruptservice

An IRPmay also be processed in interruptservice routines and completed in an arbitrary process context. Because some nal processing may need to take place in the context that initiated the IO,t h e IOmanager uses an APC to do nal IOcompletion processing in the process context of the originating thread. The IO stack model is very exible. As a driver stack is built, various drivers have the opportunity to insert themselves into the stack as lter drivers . Filter drivers can examine and potentially

. Filter drivers can examine and potentially modify each IOoperation. Mount856 Chapter 19 Windows 7 management, partition management, and disk striping and mirroring are all examples of functionality implemented using lter drivers that execute beneath the le system in the stack. Filesystem lter drivers execute above the le system and have been used to implement functionalities such as hierarchical storage management, single instancing of les for remote boot, and dynamic format conversion. Third

remote boot, and dynamic format conversion. Third parties also use lesystem lter drivers to implement virus detection. Device drivers for Windows are written to the Windows Driver Model (WDM ) specication. This model lays out all the requirements for device drivers, including how to layer lter drivers, share common code for handling power and plugandplay requests, build correct cancellation logic, and so forth. Because of the richness of the WDM ,w r i t i n gaf u l l WDM device driver for each

,w r i t i n gaf u l l WDM device driver for each new hardware device can involve a great deal of work. Fortunately, the portminiport model makes it unnecessary to do this. Within a class of similar devices, such as audio drivers, SATA devices, or Ethernet controllers, each instance of a device shares a common driver for that class, called a port driver .T h ep o r td r i v e ri m p l e m e n t st h es t a n d a r do p e r a t i o n sf o rt h ec l a s sa n d then calls devicespecic r outines in

l a s sa n d then calls devicespecic r outines in the devices miniport driver to implement devicespecic functionality. The TCPIPnetwork stack is implemented in this way, with the ndis.sys class driver implementing much of the network driver functionality and calling out to the network miniport drivers for specic hardware. Recent versions of Windows, includin gW i n d o w s7 ,p r o v i d ea d d i t i o n a l simplications for writing device drivers for hardware devices. Kernelmode drivers can now

for hardware devices. Kernelmode drivers can now be written using the KernelMode Driver Framework (KMDF ), which provides a simplied programming model for drivers on top of WDM . Another option is the UserMode Driver Framework (UMDF ). Many drivers do not need to operate in kernel mode, and it is easier to develop and deploy drivers in user mode. It also makes the system more reliable, because a failure in a usermode driver does not cause a kernelmode crash. 19.3.3.6 Cache Manager In many

a kernelmode crash. 19.3.3.6 Cache Manager In many operating systems, caching is done by the le system. Instead, Windows provides a centralized caching facility. The cache manager works closely with the VMmanager to provide cache s ervices for all components under the control of the IOmanager. Caching in Windows is based on les rather than raw blocks. The size of the cache changes dynamically according to how much free memory is available in the system. The cache manager maintains a private

the system. The cache manager maintains a private working set rather than sharing the system processs working set. The cache manager memorymaps les into kernel memory and then uses special interfaces to the VMmanager to fault pages into or trim them from this private working set. The cache is divided into blocks of 256 KB.E a c hc a c h eb l o c kc a nh o l da view (that is, a memorymapped region) of a le. Each cache block is described by a virtual address control block (VA C B )that stores the

address control block (VA C B )that stores the virtual address and le offset for the view, as well as the n umber of processes using the view. The VACB sr e s i d ei nas i n g l ea r r a ym a i n t a i n e db yt h ec a c h em a n a g e r . When the IO manager receives a les userlevel read request, the IO manager sends an IRPto the IOstack for the volume on which the le resides.19.3 System Components 857 For les that are marked as cacheable, the le system calls the cache manager to look up the

le system calls the cache manager to look up the requested data in its cached le views. The cache manager calculates which entry of that les VACB index array corresponds to the byte offset of the request. The entry either points to the view in the cache or is invalid. If it is invalid, the cache manager allocates a cache block (and the corresponding entry in the VACB array) and maps the view into the cache block. The cache manager then attempts to copy data from the mapped le to the callers

to copy data from the mapped le to the callers buffer. If the copy succeeds, the operation is completed. If the copy fails, it does so because of a page fault, which causes the VM manager to send a noncached read request to the IOmanager. The IOmanager sends another request down the driver stack, this time requesting a paging operation, which bypasses the cache manager and reads the data from the le directly into the page allocated for the cache manager. Upon completion, the VACB is set to point

manager. Upon completion, the VACB is set to point at the page. The data, now in the cache, are copied to the callers buffer, and the original IOrequest is completed. Figure 19.6 shows an overview of these operations. Ak e r n e l  l e v e lr e a do p e r a t i o ni ss i m i l a r ,e x c e p tt h a tt h ed a t ac a nb ea c c e s s e d directly from the cache rather than being copied to a buffer in user space. To use lesystem metadata (data structures that describe the le system), the kernel uses

that describe the le system), the kernel uses the cache managers mapping interface to read the metadata. To modify the metadata, the le system uses the cache managers pinning interface. Pinning a page locks the page into a physicalmemory page frame so that the VMmanager cannot move the page or page it out. After updating the metadata, the le system asks the cache manager to unpin the page. A modied page is marked dirty, and so the VMmanager ushes the page to disk. To improve performance, the

the page to disk. To improve performance, the cache manager keeps a small history of read requests and from this history attempts to predict future requests. If the cache manager nds a pattern in the previous three requests, such as sequential access forward or backward, it prefetches data into the cache before the next cache managerVM managerprocess file system disk drivernoncached IOIO manager data copycached IO page faultIO Figure 19.6 File IO.858 Chapter 19 Windows 7 request is submitted by

Chapter 19 Windows 7 request is submitted by the application. In this way, the application may nd its data already cached and not need to wait for disk IO. The cache manager is also responsible for telling the VMmanager to ush the contents of the cache. The cache manage rs default behavior is writeback caching: it accumulates writes for 4 to 5 seconds and then wakes up the cache writer thread. When writethrough cach ing is needed, a process can set a ag when opening the le, or the process can

set a ag when opening the le, or the process can call an explicit cacheush function. A fastwriting process could potentially ll all the free cache pages before the cachewriter thread had a chance to wake up and ush the pages to disk. The cache writer prevents a process from ooding the system in the following way. When the amount of free cache memory becomes low, the cache manager temporarily blocks processes attempting to write data and wakes the cache writer thread to ush pages to disk. If the

cache writer thread to ush pages to disk. If the fastwriting process is actually a network redirector for a network le system, blocking it for too long could cause network transfers to time out and be retransmitted. This retransmission would waste network bandwidth. To pr event such waste, network redirectors can instruct the cache manager to limit the backlog of writes in the cache. Because a network le system needs to move data between a disk and the network interface, the cache manager also

and the network interface, the cache manager also provides a DMA interface to move the data directly. Moving data directly avoids the need to copy data through an intermediate buffer. 19.3.3.7 Security Reference Monitor Centralizing management of system entities in the object manager enables Windows to use a uniform mechanism to perform runtime access validation and audit checks for every useraccessible entity in the system. Whenever a process opens a handle to an object, the security reference

a handle to an object, the security reference monitor (SRM ) checks the processs security token and the objects accesscontrol list to see whether the process has the necessary access rights. The SRM is also responsible for manipulating the privileges in security tokens. Special privileges are required for users to perform backup or restore operations on le systems, debug processes, and so forth. Tokens can also be marked as being restricted in their privileges so that they cannot access objects

privileges so that they cannot access objects that are available to most users. Restricted tokens are used primarily to limit the damage that can be done by execution of untrusted code. The integrity level of the code executin gi nap r o c e s si sa l s or e p r e s e n t e d by a token. Integrity levels are a type of capability mechanism, as mentioned earlier. A process cannot modify an ob ject with an integrity level higher than that of the code executing in t he process, whatever other

the code executing in t he process, whatever other permissions have been granted. Integrity levels were introduced to make it harder for code that successfully attacks outwardfacing software, like Internet Explorer, to take over a system. Another responsibility of the SRM is logging security audit events. The Department of Defenses Common Criteria (the 2005 successor to the Orange Book) requires that a secure system have the ability to detect and log all attempts to access system resources so

and log all attempts to access system resources so that it can more easily trace attempts at unauthorized access. Because the SRM is responsible for making access checks, it generates most of the audit records in the securityevent log.19.3 System Components 859 19.3.3.8 PlugandPlay Manager The operating system uses the plugandplay (PnP)manager to recognize and adapt to changes in the hardware conguration. PnP devices use standard protocols to identify themselves to the system. The PnP manager

identify themselves to the system. The PnP manager automatically recognizes installed devices and detects changes in devices as the system operates. The manager also keeps track of hardware resources used by a device, as well as potential resources that could be used, and takes care of loading the appropriate drivers. This management of hardware resources primarily interrupts and IOmemory rangeshas the goal of determining a hardware conguration in which all devices are able to operate

in which all devices are able to operate successfully. The PnP manager handles dynamic reconguration as follows. First, it gets a list of devices from each bus driver (for example, PCIorUSB). It loads the installed driver (after nding one, if necessary) and sends an adddevice request to the appropriate driver for each device. The PnP manager then gures out the optimal resource assignments and sends a startdevice request to each driver specifying the resource assign ments for the device. If a

the resource assign ments for the device. If a device needs to be recongured, the PnP manager sends a querystop request, which asks the driver whether the device c an be temporarily disabled. If the driver can disable the device, then all pending operations are completed, and new operations are prevented from starting. Fin ally, the PnP manager sends a stop request and can then recongure the device with a new startdevice request. The PnP manager also supports other requests. For example, query

also supports other requests. For example, query remove ,w h i c ho p e r a t e ss i m i l a r l yt o querystop ,i se m p l o y e dw h e nau s e r is getting ready to eject a removable device, such as a USB storage device. The surpriseremove request is used when a device fails or, more likely, when a user removes a device without telling the system to stop it rst. Finally, the remove request tells the driver to stop using a device permanently. Many programs in the system are interested in the

Many programs in the system are interested in the addition or removal of devices, so the PnP manager supports notications. Such a notication, for example, gives GUI le menus the in formation they need to update their list of disk volumes when a new storage device is attached or removed. Installing devices often results in adding new services to the svchost.exe processes in the system. These services frequently set themselves up to run whenever the system boots and continue to run even if the

the system boots and continue to run even if the original device is never plugged into the system. Windows 7 introduced a servicetrigger mechanism in the service control manager (SCM ),w h i c hm a n a g e st h es y s t e ms e r v i c e s .W i t ht h i s mechanism, services can register themselves to start only when SCM receives a notication from the PnP manager that the device of interest has been added to the system. 19.3.3.9 Power Manager Windows works with the hardware to implement

Windows works with the hardware to implement sophisticated strategies for energy efciency, as described in Section 19.2.8. The policies that drive these strategies are implemented by the power manager .T h ep o w e rm a n a g e r detects current system conditions, such as the load on CPUso r IOdevices, and improves energy efciency by reducing the performance and responsiveness of the system when need is low. The power manager can also put the entire system into a very efcient sleep mode and can

system into a very efcient sleep mode and can even write all the contents of memory to disk and turn off the power to allow the system to go into hibernation .860 Chapter 19 Windows 7 The primary advantage of sleep is that the system can enter fairly quickly, perhaps just a few seconds after the lid closes on a laptop. The return from sleep is also fairly quick. The power is turned down low on the CPUsa n d IO devices, but the memory continues to be pow ered enough that its contents are not

to be pow ered enough that its contents are not lost. Hibernation takes considerably longer b ecause the entire contents of memory must be transferred to disk before the system is turned off. However, the fact that the system is, in fact, turned off is a signicant advantage. If there is a loss of power to the system, as when the battery is swapped on a laptop or a desktop system is unplugged, the saved system data will not be lost. Unlike shutdown, hibernation saves the currently running system

hibernation saves the currently running system so a user can resume where she left off, and because hibernation does not require power, a system can remain in hibernation indenitely. Like the PnP manager, the power manager provides notications to the rest of the system about changes in the power state. Some applications want to know when the system is about to be shut down so they can start saving their states to disk. 19.3.3.10 Registry Windows keeps much of its conguration information in

keeps much of its conguration information in internal databases, called hives ,t h a ta r em a n a g e db yt h eW i n d o w sc o n  g u r a t i o nm a n a g e r ,w h i c h is commonly known as the registry .T h e r ea r es e p a r a t eh i v e sf o rs y s t e m information, default user preferences, software installation, security, and boot options. Because the information in the system hive is required to boot the system, the registry manager is implemented as a component of the executive. The

implemented as a component of the executive. The registry represents the conguration state in each hive as a hierarchical namespace of keys (directories), each of which can contain a set of typed values, such as UNICODE string, ANSI string, integer, or untyped binary data. In theory, new keys and values are created and initialized as new software is installed; then they are modied to reect changes in the conguration of that software. In practice, the registry is often used as a generalpurpose

the registry is often used as a generalpurpose database, as an interprocesscommunication mechanism, and for many other such inventive purposes. Restarting applications, or even the system, every time a conguration change was made would be a nuisance. Instead, programs rely on various kinds of notications, such as those provided by the PnP and power managers, to learn about changes in the system conguration. The registry also supplies notications; it allows threads to register to be notied when

it allows threads to register to be notied when changes are made to some part of the registry. The threads can thus detect and adapt to conguration changes recorded in the registry itself. Whenever signicant changes are made to the system, such as when updates to the operating system or drivers are installed, there is a danger that the conguration data may be corrupted (for example, if a working driver is replaced by a nonworking driver or an application fails to install correctly and leaves

application fails to install correctly and leaves partial information in the registry). Windows creates a system restore point before making such changes. The restore point contains a copy of the hives before the change and can be used to return to this version of the hives and thereby get a corrupted system working again.19.3 System Components 861 To improve the stability of the registry conguration, Windows added a transaction mechanism beginning with Windows Vista that can be used to prevent

with Windows Vista that can be used to prevent the registry from being partially updated with a collection of related conguration changes. Registry transactions can be part of more general transactions administered by the kernel transaction manager (KTM ),w h i c h can also include lesystem transactions. KTM transactions do not have the full semantics found in normal database transactions, and they have not supplanted the system restore facility for recovering from damage to the registry

for recovering from damage to the registry conguration caused by software installation. 19.3.3.11 Booting The booting of a Windows PCbegins when the hardware powers on and rmware begins executing from ROM .I no l d e rm a c h i n e s ,t h i s r m w a r ew a s known as the BIOS ,b u tm o r em o d e r ns y s t e m su s e UEFI (the Unied Extensible Firmware Interface), which is faster and more general and makes better use of the facilities in contemporary processors. The rmware runs poweron

contemporary processors. The rmware runs poweron selftest (POST )diagnostics; identies many of the devices attached to the system and initializes them to a clean, powerup state; and then builds the description used by the advanced conguration and power interface (ACPI ).N e x t ,t h e rmware nds the system disk, loads the Windows bootmgr program, and begins executing it. In a machine that has been hibernating, the winresume program is loaded next. It restores the running system from disk, and

It restores the running system from disk, and the system continues execution at the point it had reached right before hibernating. In a machine that has been shut down, the bootmgr performs further initialization of the system and then loads winload . This program loads hal.dll ,t h ek e r n e l (ntoskrnl.exe ), any drivers needed in booting, and the system hive. winload then transfers execution to the kernel. The kernel initializes itself and creates two processes. The system process contains

creates two processes. The system process contains all the internal kernel wo rker threads and never executes in user mode. The rst usermode process created is SMSS , for session manager subsystem , which is similar to the INIT (initialization) process in UNIX .SMSS performs further initialization of the system, including establishing the paging les, loading more device drivers, and managing the Windows sessions. Each session is used to represent a loggedon user, except for session 0 ,w h i c hi

a loggedon user, except for session 0 ,w h i c hi s used to run systemwide background services, such as LSASS and SERVICES . As e s s i o ni sa n c h o r e db ya ni n s t a n c eo ft h e CSRSS process. Each session other than 0 initially runs the WINLOGON process. This process logs on a user and then launches the EXPLORER process, which implements the Windows GUI experience. The following list itemizes some of these aspects of booting: SMSS completes system initialization and then starts up

completes system initialization and then starts up session 0 and the rst login session. WININIT runs in session 0 to initialize user mode and start LSASS ,SERVICES , and the local session manager, LSM. LSASS ,t h es e c u r i t ys u b s y s t e m ,i m p l e m e n t sf a c i l i t i e ss u c ha sa u t h e n t i c a t i o n of users.862 Chapter 19 Windows 7 SERVICES contains the service control manager, or SCM,w h i c hs u p e r v i s e s all background activities in the system, including usermode

activities in the system, including usermode services. A number of services will have registered to start when the system boots. Others will be started only on demand or when triggered by an event such as the arrival of a device. CSRSS is the Win 32environmental subsystem process. It is started in every sessionunlike the POSIX subsystem, which is started only on demand when a POSIX process is created. WINLOGON is run in each Windows session other than session 0 to log on au s e r . The system

than session 0 to log on au s e r . The system optimizes the boot process by prepaging from les on disk based on previous boots of the system. Disk access patterns at boot are also used to lay out system les on disk to reduce the number of IOoperations required. The processes necessary to start the system are reduced by grouping services into fewer processes. All of these approaches contribute to a dramatic reduction in system boot time. Of course, system boot time is less important than it once

system boot time is less important than it once was because of the sleep and hibernation capabilities of Windows. 19.4 Terminal Services and Fast User Switching Windows supports a GUIbased console that interfaces with the user via keyboard, mouse, and display. Most systems also support audio and video. Audio input is used by Windows voicerecognition software; voice recognition makes the system more convenient and increases its accessibility for users with disabilities. Windows 7 added support

users with disabilities. Windows 7 added support for multitouch hardware ,a l l o w i n g users to input data by touching the screen and making gestures with one or more ngers. Eventually, the videoinput capability, which is currently used for communication applications, is likely to be used for visually interpreting gestures, as Microsoft has demonstrated for its Xbox 360 Kinect product. Other future input experiences may evolve from Microsofts surface computer .M o s t often installed at

surface computer .M o s t often installed at public venues, such as hotels and conference centers, the surface computer is a table surface with special cameras underneath. It can track the actions of multiple users at once and recognize objects that are placed on top. The PCwas, of course, envisioned as a personal computer an inherently singleuser machine. Modern Window s, however, supports the sharing of a PC among multiple users. Each user that is logged on using the GUIhas a session created

is logged on using the GUIhas a session created to represent the GUIenvironment he will be using and to contain all the processes created to run his applications. Windows allows multiple sessions to exist at the same time on a single machine. However, Windows only supports a single console, consisting of all the monitors, keyboards, and mice connected to the PC.O n l yo n es e s s i o nc a nb ec o n n e c t e dt ot h ec o n s o l ea tat i m e .F r o mt h e logon screen displayed on the console,

r o mt h e logon screen displayed on the console, users can create new sessions or attach to an existing session that was previously created. This allows multiple users to share a single PCwithout having to log off and on between users. Microsoft calls this use of sessions fast user switching .19.5 File System 863 Users can also create new sessions, or connect to existing sessions, on one PCfrom a session running on another Windows PC.T h et e r m i n a ls e r v e r( TS) connects one of the

e r m i n a ls e r v e r( TS) connects one of the GUIwindows in a users local session to the new or existing session, called a remote desktop ,o nt h er e m o t ec o m p u t e r .T h em o s tc o m m o n use of remote desktops is for users to connect to a session on their work PC from their home PC. Many corporations use corporate terminalserver systems maintained in data centers to run all user sessions t hat access corporate resources, rather than allowing users to access those resources from

than allowing users to access those resources from the PCsi ne a c hu s e r so f  c e .E a c h server computer may handle many dozens of remotedesktop sessions. This is a form of thinclient computing, in which individual computers rely on a server for many functions. Relying o nd a t a  c e n t e rt e r m i n a ls e r v e r si m p r o v e s reliability, manageability, and security of the corporate computing resources. The TSis also used by Windows to implement remote assistance .Ar e m o t e

to implement remote assistance .Ar e m o t e user can be invited to share a session with the user logged on to the session on the console. The remote user can watch the users actions and even be given control of the desktop to help resolve computing problems. 19.5 File System The native le system in Windows is NTFS .I ti su s e df o ra l ll o c a lv o l u m e s . However, associated USBthumb drives, ash memory on cameras, and external disks may be formatted with the 32bit FAT le system for

may be formatted with the 32bit FAT le system for portability. FAT is am u c ho l d e r l e  s y s t e mf o r m a tt h a ti su n d e r s t o o db ym a n ys y s t e m sb e s i d e s Windows, such as the software running on cameras. A disadvantage is that the FATle system does not restrict le access to authorized users. The only solution for securing data with FATis to run an application to encrypt the data before storing it on the le system. In contrast, NTFS uses ACLst oc o n t r o la c c e s st

NTFS uses ACLst oc o n t r o la c c e s st oi n d i v i d u a l l e sa n ds u p p o r t s implicit encryption of individual les or entire volumes (using Windows BitLocker feature). NTFS implements many other features as well, including data recovery, fault tolerance, very large les and le systems, multiple data streams, UNICODE names, sparse les, journaling, volume shadow copies, and le compression. 19.5.1 NTFS Internal Layout The fundamental entity in NTFS is a volume. A volume is created by

entity in NTFS is a volume. A volume is created by the Windows logical disk management utility and is based on a logical disk partition. A volume may occupy a portion of a disk or an entire disk, or may span several disks. NTFS does not deal with individual sectors of a disk but instead uses clusters as the units of disk allocation. A cluster is a number of disk sectors that is a power of 2. The cluster size is congured when an NTFS le system is formatted. The default cluster size is based on

is formatted. The default cluster size is based on the volume size4 KBfor volumes larger than 2 GB.G i v e nt h es i z eo ft o d a y  sd i s k s ,i tm a ym a k es e n s et ou s ec l u s t e rs i z e s larger than the Windows defaults to achieve better performance, although these performance gains will come at the expense of more internal fragmentation. NTFS uses logical cluster numbers (LCNs)as disk addresses. It assigns them by numbering clusters from the beginning of the disk to the end. Using

from the beginning of the disk to the end. Using this864 Chapter 19 Windows 7 scheme, the system can calculate a physical disk offset (in bytes) by multiplying the LCN by the cluster size. A l ei n NTFS is not a simple byte stream as it is in UNIX ;r a t h e r ,i ti sa structured object consisting of typed attributes .E a c ha t t r i b u t eo fa l ei sa n independent byte stream that can be created, deleted, read, and written. Some attribute types are standard for all les, including the le name

are standard for all les, including the le name (or names, if the le has aliases, such as an MSDOS short name), the creation time, and the security descriptor that species the access control list. User data are stored in data attributes . Most traditional data les have an unnamed data attribute that contains all the les data. However, additional data streams can be created with explicit names. For instance, in Macintosh les stored on a Windows server, the resource fork is a named data stream.

server, the resource fork is a named data stream. The IProp interfaces of the Component Object Model ( COM )u s ean a m e dd a t as t r e a mt os t o r ep r o p e r t i e so no r d i n a r y les, including thumb nails of images. In general, attributes may be added as necessary and are accessed using a lename:attribute syntax. NTFS returns only the size of the unnamed attribute in response to lequery operations, such as when running the dircommand. Every le in NTFS is described by one or more

Every le in NTFS is described by one or more records in an array stored in a special le called the master le table ( MFT). The size of a record is determined when the le system is created; it ranges from 1 to 4 KB.S m a l la t t r i b u t e s are stored in the MFT record itself and are called resident attributes .L a r g e attributes, such as the unnamed bulk data, are called nonresident attributes and are stored in one or more contiguous extents on the disk. A pointer to each extent is stored

on the disk. A pointer to each extent is stored in the MFT record. For a small le, even the data attribute may t inside the MFT record. If a le has many attributesor if it is highly fragmented, so that many pointers are needed to point to all the fragments one record in the MFT might not be large enough. In this case, the le is described by a record called the base le record ,w h i c hc o n t a i n sp o i n t e r st o overow records that hold the additional pointers and attributes. Each le in an

additional pointers and attributes. Each le in an NTFS volume has a unique IDcalled a le reference .T h e l e reference is a 64bit quantity that consists of a 48bit le number and a 16bit sequence number. The le number is the record number (that is, the array slot) in the MFT that describes the le. The sequence number is incremented every time an MFT entry is reused. The sequence number enables NTFS to perform internal consistency checks, such as catching a stale reference to a deleted le after

catching a stale reference to a deleted le after the MFT entry has been reused for a new le. 19.5.1.1 NTFS B Tree As in UNIX ,t h e NTFS namespace is organized as a hierarchy of directories. Each directory uses a data structure called a B tree to store an index of the le names in that directory. In a B tree, the length of every path from the root of the tree to a leaf is the same, and the cost of reorganizing the tree is eliminated. The index root of a directory contains the top level of the B

of a directory contains the top level of the B tree. For a large directory, this top level contains pointers to disk exten ts that hold the remainder of the tree. Each entry in the directory contains the name and le reference of the le, as well as a copy of the update timestamp and le size taken from the les resident attributes in the MFT. Copies of this information are stored in the directory so that a directory listing can be efciently generated. Because all the le names, sizes, and update

Because all the le names, sizes, and update times are available from the directory itself, there is no need to gather these attributes from the MFT entries for each of the les.19.5 File System 865 19.5.1.2 NTFS Metadata The NTFS volumes metadata are all stored in les. The rst le is the MFT.T h e second le, which is used during recovery if the MFT is damaged, contains a copy of the rst 16 entries of the MFT.T h en e x tf e w l e sa r ea l s os p e c i a li n purpose. They include the les

s os p e c i a li n purpose. They include the les described below. The log le records all metadata updates to the le system. The volume le contains the name of the volume, the version of NTFS that formatted the volume, and a bit that tells whether the volume may have been corrupted and needs to be checked for consistency using the chkdsk program. The attributedenition table indicates which attribute types are used in the volume and what operations can be performed on each of them. The root

can be performed on each of them. The root directory is the toplevel directory in the lesystem hierarchy. The bitmap le indicates which clusters on a volume are allocated to les and which are free. The boot le contains the startup code for Windows and must be located at a particular disk address so that it can be found easily by a simple ROM bootstrap loader. The boot le also contains the physical address of the MFT. The badcluster le keeps track of any bad areas on the volume; NTFS uses this

of any bad areas on the volume; NTFS uses this record for error recovery. Keeping all the NTFS metadata in actual les has a useful property. As discussed in Section 19.3.3.6, the cache manager caches le data. Since all the NTFS metadata reside in les, these data can be cached using the same mechanisms used for ordinary data. 19.5.2 Recovery In many simple le systems, a power failure at the wrong time can damage the lesystem data structures so severely that the entire volume is scrambled. Many

severely that the entire volume is scrambled. Many UNIX le systems, including UFSbut not ZFS,s t o r er e d u n d a n tm e t a d a t a on the disk, and they recover from crashes by using the fsck program to check all the lesystem data structures and restore them forcibly to a consistent state. Restoring them often involves deletin gd a m a g e d l e sa n df r e e i n gd a t a clusters that had been written with user data but not properly recorded in the le systems metadata structures. This

in the le systems metadata structures. This checking can be a slow process and can cause the loss of signicant amounts of data. NTFS takes a different approach to lesystem robustness. In NTFS ,a l l l e  system datastructure updates are performed inside transactions. Before a data structure is altered, the transaction writes a log record that contains redo and undo information. After the data structure has been changed, the transaction writes a commit record to the log to signify that the

a commit record to the log to signify that the transaction succeeded. After a crash, the system can restore the lesystem data structures to ac o n s i s t e n ts t a t eb yp r o c e s s i n gt h el o gr e c o r d s , r s tr e d o i n gt h eo p e r a t i o n s for committed transactions and then undoing the operations for transactions866 Chapter 19 Windows 7 that did not commit successfully before the crash. Periodically (usually every 5s e c o n d s ) ,ac h e c k p o i n tr e c o r di sw r i t t

d s ) ,ac h e c k p o i n tr e c o r di sw r i t t e nt ot h el o g .T h es y s t e md o e sn o t need log records prior to the checkpoint to recover from a crash. They can be discarded, so the log le does not grow without bounds. The rst time after system startup that an NTFS volume is accessed, NTFS automatically performs lesystem recovery. This scheme does not guarantee that all the userle contents are correct after a crash. It ensures only that the lesystem data structures (the metadata les)

the lesystem data structures (the metadata les) are undamaged and reect some consistent state that existed prior to the crash. It would be possible to extend t he transaction scheme to cover user les, and Microsoft took some steps to do this in Windows Vista. The log is stored in the third metadata le at the beginning of the volume. It is created with a xed maximum size when the le system is formatted. It has two sections: the logging area ,w h i c hi sac i r c u l a rq u e u eo fl o gr e c o r

i c hi sac i r c u l a rq u e u eo fl o gr e c o r d s ,a n d therestart area ,w h i c hh o l d sc o n t e x ti n f o r m a t i o n ,s u c ha st h ep o s i t i o ni nt h e logging area where NTFS should start reading during a recovery. In fact, the restart area holds two copies of its information, so recovery is still possible if one copy is damaged during the crash. The logging functionality is provided by the logle service .I na d d i t i o n to writing the log records and performing r ecovery

writing the log records and performing r ecovery actions, the logle service keeps track of the free space in the log le. If the free space gets too low, the logle service queues pending transactions, and NTFS halts all new IO operations. After the inprogress operations complete, NTFS calls the cache manager to ush all data and then resets the log le and performs the queued transactions. 19.5.3 Security The security of an NTFS volume is derived from the Windows object model. Each NTFS le

from the Windows object model. Each NTFS le references a security descriptor, which species the owner of the le, and an accesscontrol list, which contains the access permissions granted or denied to each user or group listed. Early versions of NTFS used a separate security descriptor as an attribute of each le. Beginning with Windows 2000, the securitydescriptors attribute points to a shared copy, with a signicant savings in disk and caching space; ma ny, many les have identical security

space; ma ny, many les have identical security descriptors. In normal operation, NTFS does not enforce permissions on traversal of directories in le path names. However, for compatibility with POSIX ,t h e s e checks can be enabled. Traversal checks are inherently more expensive, since modern parsing of le path names uses prex matching rather than directory bydirectory parsing of path names. Prex matching is an algorithm that looks up strings in a cache and nds the entry with the longest

in a cache and nds the entry with the longest matchfor example, an entry for foobardirwould be a match for foobardir2 dir3 myfile . The prexmatching cache allows pathname traversal to begin much deeper in the tree, saving many steps. Enforcing traversal checks means that the users access must be checked at each directory level. For instance, a user might lack permission to traverse foobar,s os t a r t i n ga tt h ea c c e s sf o r foobardir would be an error.19.5 File System 867 19.5.4 Volume

be an error.19.5 File System 867 19.5.4 Volume Management and Fault Tolerance FtDisk is the faulttolerant disk driver for Windows. When installed, it provides several ways to combine multiple disk drives into one logical volume so as to improve performance, capacity, or reliability. 19.5.4.1 Volume Sets and RAID Sets One way to combine multiple disks is to concatenate them logically to form a large logical volume, as shown in Figure 19.7. In Windows, this logical volume, called a volume set ,c a

this logical volume, called a volume set ,c a nc o n s i s to fu pt o3 2p h y s i c a lp a r t i t i o n s .Av o l u m es e t that contains an NTFS volume can be extended without disturbance of the data already stored in the le system. The bitmap metadata on the NTFS volume are simply extended to cover the newly added space. NTFS continues to use the same LCN mechanism that it uses for a single physical disk, and the FtDisk driver supplies the mapping from a logicalvolume offset to the offset on

from a logicalvolume offset to the offset on one particular disk. Another way to combine multiple physical partitions is to interleave their blocks in roundrobin fashion to form a stripe set .T h i ss c h e m ei sa l s o called RAID level 0, or disk striping .( F o rm o r eo n RAID (redundant arrays of inexpensive disks), see Section 10.7.) FtDisk uses a stripe size of 64 KB.T h e rst 64 KBof the logical volume are stored in the rst physical partition, the second 64 KBin the second physical

partition, the second 64 KBin the second physical partition, and so on, until each partition has contributed 64 KBof space. Then, the allocation wraps around to the rst disk, allocating the second 64 KBblock. A stripe set forms one large logical volume, but the physical layout can improve the IObandwidth, because for al a r g e IO,a l lt h ed i s k sc a nt r a n s f e rd a t ai np a r a l l e l .W i n d o w sa l s os u p p o r t s RAID level 5, stripe set with parity, and RAID level 1,

level 5, stripe set with parity, and RAID level 1, mirroring. LCNs 0128000LCNs 128001783361disk 1 (2.5 GB) disk 2 (2.5 GB) disk C: (FAT) 2 GB logical drive D: (NTFS) 3 GB Figure 19.7 Volume set on two drives.868 Chapter 19 Windows 7 19.5.4.2 Sector Sparing and Cluster Remapping To deal with disk sectors that go bad, FtDisk uses a hardware technique called sector sparing, and NTFS uses a software technique called cluster remapping. Sector sparing is a hardware capability provided by many disk

is a hardware capability provided by many disk drives. When ad i s kd r i v ei sf o r m a t t e d ,i tc r e a t e sam a pf r o ml o g i c a lb l o c kn u m b e r st og o o d sectors on the disk. It also leaves extra sectors unmapped, as spares. If a sector fails, FtDisk instructs the disk drive to substitute a spare. Cluster remapping is a software technique performed by the le system. If a disk block goes bad, NTFS substitutes a different, unallocated block by changing any affected pointers in

block by changing any affected pointers in the MFT.NTFS also makes a note that the bad block should never be allocated to any le. When a disk block goes bad, the usual outcome is a data loss. But sector sparing or cluster remapping can be combined with faulttolerant volumes to mask the failure of a disk block. If a read fails, the system reconstructs the missing data by reading the mirror or by calculating the exclusive or parity in a stripe set with parity. The reconstructed data are stored in

with parity. The reconstructed data are stored in a new location that is obtained by sector sparing or cluster remapping. 19.5.5 Compression NTFS can perform data compression on individual les or on all data les in ad i r e c t o r y .T oc o m p r e s sa l e , NTFS divides the les data into compression units , which are blocks of 16 contiguous clusters. When a compression unit is written, a datacompression algorithm is applied. If the result ts into fewer than 16 clusters, the compressed version

fewer than 16 clusters, the compressed version is stored. When reading, NTFS can determine whether data have been compressed: if they have been, the length of the stored compression unit is less than 16 clusters. To improve performance when reading contiguous compression units, NTFS prefetches and decompresses ahead of the application requests. For sparse les or les that contain mostly zeros, NTFS uses another technique to save space. Clusters that co ntain only zeros because they have never

that co ntain only zeros because they have never been written are not actually alloc ated or stored on disk. Instead, gaps are left in the sequence of virtualcluster numbers stored in the MFT entry for the le. When reading a le, if NTFS nds a gap in the virtualcluster numbers, it just zerolls that portion of the callers buffer. This technique is also used byUNIX . 19.5.6 Mount Points, Symbolic Links, and Hard Links Mount points are a form of symbolic link specic to directories on NTFS that were

link specic to directories on NTFS that were introduced in Windows 2000. They provide a mechanism for organizing disk volumes that is more exible than the use of global names (like drive letters). A mount point is implemented as a symbolic link with associated data that contains the true volume name. Ultimately, mount points will supplant drive letters completely, but there will be a long transition due to the dependence of many applications on the driveletter scheme. Windows Vista introduced

the driveletter scheme. Windows Vista introduced support for a more general form of symbolic links, similar to those found in UNIX . The links can be absolute or relative, can point to objects that do not exist, and can point to both les and directories19.6 Networking 869 even across volumes. NTFS also supports hard links , where a single le has an entry in more than one directory of the same volume. 19.5.7 Change Journal NTFS keeps a journal describing all chan ges that have been made to the le

all chan ges that have been made to the le system. Usermode services can receive notications of changes to the journal and then identify what les have changed by reading from the journal. The search indexer service uses the change journal to identify les that need to be reindexed. The lereplication service uses it to identify les that need to be replicated across the network. 19.5.8 Volume Shadow Copies Windows implements the capability of bringing a volume to a known state and then creating a

a volume to a known state and then creating a shadow copy that can be used to back up a consistent view of the volume. This technique is known as snapshots in some other le systems. Making a shadow copy of a volume is a form of copyonwrite, where blocks modied after the shadow copy is created are stored in their original form in the copy. To achieve a con sistent state for the volume requires the cooperation of applications, since the system cannot know when the data used by the application are

know when the data used by the application are in a stable state from which the application could be safely restarted. The server version of Windows uses sha dow copies to efciently maintain old versions of les stored on le servers. This allows users to see documents stored on le servers as they existed at earlier points in time. The user can use this feature to recover les that were accidentally deleted or simply to look at ap r e v i o u sv e r s i o no ft h e l e ,a l lw i t h o u tp u l l i

r s i o no ft h e l e ,a l lw i t h o u tp u l l i n go u tb a c k u pm e d i a . 19.6 Networking Windows supports both peertopeer and clientserver networking. It also has facilities for network management. The networking components in Windows provide data transport, interprocess communication, le sharing across a network, and the ability to send print jobs to remote printers. 19.6.1 Network Interfaces To describe networking in Windows, we must rst mention two of the internal networking

we must rst mention two of the internal networking interfaces: the network device interface specication (NDIS )and thetransport driver interface (TDI).T h e NDIS interface was developed in 1989 by Microsoft and 3Com to separate network adapters from transport protocols so that either could be changed without affecting the other. NDIS resides at the interface between the datalink and network layers in the ISOmodel and enables many protocols to operate o ver many different network adapters. In

operate o ver many different network adapters. In terms of the ISOmodel, the TDIis the interface between the transport layer (layer 4) and the session layer (layer 5). This interface enables any sessionlayer component to use any available transport mechanism. (Similar reasoning led to the streams mechanism in UNIX .) The TDIsupports both connectionbased and connectionless transport and has functions to send any type of data.870 Chapter 19 Windows 7 19.6.2 Protocols Windows implements transport

7 19.6.2 Protocols Windows implements transport protocols as drivers. These drivers can be loaded and unloaded from the system dynamically, although in practice the system typically has to be rebooted after a change. Windows comes with several networking protocols. Next, we discuss a number of these protocols. 19.6.2.1 ServerMessage Block The servermessageblock (SMB )protocol was rst introduced in MSDOS 3.1. The system uses the protocol to send IOrequests over the network. The SMB protocol has

IOrequests over the network. The SMB protocol has four message types. Session control messages are commands that start and end a redirector connection to a shared resource at the server. A redirector uses File messages to access les at the server. Printer messages are used to send data to a remote print queue and to receive status information from the queue, and Message messages are used to communicate with another workstation. A version of the SMB protocol was published as the common Internet

SMB protocol was published as the common Internet le system (CIFS )and is supported on a number of operating systems. 19.6.2.2 Transmission Control ProtocolInternet Protocol The transmission control protocolInternet protocol ( TCPIP )s u i t et h a ti su s e d on the Internet has become the de facto standard networking infrastructure. Windows uses TCPIP to connect to a wide variety of operating systems and hardware platforms. The Windows TCPIP package includes the simple networkmanagement

package includes the simple networkmanagement protocol ( SNM ), the dynamic hostconguration proto col ( DHCP ), and the older Windows Internet name service ( WINS ). Windows Vista introduced a new implementation of TCPIP that supports both IPv4 and IPv6 in the same network stack. This new implementation also supports ofoading of the network stack onto advan ced hardware, to achieve very high performance for servers. Windows provides a software rewall that limits the TCP ports that can be used by

that limits the TCP ports that can be used by programs for network communication. Network rewalls are commonly implemented in routers and are a very important security measure. Having a rewall built into the operating system makes a hardware router unnecessary, and it also provides more integrated management and easier use. 19.6.2.3 PointtoPoint T unneling Protocol The pointtopoint tunneling protocol (PPTP )is a protocol provided by Windows to communicate between remoteaccess server modules

to communicate between remoteaccess server modules running on Windows server machines and other client systems that are connected over the Internet. The remoteaccess servers can encrypt data sent over the connection, and they support multiprotocol virtual private networks (VPN s) over the Internet. 19.6.2.4 HTTP Protocol The HTTP protocol is used to getput information using the World Wide Web. Windows implements HTTP using a kernelmode driver, so web servers can operate with a lowoverhead

so web servers can operate with a lowoverhead connection to the networking stack. HTTP is a19.6 Networking 871 fairly general protocol, which Windows makes available as a transport option for implementing RPC. 19.6.2.5 WebDistributed Authoring and Versioning Protocol Webdistributed authoring and versioning (Web DAV)i sa n HTTP based protocol for collaborative authoring across a network. Windows builds a Web DAV redirector into the le system. Being built directly into the le system enables Web

built directly into the le system enables Web DAV to work with other lesystem features, such as encryption. Personal les can then be stored securely in a public place. Because Web DAV uses HTTP , which is a getput protocol, Windows has to cache the les locally so programs can use read andwrite operations on parts of the les. 19.6.2.6 Named Pipes Named pipes are a connectionoriented messaging mechanism. A process can use named pipes to communicate with other processes on the same machine. Since

with other processes on the same machine. Since named pipes are accessed through the lesystem interface, the security mechanisms used for le objects also apply to named pipes. The SMB protocol supports named pipes, so named pipes can also be used for communication between processes on different systems. The format of pipe names follows the uniform naming convention (UNC ).A UNC name looks like a typical remote le name. The format is server name share name xyz,w h e r e server name identies a

share name xyz,w h e r e server name identies a server on the network; share name identies any resource that is made available to network users, such as directories, les, named pipes, and printers; and xyzis a normal le path name. 19.6.2.7 Remote Procedure Calls Ar e m o t ep r o c e d u r ec a l l( RPC)i sac l i e n t  s e r v e rm e c h a n i s mt h a te n a b l e sa n application on one machine to make a procedure call to code on another machine. The client calls a local procedurea stub

machine. The client calls a local procedurea stub routinethat packs its arguments into a message and sends them across the network to a particular server process. The clientside stub routine then blocks. Meanwhile, the server unpacks the message, calls the procedure, packs the return results into a message, and sends them back to the client stub. The client stub unblocks, receives the message, unpacks the results of the RPC,a n dr e t u r n st h e mt ot h e caller. This packing of arguments is

h e mt ot h e caller. This packing of arguments is sometimes called marshaling .T h ec l i e n t stub code and the descriptors necessary to pack and unpack the arguments for anRPC are compiled from a specication written in the Microsoft Interface Denition Language . The Windows RPC mechanism follows the widely used distributed computingenvironment standard for RPC messages, so programs written to use Windows RPCsa r eh i g h l yp o r t a b l e .T h e RPC standard is detailed. It hides many of

.T h e RPC standard is detailed. It hides many of the architectural differences among computers, such as the sizes of binary numbers and the order of bytes and bits in computer words, by specifying standard data formats for RPC messages.872 Chapter 19 Windows 7 19.6.2.8 Component Object Model The component object model (COM )is a mechanism for interprocess commu nication that was developed for Windows. COM objects provide a welldened interface to manipulate the data in the object. For instance,

manipulate the data in the object. For instance, COM is the infras tructure used by Microsofts object linking and embedding (OLE)technology for inserting spreadsheets into Microsoft Word documents. Many Windows services provide COM interfaces. Windows has a distributed extension called DCOM that can be used over a network utilizing RPC to provide a transparent method of developing distributed applications. 19.6.3 Redirectors and Servers In Windows, an application can use the Windows IO API to

an application can use the Windows IO API to access les from a remote computer as though they were local, provided that the remote computer is running a CIFS server such as those provided by Windows. A redirector is the clientside object that forwards IOrequests to a remote system, where they are satised by a server. For performance and security, the redirectors and servers run in kernel mode. In more detail, access to a remote le occurs as follows: 1.The application calls the IOmanager to

follows: 1.The application calls the IOmanager to request that a le be opened with a l en a m ei nt h es t a n d a r d UNC format. 2.The IOmanager builds an IOrequest packet, as described in Section 19.3.3.5. 3.The IOmanager recognizes that the access is for a remote le and calls a driver called a multiple universalnamingconvention provider (MUP ). 4.The MUP sends the IOrequest packet asynchronously to all registered redirectors. 5.Ar e d i r e c t o rt h a tc a ns a t i s f yt h er e q u e s tr

t o rt h a tc a ns a t i s f yt h er e q u e s tr e s p o n d st ot h e MUP .T oa v o i d asking all the redirectors the same question in the future, the MUP uses a cache to remember which redirector can handle this le. 6.The redirector sends the network request to the remote system. 7.The remotesystem network drivers receive the request and pass it to the server driver. 8.The server driver hands the request to the proper local lesystem driver. 9.The proper device driver is called to access the

9.The proper device driver is called to access the data. 10. The results are returned to the server driver, which sends the data back to the requesting redirector. The redirec tor then returns the data to the calling application via the IOmanager. As i m i l a rp r o c e s so c c u r sf o ra p p l i c a t i o n st h a tu s et h eW i n 32network API,r a t h e r than the UNC services, except that a module called a multiprovider router is used instead of a MUP . For portability, redirectors and

of a MUP . For portability, redirectors and servers use the TDI API for network transport. The requests themselves are expressed in a higherlevel protocol,19.6 Networking 873 which by default is the SMB protocol described in Section 19.6.2. The list of redirectors is maintained in the system hive of the registry. 19.6.3.1 Distributed File System UNC names are not always convenient, because multiple le servers may be available to serve the same content and UNC names explicitly include the name of

and UNC names explicitly include the name of the server. Windows supports a distributed lesystem (DFS)protocol that allows a network administrator to serve up les from multiple servers using a single distributed name space. 19.6.3.2 Folder Redirection and ClientSide Caching To improve the PCexperience for users who frequently switch among com puters, Windows allows administrators to give users roaming proles ,w h i c h keep users preferences and other settings on servers. Folder redirection is

other settings on servers. Folder redirection is then used to automatically store a users docu ments and other les on a server. This works well until one of the computers is no longer attached to the network, as when a user takes a laptop onto an airplane. To give users offline access to their redirected les, Windows uses clientside caching (CSC).CSC is also used when the computer is online to keep copies of the server les on the local machine for better performance .T h e l e sa r ep u s h e du

better performance .T h e l e sa r ep u s h e du pt ot h e server as they are changed. If the computer becomes disconnected, the les are still available, and the update of the server is deferred until the next time the computer is online. 19.6.4 Domains Many networked environments have natural groups of users, such as students in a computer laboratory at school or employees in one department in a business. Frequently, we want all the members of the group to be able to access shared resources on

the group to be able to access shared resources on their various computers in the group. To manage the global access rights within such groups, Windows uses the concept of ad o m a i n .P r e v i o u s l y ,t h e s ed o m a i n sh a dn or e l a t i o n s h i pw h a t s o e v e rt ot h e domainname system ( DNS)t h a tm a p sI n t e r n e th o s tn a m e st o IPaddresses. Now, however, they are closely related. Specically, a Windows domain is a group of Windows workstations and servers that share

of Windows workstations and servers that share a common sec urity policy and user database. Since Windows uses the Kerberos protocol for trust and authentication, a Windows domain is the same thing as a Kerberos realm. Windows uses a hierarchical approach for establishing trust relationships between related domains. The trust relationships are based on DNS and allow transitive trusts that can ow up and down the hierarchy. This approach reduces the number of trusts required forndomains from n(n1)

number of trusts required forndomains from n(n1) to O(n). The workstations in the domain trust the domain controller to give correct information about the access rights of each user (loaded into the users access token by LSASS ). All users retain the ability to restrict access to their own workstations, however, no matter what any domain controller may say.874 Chapter 19 Windows 7 19.6.5 Active Directory Active Directory is the Windows implementation of lightweight directory access protocol

of lightweight directory access protocol (LDAP )services. Active Directory stores the topology infor mation about the domain, keeps the domain based user and group accounts and passwords, and provides a domainbased store for Windows features that need it, such as Windows group policy .A d m i n i s t r a t o r su s eg r o u pp o l i c i e st o establish uniform standards for desktop preferences and software. For many corporate informationtechnology groups, uniformity drastically reduces the cost

groups, uniformity drastically reduces the cost of computing. 19.7 Programmer Interface TheWin 32 API is the fundamental interface to the capabilities of Windows. This section describes ve main aspects of the Win 32 API :a c c e s st ok e r n e lo b j e c t s , sharing of objects between processes, pr ocess management, interprocess com munication, and memory management. 19.7.1 Access to Kernel Objects The Windows kernel provides many services that application programs can use. Application

that application programs can use. Application programs obtain these services by manipulating kernel objects. A process gains access to a kernel object named XXXby calling the Create XXXfunction to open a handle to an instance of XXX.T h i sh a n d l ei s unique to the process. Depending on which object is being opened, if the Create () function fails, it may return 0, or it may return a special constant named INVALID HANDLE VALUE .Ap r o c e s sc a nc l o s ea n yh a n d l eb yc a l l i n gt h

sc a nc l o s ea n yh a n d l eb yc a l l i n gt h e CloseHandle() function, and the system may delete the object if the count of handles referencing the object in all processes drops to zero. 19.7.2 Sharing Objects between Processes Windows provides three ways to share objects between processes. The rst way is for a child process to inherit a handle to the object. When the parent calls the Create XXXfunction, the parent supplies a SECURITIES ATTRIBUTES structure with the bInheritHandle eld set

structure with the bInheritHandle eld set to TRUE . This eld creates an inheritable handle. Next, the child process is created, passing a value of TRUE to the CreateProcess() functions bInheritHandle argument. Figure 19.8 shows a code sample that creates a semaphore handle inherited by a child process. Assuming the child process knows which handles are shared, the parent and child can achieve interprocess communication through the shared objects. In the example in Figure 19.8, the child process

In the example in Figure 19.8, the child process gets the value of the handle from the rst commandline argument a nd then shares the semaphore with the parent process. The second way to share objects is for one process to give the object a name when the object is created and f or the second process to open the name. This method has two drawbacks: Windows does not provide a way to check whether an object with the chosen name already exists, and the object name space is global, without regard to

the object name space is global, without regard to the object type. For instance, two applications19.7 Programmer Interface 875 SECURITY ATTRIBUTES sa; sa.nlength  sizeof(sa); sa.lpSecurityDescriptor  NULL; sa.bInheritHandle  TRUE; Handle a semaphore  CreateSemaphore(sa, 1, 1, NULL); char comand line[132]; ostrstream ostring(command line, sizeof(command line)); ostring  a semaphore  ends; CreateProcess(another process.exe, command line, NULL, NULL, TRUE, . . .); Figure 19.8 Code enabling a child

TRUE, . . .); Figure 19.8 Code enabling a child to share an object by inheriting a handle. may create and share a single object named foowhen two distinct objects possibly of different typeswere desired. Named objects have the advantage that unrelated processes can readily share them. The rst process calls one of the Create XXXfunctions and supplies an a m ea sap a r a m e t e r .T h es e c o n dp r o c e s sg e t sah a n d l et os h a r et h eo b j e c t by calling OpenXXX() (orCreate XXX)w i t

b j e c t by calling OpenXXX() (orCreate XXX)w i t ht h es a m en a m e ,a ss h o w ni nt h e example in Figure 19.9. The third way to share objects is via the DuplicateHandle() function. This method requires some other method of interprocess communication to pass the duplicated handle. Given a handle to a process and the value of a handle within that process, a second process can get a handle to the same object and thus share it. An example of this method is shown in Figure 19.10. 19.7.3

of this method is shown in Figure 19.10. 19.7.3 Process Management In Windows, a process is a loaded instance of an application and a thread is an executable unit of code that can be s cheduled by the kernel dispatcher. Thus, ap r o c e s sc o n t a i n so n eo rm o r et h r e a d s .Ap r o c e s si sc r e a t e dw h e nat h r e a d in some other process calls the CreateProcess() API.T h i sr o u t i n el o a d s any dynamic link libraries used by the process and creates an initial thread in the

the process and creates an initial thread in the process. Additional threads can be created by the CreateThread() function. Each thread is created with its own stack, which defaults to 1 MB unless otherwise specied in an argument to CreateThread() .  Process A ... HANDLE a semaphore  CreateSemaphore(NULL, 1, 1, MySEM1); ...  Process B ... HANDLE b semaphore  OpenSemaphore(SEMAPHORE ALL ACCESS, FALSE, MySEM1); ... Figure 19.9 Code for sharing an object by name lookup.876 Chapter 19 Windows 7

an object by name lookup.876 Chapter 19 Windows 7  Process A wants to give Process B access to a semaphore  Process A HANDLE a semaphore  CreateSemaphore(NULL, 1, 1, NULL);  send the value of the semaphore to Process B  using a message or shared memory object ...  Process B HANDLE process aO p e n P r o c e s s ( P R O C E S S ALL ACCESS, FALSE, process id of A); HANDLE b semaphore; DuplicateHandle(process a, a semaphore, GetCurrentProcess(), b semaphore, 0, FALSE, DUPLICATE SAME ACCESS);  use b

0, FALSE, DUPLICATE SAME ACCESS);  use b semaphore to access the semaphore ... Figure 19.10 Code for sharing an object by passing a handle. 19.7.3.1 Scheduling Rule Priorities in the Win 32environment are based on the native kernel ( NT) scheduling model, but not all priority values may be chosen. The Win 32 API uses four priority classes: 1.IDLE PRIORITY CLASS (NTpriority level 4) 2.NORMAL PRIORITY CLASS (NTpriority level 8) 3.HIGH PRIORITY CLASS (NTpriority level 13) 4.REALTIME PRIORITY CLASS

(NTpriority level 13) 4.REALTIME PRIORITY CLASS (NTpriority level 24) Processes are typically members of the NORMAL PRIORITY CLASS unless the parent of the process was of the IDLE PRIORITY CLASS or another class was specied when CreateProcess was called. The priority class of a process is the default for all threads that execute in the process. It can be changed with theSetPriorityClass() function or by passing an argument to the START command. Only users with the increase scheduling priority

Only users with the increase scheduling priority privilege can move a process into the REALTIME PRIORITY CLASS .A d m i n i s t r a t o r sa n dp o w e ru s e r s have this privilege by default. When a user is running an interactive pr ocess, the system needs to schedule the processs threads to provide good responsiveness. For this reason, Windows has a special scheduling rule for processes in the NORMAL PRIORITY CLASS . Windows distinguishes between the process associated with the foreground

between the process associated with the foreground window on the screen and the other (background) processes. When a process moves into the foreground, Windows in creases the scheduling quantum for all its threads by a factor of 3; CPUbound threads in the foreground process will run three times longer than similar threads in background processes.19.7 Programmer Interface 877 19.7.3.2 Thread Priorities At h r e a ds t a r t sw i t ha ni n i t i a lp r i o r i t yd e t e r m i n e db yi t sc l a s

lp r i o r i t yd e t e r m i n e db yi t sc l a s s .T h ep r i o r i t y can be altered by the SetThreadPriority() function. This function takes an argument that species a priority relative to the base priority of its class: THREAD PRIORITY LOWEST :b a s e 2 THREAD PRIORITY BELOW NORMAL :b a s e 1 THREAD PRIORITY NORMAL :b a s e0 THREAD PRIORITY ABOVE NORMAL :b a s e1 THREAD PRIORITY HIGHEST :b a s e2 Two other designations are also used to adjust the priority. Recall from Section 19.3.2.2

adjust the priority. Recall from Section 19.3.2.2 that the kernel has two priority classes: 1631 for the real time class and 115 for the variable class. THREAD PRIORITY IDLE sets the priority to 16 for realtime threads and to 1 for variablepriority threads. THREAD PRIORITY TIME CRITICAL sets the priority to 31 for realtime threads and to 15 for variablepriority threads. As discussed in Section 19.3.2.2, the kernel adjusts the priority of a variable class thread dynamically depending on whether

class thread dynamically depending on whether the thread is IObound or CPU bound. The Win 32 API provides a method to disable this adjustment via SetProcessPriorityBoost() andSetThreadPriorityBoost() functions. 19.7.3.3 Thread Suspend and Resume At h r e a dc a nb ec r e a t e di na suspended state or can be placed in a suspended state later by use of the SuspendThread() function. Before a suspended thread can be scheduled by the kernel dispatcher, it must be moved out of the suspended state by

it must be moved out of the suspended state by use of the ResumeThread() function. Both functions set a counter so that if a thread is suspended twice, it must be resumed twice before it can run. 19.7.3.4 Thread Synchronization To synchronize concurrent access to shared objects by threads, the kernel pro vides synchronization objects, such as semaphores and mutexes. These are dis patcher objects, as discussed in Section 19.3.2.2. Threads can also synchronize with kernel services operating on

also synchronize with kernel services operating on kernel ob jectssuch as threads, processes, and lesbecause these are also dispatcher objects. Synchronization with ker nel dispatcher objects can be achieved by use of the WaitForSingleObject() and WaitForMultipleObjects() functions; these functions wait for one or more dispatcher objects to be signaled. Another method of synchronization is available to threads within the same process that want to execute code exclusively. The Win 32critical

to execute code exclusively. The Win 32critical section object is a usermode mutex object that can often be acquired and released without entering the kernel. On a multiprocessor, a Win 32critical section will attempt to spin while waiting for a critical section held by another thread to be released. If the spinning takes too long, the acquiring thread will allocate a kernel mutex and yield its CPU.C r i t i c a ls e c t i o n sa r ep a r t i c u l a r l ye f  c i e n tb e c a u s et h ek e r n

u l a r l ye f  c i e n tb e c a u s et h ek e r n e l mutex is allocated only when there is contention and then used only after878 Chapter 19 Windows 7 attempting to spin. Most mutexes in programs are never actually contended, so the savings are signicant. Before using a critical section, some thread in the process must call Ini tializeCriticalSection() .E a c ht h r e a dt h a tw a n t st oa c q u i r et h em u t e x calls EnterCriticalSection() and then later calls LeaveCriticalSec tion() to

and then later calls LeaveCriticalSec tion() to release the mutex. There is also a TryEnterCriticalSection() function, which attempts to acquire the mutex without blocking. For programs that want usermode readerwriter locks rather than a mutex, Win 32supports slim readerwriter (SRW )locks .SRW locks have APIs similar to those for critical sections, such as InitializeSRWLock ,AcquireS RWLockXXX ,a n d ReleaseSRWLockXXX ,w h e r e XXX is either Exclusive or Shared ,d e p e n d i n go nw h e t h e

or Shared ,d e p e n d i n go nw h e t h e rt h et h r e a dw a n t sw r i t ea c c e s so rj u s tr e a d access to the object protected by the lock. The Win 32 API also supports condition variables ,w h i c hc a nb eu s e dw i t he i t h e rc r i t i c a ls e c t i o n so r SRW locks. 19.7.3.5 Thread Pool Repeatedly creating and deleting threads can be expensive for applications and services that perform small amounts of work in each instantiation. The Win 32 thread pool provides usermode

The Win 32 thread pool provides usermode programs with three services: a queue to which work requests may be submitted (via the SubmitThreadpoolWork() function), an API that can be used to bind callbacks to waitable handles (RegisterWaitForSingleObject() ), and APIst ow o r kw i t ht i m e r s( Cre ateThreadpoolTimer() and WaitForThreadpoolTimerCallbacks() )a n d to bind callbacks to IOcompletion queues ( BindIoCompletionCallback() ). The goal of using a thread pool is to increase performance

of using a thread pool is to increase performance and reduce memory footprint. Threads are relatively expensive, and each processor can only be executing one thread at a time no matter how many threads are available. The thread pool attempts to reduce the number of runnable threads by slightly delaying work requests (reusing each thread for many requests) while providing enough threads to effectively utilize the machines CPUs. The wait and IOa n dt i m e r  c a l l b a c k APIsa l l o wt h et h

dt i m e r  c a l l b a c k APIsa l l o wt h et h r e a dp o o lt of u r t h e rr e d u c e the number of threads in a process, using far fewer threads than would be necessary if a process were to devote separate threads to servicing each waitable handle, timer, or completion port. 19.7.3.6 Fibers Aber is usermode code that is scheduled according to a userdened scheduling algorithm. Fibers are completely a usermode facility; the kernel is not aware that they exist. The ber mechanism uses Windows

that they exist. The ber mechanism uses Windows threads as if they were CPUst oe x e c u t et h e b e r s .F i b e r sa r ec o o p e r a t i v e l ys c h e d u l e d , meaning that they are never preempted but must explicitly yield the thread on which they are running. When a ber yie lds a thread, another ber can be scheduled on it by the runtime system (the programming language runtime code). The system creates a ber by calling either ConvertThreadToFiber() orCreateFiber() . The primary

orCreateFiber() . The primary difference between these functions is that CreateFiber() does not begin executing the ber that was created. To begin execution, the application must call SwitchToFiber() .T h ea p p l i c a t i o nc a n terminate a ber by calling DeleteFiber() .19.7 Programmer Interface 879 Fibers are not recommended for threads that use Win 32 API sr a t h e rt h a n standard Clibrary functions because of potential incompatibilities. Win 32user mode threads have a threadenvironment

Win 32user mode threads have a threadenvironment block (TEB)that contains numerous perthread elds used by the Win 32 API s. Fibers must share the TEBof the thread on which they are running. This can lead to problems when a Win 32interface puts state information into the TEB for one ber and then the information is overwritten by a different ber. Fibers are included in the Win 32 API to facilitate the porting of legacy UNIX applications that were written for a usermode thread model such as

were written for a usermode thread model such as Pthreads. 19.7.3.7 UserMode Scheduling ( UMS )a n d Conc RT An e wm e c h a n i s mi nW i n d o w s7 ,u s e r  m o d es c h e d u l i n g( UMS ), addresses several limitations of bers. First, recall that bers are unreliable for executing Win 32 API sb e c a u s et h e yd on o th a v et h e i ro w n TEBs. When a thread running a b e rb l o c k si nt h ek e r n e l ,t h eu s e rs c h e d u l e rl o s e sc o n t r o lo ft h e CPU for a time as the

o s e sc o n t r o lo ft h e CPU for a time as the kernel dispatcher takes over scheduling. Problems may result when bers change the kernel state of a thread, such as the priority or impersonation token, or when they start asynchronous IO. UMS provides an alternative model by recognizing that each Windows thread is actually two threads: a kernel thread ( KT)a n dau s e rt h r e a d( UT). Each type of thread has its own stack and its own set of saved registers. The KTand UTappear as a single

of saved registers. The KTand UTappear as a single thread to the programmer because UTsc a n never block but must always enter the kernel, where an implicit switch to the corresponding KTtakes place. UMS uses each UTsTEB to uniquely identify the UT.W h e na UTenters the kernel, an explicit switch is made to the KTthat corresponds to the UTidentied by the current TEB. The reason the kernel does not know which UTis running is that UTsc a ni n v o k eau s e r  m o d es c h e d u l e r , as bers do.

eau s e r  m o d es c h e d u l e r , as bers do. But in UMS ,t h es c h e d u l e rs w i t c h e s UTs, including switching the TEBs. When a UTenters the kernel, its KTmay block. When this happens, the kernel switches to a scheduling thread, which UMS calls a primary ,a n du s e s this thread to reenter the usermode scheduler so that it can pick another UT to run. Eventually, a blocked KTwill complete its operation and be ready to return to user mode. Since UMS has already reentered the

to user mode. Since UMS has already reentered the usermode scheduler to run a different UT,UMS queues the UTcorresponding to the completed KT to a completion list in user mode. When the usermode scheduler is choosing an e w UTto switch to, it can examine the completion list and treat any UTon the list as a candidate for scheduling. Unlike bers, UMS is not intended to be used directly by the program mer. The details of writing usermode schedulers can be very challenging, and UMS does not include

can be very challenging, and UMS does not include such a scheduler. Rather, the schedulers come from programming language libraries that build on top of UMS .M i c r o s o f t Visual Studio 2010 shipped with Concurrency Runtime ( ConcRT), a concurrent programming framework for C. ConcRTprovides a usermode scheduler together with facilities for decomposing programs into tasks, which can then be scheduled on the available CPUs.ConcRTprovides support for par for styles of constructs, as well as

for par for styles of constructs, as well as rudimentary resource management and task synchronization primitives. The key features of UMS are depicted in Figure 19.11.880 Chapter 19 Windows 7 NTOS executive Only primary thread runs in usermode Trap code switches to parked KT KT blocks  primary returns to usermode KT unblocks  parks  queue UT completionThread parkin g UT completion listkernel user Usermode schedulertrap codePrimary threadKT0 UT0 UT1 UT0KT1KT2KT0 blocks Figure 19.11 Usermode

UT0 UT1 UT0KT1KT2KT0 blocks Figure 19.11 Usermode scheduling. 19.7.3.8 Winsock Winsock is the Windows sockets API. Winsock is a sessionlayer interface that is largely compatible with UNIX sockets but has some added Windows extensions. It provides a standardized interface to many transport protocols that may have different addressing schemes, so that any Winsock application can run on any Winsockcompliant protocol stack. Winsock underwent a major update in Windows Vista to add tracing, IPv6

major update in Windows Vista to add tracing, IPv6 support, impersonation, new security APIs and many other features. Winsock follows the Windows Open System Architecture ( WOSA )m o d e l , which provides a standard service provider interface ( SPI)b e t w e e na p p l i c a t i o n s and networking protocols. Applications can load and unload layered protocols that build additional functionality, such as additional security, on top of the transport protocol layers. Winsock supports asynchronous

protocol layers. Winsock supports asynchronous operations and notications, reliable multicasting, secure sockets, and kernel mode sockets. There is also support for simpler usage models, like the WSAConnectByName() function, which accepts the target as strings specifying the name or IPaddress of the server and the service or port number of the destination port. 19.7.4 Interprocess Communication Using Windows Messaging Win 32applications handle interprocess c ommunication in several ways. One way

c ommunication in several ways. One way is by using shared kernel objects. Another is by using the Windows messaging facility, an approach that is particularly popular for Win 32 GUI applications. One thread can send a message to another thread or to a window by calling PostMessage() ,PostThreadMessage() ,SendMessage() , SendThreadMessage() ,o rSendMessageCallback() .Posting a message and sending a message differ in this way: the post routines are asynchronous; they return immediately, and the

are asynchronous; they return immediately, and the calling thread does not know when the message is actually delivered. The send routines are synchronous: they block the caller until the message has been delivered and processed.19.7 Programmer Interface 881  allocate 16 MB at the top of our address space void buf  VirtualAlloc(0, 0x1000000, MEM RESERVE  MEM TOP DOWN, PAGE READWRITE);  commit the upper 8 MB of the allocated space VirtualAlloc(buf  0x800000, 0x800000, MEM COMMIT, PAGE READWRITE);

0x800000, 0x800000, MEM COMMIT, PAGE READWRITE);  do something with the memory ...  now decommit the memory VirtualFree(buf  0x800000, 0x800000, MEM DECOMMIT);  release all of the allocated address space VirtualFree(buf, 0, MEM RELEASE); Figure 19.12 Code fragments for allocating virtual memory. In addition to sending a message, a thread can send data with the message. Since processes have separate address spaces, the data must be copied. The system copies data by calling SendMessage() to send

copies data by calling SendMessage() to send a message of type WM COPYDATA with a COPYDATASTRUCT data structure that contains the length and address of the data to be transferred. When the message is sent, Windows copies the data to a new block of memory and gives the virtual address of the new block to the receiving process. Every Win 32thread has its own input queue from which it receives messages. If a Win 32application does not call GetMessage() to handle events on its input queue, the queue

to handle events on its input queue, the queue lls up; and after about ve seconds, the system marks the application as Not Responding . 19.7.5 Memory Management The Win 32 API provides several ways for an application to use memory: virtual memory, memorymapped les, heaps, and threadlocal storage. 19.7.5.1 Virtual Memory An application calls VirtualAlloc() to reserve or commit virtual memory and VirtualFree() to decommit or release the memory. These functions enable the application to specify the

functions enable the application to specify the virtual address at which the memory is allocated. They operate on multiples of the memory page size. Examples of these functions appear in Figure 19.12. Ap r o c e s sm a yl o c ks o m eo fi t sc o m m i t t e dp a g e si n t op h y s i c a lm e m o r y by calling VirtualLock() .T h em a x i m u mn u m b e ro fp a g e sap r o c e s sc a nl o c k is 30, unless the process rst calls SetProcessWorkingSetSize() to increase the maximum workingset size.

to increase the maximum workingset size. 19.7.5.2 MemoryMapping Files Another way for an application to use memory is by memorymapping a le into its address space. Memory mapping is also a convenient way for two processes to share memory: both processes map the same le into their virtual memory. Memory mapping is a multistage process, as you can see in the example in Figure 19.13.882 Chapter 19 Windows 7  open the file or create it if it does not exist HANDLE hfile  CreateFile(somefile, GENERIC

exist HANDLE hfile  CreateFile(somefile, GENERIC READ  GENERIC WRITE, FILE SHARE READ  FILE SHARE WRITE, NULL, OPEN ALWAYS, FILE ATTRIBUTE NORMAL, NULL);  create the file mapping 8 MB in size HANDLE hmap  CreateFileMapping(hfile, PAGE READWRITE, SEC COMMIT, 0, 0x800000, SHM 1);  now get a view of the space mapped void buf  MapViewOfFile(hmap, FILE MAP ALL ACCESS, 0, 0, 0, 0x800000);  do something with the mapped file ...  now unmap the file UnMapViewOfFile(buf); CloseHandle(hmap);

the file UnMapViewOfFile(buf); CloseHandle(hmap); CloseHandle(hfile); Figure 19.13 Code fragments for memory mapping of a le. If a process wants to map some address space just to share a memory region with another process, no le is needed. The process calls CreateFileMap ping() with a le handle of 0xffffffff and a particular size. The resulting lemapping object can be shared by i nheritance, by name lookup, or by handle duplication. 19.7.5.3 Heaps Heaps provide a third way for applications to

Heaps provide a third way for applications to use memory, just as with malloc() and free() in standard C. A heap in the Win 32environment is ar e g i o no fr e s e r v e da d d r e s ss p a c e .W h e naW i n 32process is initialized, it is created with a default heap .S i n c em o s tW i n 32applications are multithreaded, access to the heap is synchronized to protect the heaps spaceallocation data structures from being damaged by concurrent updates by multiple threads. Win 32provides several

by multiple threads. Win 32provides several heapmanagement functions so that a process can allocate and manage a private heap. These functions are HeapCreate() ,Hea pAlloc() ,HeapRealloc() ,HeapSize() ,HeapFree() ,a n d HeapDestroy() . The Win 32 API also provides the HeapLock() andHeapUnlock() functions to enable a thread to gain exclusive access to a heap. Unlike VirtualLock() , these functions perform only synchronization; they do not lock pages into physical memory. The original Win 32heap

into physical memory. The original Win 32heap was optimized for efcient use of space. This led to signicant problems with fragmentation of the address space for larger server programs that ran for long periods of time. A new lowfragmentation heap (LFH)design introduced in Windows XPgreatly reduced the fragmentation problem. The Windows 7 heap manager automatically turns on LFH as appropriate. 19.7.5.4 ThreadLocal Storage Af o u r t hw a yf o ra p p l i c a t i o n st ou s em e m o r yi st h r o

p p l i c a t i o n st ou s em e m o r yi st h r o u g ha threadlocal storage (TLS )mechanism. Functions that rely on global or static data typically failPractice Exercises 883  reserve a slot for a variable DWORD var index  T1sAlloc();  set it to the value 10 T1sSetValue(var index, 10);  get the value int var T1sGetValue(var index);  release the index T1sFree(var index); Figure 19.14 Code for dynamic threadlocal storage. to work properly in a multithreaded environment. For instance, the C run

multithreaded environment. For instance, the C run time function strtok() uses a static variable to keep track of its current position while parsing a string. For two concurrent threads to execute strtok() correctly, they need separate current position variables. TLSprovides a way to maintain instances of variables that are global to the function being executed but not shared with any other thread. TLS provides both dynamic and static methods of creating threadlocal storage. The dynamic method

creating threadlocal storage. The dynamic method is illustrated in Figure 19.14. The TLSmechanism allocates global heap storage and attaches it to the thread environment block that Windows allocates to every usermode thread. The TEBis readily accessible by each thread and is used not just for TLS but for all the perthread state information in user mode. To use a threadlocal static variable, the application declares the variable as follows to ensure that every thread has its own private copy:

ensure that every thread has its own private copy: declspec(thread) DWORD cur pos  0 ; 19.8 Summary Microsoft designed Windows to be an extensible, portable operating system one able to take advantage of new techniques and hardware. W indows supports multiple operating environments and symmetric multiprocessing, including both 32bit and 64bit processors and NUMA computers. The use of kernel objects to provide basic services, along with support for client server computing, enables Window st os u

client server computing, enables Window st os u p p o r taw i d ev a r i e t yo fa p p l i c a  tion environments. Windows pr ovides virtual memory, integrated caching, and preemptive scheduling. It supports elaborate security mechanisms and includes internationalization features. Windows runs on a wide variety of computers, so users can choose and upgrade hardware to match their budgets and performance requirements without n eeding to alter the applications they run. Practice Exercises 19.1

the applications they run. Practice Exercises 19.1 What type of operating system is Windows? Describe two of its major features. 19.2 List the design goals of Windows. Describe two in detail.884 Chapter 19 Windows 7 19.3 Describe the booting process for a Windows system. 19.4 Describe the three main architectural layers of the Windows kernel. 19.5 What is the job of the object manager? 19.6 What types of services does the process manager provide? 19.7 What is a local procedure call? 19.8 What

19.7 What is a local procedure call? 19.8 What are the responsibilities of the IOmanager? 19.9 What types of networking does Windows support? How does Windows implement transport protocols? Describe two networking protocols. 19.10 How is the NTFS namespace organized? 19.11 How does NTFS handle data structures? How does NTFS recover from as y s t e mc r a s h ?W h a ti sg u a r a n t e e da f t e rar e c o v e r yt a k e sp l a c e ? 19.12 How does Windows allocate user memory? 19.13 Describe

does Windows allocate user memory? 19.13 Describe some of the ways in which an application can use memory via the Win 32 API . Exercises 19.14 Under what circumstances would one use the deferred procedure calls facility in Windows? 19.15 What is a handle, and how does a process obtain a handle? 19.16 Describe the management scheme of the virtual memory manager. How does the VMmanager improve performance? 19.17 Describe a useful application of the noaccess page facility provided in Windows. 19.18

noaccess page facility provided in Windows. 19.18 Describe the three techniques used for communicating data in a local procedure call. What settings are most conducive to the application of the different messagepassing techniques? 19.19 What manages caching in Windows? How is the cache managed? 19.20 How does the NTFS directory structure differ from the directory structure used in UNIX operating systems? 19.21 What is a process, and how is it managed in Windows? 19.22 What is the ber abstraction

in Windows? 19.22 What is the ber abstraction provided by Windows? How does it differ from the thread abstraction? 19.23 How does usermode scheduling ( UMS ) in Windows 7 differ from bers? What are some tradeoffs between bers and UMS ? 19.24 UMS considers a thread to have two parts, a UTand a KT.H o wm i g h ti t be useful to allow UTst oc o n t i n u ee x e c u t i n gi np a r a l l e lw i t ht h e i r KTs? 19.25 What is the performance tradeoff of allowing KTsa n d UTst oe x e c u t e on

of allowing KTsa n d UTst oe x e c u t e on different processors?Bibliography 885 19.26 Why does the selfmap occupy large amounts of virtual address space but no additional virtual memory? 19.27 How does the selfmap make it easy for the VMmanager to move the pagetable pages to and from disk? Where are the pagetable pages kept on disk? 19.28 When a Windows system hibernates, the system is powered off. Suppose you changed the CPU or the amount of RAM on a hibernating system. Do you think that

of RAM on a hibernating system. Do you think that would work? Why or why not? 19.29 Give an example showing how the use of a suspend count is helpful in suspending and resuming threads in Windows. Bibliographical Notes [Russinovich and Solomon (2009)] give an overview of Windows 7 and considerable technical detail about system internals and components. [Brown (2000)] presents details of the security architecture of Windows. The Microsoft Developer Network Library ( http:msdn.microsoft.com )

Network Library ( http:msdn.microsoft.com ) supplies a wealth of information on Windows and other Microsoft products, including documentation of all the published APIs. [Iseminger (2000)] provides a good reference on the Windows Active Directory. Detailed discussions of writing programs that use the Win 32 API appear in [Richter (1997)]. [Silberschatz et al. (2010)] supply a good discussion of B trees. The source code for a 2005 WRK version of the Windows kernel, together with a collection of

the Windows kernel, together with a collection of slides and other CRK curriculum materials, is available from www.microsoft.comWindowsAcademic for use by universities. Bibliography [Brown (2000)] K. Brown, Programming Windows Security ,A d d i s o n  W e s l e y (2000). [Iseminger (2000)] D. Iseminger, Active Directory Services for Microsoft Windows 2000. Technical Reference ,M i c r o s o f tP r e s s( 2 0 0 0 ) . [Richter (1997)] J. Richter, Advanced Windows ,M i c r o s o f tP r e s s( 1 9 9

Windows ,M i c r o s o f tP r e s s( 1 9 9 7 ) . [Russinovich and Solomon (2009)] M. E. Russinovich and D. A. Solomon, Win dows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition, Microsoft Press (2009). [Silberschatz et al. (2010)] A. Silberschatz, H. F. Korth, and S. Sudarshan, Database System Concepts, Sixth Edition, McGrawHill (2010).20CHAPTER Influential Operating Systems Now that you understand the fundamental concepts of operating systems ( CPU scheduling, memory

of operating systems ( CPU scheduling, memory management, processes, and so on), we are in a position to examine how these concepts have been applied in several older and highly inuential operating systems. Some of them (such as the XDS940 and the THE system) were oneofakind systems; others (such as OS360 )a r ew i d e l yu s e d . The order of presentation highlights the similarities and differences of the systems; it is not strictly chronological or ordered by importance. The serious student

or ordered by importance. The serious student of operating systems should be familiar with all these systems. In the bibliographical notes at the end of the chapter, we include references to further reading about these early systems. The papers, written by the designers of the systems, are important both for their technical content and for their style and avor. CHAPTER OBJECTIVES To explain how operatingsystem features migrate over time from large computer systems to smaller ones. To discuss the

computer systems to smaller ones. To discuss the features of several historically important operating systems. 20.1 Feature Migration One reason to study early architectures and operating systems is that a feature that once ran only on huge systems may eventually make its way into very small systems. Indeed, an examination of operating systems for mainframes and microcomputers shows that many features once available only on main frames have been adopted for microcomputers. The same

have been adopted for microcomputers. The same operatingsystem concepts are thus appropriate for various classes of computers: mainframes, minicomputers, microcomputers, and handhelds. To understand modern oper ating systems, then, you need to recognize the theme of feature migration and the long history of many operatingsystem features, as shown in Figure 20.1. Ag o o de x a m p l eo ff e a t u r em i g r a t i o ns t a r t e dw i t ht h eM u l t i p l e x e dI n f o r  mation and Computing

u l t i p l e x e dI n f o r  mation and Computing Services ( MULTICS )o p e r a t i n gs y s t e m . MULTICS was 887888 Chapter 20 Inuential Operating Systems mainframes1950 no sof t w are no sof t w arerossecorpitlum hctabcompilers time shareddis tributed syst ems resident monitorsfault tolerantnetworkedmultiuser no sof t w arecompilers no sof t w are i nt eract i vecompilers compilers interactive networkedtime sharedresident monitorsfault tolerantmultiuser networked clustered

tolerantmultiuser networked clustered multiusermultiprocess or multiprocess or1960 1970 MULTIC S1980 1990 2000 minicomputers desktop computers handheld computersUNIX UNIX networked UNIX smart phones2010 LINUX multiprocess or networked interactiveLINUX Figure 20.1 Migration of operatingsystem concepts and features. developed from 1965 to 1970 at the Massachusetts Institute of Technology ( MIT) as a computing utility . It ran on a large, complex mainframe computer (the GE 645). Many of the ideas

mainframe computer (the GE 645). Many of the ideas that were developed for MULTICS were subsequently used at Bell Laboratories (one of the o riginal partners in the development of MULTICS )i nt h ed e s i g no f UNIX .T h e UNIX operating system was designed around 1970 for a PDP11 minicomputer. Around 1980, the features of UNIX became the basis for UNIX like operating systems on microcomputers; and these features are included in several more recent operating systems for microcom puters, such as

operating systems for microcom puters, such as Microsoft Windows, Windows XP, and the Mac OS X operating system. Linux includes some of these same features, and they can now be found onPDAs. 20.2 Early Systems We turn our attention now to a historical overview of early computer systems. We should note that the history of computing starts far before computers with looms and calculators. We begin our discussion, however, with the computers of the twentieth century. Before the 1940s, computing

the twentieth century. Before the 1940s, computing devices were designed and implemented to perform specic, xed tasks. Modifying one of those tasks required a great deal of effort and manual labor. All that changed in the 1940s when Alan Turing and John von Neumann (and colleagues), both separately and together, worked on the idea of a more generalpurpose stored program computer. Such a machine20.2 Early Systems 889 has both a program store and a data store, where the program store provides

and a data store, where the program store provides instructions about what to do to the data. This fundamental computer concep tq u i c k l yg e n e r a t e dan u m b e ro f generalpurpose computers, but much of the history of these machines is blurred by time and the secrecy of their development during World War II. It is likely that the rst working storedprogram generalpurpose computer was the Manchester Mark 1, which ran successfully in 1949. The rst commercial computer the Ferranti Mark 1,

The rst commercial computer the Ferranti Mark 1, which went on sale in 1951was it offspring. Early computers were physically enormous machines run from consoles. The programmer, who was also the operator of the computer system, would write a program and then would operate the program directly from the operators console. First, the program would be loaded manually into memory from the front panel switches (one instruction at a time), from paper tape, or from punched cards. Then the appropriate

tape, or from punched cards. Then the appropriate buttons would be pushed to set the starting address and to start the execution of the program. As the program ran, the programmeroperator could monitor its execution by the display lights on the console. If errors were discovered, the programmer could halt the program, examine the contents of memory and registers, and debug the program directly from the console. Output was printed or was punched onto paper tape or cards for later printing. 20.2.1

paper tape or cards for later printing. 20.2.1 Dedicated Computer Systems As time went on, additional software and hardware were developed. Card readers, line printers, and magnetic tape became commonplace. Assemblers, loaders, and linkers were designed to ease the programming task. Libraries of common functions were created. Common functions could then be copied into a new program without having to be written again, providing software reusability. The routines that performed IOwere especially

The routines that performed IOwere especially important. Each new IO device had its own characteristics, requirin g careful programming. A special subroutinecalled a device driverwas written for each IOdevice. A device driver knows how the buffers, ags, registers, control bits, and status bits for a particular device should be used. Each type of device has its own driver. As i m p l et a s k ,s u c ha sr e a d i n gac h a r a c t e rf r o map a p e r  t a p er e a d e r ,m i g h t involve

map a p e r  t a p er e a d e r ,m i g h t involve complex sequences of devicespeci co p e r a t i o n s .R a t h e rt h a nw r i t i n g the necessary code every time, the devic ed r i v e rw a ss i m p l yu s e df r o mt h e library. Later, compilers for FORTRAN ,COBOL ,a n do t h e rl a n g u a g e sa p p e a r e d , making the programming task much easier but the operation of the computer more complex. To prepare a FORTRAN program for execution, for example, the programmer would rst need to

for example, the programmer would rst need to load the FORTRAN compiler into the computer. The compiler was normally kept on magnetic tape, so the proper tape would need to be mounted on a tape drive. The program would be read through the card reader and written onto another tape. The FORTRAN compiler produced assemblylanguage output, which then had to be assembled. This procedure required mounting another tape with the assembler. The output of the assembler would need to be linked to supporting

assembler would need to be linked to supporting library routines. Finally, the binary object form of the program would be ready to execute. It could be loaded into memory and debugged from the console, as before.890 Chapter 20 Inuential Operating Systems As i g n i  c a n ta m o u n to f setup time could be involved in the running of a job. Each job consisted of many separate steps: 1.Loading the FORTRAN compiler tape 2.Running the compiler 3.Unloading the compiler tape 4.Loading the assembler

the compiler tape 4.Loading the assembler tape 5.Running the assembler 6.Unloading the assembler tape 7.Loading the object program 8.Running the object program If an error occurred during any step, the programmeroperator might have to start over at the beginning. Each job step might involve the loading and unloading of magnetic tapes, paper tapes, and punch cards. The job setup time was a real problem. While tapes were being mounted or the programmer was operating the console, the CPU sat idle.

was operating the console, the CPU sat idle. Remember that, in the early days, few computers were available, and they were expensive. A computer might have cost millions of dollars, not including the operational costs of power, cooling, programmers, and so on. Thus, computer time was extremely valuable, and owners wanted their computers to be used as much as possible. They needed high utilization to get as much as they could from their investments. 20.2.2 Shared Computer Systems The solution was

20.2.2 Shared Computer Systems The solution was twofold. First, a professional computer operator was hired. The programmer no longer operated the machine. As soon as one job was nished, the operator could start the next. Since the operator had more experience with mounting tapes than a programmer, setup time was reduced. The programmer provided whatever card so rt a p e sw e r en e e d e d ,a sw e l la sa short description of how the job was to be run. Of course, the operator could not debug an

be run. Of course, the operator could not debug an incorrect program at the console, since the operator would not understand the program. Therefore, in the case of program error, a dump of memory and registers was taken, and the programmer had to debug from the dump. Dumping the memory and registers allowed the operator to continue immediately with the next job but left the programmer with the more difcult debugging problem. Second, jobs with similar needs were b atched together and run through

needs were b atched together and run through the computer as a group to reduce setup time. For instance, suppose the operator received one FORTRAN job, one COBOL job, and another FORTRAN job. If she ran them in that order, she would have to set up for FORTRAN (load the compiler tapes and so on), then set up for COBOL ,a n dt h e ns e tu pf o r FORTRAN again. If she ran the two FORTRAN programs as a batch, however, she could setup only once for FORTRAN ,s a v i n go p e r a t o rt i m e .20.2

FORTRAN ,s a v i n go p e r a t o rt i m e .20.2 Early Systems 891 loader job sequencing control card interpreter user program areamonitor Figure 20.2 Memory layout for a resident monitor. But there were still problems. For example, when a job stopped, the operator would have to notice that it had stopped (by observing the console), determine why it stopped (normal or abnormal termination), dump memory and register (if necessary), load the appropriate device with the next job, and restart the

device with the next job, and restart the computer. During this transition from one job to the next, the CPU sat idle. To overcome this idle time, people developed automatic job sequencing . With this technique, the rst rudimentary operating systems were created. As m a l lp r o g r a m ,c a l l e da resident monitor ,w a sc r e a t e dt ot r a n s f e rc o n t r o l automatically from one job to the next (Figure 20.2). The resident monitor is always in memory (or resident ). When the computer

always in memory (or resident ). When the computer was turned on, the resident monitor was invoked, and it would transfer control to a program. When the program terminated, it would return control to the resident monitor, which would then go on to the next program. Thus, the resident monitor would automatically sequence from one program to another and from one job to another. But how would the resident monitor know which program to execute? Previously, the operator had been given a short

Previously, the operator had been given a short description of what programs were to be run on what data. Control cards were introduced to provide this information directly to the monitor. The idea is simple. In addition to the program or data for a job, the programmer supplied control cards, which contained directives to the resident mon itor indicating what program to run. For example, a normal user program might require one of three programs to run: the FORTRAN compiler ( FTN), the assembler

to run: the FORTRAN compiler ( FTN), the assembler ( ASM ), or the users program (RUN ). We could use a separate control card for each of these: FTNExecute the FORTRAN compiler. ASM Execute the assembler . RUN Execute the user program. These cards tell the resident monitor which program to run.892 Chapter 20 Inuential Operating Systems We can use two additional control cards to dene the boundaries of each job: JOBFirst card of a job END Final card of a job These two cards might be useful in

card of a job These two cards might be useful in accounting for the machine resources used by the programmer. Parameters can be used to dene the job name, account number to be charged, and so on. Other control cards can be dened for other functions, such as asking the operator to load or unload a tape. One problem with control cards is how to distinguish them from data or program cards. The usual solution is to identify them by a special character or pattern on the card. Several systems used the

or pattern on the card. Several systems used the dollarsign character () in the rst column to identify a control card. Others used a different code. IBMs Job Control Language ( JCL)u s e ds l a s hm a r k s(   )i nt h e r s tt w oc o l u m n s .F i g u r e 20.3 shows a sample carddeck setup for a simple batch system. A resident monitor thus has several identiable parts: The controlcard interpreter is responsible for reading and carrying out the instructions on the cards at the point of

out the instructions on the cards at the point of execution. The loader is invoked by the controlcard interpreter to load system programs and application programs into memory at intervals. The device drivers are used by both the controlcard interpreter and the loader for the systems IO devices. Often, the system and application programs are linked to these same device drivers, providing continuity in their operation, as well as saving memory space and programming time. These batch systems work

and programming time. These batch systems work fairly well. The resident monitor provides automatic job sequencing as indicated by the control cards. When a control card indicates that a program is to be run, the monitor loads the program into memory and transfers control to it. When the program completes, itENDRUNdata for program LOADFTNJOBprogram to be compiled Figure 20.3 Card deck for a simple batch system.20.2 Early Systems 893 transfers control back to the monitor, which reads the next

control back to the monitor, which reads the next control card, loads the appropriate program, and so on. This cycle is repeated until all control cards are interpreted for the job. Then the monitor automatically continues with the next job. The switch to batch systems with automatic job sequencing was made to improve performance. The problem, quite simply, is that humans are considerably slower than computers. Con sequently, it is desirable to replace human operation with operatingsystem

to replace human operation with operatingsystem software. Automatic job sequencing eliminates the need for human setup time and job sequencing. Even with this arrangement, however, the CPU is often idle. The problem is the speed of the mechanical IO devices, which are intrinsically slower than electronic devices. Even a slow CPU works in the microsecond range, with thousands of instructions executed per second. A fast card reader, in contrast, might read 1,200 cards per minute (or 20 cards per

might read 1,200 cards per minute (or 20 cards per second). Thus, the difference in speed between the CPU and its IOdevices may be three orders of magnitude or more. Over time, of course, improvements in technology resulted in faster IOdevices. Unfortunately, CPU speeds increased even faster, so that the problem was not only unresolved but also exacerbated. 20.2.3 Overlapped IO One common solution to the IOproblem was to replace slow card readers (input devices) and line printers (output

readers (input devices) and line printers (output devices) with magnetictape units. Most computer systems in the late 1950s and early 1960s were batch systems reading from card readers and writing to line printers or card punches. The CPU did not read directly from cards, how ever; instead, the cards were rst copied onto a magnetic tape via a separate device. When the tape was sufciently full, it was taken down and carried over to the computer. When a card was needed for input to a program, the

When a card was needed for input to a program, the equivalent record was read from the tape. Similarly, output was written to the tape, and the contents of the tape were printed later. The card readers and line printers were operated offline ,r a t h e rt h a nb yt h e main computer (Figure 20.4). An obvious advantage of offline operation was that the main computer was no longer constrained by the speed of the card readers and line printers but was limited only by the speed of the much faster

was limited only by the speed of the much faster magnetic tape units. (b)(a)CPU card reader card readerline printer tape drives tape drives line printerCPUonline onlineFigure 20.4 Operation of IO devices (a) online and (b) offline.894 Chapter 20 Inuential Operating Systems The technique of using magnetic tape for all IOcould be applied with any similar equipment (such as card readers, card punches, plotters, paper tape, and printers). The real gain in offline operation comes from the possibility

in offline operation comes from the possibility of using multiple readertotape and tapetoprinter systems for one CPU.I ft h e CPU can process input twice as fast as the reader can read cards, then two readers working simultaneously can produce enough tape to keep the CPU busy. There is a disadvantage, too, howevera longer delay in getting a particular job run. The job must rst be read onto tape. Then it must wait until enough additional jobs are read onto the tape to llit. The tape must then be

read onto the tape to llit. The tape must then be rewound, unloaded, handcarried to the CPU,a n dm o u n t e do naf r e et a p ed r i v e .T h i sp r o c e s si sn o t unreasonable for batch systems, of course. Many similar jobs can be batched onto a tape before it is taken to the computer. Although offline preparation of jobs continued for some time, it was quickly replaced in most systems. Disk systems became widely available and greatly improved on offline operation. One problem with tape

on offline operation. One problem with tape systems was that the card reader could not write onto one end of the tape while the CPU read from the other. The entire tape had to be written before it was rewound and read, because tapes are by nature sequentialaccess devices .D i s ks y s t e m s eliminated this problem by being randomaccess devices .B e c a u s et h eh e a di s moved from one area of the disk to another, it can switch rapidly from the area on the disk being used by the card reader

the area on the disk being used by the card reader to store new cards to the position needed by the CPU to read the next card. In a disk system, cards are read directly from the card reader onto the disk. The location of card images is recorded in a table kept by the operating system. When a job is executed, the operating system satises its requests for cardreader input by reading from the disk. Similarly, when the job requests the printer to output a line, that line is copied into a system

output a line, that line is copied into a system buffer and is written to the disk. When the job is completed, the output is actually printed. This form of processing is called spooling (Figure 20.5); the name is an acronym for simultaneous peripheral operation online. Spooling, in essence, uses the disk CPU card reader line printerdisk IOonlineFigure 20.5 Spooling.20.3 Atlas 895 as a huge buffer for reading as far ahead as possible on input devices and for storing output les until the output

and for storing output les until the output devices are able to accept them. Spooling is also used for processing data at remote sites. The CPU sends the data via communication paths to a remote printer (or accepts an entire input job from a remote card reader). The remote processing is done at its own speed, with no CPU intervention. The CPU just needs to be notied when the processing is completed, so that it can spool the next batch of data. Spooling overlaps the IOof one job with the

data. Spooling overlaps the IOof one job with the computation of other jobs. Even in a simple system, the spooler may be reading the input of one job while printing the output of a different job. During this time, still another job (or other jobs) may be executed, reading its cards from disk and printing its output lines onto the disk. Spooling has a direct benecial effect on the performance of the system. For the cost of some disk space and a few tables, the computation of one job and the IOof

tables, the computation of one job and the IOof other jobs can take place at the same time. Thus, spooling can keep both the CPU and the IOdevices working at much higher rates. Spooling leads naturally to multiprogramming, which is the foundation of all modern operating systems. 20.3 Atlas The Atlas operating system was designed at the University of Manchester in England in the late 1950s and early 1960s. Many of its basic features that were novel at the time have become standard parts of modern

at the time have become standard parts of modern operating systems. Device drivers were a major part of the system. In addition, system calls were added by a set of special instructions called extra codes . Atlas was a batch operating system with spooling. Spooling allowed the system to schedule jobs according to the availability of peripheral devices, such as magnetic tape units, paper tape readers, paper tape punches, line printers, card readers, and card punches. The most remarkable feature

and card punches. The most remarkable feature of Atlas, however, was its memory manage ment. Core memory was new and expensive at the time. Many computers, like the IBM 650, used a drum for primary memory. The Atlas system used a drum for its main memory, but it had a small amount of core memory that was used as a cache for the drum. Demand paging was used to transfer information between core memory and the drum automatically. The Atlas system used a British computer with 48bit words. Addresses

a British computer with 48bit words. Addresses were 24 bits but were encoded in decimal, which allowed 1 million words to be addressed. At that time, this was an extremely large address space. The physical memory for Atlas was a 98 KBword drum and 16 KBwords of core. Memory was divided into 512word pages, providing 32 frames in physical memory. An associative memory of 32 registers implemented the mapping from a virtual address to a physical address. If a page fault occurred, a pagereplacement

If a page fault occurred, a pagereplacement algorithm was invoked. One memory frame was always kept empty, so that a drum transfer could start immediately. The pagereplacement algorithm attempted to predict future memoryaccessing behavior based on past behavior. A reference bit for each frame was set whenever the frame was acce ssed. The reference bits were read896 Chapter 20 Inuential Operating Systems into memory every 1,024 instructions, and the last 32 values of these bits were retained.

the last 32 values of these bits were retained. This history was used to dene the time since the most recent reference (t1)a n dt h ei n t e r v a lb e t w e e nt h el a s tt w or e f e r e n c e s( t2). Pages were chosen for replacement in the following order: 1.Any page with t1t21i sc o n s i d e r e dt ob en ol o n g e ri nu s ea n di s replaced. 2.Ift1t2for all pages, then replace the page with the largest value for t2 t1. The pagereplacement algorithm assumes that programs access memory in

algorithm assumes that programs access memory in loops. If the time between the last two references is t2,t h e na n o t h e rr e f e r e n c ei s expected t2time units later. If a reference does not occur ( t1t2), it is assumed that the page is no longer being used, and the page is replaced. If all pages are still in use, then the page that will not be needed for the longest time is replaced. The time to the next reference is expected to be t2t1. 20.4 XDS940 The XDS940 operating system was

t2t1. 20.4 XDS940 The XDS940 operating system was designed at the University of California at Berkeley in the early 1960s. Like the Atlas system, it used paging for memory management. Unlike the Atlas system, it was a timeshared system. The paging was used only for relocation; it was not used for demand paging. The virtual memory of any user process was made up of 16 KBwords, whereas the physical memory was made up of 64 KBwords. Each page was made up of 2 KBwords. The page table was kept in

made up of 2 KBwords. The page table was kept in registers. Since physical memory was larger than virtual memory, several user processes could be in memory at the same time. The number of users could be increased by page sharing when the pages contained readonly reentrant code. Processes were kept on a drum and were swapped in and out of memory as necessary. The XDS940 system was constructed from a modied XDS930 . The mod ications were typical of the changes made to a basic computer to allow an

the changes made to a basic computer to allow an operating system to be written properly. A usermonitor mode was added. Certain instructions, such as IOand halt, were dened to be privileged. An attempt to execute a privileged instruction in user mode would trap to the operating system. As y s t e m  c a l li n s t r u c t i o nw a sa d d e dt ot h eu s e r  m o d ei n s t r u c t i o ns e t . This instruction was used to create new resources, such as les, allowing the operating system to manage

as les, allowing the operating system to manage the physical resources. Files, for example, were allocated in 256word blocks on the drum. A bit map was used to manage free drum blocks. Each le had an index block with pointers to the actual data blocks. Index blocks were chained together. The XDS940 system also provided system calls to allow processes to create, start, suspend, and destroy subprocesses. A programmer could construct a system of processes. Separate processes could share memory for

Separate processes could share memory for communica tion and synchronization. Process creation dened a tree structure, where a process is the root and its subprocesses are nodes below it in the tree. Each of the subprocesses could, in turn, create more subprocesses.20.6 RC 4000 897 20.5 THE The THE operating system was designed at the Technische Hogeschool in Eindhoven in the Netherlands in the mid1960s. It was a batch system running on a Dutch computer, the EL X8 ,w i t h3 2 KBof 27bit words.

computer, the EL X8 ,w i t h3 2 KBof 27bit words. The system was mainly noted for its clean design, particularly its layer structure, and its use of as e to fc o n c u r r e n tp r o c e s s e se m p l o y i n gs e m a p h o r e sf o rs y n c h r o n i z a t i o n . Unlike the processes in the XDS940 system, the set of processes in the THE system was static. The operating system itself was designed as a set of cooperating processes. In addition, ve user processes were created that served as the

ve user processes were created that served as the active agents to compile, execute, and print user programs. When one job was nished, the process would re turn to the input queue to select another job. Ap r i o r i t y CPUscheduling algorithm was used. The priorities were recom puted every 2 seconds and were in versely proportional to the amount of CPU time used recently (in the last 8 to 10 seconds). This scheme gave higher priority toIObound processes and to new processes. Memory management

processes and to new processes. Memory management was limited by the lack of hardware support. How ever, since the system was limited and user programs could be written only in Algol, a software paging scheme was used. The Algol compiler automatically generated calls to system routines, which made sure the requested information was in memory, swapping if necessary. The backing store was a 512 KBword drum. A 512word page was used, with an LRU pagereplacement strategy. Another major concern of the

strategy. Another major concern of the THE system was deadlock control. The bankers algorithm was used to provide deadlock avoidance. Closely related to the THE system is the Venus system. The Venus system was also a layerstructured design, using semaphores to synchronize processes. The lower levels of the design were implemented in microcode, however, providing a much faster system. Pagedsegmented memory was used for memory management. In addition, the system was designed as a timesharing

addition, the system was designed as a timesharing system, rather than a batch system. 20.6 RC 4000 The RC4000 system, like the THE system, was notable primarily for its design concepts. It was designed in the late 1960s for the Danish 4000 computer by Regnecentralen, particularly by B rinchHansen. The objective was not to design a batch system, or a timesharing system, or any other specic system. Rather, the goal was to create an operatingsystem nucleus, or kernel, on which a complete operating

nucleus, or kernel, on which a complete operating system could be built. Thus, the system structure was layered, and only the lower levelscomprising the kernelwere provided. The kernel supported a collection of concurrent processes. A roundrobin CPU scheduler was used. Although processes could share memory, the primary communication and synchronization mechanism was the message system provided by the kernel. Processes could communicate with each other by exchanging xedsized messages of eight

other by exchanging xedsized messages of eight words in length. All messages were stored in buffers from a common buffer pool. When a message buffer was no longer required, it was returned to the common pool.898 Chapter 20 Inuential Operating Systems Amessage queue was associated with each process. It contained all the messages that had been sent to that process but had not yet been received. Messages were removed from the queue in FIFO order. The system supported four primitive operations,

The system supported four primitive operations, which were executed atomically: sendmessage (inreceiver, inmessage, outbuffer ) waitmessage (outsender, outmessage, outbuffer ) sendanswer (outresult, inmessage, inbuffer ) waitanswer (outresult, outmessage, inbuffer ) The last two operations allowed processes to exchange several messages at a time. These primitives required that a process service its message queue in FIFO order and that it block itself while other processes were handling its

itself while other processes were handling its messages. To remove these restrictions, the d evelopers provided two additional communication primitives that allowed a process to wait for the arrival of the next message or to answer and service its queue in any order: waitevent (inpreviousbuffer, outnextbuffer, outresult ) getevent (outbuffer ) IOdevices were also treated as processes. The device drivers were code that converted the device interrupts and registers into messages. Thus, a process

and registers into messages. Thus, a process would write to a terminal by sending that terminal a message. The device driver would receive the message and output the character to the terminal. An input character would interrupt the system and transfer to ad e v i c ed r i v e r .T h ed e v i c ed r i v e rw o u l dc r e a t eam e s s a g ef r o mt h ei n p u t character and send it to a waiting process. 20.7 CTSS The Compatible TimeSharing System ( CTSS )w a sd e s i g n e da t MITas an experi

( CTSS )w a sd e s i g n e da t MITas an experi mental timesharing system and rst appeared in 1961. It was implemented on anIBM7090 and eventually supported up to 32 interactive users. The users were provided with a set of interactive commands that allowed them to manipulate les and to compile and run programs through a terminal. The 7090 had a 32 KBmemory made up of 36bit words. The monitor used 5KBwords, leaving 27 KBfor the users. User memory images were swapped between memory and a fast

images were swapped between memory and a fast drum. CPU scheduling employed a multilevel feedbackqueue algorithm. The time quantum for level iwas 2 itime units. If a program did not nish its CPU burst in one time quantum, it was moved down to the next level of the queue, giving it twice as much time. The program at the highest level (with the shortest quantum) was run rst. The initial level of a program was determined by its size, so that the time quantum was at least as long as the swap time.

quantum was at least as long as the swap time. CTSS was extremely successful and was in use as late as 1972. Although it was limited, it succeeded in demonstrating that time sharing was a con20.9 IBM OS360 899 venient and practical mode of computing. One result of CTSS was increased development of timesharing systems. Another result was the development of MULTICS . 20.8 MULTICS The MULTICS operating system was designed from 1965 to 1970 at MIT as a natural extension of CTSS .CTSS and other early

a natural extension of CTSS .CTSS and other early timesharing systems were so successful that they created an immediate desire to proceed quickly to bigger and better systems. As larger computers became available, the designers of CTSS set out to create a timesharing utility. Computing service would be provided like electrical power. Large computer systems would be connected by telephone wires to terminals in ofces and homes throughout a city. The operating system would be a timeshared system

The operating system would be a timeshared system running continuously with a vast le system of shared programs and data. MULTICS was designed by a team from MIT,GE(which later sold its computer department to Honeywell), and Bell Laboratories (which dropped out of the project in 1969). The basic GE635 computer was modied to a new computer system called the GE645, mainly by the addition of paged segmentation memory hardware. InMULTICS ,av i r t u a la d d r e s sw a sc o m p o s e do fa n1 8  b i

a la d d r e s sw a sc o m p o s e do fa n1 8  b i ts e g m e n tn u m b e r and a 16bit word offset. The segments were then paged in 1 KBword pages. The secondchance pagereplacement algorithm was used. The segmented virtual address space was merged into the le system; each segment was a le. Segments were addressed by the name of the le. The le system itself was a multilevel tree structure, allowing users to create their own subdirectory structures. Like CTSS ,MULTICS used a multilevel feedback

Like CTSS ,MULTICS used a multilevel feedback queue for CPU scheduling. Protection was accomplished through an access list associated with each le and a set of protection rings for executing processes. The system, which was written almost entirely in PL1 ,c o m p r i s e da b o u t3 0 0 , 0 0 0l i n e so fc o d e .I tw a s extended to a multiprocessor system, allowing a CPU to be taken out of service for maintenance while the system continued running. 20.9 IBM OS360 The longest line of

running. 20.9 IBM OS360 The longest line of operatingsystem development is undoubtedly that of IBM computers. The early IBMcomputers, such as the IBM7090 and the IBM7094, are prime examples of the development of common IOsubroutines, followed by development of a resident monitor, privileged in structions, memory protection, and simple batch processing. These systems were developed separately, often at independent sites. As a result, IBMwas faced with many different computers, with different

with many different computers, with different languages and different system software. The IBM360 which rst appeared in the mid 1960s  was designed to alter this situation. The IBM360 ([Mealy et al. (1966)]) was designed as a family of computers spanning the complete range from small business machines to large scientic machines. Only one set of software would be needed for these systems, which all used the same operating system: OS360 .T h i sa r r a n g e m e n t900 Chapter 20 Inuential

h i sa r r a n g e m e n t900 Chapter 20 Inuential Operating Systems was intended to reduce maintenance problems for IBM and to allow users to move programs and applications freely from one IBM system to another. Unfortunately, OS360 tried to be all things to all people. As a result, it did none of its tasks especially well. The le system included a type eld that dened the type of each le, and different le types were dened for xedlength and variablelength records and for blocked and unblocked

records and for blocked and unblocked les. Contiguous allocation was used, so the user had to guess the size of each output le. The Job Control Language ( JCL)a d d e dp a r a m e t e r sf o re v e r yp o s s i b l e option, making it incomprehen sible to the average user. The memorymanagement routines were hampered by the architecture. Although a baseregister addressing mode was used, the program could access and modify the base register, so that absolute addresses were generated by the CPU.T h

absolute addresses were generated by the CPU.T h i sa r r a n g e m e n tp r e v e n t e dd y n a m i cr e l o c a t i o n ;t h ep r o g r a mw a sb o u n d to physical memory at load time. Two separate versions of the operating system were produced: OSMFT used xed regions and OSMVT used variable regions. The system was written in assembly language by thousands of program mers, resulting in millions of lines of code. The operating system itself required large amounts of memory for its code and

required large amounts of memory for its code and tables. Operatingsystem overhead often consumed onehalf of the total CPU cycles. Over the years, new versions were released to add new features and to x errors. However, xing one error often caused another in some remote part of the system, so that the number of known errors in the system remained fairly constant. Virtual memory was added to OS360 with the change to the IBM370 architecture. The underlying h ardware provided a segmentedpaged

The underlying h ardware provided a segmentedpaged virtual memory. New versions of OSused this hardware in different ways. OSVS1 created one large virtual address space and ran OSMFT in that virtual memory. Thus, the operating system itself was paged, as well as user programs. OSVS2 Release 1 ran OSMVT in virtual memory. Finally, OSVS2 Release 2, which is now called MVS ,p r o v i d e de a c hu s e rw i t hh i so w nv i r t u a lm e m o r y . MVS is still basically a batch operating system. The

is still basically a batch operating system. The CTSS system was run on an IBM7094, but the developers at MITdecided that the address space of the 360, IBMs successor to the 7094, was too small for MULTICS ,s ot h e ys w i t c h e d vendors. IBMthen decided to create its own timesharing system, TSS360 .L i k e MULTICS ,TSS360 was supposed to be a large, timeshared utility. The basic 360 architecture was modied in the model 67 to provide virtual memory. Several sites purchased the 36067 in

memory. Several sites purchased the 36067 in anticipation of TSS360 . TSS360 was delayed, however, so other timesharing systems were devel oped as temporary systems until TSS360 was available. A timesharing option (TSO)w a sa d d e dt o OS360 .IBMs Cambridge Scientic Center developed CMS as a singleuser system and CP67 to provide a virtual machine to run it on. When TSS360 was eventually delivered, it was a failure. It was too large and too slow. As a result, no site would switch from its

slow. As a result, no site would switch from its temporary system to TSS360 .T o d a y ,t i m es h a r i n go n IBM systems is largely provided either by TSO under MVS or by CMS under CP67 (renamed VM). Neither TSS360 nor MULTICS achieved commercial success. What went wrong? Part of the problem was that these advanced systems were too large and too complex to be understood. Another problem was the assumption that computing power would be available from a large, remote source.20.11 CPM and MSDOS

from a large, remote source.20.11 CPM and MSDOS 901 Minicomputers came along and decre ased the need for large monolithic systems. They were followed by workstations and then personal computers, which put computing power closer and closer to the end users. 20.10 TOPS20 DEC created many inuential computer systems during its history. Probably the most famous operating system associated with DEC isVMS ,ap o p u l a r businessoriented system that is still in use today as Open VMS ,ap r o d u c to f

still in use today as Open VMS ,ap r o d u c to f HewlettPackard. But perhaps the most inuential of DECs operating systems was TOPS20 . TOPS20 started life as a research project at Bolt, Beranek, and Newman (BBN)a r o u n d1 9 7 0 . BBN took the businessoriented DEC PDP10 computer running TOPS10 ,a d d e dah a r d w a r em e m o r y  p a g i n gs y s t e mt oi m p l e m e n t virtual memory, and wrote a new operating system for that computer to take advantage of the new hardware features. The

take advantage of the new hardware features. The result was TENEX ,ag e n e r a l  purpose timesharing system. DEC then purchased the rights to TENEX and created a new computer with a builtin hardware pager. The resulting system was the DECSYSTEM20 and the TOPS20 operating system. TOPS20 had an advanced commandline interpreter that provided help as needed to users. That, in combination with the power of the computer and its reasonable price, made the DECSYSTEM20 the most popular timesharing

made the DECSYSTEM20 the most popular timesharing system of its time. In 1984, DEC stopped work on its line of 36bit PDP10 computers to concentrate on 32bit VAX systems running VMS . 20.11 CPM and MSDOS Early hobbyist computers were typically built from kits and ran a single program at a time. The systems evolved into more advanced systems as computer components improved. An early standard operating system for these computers of the 1970s was CPM ,s h o r tf o rC o n t r o lP r o g r a m  M o n

,s h o r tf o rC o n t r o lP r o g r a m  M o n i t o r , written by Gary Kindall of Digital Research, Inc. CPM ran primarily on the rst personal computer CPU,t h e8  b i tI n t e l8 0 8 0 . CPM originally supported only 64 KB of memory and ran only one program at a time. Of course, it was textbased, with a command interpreter. The command interpreter resembled those in other operating systems of the time, such as the TOPS10 from DEC. When IBM entered the personal computer busin ess, it decided

the personal computer busin ess, it decided to have Bill Gates and company write a new operating system for its 16bit CPU of choice the Intel 8086. This operating system, MSDOS ,w a ss i m i l a rt o CPM but had a richer set of builtin commands, again mostly modeled after TOPS10 . MSDOS became the most popular personalcomputer operating system of its time, starting in 1981 and continuing development until 2000. It supported 640 KB of memory, with the ability to address extended and expanded

with the ability to address extended and expanded  memory to get somewhat beyond that limit. It lacked fundamental current operatingsystem features, however, especially protected memory.902 Chapter 20 Inuential Operating Systems 20.12 Macintosh Operating System and Windows With the advent of 16bit CPUs, operating systems for personal computers could become more advanced, feature rich, and usable. The Apple Macintosh computer was arguably the rst computer with a GUIdesigned for home users. It was

computer with a GUIdesigned for home users. It was certainly the most successful for a while, starting at its launch in 1984. It used a mouse for screen pointing and selecting and came with many utility programs that took advantage of the new user interface. Harddisk drives were relatively expensive in 1984, so it came only with a 400KBcapacity oppy drive by default. The original Mac OSran only on Apple computers and slowly was eclipsed by Microsoft Windows (starting with Version 1.0 in 1985),

Windows (starting with Version 1.0 in 1985), which was licensed to run on many different computers from a multitude of companies. As microprocessor CPUse v o l v e dt o3 2  b i tc h i p sw i t ha d v a n c e d features, such as protected memory and context switching, these operating systems added features that had previously been found only on mainframes and minicomputers. Over time, personal computers became as powerful as those systems and more useful for many purposes. Minicomputers died out,

useful for many purposes. Minicomputers died out, replaced by general and specialpurpose servers. Although personal computers continue to increase in capacity and performance, servers tend to stay ahead of them in amount of memory, disk space, and number and speed of available CPUs. Today, servers typically run in data centers or machine rooms, while personal computers sit on or next to desks and talk to each other and servers across a network. The desktop rivalry between Apple and Mic rosoft

The desktop rivalry between Apple and Mic rosoft continues today, with new versions of Windows and Mac OStrying to outdo each other in features, usability, and application functionality. Other operating systems, such as Amiga OSand OS2 ,h a v ea p p e a r e do v e rt i m eb u th a v en o tb e e nl o n g  t e r m competitors to the two leading desktop operating systems. Meanwhile, Linux in its many forms continues to gain in popularity among more technical users and even with nontechnical users

technical users and even with nontechnical users on systems like the One Laptop per Child (OLPC )childrens connected computer network ( http:laptop.org ). 20.13 Mach The Mach operating system traces its ancestry to the Accent operating system developed at Carnegie Mellon University ( CMU ). Machs communication system and philosophy are derived from Accent, but many other signicant portions of the system (for example, the virtual memory system and task and thread management) were developed from

task and thread management) were developed from scratch. Work on Mach began in the mid 1980s and the operating system was designed with the following three critical goals in mind: 1.Emulate 4.3 BSD UNIX so that the executable les from a UNIX system can run correctly under Mach. 2.Be a modern operating system that supports many memory models, as well as parallel and distributed computing. 3.Have a kernel that is simpler and easier to modify than 4.3 BSD .20.13 Mach 903 Machs development followed

4.3 BSD .20.13 Mach 903 Machs development followed an evolutionary path from BSD UNIX sys tems. Mach code was initially developed inside the 4.2BSD kernel, with BSD kernel components replaced by Mach c omponents as the Mach components were completed. The BSD components were updated to 4.3BSD when that became available. By 1986, the virtual memory and communication subsys tems were running on the DEC VAX computer family, including multiprocessor versions of the VAX.V e r s i o n sf o rt h e IBM

versions of the VAX.V e r s i o n sf o rt h e IBM RTPC and for SUN 3 workstations followed shortly. Then, 1987 saw the completion of the Encore Multimax and Sequent Balance multiprocessor versions, including task and thread support, as well as the rst ofcial releases of the system, Release 0 and Release 1. Through Release 2, Mach provided compatibility with the corresponding BSD systems by including much of BSDs code in the kernel. The new features and capabilities of Mach made the kernels in

and capabilities of Mach made the kernels in these releases larger than the corresponding BSD kernels. Mach 3 moved the BSD code outside the kernel, leaving a much smaller microkernel. This system implements only basic Mach features in the kernel; all UNIX specic code has been evicted to run in usermode servers. Excluding UNIX specic code from the kernel allows the replacement of BSD with another operating system or the simultaneous execution of multiple operatingsystem interfaces on top of the

multiple operatingsystem interfaces on top of the microkernel. In addition to BSD,u s e r  m o d ei m p l e m e n t a t i o n sh a v eb e e nd e v e l o p e df o r DOS,t h e Macintosh operating system, and OSF1 .T h i sa p p r o a c hh a ss i m i l a r i t i e st ot h e virtual machine concept, but here the virtual machine is dened by software (the Mach kernel interface), rather than by hardware. With Release 3.0, Mach became available on a wide variety of systems, including singleprocessor SUN,

variety of systems, including singleprocessor SUN, Intel, IBM,a n d DEC machines and multiprocessor DEC,S e q u e n t ,a n dE n c o r e systems. Mach was propelled to the forefront of industry attention when the Open Software Foundation ( OSF)a n n o u n c e di n1 9 8 9t h a ti tw o u l du s eM a c h2 . 5a s the basis for its new operating system, OSF1 .( M a c h2 . 5w a sa l s ot h eb a s i sf o r the operating system on the NeXTworkstation, the brainchild of Steve Jobs of Apple Computer fame.)

brainchild of Steve Jobs of Apple Computer fame.) The initial release of OSF1 occurred a year later, and this system competed with UNIX System V , Release 4, the operating system of choice at that time among UNIX International ( UI)m e m b e r s . OSF members included key technological companies such as IBM,DEC,a n d HP.OSFhas since changed its direction, and only DEC UNIX is based on the Mach kernel. Unlike UNIX ,w h i c hw a sd e v e l o p e dw i t h o u tr e g a r df o rm u l t i p r o c e s

i t h o u tr e g a r df o rm u l t i p r o c e s s i n g , Mach incorporates multiprocessing support throughout. This support is also exceedingly exible, ranging from sharedmemory systems to systems with no memory shared between processors. Mach uses lightweight processes, in the form of multiple threads of exec ution within one task (or address space), to support multiprocessing and parallel computation. Its extensive use of messages as the only communication method ensures that protection

only communication method ensures that protection mechanisms are complete and efcient. By integrating messages with the virtual memory system, Mach also ensures that messages can be handled efciently. Finally, by having the virtual memory system use messages to communicate with the daemons managing the backing store, Mach provides great exibility in the design and implementation of these memoryobject managing tasks. By providing lowlevel, or primitive, system calls from which more complex

or primitive, system calls from which more complex functions can be built, Mach reduces the size of the kernel904 Chapter 20 Inuential Operating Systems while permitting operatingsystem emulation at the user level, much like IBMs virtual machine systems. Some previous editions of Operating System Concepts included an entire chapter on Mach. This chapter, as it appeared in the fourth edition, is available on the Web ( http:www.osbook.com ). 20.14 Other Systems There are, of course, other

). 20.14 Other Systems There are, of course, other operating systems, and most of them have interesting properties. The MCP operating system for the Burroughs computer family was the rst to be written in a system programming language. It supported segmentation and multiple CPUs. The SCOPE operating system for the CDC 6600 was also a multi CPU system. The coordination and synchronization of the multiple processes were surprisingly well designed. History is littered with operating systems that

History is littered with operating systems that suited a purpose for a time (be it a long or a short time) and then, when faded, were replaced by operating systems that had more features, supported newer hardware, were easier to use, or were better marketed. We are sure this trend will continue in the future. Exercises 20.1 Discuss what considerations the computer operator took into account in deciding on the sequences in which programs would be run on early computer systems that were manually

run on early computer systems that were manually operated. 20.2 What optimizations were used to minimize the discrepancy between CPU and IOspeeds on early computer systems? 20.3 Consider the pagereplacement a lgorithm used by Atlas. In what ways is it different from the clock algorithm discussed in Section 9.4.5.2? 20.4 Consider the multilevel feedback queue used by CTSS and MULTICS . Suppose a program consistently uses seven time units every time it is scheduled before it performs an IO

time it is scheduled before it performs an IO operation and blocks. How many time units are allocated to this program when it is scheduled for execution at different points in time? 20.5 What are the implications of supporting BSDfunctionality in usermode servers within the Mach operating system? 20.6 What conclusions can be drawn about the evolution of operating systems? What causes some operating systems to gain in popularity and others to fade? Bibliographical Notes Looms and calculators are

Bibliographical Notes Looms and calculators are described in [Frah (2001)] and shown graphically in [Frauenfelder (2005)]. The Manchester Mark 1 is discussed by [Rojas and Hashagen (2000)], and its offspring, the Ferranti Mark 1, is described by [Ceruzzi (1998)].Bibliography 905 [Kilburn et al. (1961)] and [Howarth et al. (1961)] examine the Atlas operating system. The XDS940 operating system is described by [Lichtenberger and Pirtle (1965)]. The THE operating system is covered by [Dijkstra

The THE operating system is covered by [Dijkstra (1968)] and by [McKeag and Wilson (1976)]. The Venus system is described by [Liskov (1972)]. [BrinchHansen (1970)] and [BrinchHansen (1973)] discuss the RC4000 system. The Compatible TimeSharing System ( CTSS )i sp r e s e n t e db y[ C o r b a t oe ta l . (1962)]. The MULTICS operating system is described by [Corbato and Vyssotsky (1965)] and [Organick (1972)]. [Mealy et al. (1966)] presented the IBM360 .[ L e t ta n dK o n i g s f o r d( 1 9 6 8

IBM360 .[ L e t ta n dK o n i g s f o r d( 1 9 6 8 ) ] cover TSS360 . CP67 is described by [Meyer and Seawright (1970)] and [Parmelee et al. (1972)]. DEC VMS is discussed by [Kenah et al. (1988)], and TENEX is described by [Bobrow et al. (1972)]. Ad e s c r i p t i o no ft h eA p p l eM a c i n t o s ha p p e a r si n[ A p p l e( 1 9 8 7 ) ] .F o rm o r e information on these operating systems and their history, see [Freiberger and Swaine (2000)]. The Mach operating system and its ancestor, the

The Mach operating system and its ancestor, the Accent operating sys tem, are described by [Rashid and Robertson (1981)]. Machs communi cation system is covered by [Rashid (1986)], [Tevanian et al. (1989)], and [Accetta et al. (1986)]. The Mach scheduler is described in detail by [Tevanian et al. (1987a)] and [Black (1990)]. An early version of the Mach shared memory and memorymapping system is presented by [Tevanian et al. (1987b)]. A good resource describing the Mach project can be found at

describing the Mach project can be found at http:www.cs.cmu.eduafscsprojectmachpublicwwwmach.html . [McKeag and Wilson (1976)] discuss the MCP operating system for the Burroughs computer family as well as the SCOPE operating system for the CDC 6600. Bibliography [Accetta et al. (1986)] M. Accetta, R. Baron, W. Bolosky, D. B. Golub, R. Rashid, A. Tevanian, and M. Young, Mach: A New Kernel Foundation for UNIX Development ,Proceedings of the Summer USENIX Conference (1986), pages 93112. [Apple

USENIX Conference (1986), pages 93112. [Apple (1987)] Apple Technical Introduction to the Macintosh Family .A d d i s o n  Wesley (1987). [Black (1990)] D. L. Black, Scheduling Support for Concurrency and Parallelism in the Mach Operating System ,IEEE Computer ,V o l u m e2 3 ,N u m b e r5( 1 9 9 0 ) , pages 3543.906 Chapter 20 Inuential Operating Systems [Bobrow et al. (1972)] D. G. Bobrow, J. D. Burchel, D. L. Murphy, and R. S. Tom linson, TENEX, a Paged Time Sharing System for the PDP10

TENEX, a Paged Time Sharing System for the PDP10 ,Communications of the ACM ,V o l u m e1 5 ,N u m b e r3( 1 9 7 2 ) . [BrinchHansen (1970)] P. B r i n c h  H a n s e n , The Nucleus of a Multiprogram ming System ,Communications of the ACM ,V o l u m e1 3 ,N u m b e r4( 1 9 7 0 ) ,p a g e s 238241 and 250. [BrinchHansen (1973)] P. B r i n c h  H a n s e n , Operating System Principles ,P r e n t i c e Hall (1973). [Ceruzzi (1998)] P. E . C e r u z z i , AH i s t o r yo fM o d e r nC o m p u t i

z z i , AH i s t o r yo fM o d e r nC o m p u t i n g ,M I TP r e s s( 1 9 9 8 ) . [Corbato and Vyssotsky (1965)] F. J. Corbato and V. A. Vyssotsky, Introduction and Overview of the MULTICS System ,Proceedings of the AFIPS Fall Joint Computer Conference (1965), pages 185196. [Corbato et al. (1962)] F. J. Corbato, M. MerwinDaggett, and R. C. Daley, An Experimental TimeSharing System ,Proceedings of the AFIPS Fall Joint Computer Conference (1962), pages 335344. [Dijkstra (1968)] E. W. Dijkstra,

pages 335344. [Dijkstra (1968)] E. W. Dijkstra, The Structure of the THE Multiprogramming System ,Communications of the ACM ,V o l u m e1 1 ,N u m b e r5( 1 9 6 8 ) ,p a g e s 341346. [Frah (2001)] G. Frah, The Universal History of Computing ,J o h nW i l e ya n dS o n s (2001). [Frauenfelder (2005)] M. Frauenfelder, The ComputerAn Illustrated History , Carlton Books (2005). [Freiberger and Swaine (2000)] P . Freiberger and M. Swaine, Fire in the Valley The Making of the Personal Computer ,M c G

Valley The Making of the Personal Computer ,M c G r a w  H i l l( 2 0 0 0 ) . [Howarth et al. (1961)] D. J. Howarth, R. B. Payne, and F. H. Sumner, The Manchester University Atlas Operating System, Part II: Users Description , Computer Journal ,V o l u m e4 ,N u m b e r3( 1 9 6 1 ) ,p a g e s2 2 6  2 2 9 . [Kenah et al. (1988)] L. J. Kenah, R. E. Goldenberg, and S. F. Bate, VA X  V M S Internals and Data Structures ,D i g i t a lP r e s s( 1 9 8 8 ) . [Kilburn et al. (1961)] T. Kilburn, D. J.

8 8 ) . [Kilburn et al. (1961)] T. Kilburn, D. J. Howarth, R. B. Payne, and F. H. Sumner, The Manchester University Atlas Operating System, Part I: Internal Organiza tion,Computer Journal ,V o l u m e4 ,N u m b e r3( 1 9 6 1 ) ,p a g e s2 2 2  2 2 5 . [Lett and Konigsford (1968)] A. L. Lett and W. L. Konigsford, TSS360: A TimeShared Operating System ,Proceedings of the AFIPS Fall Joint Computer Conference (1968), pages 1528. [Lichtenberger and Pirtle (1965)] W. W. Lichtenberger and M. W. Pirtle,

(1965)] W. W. Lichtenberger and M. W. Pirtle, A Facility for Experimentation in ManMachine Interaction ,Proceedings of the AFIPS Fall Joint Computer Conference (1965), pages 589598. [Liskov (1972)] B. H. Liskov, The Design of the Venus Operating System , Communications of the ACM ,V o l u m e1 5 ,N u m b e r3( 1 9 7 2 ) ,p a g e s1 4 4  1 4 9 . [McKeag and Wilson (1976)] R. M. McKeag and R. Wilson, Studies in Operating Systems ,A c a d e m i cP r e s s( 1 9 7 6 ) .Bibliography 907 [Mealy et al.

r e s s( 1 9 7 6 ) .Bibliography 907 [Mealy et al. (1966)] G. H. Mealy, B. I. Witt, and W. A. Clark, The Functional Structure of OS360 ,IBM Systems Journal ,V o l u m e5 ,N u m b e r1( 1 9 6 6 ) ,p a g e s 311. [Meyer and Seawright (1970)] R. A. Meyer and L. H. Seawright, AV i r t u a l Machine TimeSharing System ,IBM Systems Journal ,V o l u m e9 ,N u m b e r3 (1970), pages 199218. [Organick (1972)] E. I. Organick, The Multics System: An Examination of Its Structure ,M I TP r e s s( 1 9 7 2 ) .

of Its Structure ,M I TP r e s s( 1 9 7 2 ) . [Parmelee et al. (1972)] R. P . Parmelee, T. I. Peterson, C. C. Tillman, and D. Hat eld, Virtual Storage and Virtual Machine Concepts ,IBM Systems Journal , Volume 11, Number 2 (1972), pages 99130. [Rashid (1986)] R. F. Rashid, From RIG to Accent to Mach: The Evolution of a Network Operating System ,Proceedings of the ACMIEEE Computer Society, Fall Joint Computer Conference (1986), pages 11281137. [Rashid and Robertson (1981)] R. Rashid and G.

[Rashid and Robertson (1981)] R. Rashid and G. Robertson, Accent: A Com municationOriented Network Operating System Kernel ,Proceedings of the ACM Symposium on Operating System Principles (1981), pages 6475. [Rojas and Hashagen (2000)] R. Rojas and U. Hashagen, The First Computers History and Architectures ,M I TP r e s s( 2 0 0 0 ) . [Tevanian et al. (1987a)] A. Tevanian, Jr., R. F. Rashid, D. B. Golub, D. L. Black, E. Cooper, and M. W. Young, Mach Threads and the Unix Kernel: The Battle for

Mach Threads and the Unix Kernel: The Battle for Control ,Proceedings of the Summer USENIX Conference (1987). [Tevanian et al. (1987b)] A. Tevanian, Jr., R. F. Rashid, M. W. Young, D. B. Golub, M. R. Thompson, W. Bolosky, and R. Sanzi, AU N I XI n t e r f a c ef o r Shared Memory and Memory Mapped Files Under Mach ,T e c h n i c a lr e p o r t , CarnegieMellon University (1987). [Tevanian et al. (1989)] A. Tevanian, Jr., and B. Smith, Mach: The Model for Future Unix ,Byte (1989).Credits Figure

Model for Future Unix ,Byte (1989).Credits Figure 1.11: From Hennesy and Patterson, Computer Architecture: A Quanti tative Approach, Third Edition ,C2002, Morgan Kaufmann Publishers, Figure 5.3, p. 394. Reprinted with permission of the publisher. Figure 6.24 adapted with permission from Sun Microsystems, Inc. Figure 9.18: From IBM Systems Journal ,V o l .1 0 ,N o .3 , C1971, Interna tional Business Machines Corporation. Reprinted by permission of IBM Corporation. Figure 12.9: From

permission of IBM Corporation. Figure 12.9: From LeferMcKusickKarelsQuarterman, The Design and Implementation of the 4.3BSD UNIX Operating System ,C1989 by Addison Wesley Publishing Co., Inc., Reading, Massachusetts. Figure 7.6, p. 196. Reprinted with permission of the publisher. Figure 13.4: From Pentium Processor Users Manual: Architecture and Pro gramming Manual , Volume 3, Copyright 1993. Reprinted by permission of Intel Corporation. Figures 17.5, 17.6, and 17.8: From Halsall, Data

Figures 17.5, 17.6, and 17.8: From Halsall, Data Communications, Computer Networks, and Open Systems, Third Edition ,C1992, AddisonWesley Pub lishing Co., Inc., Reading, Massachusetts. Figure 1.9, p. 14, Figure 1.10, p. 15, and Figure 1.11, p. 18. Reprinted with permission of the publisher. Figure 6.14: From KhannaSebreeZolnowsky, Realtime Scheduling in SunOS 5.0, Proceedings of Winter USENIX, January 1992, San Francisco, California. Derived with permission of the authors. 909A accesscontrol

with permission of the authors. 909A accesscontrol lists ( ACLs), 83 2 ACLs (accesscontrol lists), 83 2 ACPI (advanced con guration and power interface), 8 62 address space layout randomization (ASLR), 83 2 admissioncontrol al gorithms, 286 advanced con guration and power interface ( ACPI), 862 advanced encryption standard (AES), 677 advanced local procedure call (ALPC), 135, 854 ALPC (advanced local procedure call), 135, 854 AMD64 architecture, 38 7 Amdahls Law, 167 AMD virtualization technolo

38 7 Amdahls Law, 167 AMD virtualization technolo gy (AMDV), 720 Android operatin g system, 8 586 API (application pro gram interface), 6364 Apple i Pad, 60, 84 application containment, 713, 727728 Aqua interface, 59, 84 ARM architecture, 388 arrays, 3 1 ASIDs (addressspace identiers), 374 ASLR (address space layout randomization), 83 2 assembly lan guage, 77 asynchronous threadin g, 172augmentedreality applications, 3 6 authentication: multif actor, 689 automatic workin gset trimmin g, 446 B

actor, 689 automatic workin gset trimmin g, 446 B back ground processes, 7475, 115, 296 balanced binary search trees, 33 binary search trees, 33 binary translation, 718720 binary trees, 33 bitmaps, 34 bourne Again shell (bash), 789 brid ging, 732 bugs, 66 C CFQ (Completely Fair Queuein g), 817 children, 33 chipsets, 83 6 Chrome, 123 CIFS (common internet le system), 871 circularly linked lists, 3 2 client(s): thin, 3 5 clientserver model, 8 54855 clock al gorithm, 4 18419 clones, 715 cloud

54855 clock al gorithm, 4 18419 clones, 715 cloud computin g, 4142, 716 Cocoa Touch, 84 911Indexcode inte grity module (Windows 7), 832 COM (component object model), 8 73 common internet le system (CI FS), 871 Completely Fair Queuein g (CFQ), 817 computational kernels, 83 5836 computer environments: cloud computing, 4 142 distribut ed systems, 3738 mobile computing, 3 637 realtim e embedded systems, 43 virtualization, 4041 computin g: mobile, 3637 concurrency, 166 Concurrency Runtime (Conc RT),

concurrency, 166 Concurrency Runtime (Conc RT), 297, 880881 condition variables, 8 79 conict phase (of dispatch latency), 285 containers, 728 control partitions, 723 couplin g, symmetric, 17 CPU schedulin g: realtim e, 283290 earliestdeadlinerst sch eduling, 288289 and minimizing l atency, 283285 POSIX realtim e scheduling, 290 prioritybased scheduling, 285287 proportional share scheduling, 289290 ratemonotonic sch eduling, 287288 virtual machin es, 729 criticalsection problem: and mut ex locks,

es, 729 criticalsection problem: and mut ex locks, 212213D Dalvik virtual machine, 8 6 data parallelism, 168169 defense in depth, 689 desktop window mana ger (DWM), 831 device objects, 8 55 Digital Equipment Corporation (DEC), 3 79 digital si gnatures, 83 2 DirectCompute, 83 5 discovery protocols, 3 9 disk(s): solidstate, 469 dispatcher, 294 DMAcontroller, 595 doubly linked lists, 3 2 driver objects, 8 55 DWM (desktop window mana ger), 831 dynamic con gurations, 83 7, 838 E earliestdeadlinerst (

con gurations, 83 7, 838 E earliestdeadlinerst ( EDF) schedulin g, 288289 EC2, 41 EDF (earliestdeadlinerst) schedulin g, 288289 efciency, 83 7 emulation, 4 0, 727 emulators, 713 encryption: public key, 678 ener gy efciency, 83 7 Erlang language, 241242 event latency, 283284 eventpair objects, 8 55exit()system call, 120, 121912 Inde xext2 (second e xtended le system), 811 ext3 (third e xtended le system), 811813 ext4 (fourth e xtended le system), 811 extended le attributes, 505 extensibility, 736

811 extended le attributes, 505 extensibility, 736 F fastuser switchin g, 863864 FIFO, 3 2 le info window (Mac OS X), 505 le replication, 767 le systems: Windows 7, seeWindows 7 foreground processes, 115, 296 forkjoin strate gy, 172 fourth e xtended le system (e xt4), 811 G GCD (Grand Central Dispatch), 182183 general trees, 33 gestures, 60 global positionin g system (GPS), 3 6 GNOM E desktop, 60 GPS (global positionin g system), 3 6 Grand Central Dispatch (GCD), 182183 granularity, minimum, 797

Dispatch (GCD), 182183 granularity, minimum, 797 graphics shaders, 83 5 guard pa ges, 84 7 GUIs ( graphical user interfaces), 5962H Hadoop, 765 Hadoop distributed le system (HD FS), 767 handle tables, 844 handson computer systems, 20 hardware: virtual machin es, 720721 hash collisions, 4 71 hash functions, 3334 hash maps, 4 71 HDFS (Hadoop distributed le system), 767 hibernation, 8 60861 hybrid cloud, 4 2 hybrid operatin g systems, 838 6 Android, 8586 iOS, 848 5 Mac OS X, 84 hypercalls, 726

8586 iOS, 848 5 Mac OS X, 84 hypercalls, 726 hypervisors, 712 type 0, 723724 type 1, 724725 type 2, 725 I IA32 architecture, 38438 7 paging in, 38 5387 segmentation in, 38438 5 IA64 architecture, 38 7 IaaS (infrastructure as a service), 42 idle threads, 84 0 IDSs (intrusiondetection systems), 691694 imperative lan guages, 241 impersonation, 8 53 implicit threadin g, 177183Inde x 913Grand Central Dispatch (GC D), 182183 OpenMP and, 181182 thread pools and, 179181 infrastructure as a service

pools and, 179181 infrastructure as a service (IaaS), 4 2 Intel processors: IA3 2 archit ecture, 384387 IA64 archit ecture, 387 interface(s): choice of, 6162 Internet Key Exchan ge (IKE), 682 interpretation, 4 0 interpreted lan guages, 727 interrupt latency, 284285 interrupt service routines (IS Rs), 84 0 IO (inputoutput): virtual machin es, 73 1732 iOS operatin g system, 848 5 IO system(s): applic ation int erface: vectored IO, 603604 IP (Internet Protocol), 681683 iPad, seeApple i Pad ISRs

Protocol), 681683 iPad, seeApple i Pad ISRs (interrupt service routines), 84 0 J Java Virtual Machine (JVM), 107, 726, 736737 journalin g le systems, 569570 justintime (JIT) compilers, 727 JVM, seeJava Virtual Machine K K Desktop Environment ( KDE), 60 kernel(s): comput ational, 83 5 kernel code, 96 kernel data structures, 3 134 arrays, 31bitm aps, 34 hash functi ons and maps, 3334 lists, 3 133 queues, 32 stacks, 32 trees, 3133 kernel environment, 84 KernelMode Driver Framework (KMDF), 856

84 KernelMode Driver Framework (KMDF), 856 kernelmode threads ( KT), 844 kernel modules: Linux, 96101 kernel transaction mana ger (KTM), 862 KMDF (KernelMode Driver Framework), 8 56 KT (kernelmode threads), 844 KTM (kernel transaction mana ger), 862 L latency: in realtim e systems, 283285 target, 797 left child, 33 LFH desi gn, 883884 LIFO, 3 2 Linux: kernel modules, 96101 Linux system(s): obtaining p age size on, 37 0 lists, 3 132 live mi gration (virtual machines), 716, 733735 lock(s): mutex,

(virtual machines), 716, 733735 lock(s): mutex, 212214 looselycoupled systems, 17 love bu g virus, 694 lowfra gmentation heap ( LFH) desi gn, 883884 LPCs (local procedure calls), 834914 Inde xM Mac OS X operatin g system, 84 main memory: paging f or management of: and Oracle SPARC Solaris, 383 memory: transactional, 239240 memory leaks, 101 memory mana gement: with virtual machin es, 73 0731 memorymana gement unit (MMU), 384 micro T LBs, 388 migration: with virtual machin es, 73373 5 minimum

migration: with virtual machin es, 73373 5 minimum granularity, 797 mobile computin g, 3637 mobile systems: multit asking in, 115 swapping on, 360, 407 module entry point, 97 module e xit point, 97 Moores Law, 6, 835 multicore systems, 14, 16, 166 multifactor authentication, 689 multiprocessor systems (parallel systems, ti ghtly coupled systems), 166 multitouch hardware, 8 63 mutant (Windows 7), 84 1 mute x locks, 212214 N namespaces, 793 NAT (network address translation), 732 nested pa ge

(network address translation), 732 nested pa ge tables (N PTs), 720 network address translation (N AT), 732nonuniform memory access (NUM A), 834 NPTs (nested pa ge tables), 720 O OLE (object linkin g and embeddin g), 873 openle table, 546547 OpenM P, 181182, 240241 OpenSolaris, 4 6 operatin g system(s): hybrid systems, 838 6 portability of, 83 6837 Oracle S PARC Solaris, 383 Oran ge Book, 83 2 OSI model, 757758 OSI Reference Model, 682 overcommitment, 729 P PaaS (platform as a service), 4 2 page

729 P PaaS (platform as a service), 4 2 page address e xtension ( PAE), 396 page directory pointer table, 38 6 pageframe number ( PFN) database, 850851 pagetable entries ( PTEs), 84 7 paging: and Oracle SPARC Solaris, 383 parallelism, 166, 168169 paravirtualization, 713, 725726 partition(s): control, 723 PC systems, 8 63 PDAs (personal di gital assistants), 11 periodic processes, 286 periodic task rate, 286 personal computer ( PC) systems, 8 63 personalities, 83 PFF (pagefaultfrequency), 4

8 63 personalities, 83 PFF (pagefaultfrequency), 4 29430Inde x 915PFN database, 8 50851 platform as a service ( PaaS), 4 2 pop, 3 2 POSIX: realtim e scheduling, 290 POST (poweron selftest), 8 62 power mana ger (Windows 7), 860861 poweron selftest ( POST), 8 62 prioritybased schedulin g, 285287 private cloud, 4 2 privile ge levels, 23 procedural lan guages, 241 process(es): background, 747 5, 115, 296 foreground, 115, 296 system, 8 processor groups, 83 5 process synchronization: alternative

groups, 83 5 process synchronization: alternative approaches to, 238242 functi onal programming languages, 241242 OpenMP , 240241 transactional memory, 239240 critic alsection pr oblem: software solution to, 212213 programmin genvironment virtualization, 713, 726727 proportional share schedulin g, 289290 protection domain, 721 protocols: discovery, 39 pseudodevice driver, 730731 PTEs (pa getable entries), 84 7 PTE tables, 84 7 Pthreads: thread cancellation in, 186187 public cloud, 4 1 publickey

in, 186187 public cloud, 4 1 publickey encryption, 678 push, 3 2R RAID sets, 8 68 rate, periodic task, 286 ratemonotonic schedulin g, 287288 ratemonotonic schedulin g algorithm, 287288 RC4, 677 RDP, 717 realtime C PU schedulin g, 283290 earliestdeadlinerst sch eduling, 288289 and minimizing l atency, 283285 POSIX realtim e scheduling, 290 prioritybased scheduling, 285287 proportional share scheduling, 289290 ratemonotonic sch eduling, 287288 redblack trees, 3 5 resume, 715 right child, 33 ROM

trees, 3 5 resume, 715 right child, 33 ROM (readonly memory), 93, 48 0 routers, 754 RR schedulin g algorithm, 271273 S SaaS (software as a service), 4 2 Scala lan guage, 241242 schedulin g: earliestdeadlinerst, 288289 prioritybased, 285287 proportional share, 289290 ratemonotonic, 287288 SSDs and, 478 SCM (Service Control Mana ger), 8 60 second e xtended le system (e xt2), 811 security identity (SID), 8 53 security tokens, 8 53 Service Control Mana ger (SCM), 8 60916 Inde xservices, operatin g

Mana ger (SCM), 8 60916 Inde xservices, operatin g system, 115 session mana ger subsystem (SMSS), 862 SID (security identity), 8 53 singly linked lists, 3 2 SJF schedulin g algorithm, 267270 Skype, 4 0 slim readerwriter (S RW) locks, 8 79 SLOB allocator , 43 9 SLUB allocator , 43 9 SMB (servermessa geblock), 8 71 SMSS (session mana ger subsystem), 862 software as a service (SaaS), 4 2 solidstate disks (SSDs), 11, 469, 478 SPARC, 383 SRM (security reference monitor), 858859 SRW (slim

SRM (security reference monitor), 858859 SRW (slim readerwriter) locks, 8 79 SST F schedulin g algorithm, 4 74475 standard swappin g, 358360 stora ge: threadlocal, 187 stora ge mana gement: with virtual machin es, 73 2733 subsystems, 135 superuser, 688 Surface Computer, 8 63 suspended state, 715 swappin g: on mobile systems, 3 60, 407 standard, 358360 switchin g: fastus er, 863864 symmetric couplin g, 17 symmetric encryption al gorithm, 676 synchronous threadin g, 172 SYSG EN, 9192 system

synchronous threadin g, 172 SYSG EN, 9192 system daemons, 8 systemdevelopment time, 715 system hive, 8 61system processes, 8, 84484 5 system restore point, 8 61 T target latency, 797 task parallelism, 168169 TEBs (thread environment blocks), 880 terminal applications, 96 terminal server systems, 8 64 thin clients, 3 5 third e xtended le system (e xt3), 811813 threads: implicit thr eading, 177183 thread attach, 8 53 thread environment blocks (T EBs), 880 threadlocal stora ge, 187 thread pools,

EBs), 880 threadlocal stora ge, 187 thread pools, 179181 thunkin g, 834 time sharin g (multitaskin g), 115 time slice, 796 timestamp counters (TSCs), 840841 touch screen (touchscreen computin g), 5, 60 transactions: atomic, 210 transactional memory, 239240 Transmission Control ProtocolInternet Protocol (TCPIP), 758761 trapandemulate method, 717718 trees, 33, 3 5 TSCs (timestamp counters), 84 0841 type 0 hypervisors, 712, 723724 type 1 hypervisors, 712, 724725 type 2 hypervisors, 713, 725Inde x

712, 724725 type 2 hypervisors, 713, 725Inde x 917U UAC (User Account Control), 701 UI (user interface), 5255 UMD F (UserMode Driver Framework), 8 56 UMS, seeusermode schedulin g USBs (universal serial buses), 4 69 User Account Control (U AC), 701 user mode, 787 UserMode Driver Framework (UMD F), 856 usermode schedulin g (UMS), 296297, 835, 880881 usermode threads (UT), 844 UT (usermode threads), 844 V VACB (virtual address control block), 8 57 variables: condition, 87 9 VAX minicomputer , 3

57 variables: condition, 87 9 VAX minicomputer , 3 79380 VCPU (virtual C PU), 717 vectored IO, 603604 virtual C PU (VC PU), 717 virtualization, 4 041 advantages and disadvantages of, 714716 and applic ation containment, 727728 and emulation, 727 and operatings ystem components, 72873 5 CPU sch eduling, 7 29 IO, 73 1732 live migr ation, 73373 5 memory management, 73 0731 storage management, 73 2733para, 725726 programming environment, 726727 virtual machines, 711738. See a lso virtualization

virtual machines, 711738. See a lso virtualization advantages and disadvantages of, 714716 and binary translation, 718720 examples, 73 5737 features of, 715717 and hardware assist ance, 720721 history of, 713714 Java Virtual Machin e, 736737 life cycle of, 722723 trapandemulate systems, 717718 type 0 hypervisors, 7 23724 type 1 hypervisors, 7 24725 type 2 hypervisors, 7 25 VMware, 735736 virtual machine control structures (VMCSs), 721 virtual machine mana ger (VMM), 2223, 41, 712 virtual machine

mana ger (VMM), 2223, 41, 712 virtual machine sprawl, 723 VMCSs (virtual machine control structures), 721 VMM, seevirtual machine mana ger VM mana ger, 84 6852 VMware, 714, 735736 Wwait()system call, 120122 Win3 2 API, 875 Windows 7: dynamic device supp ort, 837, 838 and energy efciency, 837 fastus er switching with, 8 63864 securit y in, 7 00701918 Inde xsynchr onization in, 833834, 87887 9 termin al services, 863864 usermode scheduling in, 296297 Windows e xecutive: booting, 8 62863 power

296297 Windows e xecutive: booting, 8 62863 power manager, 860861 Windows group policy, 8 75 Windows Task Mana ger, 8 7, 88 Windows Vista, 83 0 securit y in, 7 00 symbolic lin ks in, 8 69870 Windows XP, 830Winsock, 88 1 Workstation (VMWare), 735736 X x8664 architecture, 38 7 Xen, 714 Z zones, 728Inde x 919

